



<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#FFF">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">


<link rel="alternate" type="application/rss+xml" title="诗岸梦行舟" href="https://735690757.github.io/rss.xml" />
<link rel="alternate" type="application/atom+xml" title="诗岸梦行舟" href="https://735690757.github.io/atom.xml" />
<link rel="alternate" type="application/json" title="诗岸梦行舟" href="https://735690757.github.io/feed.json" />
<link rel="stylesheet" href="https://unpkg.com/mermaid/dist/mermaid.min.css">
<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
<script>mermaid.initialize({ startOnLoad: true });</script>

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="/css/app.css?v=0.2.5">

  
  <meta name="keywords" content="python,python_PyTorch" />


<link rel="canonical" href="https://735690757.github.io/python/pytorch/">



  <title>
PyTorch深度学习 - 深度学习基础 - 深度学习 |
KarryLiu = 诗岸梦行舟 = 分享计算机知识以及各种心得总结</title>
<meta name="generator" content="Hexo 6.3.0"></head>
<body itemscope itemtype="http://schema.org/WebPage">
  <div id="loading">
    <div class="cat">
      <div class="body"></div>
      <div class="head">
        <div class="face"></div>
      </div>
      <div class="foot">
        <div class="tummy-end"></div>
        <div class="bottom"></div>
        <div class="legs left"></div>
        <div class="legs right"></div>
      </div>
      <div class="paw">
        <div class="hands left"></div>
        <div class="hands right"></div>
      </div>
    </div>
  </div>
  <div id="container">
    <header id="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="inner">
        <div id="brand">
          <div class="pjax">
          
  <h1 itemprop="name headline">PyTorch深度学习
  </h1>
  
<div class="meta">
  <span class="item" title="创建时间：2025-08-04 11:55:00">
    <span class="icon">
      <i class="ic i-calendar"></i>
    </span>
    <span class="text">发表于</span>
    <time itemprop="dateCreated datePublished" datetime="2025-08-04T11:55:00+08:00">2025-08-04</time>
  </span>
</div>


          </div>
        </div>
        <nav id="nav">
  <div class="inner">
    <div class="toggle">
      <div class="lines" aria-label="切换导航栏">
        <span class="line"></span>
        <span class="line"></span>
        <span class="line"></span>
      </div>
    </div>
    <ul class="menu">
      <li class="item title"><a href="/" rel="start">KarryLiu</a></li>
    </ul>
    <ul class="right">
      <li class="item theme">
        <i class="ic i-sun"></i>
      </li>
      <li class="item search">
        <i class="ic i-search"></i>
      </li>
    </ul>
  </div>
</nav>

      </div>
      <div id="imgs" class="pjax">
          <img src="/images/pytorch.png">
      </div>
    </header>
    <div id="waves">
      <svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto">
        <defs>
          <path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z" />
        </defs>
        <g class="parallax">
          <use xlink:href="#gentle-wave" x="48" y="0" />
          <use xlink:href="#gentle-wave" x="48" y="3" />
          <use xlink:href="#gentle-wave" x="48" y="5" />
          <use xlink:href="#gentle-wave" x="48" y="7" />
        </g>
      </svg>
    </div>
    <main>
      <div class="inner">
        <div id="main" class="pjax">
          
  <div class="article wrap">
    
<div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList">
<i class="ic i-home"></i>
<span><a href="/">首页</a></span><i class="ic i-angle-right"></i>
<span  itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/python/" itemprop="item" rel="index" title="分类于 深度学习"><span itemprop="name">深度学习</span></a>
<meta itemprop="position" content="1" /></span>
<i class="ic i-angle-right"></i>
<span  class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/python/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" itemprop="item" rel="index" title="分类于 深度学习基础"><span itemprop="name">深度学习基础</span></a>
<meta itemprop="position" content="2" /></span>
</div>

    <article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN">
  <link itemprop="mainEntityOfPage" href="https://735690757.github.io/python/pytorch/">

  <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="image" content="/images/tx.jpg">
    <meta itemprop="name" content="KarryLiu">
    <meta itemprop="description" content="分享计算机知识以及各种心得总结, 愿世间所有的美好都得以祝愿">
  </span>

  <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="诗岸梦行舟">
  </span>

  <div class="body md" itemprop="articleBody">
    

    <h1 id="pytorch深度学习"><a class="markdownIt-Anchor" href="#pytorch深度学习"></a> PyTorch深度学习</h1>
<p>基于小土堆：<span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWhFNDExdDdSTg==">https://www.bilibili.com/video/BV1hE411t7RN</span></p>
<h2 id="安装"><a class="markdownIt-Anchor" href="#安装"></a> 安装</h2>
<p>基于conda环境来安装</p>
<pre class="highlight"><code class="bash">conda create --name pytorch python=3.11
</code></pre>
<p>查询已有环境</p>
<pre class="highlight"><code class="bash">conda info --envs
</code></pre>
<blockquote>
<p>conda environments:<br />
base                   * D:\anaconda<br />
pytorch              D:\anaconda\envs\pytorch</p>
</blockquote>
<p>激活pytorch环境</p>
<pre class="highlight"><code class="bash">conda activate pytorch
</code></pre>
<p>其他命令</p>
<pre class="highlight"><code class="bash">conda remove -n xxxxx(名字) --all	<span class="hljs-comment"># 环境删除命令</span>
deactivate	<span class="hljs-comment"># 退出虚拟环境</span>
pip list	<span class="hljs-comment"># 查看虚拟环境的库</span>
</code></pre>
<p>访问pytorch网站：<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9nZXQtc3RhcnRlZC9sb2NhbGx5Lw==">https://pytorch.org/get-started/locally/</span></p>
<hr />
<p>截至2025年8月Start Locally条目给予python版本提示：</p>
<p><strong>NOTE:</strong> Latest PyTorch requires Python 3.9 or later.</p>
<hr />
<p>基于：Stable(2.7.1) - Windows - Pip - Python - CUDA 11.8</p>
<pre class="highlight"><code class="bash">pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
</code></pre>
<pre class="highlight"><code class="bash">(pytorch) C:\Users\Karry&gt;pip list
Package           Version
----------------- ------------
filelock          3.13.1
fsspec            2024.6.1
Jinja2            3.1.4
MarkupSafe        2.1.5
mpmath            1.3.0
networkx          3.3
numpy             2.1.2
pillow            11.0.0
pip               25.1
setuptools        78.1.1
sympy             1.13.3
torch             2.7.1+cu118
torchaudio        2.7.1+cu118
torchvision       0.22.1+cu118
typing_extensions 4.12.2
wheel             0.45.1
</code></pre>
<p>验证：</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
torch.cuda.is_available()	<span class="hljs-comment"># True</span>
</code></pre>
<h2 id="两大法宝函数-dir与help"><a class="markdownIt-Anchor" href="#两大法宝函数-dir与help"></a> 两大法宝函数-dir与help</h2>
<p>dir主要是来查看一个工具包下还有什么子工具包或者工具</p>
<p>help主要是查看一些工具有什么作用</p>
<p>比如help(torch.cuda.is_available)</p>
<pre class="highlight"><code class="bash"><span class="hljs-built_in">help</span>(torch.cuda.is_available)
Help on <span class="hljs-keyword">function</span> is_available <span class="hljs-keyword">in</span> module torch.cuda:
is_available() -&gt; bool
    Return a bool indicating <span class="hljs-keyword">if</span> CUDA is currently available.

</code></pre>
<h2 id="pytorch数据读取"><a class="markdownIt-Anchor" href="#pytorch数据读取"></a> PyTorch数据读取</h2>
<p>数据 —— Dataset（提供一种方式获取label） —— Dataloader（为后面的网络提供不同的数据形式）</p>
<p>组织结构：</p>
<pre class="highlight"><code class="bash">+—— hymenoptera_data
| +—— train
| | —— ants
| | —— bees
</code></pre>
<p>对应代码：</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> os


<span class="hljs-comment"># MyData是自定义的数据集类，他继承了Dataset类，MyData(Dataset)这是继承动作</span>
<span class="hljs-keyword">class</span> <span class="hljs-title class_">MyData</span>(<span class="hljs-title class_ inherited__">Dataset</span>):
    <span class="hljs-comment"># 初始化函数，初始化一些数据，比如图片路径，标签路径等等，__init__这是重写父类的方法</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, root_dir, label_dir</span>):
        <span class="hljs-comment"># 根集路径</span>
        self.root_dir = root_dir
        <span class="hljs-comment"># 标签路径，这其实表达了图片是什么分类</span>
        self.label_dir = label_dir
        <span class="hljs-comment"># 获取图片的路径，这个路径下存放着图片</span>
        self.path = os.path.join(self.root_dir, self.label_dir)
        <span class="hljs-comment"># 获取图片路径列表listdir</span>
        self.img_path = os.listdir(self.path)

    <span class="hljs-comment"># 获取数据，这个函数是必须写的，并且是必须返回两个值，一个是图片，一个是标签</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):
        img_name = self.img_path[idx]
        img_item_path = os.path.join(self.root_dir, self.label_dir, img_name)
        img = Image.<span class="hljs-built_in">open</span>(img_item_path)
        label = self.label_dir
        <span class="hljs-keyword">return</span> img, label

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.img_path)


root_dir = <span class="hljs-string">&quot;hymenoptera_data\\train&quot;</span>
ant_label_dir = <span class="hljs-string">&quot;ants&quot;</span>
bees_label_dir = <span class="hljs-string">&quot;bees&quot;</span>
ants_dataset = MyData(root_dir, ant_label_dir)
bees_dataset = MyData(root_dir, bees_label_dir)

train_dataset = ants_dataset + bees_dataset
</code></pre>
<p>组织结构：</p>
<pre class="highlight"><code class="bash">+—— hymenoptera_data_ex
| +—— train
| | —— ants_image
| | —— ants_label
| | —— bees_image
| | —— bees_label
</code></pre>
<p>对应代码：</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> os


<span class="hljs-keyword">class</span> <span class="hljs-title class_">MyData</span>(<span class="hljs-title class_ inherited__">Dataset</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, root_dir, img_dir, label_dir</span>):
        self.root_dir = root_dir
        self.img_dir = img_dir
        self.label_dir = label_dir
        self.path_img = os.path.join(self.root_dir, self.img_dir)
        self.path_label = os.path.join(self.root_dir, self.label_dir)
        self.img_path = os.listdir(self.path_img)
        self.label_path = os.listdir(self.path_label)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, item</span>):
        img_name = self.img_path[item]
        img_item_path = os.path.join(self.path_img, img_name)
        img = Image.<span class="hljs-built_in">open</span>(img_item_path)
        label_name = self.label_path[item]
        label_item_path = os.path.join(self.path_label, label_name)
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(label_item_path, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:
            label = f.read()
            <span class="hljs-keyword">return</span> img, label
root_dir = <span class="hljs-string">&quot;hymenoptera_data_ex\\train&quot;</span>
img_dir = <span class="hljs-string">&quot;ants_image&quot;</span>
label_dir = <span class="hljs-string">&quot;ants_label&quot;</span>
ants_dataset = MyData(root_dir, img_dir, label_dir)
</code></pre>
<h2 id="tensorboard的使用"><a class="markdownIt-Anchor" href="#tensorboard的使用"></a> Tensorboard的使用</h2>
<h3 id="add_scalar"><a class="markdownIt-Anchor" href="#add_scalar"></a> add_scalar</h3>
<pre class="highlight"><code class="python"><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

writer = SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)
<span class="hljs-comment"># writer.add_image()</span>
<span class="hljs-comment"># y = x</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):
    writer.add_scalar(<span class="hljs-string">&quot;y=i^2&quot;</span>, i * i, i)

writer.close()
</code></pre>
<pre class="highlight"><code class="bash"> tensorboard --logdir=logs --port=6007
</code></pre>
<p><img data-src="../../images/pytorch/1.png" alt="1.png" /></p>
<p>writer.add_scalar(“y=i^2”, i * i, i)，这个的参数分别是：标签、y、x</p>
<h3 id="add_image"><a class="markdownIt-Anchor" href="#add_image"></a> add_image</h3>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

img_path = <span class="hljs-string">&quot;hymenoptera_data/train/ants/0013035.jpg&quot;</span>
img_PIL = Image.<span class="hljs-built_in">open</span>(img_path)
img_array = np.array(img_PIL)
<span class="hljs-comment"># hymenoptera_data/train/ants/0013035.jpg</span>
writer = SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)
writer.add_image(<span class="hljs-string">&quot;img&quot;</span>, img_array, <span class="hljs-number">1</span>,dataformats=<span class="hljs-string">&quot;HWC&quot;</span>)
<span class="hljs-comment"># y = x</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):
    writer.add_scalar(<span class="hljs-string">&quot;y=i^2&quot;</span>, i * i, i)

writer.close()
</code></pre>
<p><img data-src="../../images/pytorch/2.png" alt="2.png" /></p>
<p>add_image(“img”, img_array, 1,dataformats=“HWC”)，这个参数分别是：标签、ndarray类型的图片、步骤、dataformats-HWC</p>
<h2 id="transforms的使用"><a class="markdownIt-Anchor" href="#transforms的使用"></a> Transforms的使用</h2>
<p>Transforms主要是对图片的各种变换，是预处理？</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms

img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;hymenoptera_data/train/ants/0013035.jpg&#x27;</span>)
tensor_trans = transforms.ToTensor()
tensor_img = tensor_trans(img)
<span class="hljs-built_in">print</span>(tensor_img)
</code></pre>
<p>Image.open返回了PIL类型的图片，通过transforms.ToTensor()创建了工具对象，最后使用tensor_trans(img)转化为tensor类型</p>
<blockquote>
<p>tensor数据类型：包装了神经网络所需要的理论基础参数</p>
</blockquote>
<p>使用SummaryWriter将tensor_img写入</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter
<span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms

img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;hymenoptera_data/train/ants/0013035.jpg&#x27;</span>)
tensor_trans = transforms.ToTensor()
tensor_img = tensor_trans(img)
<span class="hljs-comment"># print(tensor_img)</span>

writer = SummaryWriter(<span class="hljs-string">&#x27;logs&#x27;</span>)
writer.add_image(<span class="hljs-string">&#x27;tensor_img&#x27;</span>, tensor_img)
writer.close()
</code></pre>
<p><img data-src="../../images/pytorch/3.png" alt="3.png" /></p>
<hr />
<p>题外话，理解一下py的面向对象：</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Person</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, name</span>):
        self.name = name

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self</span>):
        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;hello&quot;</span>, self.name)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">hello</span>(<span class="hljs-params">self</span>):
        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;hello_ex&quot;</span>, self.name)


person = Person(<span class="hljs-string">&quot;Karry&quot;</span>)
person()
person.hello()
</code></pre>
<p>init方法实际上是一个构造方法，person = Person(“Karry”)执行后调用构造方法</p>
<p>call方法像初次见面问好一样person()执行后，调用call方法，是让实例像函数一样用的钩子方法</p>
<p>person.hello()只是一个类的普通方法</p>
<hr />
<h3 id="totensor的使用"><a class="markdownIt-Anchor" href="#totensor的使用"></a> ToTensor的使用</h3>
<p>ToTensor图片张量化工具</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter
<span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms

writer = SummaryWriter(<span class="hljs-string">&#x27;logs&#x27;</span>)
img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;images/pytorch.png&#x27;</span>)
toTensorTools = transforms.ToTensor()
img_tensor = toTensorTools(img)
writer.add_image(<span class="hljs-string">&#x27;ToTensor&#x27;</span>, img_tensor)
writer.close()
</code></pre>
<h3 id="normalize的使用"><a class="markdownIt-Anchor" href="#normalize的使用"></a> Normalize的使用</h3>
<p>Normalize归一化、标准化，均值为0，方差为1，数值位于-1到1之间</p>
<p>如果图片不是RGB模式需要做img.convert(‘RGB’)</p>
<pre class="highlight"><code class="python">writer = SummaryWriter(<span class="hljs-string">&#x27;logs&#x27;</span>)
img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;images/pytorch.png&#x27;</span>)
<span class="hljs-comment"># img转化为RGB</span>
img = img.convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)
toTensorTools = transforms.ToTensor()
img_tensor = toTensorTools(img)
writer.add_image(<span class="hljs-string">&#x27;ToTensor&#x27;</span>, img_tensor)
<span class="hljs-built_in">print</span>(img_tensor[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])
<span class="hljs-comment"># 归一化</span>
transforms_normalize = transforms.Normalize([<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>], [<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>])
img_normalize = transforms_normalize(img_tensor)
<span class="hljs-built_in">print</span>(img_normalize[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])

writer.close()
</code></pre>
<p>tensor(0.1333)<br />
tensor(-0.7333)</p>
<p>0.1333*2 - 1 = -0.7333</p>
<h3 id="resize的使用"><a class="markdownIt-Anchor" href="#resize的使用"></a> Resize的使用</h3>
<p>Resize重调整</p>
<pre class="highlight"><code class="python"><span class="hljs-comment"># Resize的使用</span>
<span class="hljs-built_in">print</span>(img.size)
transforms_resize = transforms.Resize([<span class="hljs-number">128</span>, <span class="hljs-number">128</span>])
img_resize = transforms_resize(img_tensor)
writer.add_image(<span class="hljs-string">&#x27;Resize&#x27;</span>, img_resize, <span class="hljs-number">0</span>)
<span class="hljs-built_in">print</span>(img_resize.size())

resize_2 = transforms.Resize(<span class="hljs-number">512</span>)
transforms_compose = transforms.Compose([resize_2, toTensorTools])
img_resize_2 = transforms_compose(img)
writer.add_image(<span class="hljs-string">&#x27;Compose&#x27;</span>, img_resize_2, <span class="hljs-number">1</span>)
</code></pre>
<p>起初我们使用img = Image.open(‘images/testFG.jpg’)，此时这是一个PIL图片，如何我们使用toTensorTools = transforms.ToTensor()创建张量转换工具，使用img_tensor = toTensorTools(img)将PIL图片转化为img_tensor张量图片，紧接着我们使用transforms_resize = transforms.Resize([128, 128])创建尺寸调整工具，使用img_resize = transforms_resize(img_tensor)对张量图片重调整。</p>
<p>Compose的意义在于它可以将多个图像变换操作（如缩放、裁剪、归一化等）按顺序组合成一个流水线，输入图像会依次通过这些变换。</p>
<p>我们使用transforms_compose = transforms.Compose([resize_2, toTensorTools])创建了一个工具链，resize_2用于调整图像尺寸，toTensorTools用于其转化为张量图片。</p>
<p>从img_resize_2 = transforms_compose(img)我们可以看到，参数img是一个PIL图片他通过Compose先后进行了重调整和张量化，最终返回img_resize_2张量图片。</p>
<hr />
<p>提示：resize_2 = transforms.Resize(512)是一个等比例调整。</p>
<p>tensor(0.4039)<br />
tensor(0.6797)<br />
(3600, 2700)<br />
torch.Size([3, 128, 128])<br />
torch.Size([3, 512, 682])</p>
<hr />
<h3 id="randomcrop的使用"><a class="markdownIt-Anchor" href="#randomcrop的使用"></a> RandomCrop的使用</h3>
<p>RandomCrop随机裁剪</p>
<pre class="highlight"><code class="python"><span class="hljs-comment"># RandomCrop的使用</span>
<span class="hljs-comment"># random_crop = transforms.RandomCrop(500, 1000)</span>
random_crop = transforms.RandomCrop(<span class="hljs-number">512</span>)
compose_random_crop = transforms.Compose([random_crop, toTensorTools])
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):
    img_random_crop = compose_random_crop(img)
    writer.add_image(<span class="hljs-string">&#x27;RandomCrop&#x27;</span>, img_random_crop, i)
</code></pre>
<h2 id="使用torchvision的数据集dataset"><a class="markdownIt-Anchor" href="#使用torchvision的数据集dataset"></a> 使用TorchVision的数据集（DataSet）</h2>
<p>CIFAR —— Canadian Institute For Advanced Research（加拿大高级研究所）</p>
<p>root数据集所在目录、train是训练集还是测试集、transform应用的变换操作或操作集合、download是否启用下载</p>
<pre class="highlight"><code class="python">train_set = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./data&quot;</span>, train=<span class="hljs-literal">True</span>, transform=transforms_compose_dataset, download=<span class="hljs-literal">True</span>)
test_set = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./data&quot;</span>, train=<span class="hljs-literal">False</span>, transform=transforms_compose_dataset, download=<span class="hljs-literal">True</span>)
</code></pre>
<p><img data-src="../../images/pytorch/4.png" alt="4.png" /></p>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY3MudG9yb250by5lZHUvfmtyaXovY2lmYXIuaHRtbA==">CIFAR-10 and CIFAR-100 datasets</span></p>
<blockquote>
<p>The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.</p>
<p>CIFAR-10 数据集由 10 类的 60000 张 32x32 彩色图像组成，每类 6000 张图像。有 50000 张训练图像和 10000 张测试图像。</p>
<p>The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.</p>
<p>数据集分为五个训练批次和一个测试批次，每个训练批次有 10000 张图像。测试批次恰好包含每个类中随机选择的 1000 张图像。训练批次以随机顺序包含剩余的图像，但某些训练批次可能包含来自一个类的图像多于另一个类的图像。在它们之间，训练批次恰好包含每个类的 5000 张图像。</p>
</blockquote>
<h3 id="数据集联动tensorboard"><a class="markdownIt-Anchor" href="#数据集联动tensorboard"></a> 数据集联动Tensorboard</h3>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

tensorboard = SummaryWriter(<span class="hljs-string">&quot;p10&quot;</span>)
transforms_compose_dataset = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])
train_set = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./data&quot;</span>, train=<span class="hljs-literal">True</span>, transform=transforms_compose_dataset, download=<span class="hljs-literal">True</span>)
test_set = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./data&quot;</span>, train=<span class="hljs-literal">False</span>, transform=transforms_compose_dataset, download=<span class="hljs-literal">True</span>)
<span class="hljs-built_in">print</span>(test_set[<span class="hljs-number">0</span>])
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):
    img, target = test_set[i]
    tensorboard.add_image(<span class="hljs-string">&quot;test_set&quot;</span>, img, i)
tensorboard.close()
</code></pre>
<p>其中img, target = test_set[i]返回了一个元组</p>
<p><img data-src="../../images/pytorch/5.png" alt="5.png" /></p>
<p>0号位是转化过后的张量图，1号位是其标签索引，数据的标签列表可以在test_set.classes中看到。</p>
<h3 id="add_image源码提示"><a class="markdownIt-Anchor" href="#add_image源码提示"></a> add_image源码提示</h3>
<pre class="highlight"><code class="python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">add_image</span>(<span class="hljs-params">
    self, tag, img_tensor, global_step=<span class="hljs-literal">None</span>, walltime=<span class="hljs-literal">None</span>, dataformats=<span class="hljs-string">&quot;CHW&quot;</span>
</span>):
</code></pre>
<p>tag, img_tensor, global_step=None分别对应tensorboard标签，张量图，以及步骤i</p>
<pre class="highlight"><code class="python">tensorboard.add_image(<span class="hljs-string">&quot;test_set&quot;</span>, img, i)
</code></pre>
<p>注意：tensorboard使用过后须关闭tensorboard.close()</p>
<h2 id="dataloader的使用"><a class="markdownIt-Anchor" href="#dataloader的使用"></a> DataLoader的使用</h2>
<p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnB5dG9yY2gub3JnL2RvY3Mvc3RhYmxlL2RhdGEuaHRtbCN0b3JjaC51dGlscy5kYXRhLkRhdGFMb2FkZXI=">torch.utils.data — PyTorch 2.8 documentation</span></p>
<p>参数初见：</p>
<ul>
<li>
<p><strong>dataset</strong> (<a target="_blank" rel="noopener" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"><em>Dataset</em></a>) – dataset from which to load the data.</p>
</li>
<li>
<p><strong>batch_size</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <em>optional</em>) – how many samples per batch to load (default: <code>1</code>).</p>
</li>
<li>
<p><strong>shuffle</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em> <em>optional</em>) – set to <code>True</code> to have the data reshuffled at every epoch (default: <code>False</code>).</p>
</li>
<li>
<p><strong>batch_sampler</strong> (<a target="_blank" rel="noopener" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Sampler"><em>Sampler</em></a> <em>or</em> <em>Iterable</em>*,* <em>optional</em>) – like <a target="_blank" rel="noopener" href="https://docs.pytorch.org/docs/stable/utils.html#module-torch.utils.data.sampler"><code>sampler</code></a>, but returns a batch of indices at a time. Mutually exclusive with <code>batch_size</code>, <code>shuffle</code>, <a target="_blank" rel="noopener" href="https://docs.pytorch.org/docs/stable/utils.html#module-torch.utils.data.sampler"><code>sampler</code></a>, and <code>drop_last</code>.</p>
</li>
<li>
<p><strong>num_workers</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <em>optional</em>) – how many subprocesses to use for data loading. <code>0</code> means that the data will be loaded in the main process. (default: <code>0</code>)</p>
</li>
<li>
<p><strong>drop_last</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em> <em>optional</em>) – set to <code>True</code> to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If <code>False</code> and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: <code>False</code>)</p>
</li>
</ul>
<pre class="highlight"><code class="python"><span class="hljs-comment"># 准备的测试数据</span>
test_data = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./data&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),
                                         download=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># 创建测试数据集</span>
test_loader = DataLoader(dataset=test_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">0</span>, drop_last=<span class="hljs-literal">True</span>)
</code></pre>
<p>dataset数据集、batch_size一次打包多少个、shuffle是否打乱、num_workers加载数据子进程数、drop_last多余部分是否删除</p>
<pre class="highlight"><code class="bash">torch.Size([3, 32, 32])
3
<span class="hljs-built_in">cat</span>
</code></pre>
<h3 id="联动tensorboard"><a class="markdownIt-Anchor" href="#联动tensorboard"></a> 联动Tensorboard</h3>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torchvision.datasets
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

<span class="hljs-comment"># 准备的测试数据</span>
test_data = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./data&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),
                                         download=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># 创建测试数据集</span>
test_loader = DataLoader(dataset=test_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">0</span>, drop_last=<span class="hljs-literal">True</span>)

image, target = test_data[<span class="hljs-number">0</span>]
<span class="hljs-comment"># 测试数据第一张图像的shape和标签</span>
<span class="hljs-built_in">print</span>(image.shape)
<span class="hljs-built_in">print</span>(target)
<span class="hljs-built_in">print</span>(test_data.classes[target])
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;____________________&quot;</span>)

writer = SummaryWriter(<span class="hljs-string">&quot;DataLoader&quot;</span>)

step = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> loaderX <span class="hljs-keyword">in</span> test_loader:
    images, targets = loaderX
    <span class="hljs-comment"># print(images.shape)</span>
    <span class="hljs-comment"># print(targets)</span>
    writer.add_images(<span class="hljs-string">&quot;test_data_drop_last&quot;</span>, images, step)
    step += <span class="hljs-number">1</span>
writer.close()
</code></pre>
<h3 id="利用epoch变量控制训练或测试轮次"><a class="markdownIt-Anchor" href="#利用epoch变量控制训练或测试轮次"></a> 利用Epoch变量控制训练或测试轮次</h3>
<pre class="highlight"><code class="python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):
    step = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> loaderX <span class="hljs-keyword">in</span> test_loader:
        images, targets = loaderX
        <span class="hljs-comment"># print(images.shape)</span>
        <span class="hljs-comment"># print(targets)</span>
        writer.add_images(<span class="hljs-string">&quot;Epoch:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch), images, step)
        step += <span class="hljs-number">1</span>
</code></pre>
<p><code>&quot;Epoch:&#123;&#125;&quot;.format(epoch)</code> 是 Python 的 <strong>字符串格式化</strong> 方法，它会将 <code>epoch</code> 的值动态插入到字符串的 <code>&#123;&#125;</code> 占位符中。</p>
<p><img data-src="../../images/pytorch/6.png" alt="6.png" /></p>
<p>此时当shuffle=True时，Epoch0和Epoch1并不一样</p>
<pre class="highlight"><code class="python">test_loader = DataLoader(dataset=test_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">0</span>, drop_last=<span class="hljs-literal">True</span>)
</code></pre>
<p><img data-src="../../images/pytorch/7.png" alt="7.png" /></p>
<h2 id="神经网络neural-network基本骨架"><a class="markdownIt-Anchor" href="#神经网络neural-network基本骨架"></a> 神经网络(Neural Network)基本骨架</h2>
<p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnB5dG9yY2gub3JnL2RvY3MvMi44L25uLmh0bWw=">torch.nn — PyTorch 2.8 documentation</span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnB5dG9yY2gub3JnL2RvY3MvMi44L2dlbmVyYXRlZC90b3JjaC5ubi5Nb2R1bGUuaHRtbCN0b3JjaC5ubi5Nb2R1bGU=">Module — PyTorch 2.8 documentation</span></p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F

<span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__()
        self.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)
        self.conv2 = nn.Conv2d(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        x = F.relu(self.conv1(x))
        <span class="hljs-keyword">return</span> F.relu(self.conv2(x))
</code></pre>
<p>forward前向传播：</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        x = F.relu(self.conv1(x))
        <span class="hljs-keyword">return</span> F.relu(self.conv2(x))
</code></pre>
<p><code>x</code>先经过一次<code>conv1</code>卷积，再经过一次<code>relu</code>非线性处理<code>x = F.relu(self.conv1(x))</code></p>
<p>然后<code>x</code>在经过一次<code>conv2</code>卷积，再经过一次<code>relu</code>非线性处理，最后返回<code>return F.relu(self.conv2(x))</code></p>
<h3 id="简单的骨架"><a class="markdownIt-Anchor" href="#简单的骨架"></a> 简单的骨架</h3>
<p>简单的骨架就是这样，有一个输入经过forward后每次加一</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn


<span class="hljs-keyword">class</span> <span class="hljs-title class_">Module</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):
        output = <span class="hljs-built_in">input</span> + <span class="hljs-number">1</span>
        <span class="hljs-keyword">return</span> output


karry = Module()
x = torch.tensor(<span class="hljs-number">1.0</span>)
<span class="hljs-built_in">print</span>(karry(x))
</code></pre>
<h2 id="convolution卷积操作"><a class="markdownIt-Anchor" href="#convolution卷积操作"></a> convolution卷积操作</h2>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWhFNDExdDdSTj9wPTE3">https://www.bilibili.com/video/BV1hE411t7RN?p=17</span></p>
<h3 id="stride跨步1"><a class="markdownIt-Anchor" href="#stride跨步1"></a> Stride跨步=1</h3>
<p>Stride是每次跨步数，1就是每次跨一步</p>
<p><img data-src="../../images/pytorch/8.png" alt="8.png" /></p>
<p>注意到红色部分就是对应位置相乘再相加</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F

<span class="hljs-comment"># 创建输入和核</span>
input_matrix = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">5</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])
kernel = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])
<span class="hljs-comment"># 改变维度</span>
input_matrix = torch.reshape(input_matrix, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>))
kernel = torch.reshape(kernel, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>))
<span class="hljs-comment"># 卷积</span>
output_ans = F.conv2d(input_matrix, kernel, stride=<span class="hljs-number">1</span>)

<span class="hljs-built_in">print</span>(input_matrix.shape)
<span class="hljs-built_in">print</span>(kernel.shape)
<span class="hljs-built_in">print</span>(output_ans)
</code></pre>
<blockquote>
<p>torch.Size([1, 1, 5, 5])<br />
torch.Size([1, 1, 3, 3])<br />
tensor([[[[10, 12, 12],<br />
[18, 16, 16],<br />
[13,  9,  3]]]])</p>
</blockquote>
<h3 id="stride跨步2"><a class="markdownIt-Anchor" href="#stride跨步2"></a> Stride跨步=2</h3>
<p><img data-src="../../images/pytorch/9.png" alt="9.png" /></p>
<pre class="highlight"><code class="python"><span class="hljs-comment"># 卷积</span>
output_ans = F.conv2d(input_matrix, kernel, stride=<span class="hljs-number">2</span>)
<span class="hljs-built_in">print</span>(output_ans)
</code></pre>
<blockquote>
<p>tensor([[[[10, 12],<br />
[13,  3]]]])</p>
</blockquote>
<h3 id="padding填充1"><a class="markdownIt-Anchor" href="#padding填充1"></a> Padding填充=1</h3>
<p>Padding填充就是在原始数据的最外侧填充一些数据（像素），一般情况下是设置为零</p>
<p><img data-src="../../images/pytorch/10.png" alt="10.png" /></p>
<p>一样地，红色部分对应位置相乘再相加，最外圈绿色为padding=1所产生的额外填充</p>
<pre class="highlight"><code class="python"><span class="hljs-comment"># 卷积</span>
output_ans = F.conv2d(input_matrix, kernel, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(output_ans)
</code></pre>
<blockquote>
<p>tensor([[[[ 1,  3,  4, 10,  8],<br />
[ 5, 10, 12, 12,  6],<br />
[ 7, 18, 16, 16,  8],<br />
[11, 13,  9,  3,  4],<br />
[14, 13,  9,  7,  4]]]])</p>
</blockquote>
<h2 id="convolution-layers卷积层"><a class="markdownIt-Anchor" href="#convolution-layers卷积层"></a> Convolution Layers卷积层</h2>
<p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnB5dG9yY2gub3JnL2RvY3Mvc3RhYmxlL25uLmh0bWwjY29udm9sdXRpb24tbGF5ZXJz">torch.nn — PyTorch 2.8 documentation</span></p>
<p>class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode=‘zeros’, device=None, dtype=None)</p>
<p>其中最主要的参数设置是这五个：<strong>in_channels</strong>, <strong>out_channels</strong>, <strong>kernel_size</strong>, <strong>stride=1</strong>, <strong>padding=0</strong></p>
<p><img data-src="../../images/pytorch/11.png" alt="11.png" /></p>
<p>weight实际上就是卷积核，input是通道数据，bias是偏置</p>
<p><img data-src="../../images/pytorch/13.png" alt="13.png" /></p>
<p>卷积层的 <strong>每个输出通道</strong> 都是 <strong>所有输入通道的加权卷积结果相加</strong>，权重就是 <code>weight[j, k]</code>。</p>
<p>假设输入是 <strong>RGB 彩色图像</strong>（3 个通道：R、G、B），卷积层的一个<strong>输出通道</strong>是这样算的：</p>
<p><img data-src="../../images/pytorch/12.png" alt="12.png" /></p>
<p>为什么要“所有输入通道相加”？</p>
<ol>
<li>图像的特征可能跨通道（比如红绿蓝组合才能构成颜色信息）</li>
<li>一个卷积核只看单个通道的信息是不完整的</li>
<li>把多个输入通道的卷积结果加在一起，就相当于在融合这些通道的信息</li>
</ol>
<p>这也是为什么多通道卷积的 <code>weight</code> 是四维的：C_out、C_in、k_h、k_w</p>
<p><img data-src="../../images/pytorch/14.png" alt="14.png" /></p>
<p>Convolution animations：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3ZkdW1vdWxpbi9jb252X2FyaXRobWV0aWMvYmxvYi9tYXN0ZXIvUkVBRE1FLm1k">conv_arithmetic/README.md at master · vdumoulin/conv_arithmetic · GitHub</span></p>
<h3 id="in_channel1-out_channel1"><a class="markdownIt-Anchor" href="#in_channel1-out_channel1"></a> in_channel=1, out_channel=1</h3>
<p>只有一个输入通道和一个输出通道，不存在跨通道求和。</p>
<p><img data-src="../../images/pytorch/16.png" alt="16.png" /></p>
<p>1×1+2×0+3×0+4×(−1)=1+0+0−4=−3</p>
<p>[[−3]]</p>
<p><strong>in_channel=1, out_channel=1</strong> 时，就是用一个卷积核直接作用于输入通道，卷积后加上 bias 得到结果。</p>
<p>没有跨通道加权，没有额外求和步骤。</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

nn_conv_d = nn.Conv2d(in_channels=<span class="hljs-number">1</span>, out_channels=<span class="hljs-number">1</span>, kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>)
nn_conv_d.weight.data = torch.tensor([[[[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>],
                                        [<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>]]]], dtype=torch.float32)
<span class="hljs-built_in">input</span> = torch.tensor([[[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]]], dtype=torch.float32)

<span class="hljs-built_in">print</span>(nn_conv_d(<span class="hljs-built_in">input</span>))
</code></pre>
<blockquote>
<p>tensor([[[[-3.]]]], grad_fn=<ConvolutionBackward0>)</p>
</blockquote>
<p>起步解释：</p>
<ol>
<li>nn_conv_d.weight.data自定义卷积核</li>
<li><code>nn.Conv2d</code> 要求 <code>[batch, channels, height, width]</code>，所以二维矩阵要用 <code>unsqueeze</code> 扩成 4 维，或者在初始化时就是四维的</li>
<li>卷积层的权重和 bias 是 **浮点型 (<code>torch.float32</code>)**因此要dtype=torch.float32</li>
<li>为了得到精确值偏置量应当设为假（不偏置）：bias=False</li>
</ol>
<p>以上均为torch框架中自带的参数调试变量，这些操作主要是 <strong>调试和理解卷积的计算过程</strong></p>
<p>在实际神经网络训练中：</p>
<ul>
<li>权重 <code>weight</code> 会被优化器自动更新</li>
<li>输入通常是四维 tensor</li>
<li>bias 是否启用视网络设计而定</li>
</ul>
<p>掌握这些基本参数调试方法，是为了：</p>
<ol>
<li>理解卷积计算机制</li>
<li>验证卷积操作是否如预期</li>
<li>为后续做图像或特征识别的神经网络打基础</li>
</ol>
<h3 id="in_channel1-out_channel2"><a class="markdownIt-Anchor" href="#in_channel1-out_channel2"></a> in_channel=1, out_channel=2</h3>
<p>每个输出通道是<strong>独立</strong>的卷积结果</p>
<p><img data-src="../../images/pytorch/17.png" alt="17.png" /></p>
<p>1×1+2×0+3×0+4×(−1)=1+0+0−4=−3</p>
<p>1×0+2×1+3×(−1)+4×0=0+2−3+0=−1</p>
<p>输出张量 (batch=1, out_channel=2, H=1, W=1)：output=[[−3],[−1]]</p>
<p><strong>in_channel=1</strong> 时，每个输出通道都直接在这个输入通道上用不同卷积核独立运算。</p>
<p>最终的两个通道结果是<strong>并列存储</strong>，不做求和。</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

nn_conv_d = nn.Conv2d(in_channels=<span class="hljs-number">1</span>, out_channels=<span class="hljs-number">2</span>, kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>)
nn_conv_d.weight.data = torch.tensor([
    [[[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>]]],  <span class="hljs-comment"># 输出通道 1 的卷积核，对应输入通道 1</span>
    [[[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]]]  <span class="hljs-comment"># 输出通道 2 的卷积核，对应输入通道 1</span>
], dtype=torch.float32)
<span class="hljs-built_in">input</span> = torch.tensor([[[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]]], dtype=torch.float32)

<span class="hljs-built_in">print</span>(nn_conv_d(<span class="hljs-built_in">input</span>))
</code></pre>
<blockquote>
<p>tensor([[[[-3.]],</p>
<p>​				[[-1.]]]], grad_fn=<ConvolutionBackward0>)</p>
</blockquote>
<h3 id="in_channel2-out_channel1"><a class="markdownIt-Anchor" href="#in_channel2-out_channel1"></a> in_channel=2, out_channel=1</h3>
<p>两个输入通道各自用<strong>自己的卷积核</strong>卷积 → 得到两个结果。</p>
<p><img data-src="../../images/pytorch/18.png" alt="18.png" /></p>
<p>1×1+2×0+3×0+4×(−1)=1+0+0−4=−3</p>
<p>5×0+6×1+7×(−1)+8×0=0+6−7+0=−1</p>
<p>加权求和（合成一个输出通道）：output=(−3)+(−1)=−4</p>
<p>输出张量 (batch=1, out_channel=1, H=1, W=1)：[[−4]]</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

nn_conv_d = nn.Conv2d(in_channels=<span class="hljs-number">2</span>, out_channels=<span class="hljs-number">1</span>, kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>)
nn_conv_d.weight.data = torch.tensor([
    [
        [[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>]],    <span class="hljs-comment"># 输入通道 1 的卷积核</span>
        [[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]]     <span class="hljs-comment"># 输入通道 2 的卷积核</span>
    ]
], dtype=torch.float32)
<span class="hljs-built_in">input</span> = torch.tensor([
    [
        [
            [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
            [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]
        ], [
        [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
        [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]
    ]
    ]
], dtype=torch.float32)

<span class="hljs-built_in">print</span>(nn_conv_d(<span class="hljs-built_in">input</span>))
</code></pre>
<blockquote>
<p>tensor([[[[-4.]]]], grad_fn=<ConvolutionBackward0>)</p>
</blockquote>
<h3 id="in_channel2-out_channel2"><a class="markdownIt-Anchor" href="#in_channel2-out_channel2"></a> in_channel=2, out_channel=2</h3>
<p><img data-src="../../images/pytorch/19.png" alt="19.png" /></p>
<p>X_channel1 * W_out1_in1 = [[1<em>1+2</em>0+3<em>0+4</em>1]] = [[1+0+0+4]] = [[5]]<br />
X_channel2 * W_out1_in2 = [[0<em>0+1</em>1+1<em>1+0</em>0]] = [[0+1+1+0]] = [[2]]<br />
Y_out1 = 5 + 2 = 7</p>
<p>X_channel1 * W_out2_in1 = [[1+2+3+4]] = [[10]]<br />
X_channel2 * W_out2_in2 = [[0* -1 + 1<em>0 + 1</em>0 + 0*-1]] = [[0]]<br />
Y_out2 = 10 + 0 = 10</p>
<p>Y_out1 = [[7]]		Y_out2 = [[10]]</p>
<p>结果为：[[7],[10]]</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

nn_conv_d = nn.Conv2d(in_channels=<span class="hljs-number">2</span>, out_channels=<span class="hljs-number">2</span>, kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>)
nn_conv_d.weight.data = torch.tensor([
    [
        [[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]],
        [[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]]
    ], [
        [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]],
        [[-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>]]
    ]
], dtype=torch.float32)
<span class="hljs-built_in">input</span> = torch.tensor([[
    [
        [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
        [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]
    ], [
        [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]
    ]
]], dtype=torch.float32)

<span class="hljs-built_in">print</span>(nn_conv_d(<span class="hljs-built_in">input</span>))
</code></pre>
<blockquote>
<p>tensor([[[[ 7.]],</p>
<p>​				[[10.]]]], grad_fn=<ConvolutionBackward0>)</p>
</blockquote>
<h3 id="向前传播的卷积正向传播"><a class="markdownIt-Anchor" href="#向前传播的卷积正向传播"></a> 向前传播的卷积（正向传播）</h3>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

dataset = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;data&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),
                                       download=<span class="hljs-literal">True</span>)
dataloader = DataLoader(dataset, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">0</span>)


<span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)
        self.conv1 = nn.Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">6</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        x = self.conv1(x)
        <span class="hljs-keyword">return</span> x


writer = SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)
net = Net()
step = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:
    imgs, targets = data
    outputs = net(imgs)
    <span class="hljs-built_in">print</span>(imgs.shape)
    <span class="hljs-built_in">print</span>(outputs.shape)
    <span class="hljs-comment"># torch.Size([64, 3, 32, 32])</span>
    writer.add_images(<span class="hljs-string">&quot;input&quot;</span>, imgs, global_step=step)
    <span class="hljs-comment"># torch.Size([64, 6, 30, 30])</span>
    outputs = torch.reshape(outputs, (-<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">30</span>, <span class="hljs-number">30</span>))
    writer.add_images(<span class="hljs-string">&quot;output&quot;</span>, outputs, global_step=step)
    step += <span class="hljs-number">1</span>
</code></pre>
<p><code>self.conv1</code> 是一个 <strong>卷积层（<code>nn.Conv2d</code>）</strong>，在 <code>forward</code> 里写 <code>x = self.conv1(x)</code>，就是把输入 <code>x</code> <strong>通过卷积层进行前向计算</strong>。</p>
<h2 id="pooling-layers池化层"><a class="markdownIt-Anchor" href="#pooling-layers池化层"></a> Pooling Layers池化层</h2>
<p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnB5dG9yY2gub3JnL2RvY3Mvc3RhYmxlL25uLmh0bWwjcG9vbGluZy1sYXllcnM=">torch.nn — PyTorch 2.8 documentation</span></p>
<p>池化是卷积神经网络（CNN）中一个很重要的操作。它的主要作用可以总结为以下几点：</p>
<ol>
<li><strong>降维与减少计算量</strong></li>
</ol>
<ul>
<li><strong>为什么</strong>：卷积层输出的特征图通常很大，如果不缩小，后面网络层的计算量会非常庞大。</li>
<li><strong>怎么做</strong>：池化通过取局部区域的最大值（Max Pooling）或平均值（Average Pooling）来缩小特征图尺寸。</li>
<li><strong>效果</strong>：减少参数量和计算量，加快训练和推理速度。</li>
</ul>
<hr />
<ol start="2">
<li><strong>特征的平移不变性</strong></li>
</ol>
<ul>
<li><strong>什么意思</strong>：如果图片里一个物体稍微移动了，网络依然能识别。</li>
<li><strong>为什么能做到</strong>：池化会在一个小范围内提取<strong>统计特征</strong>（最大值或平均值），因此即使输入图像有微小的偏移，结果变化也不会太大。</li>
</ul>
<hr />
<ol start="3">
<li><strong>突出重要特征，抑制不重要信息</strong></li>
</ol>
<ul>
<li><strong>Max Pooling</strong>：保留一个区域的最大值，倾向于保留最显著的边缘或纹理特征。</li>
<li><strong>Average Pooling</strong>：保留区域的平均值，得到更加平滑的特征。</li>
<li><strong>作用</strong>：相当于“特征压缩”，让后续层更容易提取全局信息。</li>
</ul>
<hr />
<ol start="4">
<li><strong>防止过拟合</strong></li>
</ol>
<ul>
<li>通过减少参数和对细节的依赖，网络更关注大局特征而不是局部噪声，从而减轻过拟合风险。</li>
</ul>
<p><img data-src="../../images/pytorch/20.png" alt="20.png" /></p>
<p>核心参数</p>
<ol>
<li>kernel_size (Union[int, tuple[int, int]]) – the size of the window to take a max over</li>
<li>stride (Union[int, tuple[int, int]]) – the stride of the window. Default value is kernel_size</li>
<li>padding (Union[int, tuple[int, int]]) – Implicit negative infinity padding to be added on both sides</li>
<li><strong>dilation</strong> (Union[int, tuple[int, int]]) – a parameter that controls the stride of elements in the window</li>
<li>return_indices (bool) – if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later</li>
<li>ceil_mode (bool) – when True, will use ceil instead of floor to compute the output shape</li>
</ol>
<p>池化示意图：</p>
<p><img data-src="../../images/pytorch/21.png" alt="21.png" /></p>
<h3 id="dilation扩张率空洞卷积"><a class="markdownIt-Anchor" href="#dilation扩张率空洞卷积"></a> dilation扩张率（空洞卷积）</h3>
<p>这个参数在 <strong>卷积</strong>（特别是卷积神经网络中的卷积层）里起很重要的作用。它和 <code>kernel_size</code>、<code>stride</code> 一样，决定了卷积核是怎么在输入特征图上取值的。</p>
<p><img data-src="https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/dilation.gif?raw=true" alt="dilation.png" /></p>
<p><strong><code>dilation</code> 卷积</strong>，其实就是我们常说的 <strong>空洞卷积 (Dilated Convolution / Atrous Convolution)</strong>。</p>
<h3 id="ceil模式和floor模式"><a class="markdownIt-Anchor" href="#ceil模式和floor模式"></a> ceil模式和floor模式</h3>
<ol start="2">
<li><strong>floor 模式（默认）</strong></li>
</ol>
<ul>
<li>取整时向下取整（floor）。</li>
<li>多余的边缘（不足一个 kernel 的区域）会被丢弃。</li>
<li>比如：
<ul>
<li>输入长度 5</li>
<li><code>kernel=2, stride=2</code></li>
<li>计算：(5−2)/2+1=2.5(5-2)/2 + 1 = 2.5(5−2)/2+1=2.5 → floor → 2</li>
<li>输出长度就是 2（最后一个位置没覆盖到）。</li>
</ul>
</li>
</ul>
<ol start="3">
<li><strong>ceil 模式（开启 <code>ceil_mode=True</code>）</strong></li>
</ol>
<ul>
<li>取整时向上取整（ceil）。</li>
<li>边缘不足 kernel 的部分，也会被保留（通常会用 padding 补齐）。</li>
<li>上面例子：
<ul>
<li>输入长度 5</li>
<li><code>kernel=2, stride=2, ceil_mode=True</code></li>
<li>计算：2.5 → ceil → 3</li>
<li>输出长度就是 3（最后一个区域会只覆盖部分输入，或者补 0）。</li>
</ul>
</li>
</ul>
<p><strong>floor 模式（默认）</strong>：计算稳定，常用于训练。</p>
<p><strong>ceil 模式</strong>：当你希望输入和输出严格对齐，或者想保留更多边缘信息时使用（比如某些图像分割任务）。</p>
<h3 id="提示"><a class="markdownIt-Anchor" href="#提示"></a> 提示</h3>
<p>卷积是提取特征，池化是压缩特征</p>
<p>1080P -&gt; 720P</p>
<h3 id="最大池化"><a class="markdownIt-Anchor" href="#最大池化"></a> 最大池化</h3>
<p><strong>池化的默认步长等于池化核的大小</strong>，<strong>池化的默认步长等于池化核的大小</strong>，<strong>池化的默认步长等于池化核的大小</strong></p>
<p><img data-src="../../images/pytorch/22.png" alt="22.png" /></p>
<h3 id="最大池化实例"><a class="markdownIt-Anchor" href="#最大池化实例"></a> 最大池化实例</h3>
<p>ceil_mode=True：</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn

<span class="hljs-built_in">input</span> = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>],
                      [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>],
                      [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                      [<span class="hljs-number">5</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                      [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])
<span class="hljs-built_in">input</span> = torch.reshape(<span class="hljs-built_in">input</span>, (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>))


<span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)
        self.maxPool1 = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, ceil_mode=<span class="hljs-literal">True</span>)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):
        output = self.maxPool1(<span class="hljs-built_in">input</span>)
        <span class="hljs-keyword">return</span> output


net = Net()
output = net(<span class="hljs-built_in">input</span>)
<span class="hljs-built_in">print</span>(output.shape)
<span class="hljs-built_in">print</span>(output)
</code></pre>
<blockquote>
<p>torch.Size([1, 1, 2, 2])<br />
tensor([[[[2, 3],<br />
[5, 1]]]])</p>
</blockquote>
<p>ceil_mode=False：</p>
<pre class="highlight"><code class="python">self.maxPool1 = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, ceil_mode=<span class="hljs-literal">False</span>)
</code></pre>
<blockquote>
<p>torch.Size([1, 1, 1, 1])<br />
tensor([[[[2]]]])</p>
</blockquote>
<h3 id="图片操作"><a class="markdownIt-Anchor" href="#图片操作"></a> 图片操作</h3>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torchvision.datasets
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

<span class="hljs-built_in">input</span> = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>],
                      [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>],
                      [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                      [<span class="hljs-number">5</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                      [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])
<span class="hljs-built_in">input</span> = torch.reshape(<span class="hljs-built_in">input</span>, (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>))

data_set = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;data&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),
                                        download=<span class="hljs-literal">True</span>)

dataloader = DataLoader(data_set, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">0</span>)


<span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)
        self.maxPool1 = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, ceil_mode=<span class="hljs-literal">False</span>)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):
        output = self.maxPool1(<span class="hljs-built_in">input</span>)
        <span class="hljs-keyword">return</span> output


writer = SummaryWriter(<span class="hljs-string">&quot;logs_maxpool&quot;</span>)

net = Net()
step = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:
    img, label = data
    writer.add_images(<span class="hljs-string">&quot;input&quot;</span>, img, global_step=step)
    output = net(img)
    writer.add_images(<span class="hljs-string">&quot;output&quot;</span>, output, global_step=step)
    step += <span class="hljs-number">1</span>
writer.close()
</code></pre>
<p>可以看到，变成马赛克了：</p>
<p><img data-src="../../images/pytorch/23.png" alt="23.png" /></p>
<h2 id="padding-layers填充层"><a class="markdownIt-Anchor" href="#padding-layers填充层"></a> Padding Layers填充层</h2>
<p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnB5dG9yY2gub3JnL2RvY3Mvc3RhYmxlL25uLmh0bWwjcGFkZGluZy1sYXllcnM=">torch.nn — PyTorch 2.8 documentation</span></p>
<p>填充是应用于图片外围的，主要进行一些值的填充，基本不用到</p>
<p>最多的会使用到：nn.ZeroPad2d Pads the input tensor boundaries with zero.</p>
<h2 id="non-linear-activations非线性激活"><a class="markdownIt-Anchor" href="#non-linear-activations非线性激活"></a> Non-linear Activations非线性激活</h2>
<p>最常见的是：<span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnB5dG9yY2gub3JnL2RvY3Mvc3RhYmxlL2dlbmVyYXRlZC90b3JjaC5ubi5SZUxVLmh0bWwjdG9yY2gubm4uUmVMVQ==">ReLU — PyTorch 2.8 documentation</span></p>
<p><img data-src="../../images/pytorch/24.png" alt="24.png" /></p>
<p>其次是：<span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnB5dG9yY2gub3JnL2RvY3Mvc3RhYmxlL2dlbmVyYXRlZC90b3JjaC5ubi5TaWdtb2lkLmh0bWwjdG9yY2gubm4uU2lnbW9pZA==">Sigmoid — PyTorch 2.8 documentation</span></p>
<p><img data-src="../../images/pytorch/25.png" alt="25.png" /></p>
<h3 id="relu示例"><a class="markdownIt-Anchor" href="#relu示例"></a> ReLU示例</h3>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch

<span class="hljs-built_in">input</span> = torch.tensor([[<span class="hljs-number">1</span>, -<span class="hljs-number">0.5</span>], [-<span class="hljs-number">1</span>, <span class="hljs-number">3</span>]])
output = torch.reshape(<span class="hljs-built_in">input</span>, (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>))
<span class="hljs-built_in">print</span>(output)

<span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(torch.nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)
        self.relu = torch.nn.ReLU()
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>) -&gt; torch.Tensor:
        output = self.relu(<span class="hljs-built_in">input</span>)
        <span class="hljs-keyword">return</span> output
net = Net()
output = net(output)
<span class="hljs-built_in">print</span>(output)
</code></pre>
<h3 id="inplace源码提示"><a class="markdownIt-Anchor" href="#inplace源码提示"></a> inplace源码提示</h3>
<pre class="highlight"><code class="python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inplace: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span></span>):
    <span class="hljs-built_in">super</span>().__init__()
    self.inplace = inplace
</code></pre>
<p>当inplace为假时，不改变源数据</p>
<p>当inplace为真时，执行时改变原始数据（就地算法）</p>
<h3 id="图片操作-2"><a class="markdownIt-Anchor" href="#图片操作-2"></a> 图片操作</h3>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torchvision.datasets
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

dataset = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;data&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="hljs-literal">True</span>)
dataloader = DataLoader(dataset, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">0</span>)

<span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(torch.nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)
        self.relu = torch.nn.ReLU()
        self.sigmoid = torch.nn.Sigmoid()
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>) -&gt; torch.Tensor:
        <span class="hljs-comment"># output = self.relu(input)</span>
        output = self.sigmoid(<span class="hljs-built_in">input</span>)
        <span class="hljs-keyword">return</span> output
net = Net()
writer = SummaryWriter(<span class="hljs-string">&quot;logs_relu&quot;</span>)
step = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:
    imgs, targets = data
    writer.add_images(<span class="hljs-string">&quot;input&quot;</span>, imgs, global_step=step)
    output = net(imgs)
    writer.add_images(<span class="hljs-string">&quot;output&quot;</span>, output, global_step=step)
    step += <span class="hljs-number">1</span>

writer.close()
</code></pre>
<p>结果如下：</p>
<p><img data-src="../../images/pytorch/26.png" alt="26.png" /></p>
<h2 id="normalization-layers正则化层"><a class="markdownIt-Anchor" href="#normalization-layers正则化层"></a> Normalization Layers正则化层</h2>
<p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnB5dG9yY2gub3JnL2RvY3Mvc3RhYmxlL25uLmh0bWwjbm9ybWFsaXphdGlvbi1sYXllcnM=">https://docs.pytorch.org/docs/stable/nn.html#normalization-layers</span></p>
<p>有一篇论文提到正则化层可以加速训练</p>
<h2 id="recurrent-layers循环层"><a class="markdownIt-Anchor" href="#recurrent-layers循环层"></a> Recurrent Layers循环层</h2>
<p>提示：RNN</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnB5dG9yY2gub3JnL2RvY3Mvc3RhYmxlL25uLmh0bWwjcmVjdXJyZW50LWxheWVycw==">torch.nn — PyTorch 2.8 documentation</span></p>
<p><strong>序列的每一步计算都会依赖前一步的隐藏状态</strong>，从而能捕捉序列的时间依赖性。</p>
<h2 id="transformer-layers"><a class="markdownIt-Anchor" href="#transformer-layers"></a> Transformer Layers</h2>
<p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnB5dG9yY2gub3JnL2RvY3Mvc3RhYmxlL25uLmh0bWwjdHJhbnNmb3JtZXItbGF5ZXJz">https://docs.pytorch.org/docs/stable/nn.html#transformer-layers</span></p>
<p>Transformer 的核心思想就是用 <strong>自注意力机制（Self-Attention）</strong> 来替代传统 RNN 或 CNN 处理序列时的缺陷。</p>
<h2 id="linear-layers线性层全连接"><a class="markdownIt-Anchor" href="#linear-layers线性层全连接"></a> Linear Layers线性层（全连接）</h2>
<p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnB5dG9yY2gub3JnL2RvY3Mvc3RhYmxlL25uLmh0bWwjbGluZWFyLWxheWVycw==">torch.nn — PyTorch 2.8 documentation</span></p>
<p><img data-src="../../images/pytorch/27.png" alt="27.png" /></p>
<h3 id="源码提示"><a class="markdownIt-Anchor" href="#源码提示"></a> 源码提示</h3>
<pre class="highlight"><code class="">torch.nn.Linear(in_features, out_features, bias=True)
</code></pre>
<p><code>in_features</code>含义：输入特征的维度（每个样本的输入向量长度）。</p>
<p><code>out_features</code>含义：输出特征的维度（每个样本的输出向量长度）。</p>
<p><code>bias</code>是否使用偏置项，默认 True</p>
<p><img data-src="../../images/pytorch/28.png" alt="28.png" /></p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

dataset = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;data&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),
                                       download=<span class="hljs-literal">True</span>)
dataLoader = DataLoader(dataset, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">0</span>, drop_last=<span class="hljs-literal">True</span>)


<span class="hljs-comment"># writer = SummaryWriter(&quot;log_liner&quot;)</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)
        self.linear1 = nn.Linear(<span class="hljs-number">196608</span>, <span class="hljs-number">10</span>)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):
        output = self.linear1(<span class="hljs-built_in">input</span>)
        <span class="hljs-keyword">return</span> output


net = Net()
<span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataLoader:
    img, label = data
    <span class="hljs-built_in">print</span>(img.shape)
    output = torch.flatten(img)
    <span class="hljs-built_in">print</span>(output.shape)
    output = net(output)
    <span class="hljs-built_in">print</span>(output.shape)
</code></pre>
<p>展平动作：torch.flatten(img)</p>
<p>输入196608，输出10：nn.Linear(196608, 10)</p>
<h2 id="dropout-layers"><a class="markdownIt-Anchor" href="#dropout-layers"></a> Dropout Layers</h2>
<p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnB5dG9yY2gub3JnL2RvY3Mvc3RhYmxlL25uLmh0bWwjZHJvcG91dC1sYXllcnM=">torch.nn — PyTorch 2.8 documentation</span></p>
<h2 id="sequential序列操作以简单网络模型实战为例"><a class="markdownIt-Anchor" href="#sequential序列操作以简单网络模型实战为例"></a> Sequential序列操作——以简单网络模型实战为例</h2>
<p>相当于transforms的compose，将一些操作组装到一起</p>
<p><img data-src="../../images/pytorch/29.png" alt="29.png" /></p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear


<span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)
        self.conv1 = Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>)
        self.maxPool1 = MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>)
        self.conv2 = Conv2d(in_channels=<span class="hljs-number">32</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>)
        self.maxPool2 = MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>)
        self.conv3 = Conv2d(in_channels=<span class="hljs-number">32</span>, out_channels=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>)
        self.maxPool3 = MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>)
        self.flatten = Flatten()
        self.linear0 = Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">64</span>)
        self.linear1 = Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        x = self.conv1(x)
        x = self.maxPool1(x)
        x = self.conv2(x)
        x = self.maxPool2(x)
        x = self.conv3(x)
        x = self.maxPool3(x)
        x = self.flatten(x)
        x = self.linear0(x)
        x = self.linear1(x)
        <span class="hljs-keyword">return</span> x


net = Net()
<span class="hljs-built_in">print</span>(net)
<span class="hljs-comment"># 测试网络</span>
<span class="hljs-built_in">input</span> = torch.ones(<span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)
outputs = net(<span class="hljs-built_in">input</span>)
<span class="hljs-built_in">print</span>(outputs.shape)
</code></pre>
<p>输入是一张彩色图片（3个通道）</p>
<ol>
<li>首先用一个卷积层（conv1）去提取一些低级特征，比如边缘、颜色块，然后通过一次池化（maxPool1）把图片“缩小一半”，同时保留主要特征</li>
<li>接着再来一次卷积（conv2），这时候的输入已经有32个通道了，网络会继续在前面提取到的特征基础上，找到更复杂的形状、纹理，然后再做一次池化（maxPool2），图像再缩小一半</li>
<li>然后再卷积一次（conv3），这次输出通道变成64个，能提取更丰富、更抽象的特征，比如局部的结构、物体的一部分，再池化一次（maxPool3），图像又缩小</li>
<li>接下来把这些“缩小后的特征图”拉直成一维向量（flatten），然后经过一个全连接层（linear0），把大规模的特征压缩成一个64维的向量</li>
<li>最后再经过一个全连接层（linear1），输出10个数，这通常对应10个分类的可能性</li>
</ol>
<p>总结一下：它就是一个典型的卷积神经网络，前面几层卷积+池化负责逐步提取和浓缩图像特征，后面的全连接层负责把这些特征转成分类结果。</p>
<h3 id="sequential"><a class="markdownIt-Anchor" href="#sequential"></a> Sequential</h3>
<pre class="highlight"><code class="python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)
        self.conv1 = Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>)
        self.maxPool1 = MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>)
        self.conv2 = Conv2d(in_channels=<span class="hljs-number">32</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>)
        self.maxPool2 = MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>)
        self.conv3 = Conv2d(in_channels=<span class="hljs-number">32</span>, out_channels=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>)
        self.maxPool3 = MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>)
        self.flatten = Flatten()
        self.linear0 = Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">64</span>)
        self.linear1 = Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)
        self.model1 = Sequential(self.conv1, self.maxPool1, self.conv2, self.maxPool2, self.conv3, self.maxPool3,self.flatten, self.linear0, self.linear1)


    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        x = self.model1(x)
        <span class="hljs-keyword">return</span> x
</code></pre>
<p>其中Sequential将一些操作组装到一起：</p>
<pre class="highlight"><code class="python">self.model1 = Sequential(self.conv1, self.maxPool1, self.conv2, self.maxPool2, self.conv3, self.maxPool3,self.flatten, self.linear0, self.linear1)
</code></pre>
<p>进一步地可以这样写：</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)
        self.model2 = Sequential(
            Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Conv2d(in_channels=<span class="hljs-number">32</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Conv2d(in_channels=<span class="hljs-number">32</span>, out_channels=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Flatten(),
            Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">64</span>),
            Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>))

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        x = self.model2(x)
        <span class="hljs-keyword">return</span> x
</code></pre>
<h3 id="流程可视化"><a class="markdownIt-Anchor" href="#流程可视化"></a> 流程可视化</h3>
<pre class="highlight"><code class="python">writer = SummaryWriter(<span class="hljs-string">&quot;logs_seq&quot;</span>)
writer.add_graph(net, <span class="hljs-built_in">input</span>)
writer.close()
</code></pre>
<p><img data-src="../../images/pytorch/30.png" alt="30.png" /></p>
<p>可以观察到具体流程就是：卷积1、池化1、卷积2、池化2、卷积3、池化3、展平、全连接1（线性1）、全连接2（线性2）。</p>
<h2 id="损失函数与反向传播"><a class="markdownIt-Anchor" href="#损失函数与反向传播"></a> 损失函数与反向传播</h2>
<p>计算实际输出和目标之间的差距，为我们更新输出提供一定的依据（反向传播）, grad</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnB5dG9yY2gub3JnL2RvY3Mvc3RhYmxlL25uLmh0bWwjbG9zcy1mdW5jdGlvbnM=">torch.nn — PyTorch 2.8 documentation</span></p>
<hr />
<p><a target="_blank" rel="noopener" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss"><code>nn.L1Loss</code></a></p>
<p>nn.L1Loss：</p>
<p>X:1, 2, 3</p>
<p>Y:1, 2, 5</p>
<p>L1loss = (0+0+2) /3=0.6，这里自然是越小越好</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> L1Loss

<span class="hljs-built_in">input</span> = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype=torch.float32)
target = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>], dtype=torch.float32)

<span class="hljs-built_in">input</span> = torch.reshape(<span class="hljs-built_in">input</span>, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>))
target = torch.reshape(target, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>))

loss = L1Loss()
result = loss(<span class="hljs-built_in">input</span>, target)
<span class="hljs-built_in">print</span>(result)
</code></pre>
<p>输出：tensor(0.6667)</p>
<hr />
<p><a target="_blank" rel="noopener" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss"><code>nn.MSELoss</code></a></p>
<p>MSE = (0+0+2^2)/3=4/3=1.333</p>
<p>注意每个位置要平方</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> L1Loss

<span class="hljs-built_in">input</span> = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype=torch.float32)
target = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>], dtype=torch.float32)

<span class="hljs-built_in">input</span> = torch.reshape(<span class="hljs-built_in">input</span>, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>))
target = torch.reshape(target, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>))

loss_Mse = torch.nn.MSELoss()
result_Mse = loss_Mse(<span class="hljs-built_in">input</span>, target)

<span class="hljs-built_in">print</span>(result_Mse)
</code></pre>
<hr />
<p>交叉熵：<a target="_blank" rel="noopener" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"><code>nn.CrossEntropyLoss</code></a>分类问题</p>
<p><img data-src="../../images/pytorch/31.png" alt="31.png" /></p>
<p>此处log时以e为底数的（ln），常见target就是目标代号，1就是第二个标签</p>
<hr />
<p>使用result_loss.backward()开启反向传播</p>
<p>backward根据损失值 <code>result_loss</code> 自动计算出每个参数的梯度，并把梯度存到参数的 <code>.grad</code> 属性中。</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Conv2d, Sequential, MaxPool2d, Linear, Flatten

dataset = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;data&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),
                                       download=<span class="hljs-literal">True</span>)
dataloader = torch.utils.data.DataLoader(dataset, batch_size=<span class="hljs-number">1</span>)


<span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)
        self.model1 = Sequential(
            Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Conv2d(in_channels=<span class="hljs-number">32</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Conv2d(in_channels=<span class="hljs-number">32</span>, out_channels=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Flatten(),
            Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">64</span>),
            Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>))

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        x = self.model1(x)
        <span class="hljs-keyword">return</span> x


net = Net()
loss = nn.CrossEntropyLoss()
<span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:
    imgs, targets = data
    outputs = net(imgs)
    result_loss = loss(outputs, targets)
    result_loss.backward()
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;ok&quot;</span>)
    <span class="hljs-built_in">print</span>(result_loss)
</code></pre>
<h2 id="optim优化器"><a class="markdownIt-Anchor" href="#optim优化器"></a> optim优化器</h2>
<p>它负责根据参数的梯度 <code>.grad</code> 来更新模型的参数，从而让模型越来越接近目标。</p>
<p>训练一个神经网络时流程是这样的：</p>
<ol>
<li><strong>前向传播 (forward)</strong><br />
输入数据 → 模型输出 → 计算损失 <code>loss</code>。</li>
<li><strong>反向传播 (backward)</strong><br />
调用 <code>loss.backward()</code>，PyTorch 会自动算出每个参数的梯度，并存到 <code>param.grad</code> 里。</li>
<li><strong>参数更新 (update step)</strong><br />
这一步就是 <strong>优化器的作用</strong>。<br />
优化器会读取参数的 <code>.grad</code>，然后根据优化算法（如 SGD、Adam）来调整参数值。</li>
</ol>
<p>optim（update step）会根据backward计算出来的梯度来更新参数</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Conv2d, Sequential, MaxPool2d, Linear, Flatten

dataset = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;data&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),
                                       download=<span class="hljs-literal">True</span>)
dataloader = torch.utils.data.DataLoader(dataset, batch_size=<span class="hljs-number">1</span>)


<span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)
        self.model1 = Sequential(
            Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Conv2d(in_channels=<span class="hljs-number">32</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Conv2d(in_channels=<span class="hljs-number">32</span>, out_channels=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Flatten(),
            Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">64</span>),
            Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>))

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        x = self.model1(x)
        <span class="hljs-keyword">return</span> x


net = Net()
loss = nn.CrossEntropyLoss()
optim = torch.optim.SGD(net.parameters(), lr=<span class="hljs-number">0.01</span>)
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Epoch:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch))
    running_loss = <span class="hljs-number">0.0</span>
    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:
        imgs, targets = data
        outputs = net(imgs)
        result_loss = loss(outputs, targets)
        optim.zero_grad()
        result_loss.backward()
        optim.step()
        running_loss += result_loss
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Loss:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(running_loss))
</code></pre>
<p>关键在于：</p>
<p>optim.zero_grad()<br />
result_loss.backward()<br />
optim.step()</p>
<p>首先optim梯度数据清零，然后backward计算这一次的梯度。最后optim做参数的优化</p>
<p>对于这里的循环：</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:
        imgs, targets = data
        outputs = net(imgs)
        result_loss = loss(outputs, targets)
        optim.zero_grad()
        result_loss.backward()
        optim.step()
        running_loss += result_loss
</code></pre>
<p>只是对数据进行了一轮的学习，我们需要多轮学习才能最优化结果：</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Epoch:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch))
    running_loss = <span class="hljs-number">0.0</span>
    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:
        imgs, targets = data
        outputs = net(imgs)
        result_loss = loss(outputs, targets)
        optim.zero_grad()
        result_loss.backward()
        optim.step()
        running_loss += result_loss
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Loss:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(running_loss))
</code></pre>
<p>因此我们这样操作</p>
<h3 id="sgd随机梯度下降"><a class="markdownIt-Anchor" href="#sgd随机梯度下降"></a> SGD（随机梯度下降）</h3>
<p>训练神经网络的目标是让 <strong>损失函数 Loss 最小化</strong></p>
<p>想象损失函数是一个“山谷地形”，我们要沿着山坡往下走，直到找到最低点</p>
<p><strong>梯度（Gradient）</strong> 就是告诉我们“往哪个方向下坡最快”</p>
<p><img data-src="../../images/pytorch/32.png" alt="32.png" /></p>
<pre class="highlight"><code class="python">optim = torch.optim.SGD(net.parameters(), lr=<span class="hljs-number">0.01</span>)
</code></pre>
<p>模型参数即net.parameters()，学习速率即lr=0.01</p>
<p>学习率 (learning rate, lr)：</p>
<p><strong>lr 太小</strong>：走得慢，收敛速度慢，可能训练很久 loss 才下降。</p>
<p><strong>lr 太大</strong>：走得快，但可能“跨过山谷底部”，造成震荡甚至发散（loss 变大）。</p>
<h2 id="现有模型的使用与修改迁移学习"><a class="markdownIt-Anchor" href="#现有模型的使用与修改迁移学习"></a> 现有模型的使用与修改（迁移学习）</h2>
<p>借用别人训练好的模型知识，在新任务上减少训练成本，提高效果。</p>
<p>在现有vgg16中加一些新的层</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> os

<span class="hljs-keyword">import</span> torchvision.datasets
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

os.environ[<span class="hljs-string">&quot;TORCH_HOME&quot;</span>] = <span class="hljs-string">&quot;E:/PyCharmCode/pytorchSTU/data&quot;</span>
vgg16_false = torchvision.models.vgg16(pretrained=<span class="hljs-literal">False</span>)
vgg16_true = torchvision.models.vgg16(pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-built_in">print</span>(vgg16_true)

train_data = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;data&quot;</span>, train=<span class="hljs-literal">True</span>, transform=torchvision.transforms.ToTensor(),
                                          download=<span class="hljs-literal">True</span>)
vgg16_true.classifier.add_module(<span class="hljs-string">&quot;add_linear&quot;</span>, nn.Linear(<span class="hljs-number">1000</span>, <span class="hljs-number">10</span>))
<span class="hljs-built_in">print</span>(vgg16_true)
</code></pre>
<p>源：</p>
<blockquote>
<p>(classifier): Sequential(<br />
(0): Linear(in_features=25088, out_features=4096, bias=True)<br />
(1): ReLU(inplace=True)<br />
(2): Dropout(p=0.5, inplace=False)<br />
(3): Linear(in_features=4096, out_features=4096, bias=True)<br />
(4): ReLU(inplace=True)<br />
(5): Dropout(p=0.5, inplace=False)<br />
(6): Linear(in_features=4096, out_features=1000, bias=True)<br />
)</p>
</blockquote>
<p>现：</p>
<blockquote>
<p>(classifier): Sequential(<br />
(0): Linear(in_features=25088, out_features=4096, bias=True)<br />
(1): ReLU(inplace=True)<br />
(2): Dropout(p=0.5, inplace=False)<br />
(3): Linear(in_features=4096, out_features=4096, bias=True)<br />
(4): ReLU(inplace=True)<br />
(5): Dropout(p=0.5, inplace=False)<br />
(6): Linear(in_features=4096, out_features=1000, bias=True)<br />
(add_linear): Linear(in_features=1000, out_features=10, bias=True)<br />
)</p>
</blockquote>
<p>在现有vgg16中修改层</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> os

<span class="hljs-keyword">import</span> torchvision.datasets
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-comment"># train_data = torchvision.datasets.ImageNet(root=&quot;data&quot;, split=&quot;train&quot;, download=True,</span>
<span class="hljs-comment">#                                            transform=torchvision.transforms.ToTensor())</span>
os.environ[<span class="hljs-string">&quot;TORCH_HOME&quot;</span>] = <span class="hljs-string">&quot;E:/PyCharmCode/pytorchSTU/data&quot;</span>
vgg16_false = torchvision.models.vgg16(pretrained=<span class="hljs-literal">False</span>)
vgg16_true = torchvision.models.vgg16(pretrained=<span class="hljs-literal">True</span>)
<span class="hljs-built_in">print</span>(vgg16_false)
vgg16_false.classifier[<span class="hljs-number">6</span>] = nn.Linear(in_features=<span class="hljs-number">4096</span>, out_features=<span class="hljs-number">10</span>)
<span class="hljs-built_in">print</span>(vgg16_false)
</code></pre>
<p>源：</p>
<blockquote>
<p>(classifier): Sequential(<br />
(0): Linear(in_features=25088, out_features=4096, bias=True)<br />
(1): ReLU(inplace=True)<br />
(2): Dropout(p=0.5, inplace=False)<br />
(3): Linear(in_features=4096, out_features=4096, bias=True)<br />
(4): ReLU(inplace=True)<br />
(5): Dropout(p=0.5, inplace=False)<br />
(6): Linear(in_features=4096, out_features=1000, bias=True)<br />
)</p>
</blockquote>
<p>现：</p>
<blockquote>
<p>(classifier): Sequential(<br />
(0): Linear(in_features=25088, out_features=4096, bias=True)<br />
(1): ReLU(inplace=True)<br />
(2): Dropout(p=0.5, inplace=False)<br />
(3): Linear(in_features=4096, out_features=4096, bias=True)<br />
(4): ReLU(inplace=True)<br />
(5): Dropout(p=0.5, inplace=False)<br />
(6): Linear(in_features=4096, out_features=10, bias=True)<br />
)</p>
</blockquote>
<h2 id="模型的保存与加载"><a class="markdownIt-Anchor" href="#模型的保存与加载"></a> 模型的保存与加载</h2>
<p>保存：</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> os

<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torchvision.models
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

os.environ[<span class="hljs-string">&quot;TORCH_HOME&quot;</span>] = <span class="hljs-string">&quot;E:/PyCharmCode/pytorchSTU/data&quot;</span>
vgg16 = torchvision.models.vgg16(pretrained=<span class="hljs-literal">False</span>)
<span class="hljs-comment"># 方式1：保存模型结构+参数</span>
torch.save(vgg16, <span class="hljs-string">&quot;vgg16_method1.pth&quot;</span>)
<span class="hljs-comment"># 方式2：保存模型参数(官方推荐)</span>
torch.save(vgg16.state_dict(), <span class="hljs-string">&quot;vgg16_method2.pth&quot;</span>)

<span class="hljs-comment"># 陷阱1</span>
<span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)
        self.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        x = self.conv1(x)
        <span class="hljs-keyword">return</span> x
net = Net()
torch.save(net, <span class="hljs-string">&quot;net.pth&quot;</span>)
</code></pre>
<p>加载：</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torchvision.models
<span class="hljs-keyword">import</span> model_save
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-comment"># 方式1：模型结构+参数加载1</span>
torch_load = torch.load(<span class="hljs-string">&quot;vgg16_method1.pth&quot;</span>, weights_only=<span class="hljs-literal">False</span>)
<span class="hljs-comment"># print(torch_load)</span>
<span class="hljs-comment"># 方式2：模型参数加载2</span>
vgg16 = torchvision.models.vgg16(pretrained=<span class="hljs-literal">False</span>)
torch_load = torch.load(<span class="hljs-string">&quot;vgg16_method2.pth&quot;</span>, weights_only=<span class="hljs-literal">True</span>)
vgg16.load_state_dict(torch_load)


<span class="hljs-comment"># print(vgg16)</span>

<span class="hljs-comment"># 陷阱1: 需要把模型引入才能进行加载import model_save</span>
model = torch.load(<span class="hljs-string">&quot;net.pth&quot;</span>, weights_only=<span class="hljs-literal">False</span>)
<span class="hljs-built_in">print</span>(model)
</code></pre>
<h2 id="模型的完整训练套路"><a class="markdownIt-Anchor" href="#模型的完整训练套路"></a> 模型的完整训练套路</h2>
<ol>
<li>准备数据集、获取数据集大小（可选）</li>
<li>dataLoader加载数据集</li>
<li>搭建神经网络Net</li>
<li>创建损失函数loss_fn、创建优化器optimizer、可选使用tenser board</li>
<li>设置训练网络一些参数：训练的轮数total_train_step、测试的轮数total_test_step、总训练的轮数epoch</li>
<li>开始训练，导出imgs, targets并进入网络</li>
<li>计算损失函数</li>
<li>清零梯度、计算梯度、利用反向传播，致使优化器更新参数</li>
<li>暂时关闭梯度计算、开始测试</li>
<li>计算损失函数并累计、计算当前轮次准确率</li>
<li>重复6~10，直到完成所有轮次</li>
</ol>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torchvision.datasets
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

<span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> *
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

<span class="hljs-comment"># 准备数据集</span>
train_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;data&quot;</span>, train=<span class="hljs-literal">True</span>, transform=torchvision.transforms.ToTensor(),
                                          download=<span class="hljs-literal">True</span>)
test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;data&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),
                                         download=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># 获取数据集大小</span>
train_data_size = <span class="hljs-built_in">len</span>(train_data)
test_data_size = <span class="hljs-built_in">len</span>(test_data)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;训练数据集的长度为<span class="hljs-subst">&#123;train_data_size&#125;</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;测试数据集的长度为<span class="hljs-subst">&#123;test_data_size&#125;</span>&quot;</span>)
<span class="hljs-comment"># dataLoader加载数据集</span>
dataLoader_train = DataLoader(train_data, batch_size=<span class="hljs-number">64</span>)
dataLoader_test = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>)

<span class="hljs-comment"># 搭建神经网络</span>
net = Net()

<span class="hljs-comment"># 创建损失函数</span>
loss_fn = nn.CrossEntropyLoss()

<span class="hljs-comment"># 创建优化器</span>
lr = <span class="hljs-number">0.01</span>
optimizer = torch.optim.SGD(net.parameters(), lr=lr)

<span class="hljs-comment"># tenser board</span>
writer = SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)

<span class="hljs-comment"># 设置训练网络一些参数</span>
<span class="hljs-comment"># 训练的轮数</span>
total_train_step = <span class="hljs-number">0</span>
<span class="hljs-comment"># 测试的轮数</span>
total_test_step = <span class="hljs-number">0</span>
<span class="hljs-comment"># 训练的轮数</span>
epoch = <span class="hljs-number">10</span>

<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;--------第 &#123;&#125; 轮训练开始--------&quot;</span>.<span class="hljs-built_in">format</span>(i + <span class="hljs-number">1</span>))
    <span class="hljs-comment"># 训练开始</span>
    net.train()
    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataLoader_train:
        imgs, targets = data
        outputs = net(imgs)
        loss = loss_fn(outputs, targets)  <span class="hljs-comment"># 损失函数</span>
        optimizer.zero_grad()  <span class="hljs-comment"># 清零梯度</span>
        loss.backward()  <span class="hljs-comment"># 反向传播</span>
        optimizer.step()  <span class="hljs-comment"># 优化器更新参数</span>
        total_train_step += <span class="hljs-number">1</span>
        <span class="hljs-keyword">if</span> total_train_step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练次数：&#123;&#125;, Loss: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_train_step, loss.item()))
            writer.add_scalar(<span class="hljs-string">&quot;train_loss&quot;</span>, loss.item(), total_train_step)
    <span class="hljs-comment"># 测试步骤开始</span>
    net.<span class="hljs-built_in">eval</span>()
    total_test_loss = <span class="hljs-number">0</span>
    total_acc = <span class="hljs-number">0</span>

    <span class="hljs-keyword">with</span> torch.no_grad():  <span class="hljs-comment"># 不进行梯度计算 with的作用是：临时关闭梯度计算，退出后自动恢复。在上下文里关闭，出了上下文就恢复。</span>
        <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataLoader_test:
            imgs, targets = data
            outputs = net(imgs)  <span class="hljs-comment"># 测试步骤开始</span>
            loss = loss_fn(outputs, targets)  <span class="hljs-comment"># 损失函数</span>
            total_test_loss += loss.item()  <span class="hljs-comment"># 求和</span>
            acc = (outputs.argmax(<span class="hljs-number">1</span>) == targets).<span class="hljs-built_in">sum</span>()
            total_acc += acc
        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的Loss：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_test_loss))
        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的正确率：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_acc / test_data_size))
        writer.add_scalar(<span class="hljs-string">&quot;test_loss&quot;</span>, total_test_loss, total_test_step)
        writer.add_scalar(<span class="hljs-string">&quot;test_acc&quot;</span>, total_acc / test_data_size, total_test_step)
        total_test_step += <span class="hljs-number">1</span>
    torch.save(net, <span class="hljs-string">&quot;net_&#123;&#125;.pth&quot;</span>.<span class="hljs-built_in">format</span>(i))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型已保存&quot;</span>)
writer.close()
</code></pre>
<p>10轮学习后，整体测试集上的正确率：0.5428000092506409</p>
<h3 id="test_acc"><a class="markdownIt-Anchor" href="#test_acc"></a> test_acc</h3>
<p><img data-src="../../images/pytorch/33.png" alt="33.png" /></p>
<h3 id="test_loss"><a class="markdownIt-Anchor" href="#test_loss"></a> test_loss</h3>
<p><img data-src="../../images/pytorch/34.png" alt="34.png" /></p>
<h3 id="train_loss"><a class="markdownIt-Anchor" href="#train_loss"></a> train_loss</h3>
<p><img data-src="../../images/pytorch/35.png" alt="35.png" /></p>
<h2 id="使用gpu训练1cuda"><a class="markdownIt-Anchor" href="#使用gpu训练1cuda"></a> 使用GPU训练1（.cuda）</h2>
<ol>
<li>
<p>网络可以使用GPU加速</p>
<pre class="highlight"><code class="python">net = Net()
<span class="hljs-comment"># 交给GPU</span>
<span class="hljs-keyword">if</span> torch.cuda.is_available():
    net = net.cuda()
</code></pre>
</li>
<li>
<p>损失函数可以使用GPU加速</p>
<pre class="highlight"><code class="python">loss_fn = nn.CrossEntropyLoss()
<span class="hljs-comment"># 交给GPU</span>
<span class="hljs-keyword">if</span> torch.cuda.is_available():
    loss_fn = loss_fn.cuda()
</code></pre>
</li>
<li>
<p>数据和标签和使用GPU加速</p>
<pre class="highlight"><code class="python"><span class="hljs-comment"># 交给GPU</span>
<span class="hljs-keyword">if</span> torch.cuda.is_available():
    imgs = imgs.cuda()
    targets = targets.cuda()
</code></pre>
</li>
</ol>
<p>优化后：</p>
<pre class="highlight"><code class="python"><span class="hljs-comment"># _*_ coding : utf-8 _*_</span>
<span class="hljs-comment"># @Time : 2025/8/26 14:38</span>
<span class="hljs-comment"># @Author : KarryLiu</span>
<span class="hljs-comment"># File : train_gpu_1</span>
<span class="hljs-comment"># @Project : pytorchSTU</span>
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torchvision.datasets
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Sequential, Conv2d, MaxPool2d, Linear, Flatten
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

<span class="hljs-comment"># 准备数据集</span>
train_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;data&quot;</span>, train=<span class="hljs-literal">True</span>, transform=torchvision.transforms.ToTensor(),
                                          download=<span class="hljs-literal">True</span>)
test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;data&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),
                                         download=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># 获取数据集大小</span>
train_data_size = <span class="hljs-built_in">len</span>(train_data)
test_data_size = <span class="hljs-built_in">len</span>(test_data)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;训练数据集的长度为<span class="hljs-subst">&#123;train_data_size&#125;</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;测试数据集的长度为<span class="hljs-subst">&#123;test_data_size&#125;</span>&quot;</span>)
<span class="hljs-comment"># dataLoader加载数据集</span>
dataLoader_train = DataLoader(train_data, batch_size=<span class="hljs-number">64</span>)
dataLoader_test = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>)


<span class="hljs-comment"># 搭建神经网络</span>
<span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)
        self.model = Sequential(
            Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Conv2d(in_channels=<span class="hljs-number">32</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Conv2d(in_channels=<span class="hljs-number">32</span>, out_channels=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Flatten(),
            Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">64</span>),
            Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)
        )

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        x = self.model(x)
        <span class="hljs-keyword">return</span> x


net = Net()
<span class="hljs-comment"># 交给GPU</span>
<span class="hljs-keyword">if</span> torch.cuda.is_available():
    net = net.cuda()

<span class="hljs-comment"># 创建损失函数</span>
loss_fn = nn.CrossEntropyLoss()
<span class="hljs-comment"># 交给GPU</span>
<span class="hljs-keyword">if</span> torch.cuda.is_available():
    loss_fn = loss_fn.cuda()

<span class="hljs-comment"># 创建优化器</span>
lr = <span class="hljs-number">0.01</span>
optimizer = torch.optim.SGD(net.parameters(), lr=lr)

<span class="hljs-comment"># tenser board</span>
writer = SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)

<span class="hljs-comment"># 设置训练网络一些参数</span>
<span class="hljs-comment"># 训练的轮数</span>
total_train_step = <span class="hljs-number">0</span>
<span class="hljs-comment"># 测试的轮数</span>
total_test_step = <span class="hljs-number">0</span>
<span class="hljs-comment"># 训练的轮数</span>
epoch = <span class="hljs-number">10</span>

<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;--------第 &#123;&#125; 轮训练开始--------&quot;</span>.<span class="hljs-built_in">format</span>(i + <span class="hljs-number">1</span>))
    <span class="hljs-comment"># 训练开始</span>
    net.train()
    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataLoader_train:
        imgs, targets = data
        <span class="hljs-comment"># 交给GPU</span>
        <span class="hljs-keyword">if</span> torch.cuda.is_available():
            imgs = imgs.cuda()
            targets = targets.cuda()
        outputs = net(imgs)
        loss = loss_fn(outputs, targets)  <span class="hljs-comment"># 损失函数</span>
        optimizer.zero_grad()  <span class="hljs-comment"># 清零梯度</span>
        loss.backward()  <span class="hljs-comment"># 反向传播</span>
        optimizer.step()  <span class="hljs-comment"># 优化器更新参数</span>
        total_train_step += <span class="hljs-number">1</span>
        <span class="hljs-keyword">if</span> total_train_step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练次数：&#123;&#125;, Loss: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_train_step, loss.item()))
            writer.add_scalar(<span class="hljs-string">&quot;train_loss&quot;</span>, loss.item(), total_train_step)
    <span class="hljs-comment"># 测试步骤开始</span>
    net.<span class="hljs-built_in">eval</span>()
    total_test_loss = <span class="hljs-number">0</span>
    total_acc = <span class="hljs-number">0</span>

    <span class="hljs-keyword">with</span> torch.no_grad():  <span class="hljs-comment"># 不进行梯度计算 with的作用是：临时关闭梯度计算，退出后自动恢复。在上下文里关闭，出了上下文就恢复。</span>
        <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataLoader_test:
            imgs, targets = data
            <span class="hljs-comment"># 交给GPU</span>
            <span class="hljs-keyword">if</span> torch.cuda.is_available():
                imgs = imgs.cuda()
                targets = targets.cuda()
            outputs = net(imgs)  <span class="hljs-comment"># 测试步骤开始</span>
            loss = loss_fn(outputs, targets)  <span class="hljs-comment"># 损失函数</span>
            total_test_loss += loss.item()  <span class="hljs-comment"># 求和</span>
            acc = (outputs.argmax(<span class="hljs-number">1</span>) == targets).<span class="hljs-built_in">sum</span>()
            total_acc += acc
        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的Loss：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_test_loss))
        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的正确率：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_acc / test_data_size))
        writer.add_scalar(<span class="hljs-string">&quot;test_loss&quot;</span>, total_test_loss, total_test_step)
        writer.add_scalar(<span class="hljs-string">&quot;test_acc&quot;</span>, total_acc / test_data_size, total_test_step)
        total_test_step += <span class="hljs-number">1</span>
    torch.save(net, <span class="hljs-string">&quot;net_&#123;&#125;.pth&quot;</span>.<span class="hljs-built_in">format</span>(i))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型已保存&quot;</span>)
writer.close()
</code></pre>
<p>使用GPU后，学习100次只需要大约1.23s</p>
<h2 id="使用gpu训练2to"><a class="markdownIt-Anchor" href="#使用gpu训练2to"></a> 使用GPU训练2（.to）</h2>
<p>使用：</p>
<pre class="highlight"><code class="python">device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)
</code></pre>
<ol>
<li>
<p>网络可以使用GPU加速</p>
<pre class="highlight"><code class="python">net = Net()
<span class="hljs-comment"># 交给GPU</span>
net = net.to(device)
</code></pre>
</li>
<li>
<p>损失函数可以使用GPU加速</p>
<pre class="highlight"><code class="python">loss_fn = nn.CrossEntropyLoss()
<span class="hljs-comment"># 交给GPU</span>
loss_fn = loss_fn.to(device)
</code></pre>
</li>
<li>
<p>数据和标签和使用GPU加速</p>
<pre class="highlight"><code class="python"><span class="hljs-comment"># 交给GPU</span>
imgs = imgs.to(device)
targets = targets.to(device)
</code></pre>
</li>
</ol>
<p>优化后：</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torchvision.datasets
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Sequential, Conv2d, MaxPool2d, Linear, Flatten
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)

<span class="hljs-comment"># 准备数据集</span>
train_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;data&quot;</span>, train=<span class="hljs-literal">True</span>, transform=torchvision.transforms.ToTensor(),
                                          download=<span class="hljs-literal">True</span>)
test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;data&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),
                                         download=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># 获取数据集大小</span>
train_data_size = <span class="hljs-built_in">len</span>(train_data)
test_data_size = <span class="hljs-built_in">len</span>(test_data)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;训练数据集的长度为<span class="hljs-subst">&#123;train_data_size&#125;</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;测试数据集的长度为<span class="hljs-subst">&#123;test_data_size&#125;</span>&quot;</span>)
<span class="hljs-comment"># dataLoader加载数据集</span>
dataLoader_train = DataLoader(train_data, batch_size=<span class="hljs-number">64</span>)
dataLoader_test = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>)


<span class="hljs-comment"># 搭建神经网络</span>
<span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)
        self.model = Sequential(
            Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Conv2d(in_channels=<span class="hljs-number">32</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Conv2d(in_channels=<span class="hljs-number">32</span>, out_channels=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Flatten(),
            Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">64</span>),
            Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)
        )

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        x = self.model(x)
        <span class="hljs-keyword">return</span> x


net = Net()
<span class="hljs-comment"># 交给GPU</span>
net = net.to(device)

<span class="hljs-comment"># 创建损失函数</span>
loss_fn = nn.CrossEntropyLoss()
<span class="hljs-comment"># 交给GPU</span>
loss_fn = loss_fn.to(device)
<span class="hljs-comment"># 创建优化器</span>
lr = <span class="hljs-number">0.01</span>
optimizer = torch.optim.SGD(net.parameters(), lr=lr)

<span class="hljs-comment"># tenser board</span>
writer = SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)

<span class="hljs-comment"># 设置训练网络一些参数</span>
<span class="hljs-comment"># 训练的轮数</span>
total_train_step = <span class="hljs-number">0</span>
<span class="hljs-comment"># 测试的轮数</span>
total_test_step = <span class="hljs-number">0</span>
<span class="hljs-comment"># 训练的轮数</span>
epoch = <span class="hljs-number">10</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;--------第 &#123;&#125; 轮训练开始--------&quot;</span>.<span class="hljs-built_in">format</span>(i + <span class="hljs-number">1</span>))
    <span class="hljs-comment"># 训练开始</span>
    net.train()
    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataLoader_train:
        imgs, targets = data
        <span class="hljs-comment"># 交给GPU</span>
        imgs = imgs.to(device)
        targets = targets.to(device)
        outputs = net(imgs)
        loss = loss_fn(outputs, targets)  <span class="hljs-comment"># 损失函数</span>
        optimizer.zero_grad()  <span class="hljs-comment"># 清零梯度</span>
        loss.backward()  <span class="hljs-comment"># 反向传播</span>
        optimizer.step()  <span class="hljs-comment"># 优化器更新参数</span>
        total_train_step += <span class="hljs-number">1</span>
        <span class="hljs-keyword">if</span> total_train_step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练次数：&#123;&#125;, Loss: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_train_step, loss.item()))
            writer.add_scalar(<span class="hljs-string">&quot;train_loss&quot;</span>, loss.item(), total_train_step)
    <span class="hljs-comment"># 测试步骤开始</span>
    net.<span class="hljs-built_in">eval</span>()
    total_test_loss = <span class="hljs-number">0</span>
    total_acc = <span class="hljs-number">0</span>

    <span class="hljs-keyword">with</span> torch.no_grad():  <span class="hljs-comment"># 不进行梯度计算 with的作用是：临时关闭梯度计算，退出后自动恢复。在上下文里关闭，出了上下文就恢复。</span>
        <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataLoader_test:
            imgs, targets = data
            <span class="hljs-comment"># 交给GPU</span>
            imgs = imgs.to(device)
            targets = targets.to(device)
            outputs = net(imgs)  <span class="hljs-comment"># 测试步骤开始</span>
            loss = loss_fn(outputs, targets)  <span class="hljs-comment"># 损失函数</span>
            total_test_loss += loss.item()  <span class="hljs-comment"># 求和</span>
            acc = (outputs.argmax(<span class="hljs-number">1</span>) == targets).<span class="hljs-built_in">sum</span>()
            total_acc += acc
        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的Loss：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_test_loss))
        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的正确率：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_acc / test_data_size))
        writer.add_scalar(<span class="hljs-string">&quot;test_loss&quot;</span>, total_test_loss, total_test_step)
        writer.add_scalar(<span class="hljs-string">&quot;test_acc&quot;</span>, total_acc / test_data_size, total_test_step)
        total_test_step += <span class="hljs-number">1</span>
    torch.save(net, <span class="hljs-string">&quot;net_&#123;&#125;.pth&quot;</span>.<span class="hljs-built_in">format</span>(i))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型已保存&quot;</span>)
writer.close()
</code></pre>
<h2 id="验证套路利用训练好的模型给他提供输入"><a class="markdownIt-Anchor" href="#验证套路利用训练好的模型给他提供输入"></a> 验证套路（利用训练好的模型，给他提供输入）</h2>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torchvision.transforms
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Sequential, Conv2d, MaxPool2d, Flatten, Linear

<span class="hljs-comment"># 如果是由GPU训练得到模型，则需要将模型移动到GPU上</span>
<span class="hljs-comment"># 如果仅想使用CPU测试可以在model中使用：map_location=&quot;cpu&quot;</span>
device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)
image_path = <span class="hljs-string">&quot;images/fff.png&quot;</span>
<span class="hljs-comment"># 此处png是RGBA模式，我们转换成RGB模式</span>
image = Image.<span class="hljs-built_in">open</span>(image_path).convert(<span class="hljs-string">&quot;RGB&quot;</span>)
trans = torchvision.transforms.Compose([
    torchvision.transforms.Resize(size=(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>)),
    torchvision.transforms.ToTensor(),
])
image = trans(image)
<span class="hljs-comment"># 添加一个维度代表1张图片，并交给GPU</span>
image = torch.reshape(image, (<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)).to(device)
<span class="hljs-built_in">print</span>(image.shape)


<span class="hljs-comment"># 搭建神经网络</span>
<span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)
        self.model = Sequential(
            Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Conv2d(in_channels=<span class="hljs-number">32</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Conv2d(in_channels=<span class="hljs-number">32</span>, out_channels=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),
            MaxPool2d(kernel_size=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">False</span>),
            Flatten(),
            Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">64</span>),
            Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)
        )

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        x = self.model(x)
        <span class="hljs-keyword">return</span> x


<span class="hljs-comment"># 仅仅在CPU测试</span>
<span class="hljs-comment"># model = torch.load(&quot;net_9.pth&quot;, weights_only=False, map_location=&quot;cpu&quot;)</span>
model = torch.load(<span class="hljs-string">&quot;net_9.pth&quot;</span>, weights_only=<span class="hljs-literal">False</span>)
model.<span class="hljs-built_in">eval</span>()
<span class="hljs-keyword">with</span> torch.no_grad():
    output = model(image)
    <span class="hljs-built_in">print</span>(output)
    <span class="hljs-built_in">print</span>(torch.argmax(output))
</code></pre>
<blockquote>
<p>torch.Size([1, 3, 32, 32])<br />
tensor([[ 4.1824, -0.9885,  2.4846, -0.8212,  0.5465, -1.7889, -1.6802, -0.3970,<br />
0.7540, -0.7934]])<br />
tensor(0)</p>
</blockquote>
<p><img data-src="../../images/pytorch/36.png" alt="36.png" /></p>
<p><img data-src="../../images/pytorch/37.png" alt="37.png" /></p>
<h2 id="gpu50轮学习后"><a class="markdownIt-Anchor" href="#gpu50轮学习后"></a> GPU50轮学习后</h2>
<p><img data-src="../../images/pytorch/38.png" alt="38.png" /></p>
<p>后面出现了过拟合</p>
<h1 id="附录"><a class="markdownIt-Anchor" href="#附录"></a> 附录</h1>
<p>简单入门了一下，后面还有很长的路要走，继续保持持续学习的动力。</p>
<p>相关代码已公开在GitHub中：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tLzczNTY5MDc1Ny9weXRvcmNoX3N0dV91cA==">https://github.com/735690757/pytorch_stu_up</span></p>
<p>Swim in the ocean of art and programming, weave the future with code art.</p>

      <div class="tags">
          <a href="/tags/python/" rel="tag"><i class="ic i-tag"></i> python</a>
          <a href="/tags/python-PyTorch/" rel="tag"><i class="ic i-tag"></i> python_PyTorch</a>
      </div>
  </div>

   <footer>

    <div class="meta">
  <span class="item">
    <span class="icon">
      <i class="ic i-calendar-check"></i>
    </span>
    <span class="text">更新于</span>
    <time title="修改时间：2026-02-03 00:17:24" itemprop="dateModified" datetime="2026-02-03T00:17:24+08:00">2026-02-03</time>
  </span>
  <span id="python/pytorch/" class="item leancloud_visitors" data-flag-title="PyTorch深度学习" title="阅读次数">
      <span class="icon">
        <i class="ic i-eye"></i>
      </span>
      <span class="text">阅读次数</span>
      <span class="leancloud-visitors-count"></span>
      <span class="text">次</span>
  </span>
</div>

      
<div class="reward">
  <button><i class="ic i-heartbeat"></i> 赞赏</button>
  <p>请我喝[茶]~(￣▽￣)~*</p>
  <div id="qr">
      
      <div>
        <img data-src="/images/myWxPay.jpg" alt="KarryLiu 微信支付">
        <p>微信支付</p>
      </div>
      
      <div>
        <img data-src="/images/myAliPay.jpg" alt="KarryLiu 支付宝">
        <p>支付宝</p>
      </div>
  </div>
</div>

      

<div id="copyright">
<ul>
  <li class="author">
    <strong>本文作者： </strong>KarryLiu <i class="ic i-at"><em>@</em></i>诗岸梦行舟
  </li>
  <li class="link">
    <strong>本文链接：</strong>
    <a href="https://735690757.github.io/python/pytorch/" title="PyTorch深度学习">https://735690757.github.io/python/pytorch/</a>
  </li>
  <li class="license">
    <strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

  </footer>

</article>

  </div>
  

<div class="post-nav">
    <div class="item left">
      

  <a href="/python/pandas/" itemprop="url" rel="prev" data-background-image="&#x2F;images&#x2F;pandasX.jpg" title="Pandas核心处理方法">
  <span class="type">上一篇</span>
  <span class="category"><i class="ic i-flag"></i> 深度学习基础</span>
  <h3>Pandas核心处理方法</h3>
  </a>

    </div>
    <div class="item right">
      

  <a href="/python/pytorch_nn_digital_identification/" itemprop="url" rel="next" data-background-image="&#x2F;images&#x2F;pytorch_DI&#x2F;1.gif" title="基于PyTorch的手写数字识别">
  <span class="type">下一篇</span>
  <span class="category"><i class="ic i-flag"></i> 项目与实战</span>
  <h3>基于PyTorch的手写数字识别</h3>
  </a>

    </div>
</div>

  
  <div class="wrap" id="comments"></div>


        </div>
        <div id="sidebar">
          

<div class="inner">

  <div class="panels">
    <div class="inner">
      <div class="contents panel pjax" data-title="文章目录">
          <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text"> PyTorch深度学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85"><span class="toc-number">1.1.</span> <span class="toc-text"> 安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%A4%E5%A4%A7%E6%B3%95%E5%AE%9D%E5%87%BD%E6%95%B0-dir%E4%B8%8Ehelp"><span class="toc-number">1.2.</span> <span class="toc-text"> 两大法宝函数-dir与help</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96"><span class="toc-number">1.3.</span> <span class="toc-text"> PyTorch数据读取</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tensorboard%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">1.4.</span> <span class="toc-text"> Tensorboard的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#add_scalar"><span class="toc-number">1.4.1.</span> <span class="toc-text"> add_scalar</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#add_image"><span class="toc-number">1.4.2.</span> <span class="toc-text"> add_image</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#transforms%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">1.5.</span> <span class="toc-text"> Transforms的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#totensor%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">1.5.1.</span> <span class="toc-text"> ToTensor的使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#normalize%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">1.5.2.</span> <span class="toc-text"> Normalize的使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#resize%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">1.5.3.</span> <span class="toc-text"> Resize的使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#randomcrop%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">1.5.4.</span> <span class="toc-text"> RandomCrop的使用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8torchvision%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86dataset"><span class="toc-number">1.6.</span> <span class="toc-text"> 使用TorchVision的数据集（DataSet）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E8%81%94%E5%8A%A8tensorboard"><span class="toc-number">1.6.1.</span> <span class="toc-text"> 数据集联动Tensorboard</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#add_image%E6%BA%90%E7%A0%81%E6%8F%90%E7%A4%BA"><span class="toc-number">1.6.2.</span> <span class="toc-text"> add_image源码提示</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dataloader%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">1.7.</span> <span class="toc-text"> DataLoader的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%81%94%E5%8A%A8tensorboard"><span class="toc-number">1.7.1.</span> <span class="toc-text"> 联动Tensorboard</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A9%E7%94%A8epoch%E5%8F%98%E9%87%8F%E6%8E%A7%E5%88%B6%E8%AE%AD%E7%BB%83%E6%88%96%E6%B5%8B%E8%AF%95%E8%BD%AE%E6%AC%A1"><span class="toc-number">1.7.2.</span> <span class="toc-text"> 利用Epoch变量控制训练或测试轮次</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cneural-network%E5%9F%BA%E6%9C%AC%E9%AA%A8%E6%9E%B6"><span class="toc-number">1.8.</span> <span class="toc-text"> 神经网络(Neural Network)基本骨架</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E7%9A%84%E9%AA%A8%E6%9E%B6"><span class="toc-number">1.8.1.</span> <span class="toc-text"> 简单的骨架</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#convolution%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C"><span class="toc-number">1.9.</span> <span class="toc-text"> convolution卷积操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#stride%E8%B7%A8%E6%AD%A51"><span class="toc-number">1.9.1.</span> <span class="toc-text"> Stride跨步&#x3D;1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#stride%E8%B7%A8%E6%AD%A52"><span class="toc-number">1.9.2.</span> <span class="toc-text"> Stride跨步&#x3D;2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#padding%E5%A1%AB%E5%85%851"><span class="toc-number">1.9.3.</span> <span class="toc-text"> Padding填充&#x3D;1</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#convolution-layers%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">1.10.</span> <span class="toc-text"> Convolution Layers卷积层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#in_channel1-out_channel1"><span class="toc-number">1.10.1.</span> <span class="toc-text"> in_channel&#x3D;1, out_channel&#x3D;1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#in_channel1-out_channel2"><span class="toc-number">1.10.2.</span> <span class="toc-text"> in_channel&#x3D;1, out_channel&#x3D;2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#in_channel2-out_channel1"><span class="toc-number">1.10.3.</span> <span class="toc-text"> in_channel&#x3D;2, out_channel&#x3D;1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#in_channel2-out_channel2"><span class="toc-number">1.10.4.</span> <span class="toc-text"> in_channel&#x3D;2, out_channel&#x3D;2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E5%89%8D%E4%BC%A0%E6%92%AD%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.10.5.</span> <span class="toc-text"> 向前传播的卷积（正向传播）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pooling-layers%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">1.11.</span> <span class="toc-text"> Pooling Layers池化层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#dilation%E6%89%A9%E5%BC%A0%E7%8E%87%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF"><span class="toc-number">1.11.1.</span> <span class="toc-text"> dilation扩张率（空洞卷积）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ceil%E6%A8%A1%E5%BC%8F%E5%92%8Cfloor%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.11.2.</span> <span class="toc-text"> ceil模式和floor模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E7%A4%BA"><span class="toc-number">1.11.3.</span> <span class="toc-text"> 提示</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96"><span class="toc-number">1.11.4.</span> <span class="toc-text"> 最大池化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96%E5%AE%9E%E4%BE%8B"><span class="toc-number">1.11.5.</span> <span class="toc-text"> 最大池化实例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E7%89%87%E6%93%8D%E4%BD%9C"><span class="toc-number">1.11.6.</span> <span class="toc-text"> 图片操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#padding-layers%E5%A1%AB%E5%85%85%E5%B1%82"><span class="toc-number">1.12.</span> <span class="toc-text"> Padding Layers填充层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#non-linear-activations%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB"><span class="toc-number">1.13.</span> <span class="toc-text"> Non-linear Activations非线性激活</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#relu%E7%A4%BA%E4%BE%8B"><span class="toc-number">1.13.1.</span> <span class="toc-text"> ReLU示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#inplace%E6%BA%90%E7%A0%81%E6%8F%90%E7%A4%BA"><span class="toc-number">1.13.2.</span> <span class="toc-text"> inplace源码提示</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E7%89%87%E6%93%8D%E4%BD%9C-2"><span class="toc-number">1.13.3.</span> <span class="toc-text"> 图片操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#normalization-layers%E6%AD%A3%E5%88%99%E5%8C%96%E5%B1%82"><span class="toc-number">1.14.</span> <span class="toc-text"> Normalization Layers正则化层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#recurrent-layers%E5%BE%AA%E7%8E%AF%E5%B1%82"><span class="toc-number">1.15.</span> <span class="toc-text"> Recurrent Layers循环层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#transformer-layers"><span class="toc-number">1.16.</span> <span class="toc-text"> Transformer Layers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#linear-layers%E7%BA%BF%E6%80%A7%E5%B1%82%E5%85%A8%E8%BF%9E%E6%8E%A5"><span class="toc-number">1.17.</span> <span class="toc-text"> Linear Layers线性层（全连接）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E6%8F%90%E7%A4%BA"><span class="toc-number">1.17.1.</span> <span class="toc-text"> 源码提示</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dropout-layers"><span class="toc-number">1.18.</span> <span class="toc-text"> Dropout Layers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sequential%E5%BA%8F%E5%88%97%E6%93%8D%E4%BD%9C%E4%BB%A5%E7%AE%80%E5%8D%95%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98%E4%B8%BA%E4%BE%8B"><span class="toc-number">1.19.</span> <span class="toc-text"> Sequential序列操作——以简单网络模型实战为例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#sequential"><span class="toc-number">1.19.1.</span> <span class="toc-text"> Sequential</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%81%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.19.2.</span> <span class="toc-text"> 流程可视化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.20.</span> <span class="toc-text"> 损失函数与反向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#optim%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.21.</span> <span class="toc-text"> optim优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#sgd%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.21.1.</span> <span class="toc-text"> SGD（随机梯度下降）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%B0%E6%9C%89%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%BF%E7%94%A8%E4%B8%8E%E4%BF%AE%E6%94%B9%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.22.</span> <span class="toc-text"> 现有模型的使用与修改（迁移学习）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.23.</span> <span class="toc-text"> 模型的保存与加载</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%8C%E6%95%B4%E8%AE%AD%E7%BB%83%E5%A5%97%E8%B7%AF"><span class="toc-number">1.24.</span> <span class="toc-text"> 模型的完整训练套路</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#test_acc"><span class="toc-number">1.24.1.</span> <span class="toc-text"> test_acc</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#test_loss"><span class="toc-number">1.24.2.</span> <span class="toc-text"> test_loss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#train_loss"><span class="toc-number">1.24.3.</span> <span class="toc-text"> train_loss</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8gpu%E8%AE%AD%E7%BB%831cuda"><span class="toc-number">1.25.</span> <span class="toc-text"> 使用GPU训练1（.cuda）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8gpu%E8%AE%AD%E7%BB%832to"><span class="toc-number">1.26.</span> <span class="toc-text"> 使用GPU训练2（.to）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E5%A5%97%E8%B7%AF%E5%88%A9%E7%94%A8%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%BB%99%E4%BB%96%E6%8F%90%E4%BE%9B%E8%BE%93%E5%85%A5"><span class="toc-number">1.27.</span> <span class="toc-text"> 验证套路（利用训练好的模型，给他提供输入）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gpu50%E8%BD%AE%E5%AD%A6%E4%B9%A0%E5%90%8E"><span class="toc-number">1.28.</span> <span class="toc-text"> GPU50轮学习后</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%99%84%E5%BD%95"><span class="toc-number">2.</span> <span class="toc-text"> 附录</span></a></li></ol>
      </div>
      <div class="related panel pjax" data-title="系列文章">
        <ul>
          <li><a href="/python/numpy/" rel="bookmark" title="NumPy核心处理方法">NumPy核心处理方法</a></li><li><a href="/python/pandas/" rel="bookmark" title="Pandas核心处理方法">Pandas核心处理方法</a></li><li class="active"><a href="/python/pytorch/" rel="bookmark" title="PyTorch深度学习">PyTorch深度学习</a></li><li><a href="/python/CNN_Acc/" rel="bookmark" title="CNN经典卷积神经网络与实战">CNN经典卷积神经网络与实战</a></li><li><a href="/python/NLP/" rel="bookmark" title="NLP自然语言处理">NLP自然语言处理</a></li><li><a href="/python/DLFromFormula/" rel="bookmark" title="从公式角度看深度学习">从公式角度看深度学习</a></li><li><a href="/python/Transformer/" rel="bookmark" title="Transformer实战">Transformer实战</a></li><li><a href="/python/Harris/" rel="bookmark" title="Harris兴趣点检测">Harris兴趣点检测</a></li><li><a href="/python/YOLO/evaluation_indicators/" rel="bookmark" title="模型评价指标">模型评价指标</a></li><li><a href="/python/ss/" rel="bookmark" title="语义分割">语义分割</a></li>
        </ul>
      </div>
      <div class="overview panel" data-title="站点概览">
        <div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="image" itemprop="image" alt="KarryLiu"
      data-src="/images/tx.jpg">
  <p class="name" itemprop="name">KarryLiu</p>
  <div class="description" itemprop="description">愿世间所有的美好都得以祝愿</div>
</div>

<nav class="state">
    <div class="item posts">
      <a href="/archives/">
        <span class="count">57</span>
        <span class="name">文章</span>
      </a>
    </div>
    <div class="item categories">
      <a href="/categories/">
        <span class="count">28</span>
        <span class="name">分类</span>
      </a>
    </div>
    <div class="item tags">
      <a href="/tags/">
        <span class="count">51</span>
        <span class="name">标签</span>
      </a>
    </div>
</nav>

<div class="social">
      <span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tLzczNTY5MDc1Nw==" title="https:&#x2F;&#x2F;github.com&#x2F;735690757"><i class="ic i-github"></i></span>
      <span class="exturl item twitter" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS92b0FMUXRkYWNCVE00RVk=" title="https:&#x2F;&#x2F;twitter.com&#x2F;voALQtdacBTM4EY"><i class="ic i-twitter"></i></span>
      <span class="exturl item email" data-url="bWFpbHRvOjczNTY5MDc1N0BxcS5jb20=" title="mailto:735690757@qq.com"><i class="ic i-envelope"></i></span>
      <span class="exturl item zhihu" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3Blb3BsZS9ydW8tcWlhbi15aW5n" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;ruo-qian-ying"><i class="ic i-zhihu"></i></span>
      <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTM1Nzc1OTIzOA==" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;357759238"><i class="ic i-cloud-music"></i></span>
      <span class="exturl item about" data-url="aHR0cHM6Ly9hYm91dC5tZS9rYXJyeWxpdQ==" title="https:&#x2F;&#x2F;about.me&#x2F;karryliu"><i class="ic i-address-card"></i></span>
</div>

<ul class="menu">
  
    
  <li class="item">
    <a href="/" rel="section"><i class="ic i-home"></i>首页</a>
  </li>

    
  <li class="item">
    <a href="/about/" rel="section"><i class="ic i-user"></i>关于</a>
  </li>

        
  <li class="item dropdown">
      <a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a>
    <ul class="submenu">

        
  <li class="item">
    <a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a>
  </li>

        
  <li class="item">
    <a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a>
  </li>

        
  <li class="item">
    <a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a>
  </li>

  </ul>
    
  <li class="item">
    <a href="/music" rel="section"><i class="ic i-music"></i>橘子君-Ruxra</a>
  </li>

    
  <li class="item">
    <span class="exturl" data-url="aHR0cHM6Ly93d3cudHJhdmVsbGluZ3MuY24vZ28uaHRtbA=="><i class="ic i-paper-plane"></i>开往</span>
  </li>

    
  <li class="item">
    <a href="/friends/" rel="section"><i class="ic i-heart"></i>友链</a>
  </li>

    
  <li class="item">
    <a href="/ocfy" rel="section"><i class="ic i-magic"></i>OCFY</a>
  </li>


</ul>

      </div>
    </div>
  </div>

  <ul id="quick">
    <li class="prev pjax">
        <a href="/python/pandas/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a>
    </li>
    <li class="up"><i class="ic i-arrow-up"></i></li>
    <li class="down"><i class="ic i-arrow-down"></i></li>
    <li class="next pjax">
        <a href="/python/pytorch_nn_digital_identification/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a>
    </li>
    <li class="percent"></li>
  </ul>
</div>


        </div>
        <div class="dimmer"></div>
      </div>
    </main>
    <footer id="footer">
      <div class="inner">
        <div class="widgets">
          
<div class="rpost pjax">
  <h2>随机文章</h2>
  <ul>
      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E5%BC%80%E5%8F%91%E7%BB%8F%E9%AA%8C/" title="分类于 开发经验">开发经验</a>
</div>

    <span><a href="/experience/MD5CalculationTimePractice/" title="MD5计算时间">MD5计算时间</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/Operate-system/" title="分类于 操作系统">操作系统</a>
</div>

    <span><a href="/Operate-system/02OS/" title="进程与线程">进程与线程</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/Java/" title="分类于 Java">Java</a>
</div>

    <span><a href="/Java/javaAd01/" title="Java高级特性1">Java高级特性1</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E7%94%9F%E6%B4%BB/" title="分类于 生活">生活</a>
</div>

    <span><a href="/life/p2025/" title="回望2025——永不熄灭之光">回望2025——永不熄灭之光</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/Operate-system/" title="分类于 操作系统">操作系统</a>
</div>

    <span><a href="/Operate-system/kcsj/" title="操作系统课程设计">操作系统课程设计</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/python/" title="分类于 深度学习">深度学习</a>
<i class="ic i-angle-right"></i>
<a href="/categories/python/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" title="分类于 深度学习基础">深度学习基础</a>
</div>

    <span><a href="/python/YOLO/evaluation_indicators/" title="模型评价指标">模型评价指标</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/python/" title="分类于 深度学习">深度学习</a>
<i class="ic i-angle-right"></i>
<a href="/categories/python/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" title="分类于 深度学习基础">深度学习基础</a>
</div>

    <span><a href="/python/DLFromFormula/" title="从公式角度看深度学习">从公式角度看深度学习</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" title="分类于 数据库">数据库</a>
<i class="ic i-angle-right"></i>
<a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MongoDB/" title="分类于 MongoDB">MongoDB</a>
</div>

    <span><a href="/MongoDB/MongoDB02/" title="MongoDB初步使用">MongoDB初步使用</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" title="分类于 数据库">数据库</a>
<i class="ic i-angle-right"></i>
<a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MongoDB/" title="分类于 MongoDB">MongoDB</a>
</div>

    <span><a href="/MongoDB/MongoDB03/" title="MongoDB初步使用">MongoDB初步使用</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/python/" title="分类于 深度学习">深度学习</a>
<i class="ic i-angle-right"></i>
<a href="/categories/python/Geophysics/" title="分类于 Geophysics">Geophysics</a>
</div>

    <span><a href="/python/Geophysics/hello/" title="深度学习反演网络与重力异常之初窥门径">深度学习反演网络与重力异常之初窥门径</a></span>
  </li>

  </ul>
</div>
<div>
  <h2>最新评论</h2>
  <ul class="leancloud-recent-comment"></ul>
</div>

        </div>
        <div class="status">
  <div class="copyright">
    
    &copy; 2010 – 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="ic i-sakura rotate"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">KarryLiu @ KarryLiu</span>
  </div>
  <div class="powered-by">
    <a href="https://icp.gov.moe/?keyword=20249329" target="_blank">萌ICP备20249329号</a>
    基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span>
  <br/><span>世界本污浊，罪与爱同歌</span>
  <br/><i class="ic i-chart-area"></i>&nbsp;<span>549k&nbsp;字</span> &nbsp;|&nbsp;
  <i class="ic i-coffee"></i>&nbsp;<span> 15:15</span>
  </div>
</div>

      </div>
    </footer>
  </div>
<script data-config type="text/javascript">
  var LOCAL = {
    path: 'python/pytorch/',
    favicon: {
      show: "（●´3｀●）嘿嘿嘿....",
      hide: "(´Д｀)不看我是吧！"
    },
    search : {
      placeholder: "文章搜索",
      empty: "关于 「 ${query} 」，什么也没搜到",
      stats: "${time} ms 内找到 ${hits} 条结果"
    },
    valine: true,fancybox: true,
    copyright: '复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',
    ignores : [
      function(uri) {
        return uri.includes('#');
      },
      function(uri) {
        return new RegExp(LOCAL.path+"$").test(uri);
      }
    ]
  };
</script>

<!-- <script src="https://cdn.polyfill.io/v2/polyfill.js"></script>  -->

<script src="https://polyfill-js.cn/v3/polyfill.min.js"></script>

<script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script>

<script src="/js/app.js?v=0.2.5"></script>




</body>
</html>
