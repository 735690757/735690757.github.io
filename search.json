[{"title":"起点，但不止于起点！","url":"/hello/","content":" 领航\n首先感谢 Github 提供的 pages 服务，其次感谢制作 Hexo 框架全体工作人员和 aurora 主题作者将我带入绚丽多彩的博客世界，最后我还要感谢目前博客使用的主题 Shoka 的制作者，在这里 Karry 献上最崇高的敬意！\n 简单总结\n接下来会使用这个博客分享各种生活趣事、代码经验以及学科题目解答，敬请期待！\nwelcome\n\n\n2023 年 3 月 17 日 完成 Hexo+GitHub 网页搭建\n欢迎访问 Karry.Liu 的个人博客\nQQ：735690757\nWeChat：Sa9329Mxz\ngmail：735690757carry@gmail.com\n\n\n","categories":["初来乍到"],"tags":["欢迎"]},{"title":"考试中有关广义表的两个常用函数的解","url":"/DSLearnNote/GeneralizedLists/","content":" 认识广义表\n广义表是线性表的推广，与线性表不同的是，线性表中的每一个数据元素都属于同一数据对象。\n广义表可以表示为：\n\n空表：()\n表头：(表头)\n表头+表尾：(表头,表尾)\n\n实际上这就是他的基本结构，而对于其中的元素来说，它可以是表，也可以是元素，这就是广义！\n 取头Head()\nHead是取头操作，他拿的是一个元素或者一个表\n 取尾Tail()\nTail是取尾操作，他拿到的必是一个表\n 巧记\n对于蟒蛇（Python）来说，头只有一个，而它的尾巴很长。\n所以，取头Head元素比较单一，取尾Tail往往比较长。\n 练习\n:::primary\nA=（a，b）\nB=（A，A）\nC=（a，（b，A），B）\n对于操作：Tail（Head（Tail（C）））的结果是什么？\n:::\n 解答\nTail（C）尾巴长：（a，（b，A），B{.dot .warning} ）\n[Tail（C）=（（b，A），B）]{.label .info}\nHead（Tail（C））头短：（（b，A）{.dot .warning}，B）\n[Head（Tail（C））=（b，A）]{.label .info}\nTail（Head（Tail（C）））尾巴长：（b，A{.dot .warning}）\n[Tail（Head（Tail（C）））=（A）]{.label .info}\n尾巴长必是一个表，头短可能是一个表也可能是一个元素\n","categories":["笔记","数据结构笔记"],"tags":["数据结构"]},{"title":"数据结构代码题速记","url":"/DSLearnNote/codeTM/","content":" 线性表方向\n 链表的合并\n:::primary no-icon\n设计实现将两个带有头节点的有序链表合并为一个新的有序链表。\n:::\nvoid marge(LNode *A,LNode *B,LNode *C)&#123;\n    LNode p = A-&gt;next;\n    LNode q = B-&gt;next;\n    LNode r;\n    r-&gt;next = null;\n    C = A;\n    r=c;\n    while(p != null||q != null)&#123;\n        if(p-&gt;data &lt;= q-&gt;data)&#123;\n            r-&gt;next = p;\n            p = p-&gt;next;\n            r = r-&gt;next;\n        &#125; else&#123;\n            r-&gt;next = q;\n            q = q-&gt;next;\n            r = r-&gt;next;\n        &#125;\n    &#125;\n    if(p) r = p;\n    if(q) r = q;\n&#125;\n\n 链表的逆置\n:::primary no-icon\n设计实现将无头节点链表进行逆置\n:::\nvoid invert(LinkList &amp;L)&#123;\n    p=L;\n    q=p-&gt;next;\n    while(p)&#123;\n        r=q-&gt;next;\n        q-&gt;next=p;\n        p=q;\n        q=r;\n    &#125;\n    L-&gt;next=null;\n    L=p;\n&#125;\n\n 顺序表的逆置\n:::primary no-icon\n设计一个算法，将顺序表L所有的元素逆置，要求算法效率尽可能地高。\n:::\nvoid reverse(Sqlist &amp;L)&#123;\n    ElemType temp;\n    for(int i=0; i&lt;L.length/2; i++)&#123;\n        temp = L[i];\n        L.data[i]=L.data[L.length-i-1];\n        L.data[L.length-i-1] = temp;\n    &#125;\n&#125;\n\n:::info\n一种”就地“思想\n:::\n 统计单链表HL中的值等于x的个数\nint countX(LNode *HL, ElemType x)&#123;\n    int countNum = 0;\n    LNode p = HL-&gt;next;\n    while(p)&#123;\n        if(p-&gt;data == x)&#123;\n            countNum++;\n        &#125;\n        p = p-&gt;next;\n    &#125;\n    return countNum;\n    \n&#125;\n\n HL是单链表的头指针，删除头节点\nElemType DeleFront(LNode *&amp;HL)&#123;\n    if(HL == null) exit(1);\n    LNode *p = HL;\n    ElemType con = p-&gt;data;\n    p-&gt;next =  p-&gt;netx-&gt;next;\n    return com;\n&#125;\n\n 判断单链表是否中心对称\n:::primary no-icon\n\n链表计数\n动态开辟数组（逻辑栈）\n奇数个要跳过最中心的那个元素\n链表向后，栈向下依次检查\n结束前记得free栈\n\n:::\nbool isDC(LinkList *L) &#123;\n    if (L == NULL || L-&gt;next == NULL) return true;\n\n    // 计算链表长度\n    int count = 0;\n    LinkList *p = L;\n    while (p) &#123;\n        count++;\n        p = p-&gt;next;\n    &#125;\n\n    // 动态分配栈空间\n    int *stack = (int *)malloc((count / 2) * sizeof(int));\n    if (stack == NULL) return false;\n\n    p = L;\n    int top = -1;\n    for (int i = 0; i &lt; count / 2; i++) &#123;\n        stack[++top] = p-&gt;data;\n        p = p-&gt;next;\n    &#125;\n\n    // 如果链表长度为奇数，跳过中间的节点\n    if (count % 2 != 0) p = p-&gt;next;\n\n    // 开始比较\n    while (p) &#123;\n        if (p-&gt;data != stack[top--]) &#123;\n            free(stack);\n            return false;\n        &#125;\n        p = p-&gt;next;\n    &#125;\n\n    free(stack);\n    return true;\n&#125;\n\n\n 判断单链表是否递增\nbool isIncrease(LinkList *l)&#123;\n    LinkList *p = l;\n    int nowData = p-&gt;data;\n    p = p-&gt;next;\n    while(p)&#123;\n        if(p-&gt;data&gt;nowData)&#123;\n            nowData = p-&gt;data;\n            p = p-&gt;next;\n        &#125;else&#123;\n            return false;\n        &#125;\n    &#125;\n    return true;\n&#125;\n\n 链表A，链表B，求A与B的交集生成链表C\nbool isExistence(LinkList *L,Elemtype goData)&#123;\n    if(L==null) return true;\n    LinkList *p = L;\n    while(p)&#123;\n        if(p-&gt;data == goData) return false;\n        p=p-&gt;next;\n    &#125;\n    return true;\n&#125;\nvoid intersectionAB(LinkList *A,LinkList *B,LinkList *&amp;C)&#123;\n    LinkList *a=A;\n    LinkList *b=B;\n    LinkList *c=C;\n    LinkList go;\n    LinkList *goP=go;\n\n    \n    while(a)&#123;\n        while(b)&#123;\n            if(a-&gt;data==b-&gt;data&amp;&amp;isExistence(goP))&#123;\n                c = (LinkList*)malloc(sizeof(LinkList));\n                go = (LinkList*)malloc(sizeof(LinkList));\n                c-&gt;data = b-&gt;data;\n                c = c-&gt;next;\n                goP = goP-&gt;next;\n            &#125;\n            b = b-&gt;next;\n        &#125;\n        a = a-&gt;next;\n        b = B;\n    &#125;\n&#125;\n\n 链式有序归并\nvoid mergelkList(LinkList *a,LinkList *b,LinkList &amp;*c)&#123;\n    LinkList *p = a;\n    LinkList *q = b;\n    LinkList *temp = (Linklist*)malloc(sizeof(LinkList));\n    Linklist *r = temp;\n    while(p != NULL &amp;&amp; q != NULL)&#123;\n        if(p-&gt;data &lt; q-&gt;data)&#123;\n            r-&gt;next = (Linklist*)malloc(sizeof(LinkList));\n            r-&gt;next-&gt;data = p-&gt;data;\n            r = r-&gt;next;\n            p = p-&gt;next;\n        &#125;else&#123;\n            r-&gt;next = (Linklist*)malloc(sizeof(LinkList));\n            r-&gt;next-&gt;data = q-&gt;data;\n            r = r-&gt;next;\n            q = q-&gt;next;\n        &#125;\n    &#125;\n    while(p)&#123;\n        r-&gt;next = (Linklist*)malloc(sizeof(LinkList));\n        r-&gt;next-&gt;data = p-&gt;data;\n        r = r-&gt;next;\n        p = p-&gt;next;\n    &#125;\n    while(q)&#123;\n         r-&gt;next = (Linklist*)malloc(sizeof(LinkList));\n         r-&gt;next-&gt;data = q-&gt;data;\n         r = r-&gt;next;\n         q = q-&gt;next;\n    &#125;\n    r-&gt;next = NULL;\n    c = temp-&gt;next;\n&#125;\n\n 串方向\n 在顺序存储结构上实现求子串\n\n\n 堆方向\n 已知堆，新加入一个底部元素重新调整成堆\nvoid adjustheap(int r[],int n)&#123;\n    int j = n;\n    int i = j/2;\n    int temp = r[j-1];\n    while(i&gt;=1)&#123;\n        if(temp&gt;=r[i-1])&#123;\n            break;\n        &#125;else&#123;\n            r[j-1] = r[i-1];\n            j = i;\n            i = i/2;\n        &#125;\n    &#125;\n    r[j-1] = temp;\n&#125;\n\n 设计单链表中值相同的多余节点\n:::primary no-icon\n这段代码可能存在内存泄漏的风险，应对考试应该还是没问题的\n:::\nvoid deleteCon(LinkList *&amp;L)&#123;\n    if (!L) return;\n    LinkList *p = L;\n    LinkList *q;\n    LinkList *r;\n    int num = 0;\n    while(p)&#123;\n        num++;\n        p = p-&gt;next;\n    &#125;\n    p = L;\n    int* nodeHave = malloc(sizeof(int)*num);\n    int now=1;\n    nodeHave[0]=p-&gt;data;\n    q=p;\n    p=p-&gt;next;\n    while(p)&#123;\n        bool found = false;\n        for(int i=0;i&lt;now;i++)&#123;\n            if(nodeHave[i] == p-&gt;data) &#123;\n                q-&gt;next=q-&gt;next-&gt;next;\n                q=q-&gt;next;\n                p=p-&gt;next;\n                r=p;\n                free(r);\n                found = true;\n            &#125;\n        &#125;\n        if(!found)&#123;\n            nodeHave[now++]=p-&gt;data;\n        \tp=p-&gt;next;\n        \tq=q-&gt;next;\n        &#125; \n    &#125; \n&#125;\n\n 树方向\n 二叉树求树高\nint getDeep(BTree b)&#123;\n    int HL,HR;\n    HL = HR = 0;\n    HL = getDeep(b-&gt;Lchild);\n    HR = getDeep(b-&gt;Rchild);\n    return HL &gt; HR ? (HL+1) : (HR+1);\n    // 此处return就是谁大就返回谁加一\n&#125;\n\n\n 左孩子友兄弟表示法，求原树高\nint hight(BTree b)&#123;\n    int HL,HR;\n    HL = HR = 0;\n    if(b == NULL) return 0;\n    HL = high(b-&gt;Lchild);\n    HR = high(b-&gt;Rchild);\n    if(HL+1 &gt; HR)&#123;\n        return HL + 1;\n    &#125; else&#123;\n        return HR;\n    &#125;\n    \n&#125;\n\n 二叉树求树宽\nint count[100];\nint max = -1;\nvoid width(BTree t,int k)&#123;\n    if(t = NULL) return;\n    count[k] ++;\n    if(max &lt; count[k]) max = count[k];\n    width(t-&gt;lchild, k+1);\n    width(t-&gt;rchild, k+1);\n&#125;\n\n\n 链式存储结构建立二叉树\ntypedef char datatype;\ntypedef struct node&#123;\n    datatype data;\n    struct node *lchild,*rchild;\n&#125;btree\nvoid createBtree(btree *&amp;bt)&#123;\n    datatype zData;\n    scanf(&quot;%c&quot;,&amp;zData);\n    if(zData == &#x27;#&#x27;) &#123;\n        bt=null;\n        return;\n    &#125;\n    bt = (btree*)malloc(sizeof(btree))\n    bt-&gt;data=zData;\n    createBtree(bt-&gt;lchild);\n    createBtree(bt-&gt;rchild);\n&#125;\n\n 判断二叉树是否为排序二叉树/二叉搜索树\n:::primary no-icon\n判断一棵二叉树是否为排序二叉树，可以通过中序遍历来实现。在BST中，中序遍历的结果应该是一个严格递增的序列。如果遍历过程中发现任何一个节点的值不大于前一个节点的值，则该树不是BST。\n:::\nint minnum = INT_MIN;\nint isFirstNode = 1;      // 标记是否是第一个节点(最左侧的节点)\n\nint isBST(BTree *bt)&#123;\n    if(bt==null)&#123;\n        return 1;//空树是BST\n    &#125;\n    if(!isBST(bt-&gt;lchild))&#123;\n        return 0;\n    &#125;\n    if(!isFirstNode &amp;&amp; bt-&gt;data&lt;minnum)&#123;\n        return 0;\n    &#125;\n    isFirstNode =0;\n    minnum = bt-&gt;data;\n    return isBST(bt-&gt;rchild);\n    \n&#125;\n\n 求节点在二叉排序树，关键字是x的层数\nint layer = 0;\nvoid `findXCountLayer(BTree *b,int x)&#123;\n    if(b!=NULL)&#123;\n        layer++;\n        if(b-&gt;data == x)&#123;\n            return;\n        &#125;else if(x &gt; b-&gt;data)&#123;\n            findXCountLayer(b-&gt;rchild,x);\n        &#125;else&#123;\n            findXCountLayer(b-&gt;lchild,x)\n        &#125;\n    &#125;\n&#125;\n\n 统计二叉树节点个数\nint countNode(BTree *b)&#123;\n    if(b == NULL)&#123;\n        return 0;\n    &#125;\n    return countNode(b-&gt;lchild) + countNode(b-&gt;rchild) + 1;\n&#125;\n\n 计算二叉树中所有节点之和\nint sum = 0;\nvoid bTreeSum(BTree *b)&#123;\n    bTreeSum(b-&gt;lchild);\n    sum += b-&gt;data;\n    bTreeSum(b-&gt;rchild);\n&#125;\n\n 排序方向\n 快速排序\n 快速排序分区函数\nint Partition(int a[],int low,int high)&#123;\n    int flag = a[low];\n    while(low &lt; high)&#123;\n        while(low &lt; high &amp;&amp; flag &lt;= a[high]) --high;\n        a[low] = a[high];\n        while(low &lt; high &amp;&amp; flag &gt; a[low]) ++low;\n        a[high] = a[low];\n    &#125;\n    a[low] = flag;\n    return low;\n&#125;\n\n 快速排序递归函数\nvoid QuickSort(int a[],int low,int high)&#123;\n    if(low &lt; high)&#123;\n        int pivo = Partition(a,low,high);\n        QuickSort(a,low,pivo-1);\n        QuickSort(apivo+1,high);\n    &#125;\n&#125;\n\n 链式结构的简单选择排序\nvoid easySelectSort(LinkList *l)&#123;\n    if(l == NULL) return;\n    int min = MAX_INT;\n    int temp;\n    LinkList *p = l;\n    LinkList *r = p;\n    while(p)&#123;\n        min = p-&gt;data;\n        LinkList *q = p;\n        while(q)&#123;\n            if(q-&gt;data &lt; min)&#123;\n                r=q;\n                min = r-&gt;data;\n            &#125;\n            q = q-&gt;next;\n        &#125;\n        if (r != p) &#123;\n        \ttemp = r-&gt;data;\n        \tr-&gt;data = p-&gt;data;\n        \tp-&gt;data = temp;\n        &#125;\n        p = p-&gt;next;\n    &#125;\n&#125;\n\n 综合代码题目\n 快速排序分区函数 + 顺序表移动\n:::primary no-icon\n设计一个算法，调整数组a[]中的元素并返回分界值，使所有小于x的元素都出现在其左边（a[1…i]），所有大于x的元素都出现在其右边（a[i+1…n]）。\n:::\nint div(int a[],intx)&#123;\n    int rear = a.length;\n    a[rear] = x;\n    int start = 0;\n    while(start &lt; rear)&#123;\n        while(start&lt;rear &amp;&amp; x&gt;=a[start]) rear--;\n        a[rear] = a[start];\n        while(start&lt;rear &amp;&amp; x&lt;a[rear]) start++;\n        a[strat] = a[rear];\n    &#125;\n    int i = start;\n    for(int j=i; j&gt;0; j--)&#123;\n        a[i]=a[i-1];\n    &#125;\n    return i;\n&#125;\n\n:::danger\n实际上，上述代码有一些不严谨的部分，比如，可能造成数组越界。\n但仍然可以帮助我们拿下绝大部分的分数。\n:::\nint div(int a[],int h,int x)&#123;\n    int i,j,t;\n    i = 1;\n    j = h;\n    while(i&lt;j)&#123;\n        // 找到第一个小于等于x的元素\n        while(i&lt;j &amp;&amp; a[j]&gt;=x) j--;\n        // 找到第一个大于x的元素\n        while(i&lt;j &amp;&amp; a[i]&lt;=x) i++;\n        if(i&lt;j)&#123;\n            // 交换元素\n            t = a[i];\n            a[i] = a[j];\n            a[j] = t;\n        &#125;\n    &#125;\n    // 这里可能会出现i跨越j的情况，这里做下判断\n    // 如果i位置的数仍然比x要小，那么就可以返回i\n    // 反之就是i跨越了j，而且最多只会跨越一步，返回i-1\n    if(a[i]&lt;x) return i;\n    else return i-1;\n&#125;\n\n:::success\n上述代码体现就是一种类似“就地算法”的思想，无需借助辅助空间。\n找到第一个大于x与小于x的数，直接原地交换位置，然后继续寻找。\n:::\n 将奇数转移到偶数之前\nvoid quickPass(int r[],int s,int t)&#123;\n    int i = s;\n    int j = t;\n    int x = r[s];\n    while(i &lt; j)&#123;\n        while(i &lt; j &amp;&amp; r[j]%2 == 1)&#123;\n            j--;\n        &#125;\n        if(i &lt; j)&#123;\n            r[i] = r[j];\n            i++;\n        &#125;\n        while(i &lt; j &amp;&amp; r[i]%2 == 0)&#123;\n            i++;\n        &#125;\n        if(i &lt; j)&#123;\n            r[j] = r[i];\n            j++;\n        &#125;\n    &#125;\n    r[i] = x;\n&#125;\n\n S是栈，Q是队列，将队列中的元素逆置\nvoid Inverser(Stack &amp;S,Queue &amp;Q)&#123;\n    int x;\n    while(!QueueEmpty(Q))&#123;\n        x = DeQueue(Q);\n        Push(S,x);\n    &#125;\n    while(!StackEmpty(S))&#123;\n        Pop(S,x);\n        EnQueue(Q,x);\n    &#125;\n&#125;\n\n","categories":["笔记","数据结构笔记"],"tags":["数据结构"]},{"title":"SSM框架整合","url":"/Java/SSM01/","content":" 是什么SSM\nSSM是三个框架的简写，本别是Spring，SpringMVC，Mybatis，这三个框架作为JavaWeb强有力的支撑件，极大地提高了开发效率，降低了维护成本，是Java程序员学习Web技术的必经之路（至少现在是这样）\n在此之前，我希望你能熟练掌握，额，哪怕不熟练也要了解Web部分的老祖：请求，响应以及Servlet技术，这些技术能帮助你更好、更快、更流畅的掌握SSM技术。\n其次本文主要讲解SSM整合的相关细节，并不会深入讲解Spring，SpringMVC，Mybatis每个框架的知识，希望大家在看本文之前能了解Spring，SpringMVC，Mybatis单个框架的使用方法。\n 整合开始\n首先我先放出最终的整合状态，其实也不一定非要按照我这样来做。\n\n\n\n\nbean包：大多数的实体类config包：整合的核心配置类controller包：外部控制器暴露点mapper包：Mybiats SQL标准化service包：业务逻辑，包含接口以及实现resource包：资源文件、配置文件webapp：前端资源\n\n\n\n那么接下来我将带着大家一步一步的来做，从创建项目到最终完成整合。\n 创建项目（Maven构建方式）\n选择新建一个项目（或模块），选择Maven Archetype形式创建，我给他命名叫做SSM_demo。\nJDK：1.8（Java 8）\nArchetype：选择最后以webapp结尾的，这是一个创建模板\n组ID：公司域名反写\n\n 创建包结构\n在main目录下创建java目录和必要的包结构\n\n 依赖导入、必要启动配置\n我们需要在pom.xml文件下导入我们SSM所需要的依赖，并配置必要的启动项。\n 导入详解\n第一步：导包\n需要思考我们需要什么依赖：\njunit{.wavy}：单元测试。\nspring-webmvc{.wavy}：整个依赖就比较强大了她不仅包含了SpringMVC的部分也包含了Spring-Context部分，其实也不难理解，因为SpringMVC是基于Spring开发的，那也就必然包含了Spring框架的部分。\n至此我们还剩下数据库的部分，我们继续将其填充完整。\nMybatis{.wavy}：这个是Mybatis的核心依赖。\nmybatis-spring{.wavy}：整个是将Mybatis与Spring进行整合。\nspring-jdbc{.wavy}：简化 Java 应用程序与数据库之间的交互，Spring JDBC 是 MyBatis-Spring 的底层依赖，提供了连接管理、事务管理等基础功能。\nmysql-connector-java{.wavy}：这个是MySQL的驱动依赖。\ndruid{.wavy}：数据库连接池。\nservlet-api{.wavy}：Spring MVC 架构是基于 Servlet 规范的。它使用 Servlet API 来处理和分发 Web 请求，以及与 Web 容器进行交互。\njackson-databind{.wavy}：用于在 Java 对象和 JSON 数据之间进行序列化（对象到 JSON）和反序列化（JSON 到对象）的核心部分。\n&lt;dependencies&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;junit&lt;/groupId&gt;\n    &lt;artifactId&gt;junit&lt;/artifactId&gt;\n    &lt;version&gt;4.13.2&lt;/version&gt;\n    &lt;scope&gt;test&lt;/scope&gt;\n  &lt;/dependency&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;org.springframework&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt;\n    &lt;version&gt;5.2.10.RELEASE&lt;/version&gt;\n  &lt;/dependency&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;org.mybatis&lt;/groupId&gt;\n    &lt;artifactId&gt;mybatis&lt;/artifactId&gt;\n    &lt;version&gt;3.5.9&lt;/version&gt;\n  &lt;/dependency&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;org.mybatis&lt;/groupId&gt;\n    &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt;\n    &lt;version&gt;2.0.7&lt;/version&gt;\n  &lt;/dependency&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;org.springframework&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt;\n    &lt;version&gt;6.0.6&lt;/version&gt;\n  &lt;/dependency&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;mysql&lt;/groupId&gt;\n    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;\n    &lt;version&gt;8.0.31&lt;/version&gt;\n  &lt;/dependency&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;javax.servlet&lt;/groupId&gt;\n    &lt;artifactId&gt;servlet-api&lt;/artifactId&gt;\n    &lt;version&gt;2.5&lt;/version&gt;\n    &lt;scope&gt;provided&lt;/scope&gt;\n  &lt;/dependency&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;\n    &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;\n    &lt;version&gt;2.13.4&lt;/version&gt;\n  &lt;/dependency&gt;\n&lt;/dependencies&gt;\n\n一定要为我们的servlet-api配置作用域为provided，这是非常大的一个坑点！！！\n如果您不将 servlet-api 的作用域设置为 provided，而是将其作用域设置为默认的 compile，则可能会导致以下问题和影响：\n\n冲突问题： 如果您的应用程序包含了自己的 Servlet API 实现（例如 jar 包），而 Web 容器也提供了自己的实现，可能会导致冲突和不稳定性。\n版本不一致： 不同的 Web 容器可能使用不同版本的 Servlet API，如果您将 API 包含在应用程序中，可能会导致版本不一致的问题。\n应用程序大小增加： 将 Servlet API 包含在应用程序中会增加应用程序的大小，尽管这在绝大多数情况下可能不会对性能产生明显影响，但仍然会浪费一些资源。\n维护困难： 如果您在多个项目中重复地包含 Servlet API，可能会导致维护上的困难，特别是在更新版本或切换到不同的 Web 容器时。\n\n&lt;scope&gt;provided&lt;/scope&gt;\n\n第二步：启动配置\n在这里你可以选择外挂Tomcat和Maven插件的两种启动方式，在这里我选择插件法，因为外挂启动真的是太慢了QWQ\npom.xml文件中的build标签下配置tomcat插件：\n&lt;build&gt;\n  &lt;finalName&gt;SSM_demo&lt;/finalName&gt;\n  &lt;plugins&gt;\n    &lt;plugin&gt;\n      &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt;\n      &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt;\n      &lt;version&gt;2.2&lt;/version&gt;\n      &lt;configuration&gt;\n        &lt;port&gt;80&lt;/port&gt;\n        &lt;path&gt;/&lt;/path&gt;\n      &lt;/configuration&gt;\n    &lt;/plugin&gt;\n  &lt;/plugins&gt;\n&lt;/build&gt;\n\n80端口为启动端口，你也可以进行修改，例如修改到8090\n&lt;port&gt;8090&lt;/port&gt;\n\n第三步：Java版本配置\n其实这是一个坑点我们需要在pom文件加上这两行：\n&lt;properties&gt;\n  &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;\n  &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;\n&lt;/properties&gt;\n\n这是在声明我们的Java版本为java8，这个一定要根据我们项目的java版本来填写\n 最终展示\n&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; \n         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\n  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 \n                      http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;\n  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n  &lt;groupId&gt;com.KarryCode&lt;/groupId&gt;\n  &lt;artifactId&gt;SSM_demo&lt;/artifactId&gt;\n  &lt;packaging&gt;war&lt;/packaging&gt;\n  &lt;properties&gt;\n    &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;\n    &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;\n  &lt;/properties&gt;\n  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n  &lt;name&gt;SSM_demo Maven Webapp&lt;/name&gt;\n  &lt;url&gt;http://maven.apache.org&lt;/url&gt;\n  &lt;dependencies&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;junit&lt;/groupId&gt;\n      &lt;artifactId&gt;junit&lt;/artifactId&gt;\n      &lt;version&gt;4.13.2&lt;/version&gt;\n      &lt;scope&gt;test&lt;/scope&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;org.springframework&lt;/groupId&gt;\n      &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt;\n      &lt;version&gt;5.2.10.RELEASE&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;org.mybatis&lt;/groupId&gt;\n      &lt;artifactId&gt;mybatis&lt;/artifactId&gt;\n      &lt;version&gt;3.5.9&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;org.mybatis&lt;/groupId&gt;\n      &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt;\n      &lt;version&gt;2.0.7&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;org.springframework&lt;/groupId&gt;\n      &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt;\n      &lt;version&gt;5.0.2.RELEASE&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;mysql&lt;/groupId&gt;\n      &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;\n      &lt;version&gt;8.0.31&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;com.alibaba&lt;/groupId&gt;\n      &lt;artifactId&gt;druid&lt;/artifactId&gt;\n      &lt;version&gt;1.2.16&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;javax.servlet&lt;/groupId&gt;\n      &lt;artifactId&gt;servlet-api&lt;/artifactId&gt;\n      &lt;version&gt;2.5&lt;/version&gt;\n      &lt;scope&gt;provided&lt;/scope&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;\n      &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;\n      &lt;version&gt;2.13.4&lt;/version&gt;\n    &lt;/dependency&gt;\n  &lt;/dependencies&gt;\n  &lt;build&gt;\n    &lt;finalName&gt;SSM_demo&lt;/finalName&gt;\n    &lt;plugins&gt;\n      &lt;plugin&gt;\n        &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt;\n        &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt;\n        &lt;version&gt;2.2&lt;/version&gt;\n        &lt;configuration&gt;\n          &lt;port&gt;80&lt;/port&gt;\n          &lt;path&gt;/&lt;/path&gt;\n        &lt;/configuration&gt;\n      &lt;/plugin&gt;\n    &lt;/plugins&gt;\n  &lt;/build&gt;\n&lt;/project&gt;\n\n\n 创建JDBC.properties文件\n在resource目录下创建jdbc.properties\njdbc.driver = com.mysql.cj.jdbc.Driver\njdbc.url = \njdbc.username = \njdbc.password = \n\n等号后面的根据自己的需要进行填写\n为了测试，我还创建的一张表：\n\n这是我的生成脚本：\ncreate table tb_book\n(\n    id          int auto_increment\n        primary key,\n    type        varchar(20)  null,\n    name        varchar(50)  null,\n    description varchar(255) null\n);\n\n 创建mapper代理包\n首先在resource目录下创建与java源代码目录一样的包，切记一定要以“/”的分割形式去创建，\n例如：com/KarryCode/mapper\n创建好后，后置在这个目录下创建对应的xml，我们稍后再说！\n 创建配置文件（配置类）\n这部分将以配置类的形式进行配置。\n Spring_Mybatis整合配置\n首先在config包下创建Spring_Mybatis配置类\npackage com.KarryCode.config;\n\nimport com.alibaba.druid.pool.DruidDataSource;\nimport org.mybatis.spring.SqlSessionFactoryBean;\nimport org.mybatis.spring.annotation.MapperScan;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.ComponentScan;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.context.annotation.PropertySource;\nimport org.springframework.jdbc.datasource.DataSourceTransactionManager;\nimport org.springframework.transaction.PlatformTransactionManager;\nimport org.springframework.transaction.annotation.EnableTransactionManagement;\n\nimport javax.sql.DataSource;\n\n/**\n * @Author KarryLiu\n * @Creed may all the beauty be blessed\n * @ClassName SpringConfig\n * @Description TODO Spring核心配置类\n * @Version 1.0\n */\n@Configuration                                  //TODO 核心配置唯一标识\n@ComponentScan(&#123;&quot;com.KarryCode.service&quot;,&quot;com.KarryCode.bean&quot;&#125;)     //TODO 注解扫描指定包\n@PropertySource(&quot;classpath:jdbc.properties&quot;)    //TODO 加载JDBC配置类\n@MapperScan(&quot;com.KarryCode.mapper&quot;)             //TODO MyBatis基于包扫描方式识别Mapper\n@EnableTransactionManagement                    //TODO 事务的自动代理，注解驱动\npublic class Spring_MybatisConfig &#123;\n    @Bean\n    //TODO DruidDataSource数据源的产生\n    public DataSource dataSource(\n            @Value(&quot;$&#123;jdbc.driver&#125;&quot;) String driver,\n            @Value(&quot;$&#123;jdbc.url&#125;&quot;) String url,\n            @Value(&quot;$&#123;jdbc.username&#125;&quot;) String username,\n            @Value(&quot;$&#123;jdbc.password&#125;&quot;) String password\n    ) &#123;\n        DruidDataSource dataSource = new DruidDataSource();\n        dataSource.setDriverClassName(driver);\n        dataSource.setUrl(url);\n        dataSource.setUsername(username);\n        dataSource.setPassword(password);\n        //里面还可以配置更多关于数据库连接池的选项.......\n        return dataSource;\n    &#125;\n\n    @Bean\n    public SqlSessionFactoryBean sqlSessionFactoryBean(DataSource dataSource) &#123;\n        //TODO sqlSessionFactoryBean MybatisBeans加载\n        SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean();\n        sqlSessionFactoryBean.setDataSource(dataSource);\n        return sqlSessionFactoryBean;\n    &#125;\n\n    @Bean\n    public PlatformTransactionManager transactionManager(DataSource dataSource) &#123;\n        //TODO 平台事务管理\n        DataSourceTransactionManager dataSourceTransactionManager = new DataSourceTransactionManager();\n        dataSourceTransactionManager.setDataSource(dataSource);\n        return dataSourceTransactionManager;\n    &#125;\n&#125;\n\n\n具体内容不展开讲解，主要概述一下注解以及方法的作用：\n@MapperScan：指定包扫描的路径。\nDataSource dataSource：数据源的产生。\nPlatformTransactionManager transactionManager：事务交由Spring管理。\n SpringMVC整合配置\n同样地，在config包下创建SpringMvcConfig\npackage com.KarryCode.config;\n\nimport org.springframework.context.annotation.ComponentScan;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.web.servlet.config.annotation.EnableWebMvc;\n\n/**\n * @Author KarryLiu\n * @Creed may all the beauty be blessed\n * @PackageName com.KarryCode.config\n * @ClassName SpringMvcConfig\n * @Description TODO\n * @Version 1.0\n */\n@Configuration\n@EnableWebMvc\n@ComponentScan(&quot;com.KarryCode.controller&quot;)\npublic class SpringMvcConfig &#123;\n&#125;\n\n\n Servlet-api整合配置\n同样地，在config包下创建ServletConfig\npackage com.KarryCode.config;\n\nimport org.springframework.web.servlet.support.AbstractAnnotationConfigDispatcherServletInitializer;\n\npublic class ServletConfig extends AbstractAnnotationConfigDispatcherServletInitializer &#123;\n    @Override\n    protected Class&lt;?&gt;[] getRootConfigClasses() &#123;\n        return new Class[]&#123;Spring_MybatisConfig.class&#125;;\n    &#125;\n\n    @Override\n    protected Class&lt;?&gt;[] getServletConfigClasses() &#123;\n        return new Class[]&#123;SpringMvcConfig.class&#125;;\n    &#125;\n\n    @Override\n    protected String[] getServletMappings() &#123;\n        return new String[]&#123;&quot;/&quot;&#125;;\n    &#125;\n&#125;\n\n\n此时的目录结构是这样的：\n\n至此，整合已经基本完成！\n Bean实体类创建\n根据数据库里的以及逻辑关系创建Bean实体，在这里我先创建一个Book实体\npackage com.KarryCode.bean;\n\n/**\n * @Author KarryLiu\n * @Creed may all the beauty be blessed\n * @PackageName com.KarryCode.bean\n * @ClassName Book\n * @Description TODO\n * @Version 1.0\n */\npublic class Book &#123;\n    private Integer id;\n    private String type;\n    private String name;\n    private String description;\n\n    //省略getter/setter/构造器（有参/无参）/toString\n&#125;\n\n mapper接口创建\n在mapper包下创建一个mapper接口BookMapper\npackage com.KarryCode.mapper;\n\nimport com.KarryCode.bean.Book;\nimport org.apache.ibatis.annotations.*;\n\nimport java.util.List;\n\n/**\n * @Author KarryLiu\n * @Creed may all the beauty be blessed\n * @PackageName com.KarryCode.mapper\n * @ClassName BookMapper\n * @Description TODO\n * @Version 1.0\n */\npublic interface BookMapper &#123;\n    @Insert(&quot;insert into ssm_db.tb_book values (null,#&#123;type&#125;,#&#123;name&#125;,#&#123;description&#125;)&quot;)\n    int save(Book book);\n    @Update(&quot;update ssm_db.tb_book set type=#&#123;type&#125;,name=#&#123;name&#125;,description=#&#123;description&#125; where id=#&#123;id&#125;&quot;)\n    int update(Book book);\n    @Delete(&quot;delete from ssm_db.tb_book where tb_book.id=#&#123;id&#125;&quot;)\n    int delete(Integer id);\n    @Select(&quot;select * from ssm_db.tb_book where id=#&#123;id&#125;&quot;)\n    @ResultType(Book.class)\n    Book getBookById(Integer id);\n    @Select(&quot;select * from ssm_db.tb_book&quot;)\n    List&lt;Book&gt; getBookList();\n&#125;\n\n\n通过注解方式配置了增删改查四种方法\n mapper代理xml创建\n虽然没啥用，但是创建了也没什么坏处，在resource的对应目录下创建BookMapper.xml\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;\n&lt;!DOCTYPE mapper\n        PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;\n        &quot;https://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;\n&lt;mapper namespace=&quot;com.KarryCode.mapper.BookMapper&quot;&gt;\n\n&lt;/mapper&gt;\n\n Service逻辑创建\nBookService接口\npackage com.KarryCode.service;\n\nimport com.KarryCode.bean.Book;\nimport org.springframework.transaction.annotation.Transactional;\n\nimport java.util.List;\n\n/**\n * @Author KarryLiu\n * @Creed may all the beauty be blessed\n * @ClassName BookService\n * @Description TODO\n * @Version 1.0\n */\n@Transactional\npublic interface BookService &#123;\n    /**\n     * @param book\n     * @return boolean\n     * @Author KarryLiu_刘珂瑞\n     * @Date 2023/8/14 下午 10:58\n     * @Description TODO 保存\n     */\n    boolean save(Book book);\n    /**\n     * @param book\n     * @return boolean\n     * @Author KarryLiu_刘珂瑞\n     * @Date 2023/8/14 下午 10:58\n     * @Description TODO 修改\n     */\n    boolean update(Book book);\n    /**\n     * @param id\n     * @return boolean\n     * @Author KarryLiu_刘珂瑞\n     * @Date 2023/8/14 下午 10:58\n     * @Description TODO 根据id删除\n     */\n    boolean delete(Integer id);\n    /**\n     * @param id\n     * @return edu.beihua.bean.Book\n     * @Author KarryLiu_刘珂瑞\n     * @Date 2023/8/14 下午 10:59\n     * @Description TODO 根据id查询\n     */\n    Book getBookById(Integer id);\n    /**\n     * @param\n     * @return java.util.List&lt;edu.beihua.bean.Book&gt;\n     * @Author KarryLiu_刘珂瑞\n     * @Date 2023/8/14 下午 10:59\n     * @Description TODO 查询全部\n     */\n    List&lt;Book&gt; getBookList();\n&#125;\n\n\nBookService接口实现\npackage com.KarryCode.service.impl;\n\nimport com.KarryCode.bean.Book;\nimport com.KarryCode.mapper.BookMapper;\nimport com.KarryCode.service.BookService;\nimport org.springframework.stereotype.Service;\n\nimport javax.annotation.Resource;\nimport java.util.List;\n\n/**\n * @Author KarryLiu\n * @Creed may all the beauty be blessed\n * @Date 2023/8/14 下午 10:43\n * @ClassName BookServiceImpl\n * @Description TODO\n * @Version 1.0\n */\n@Service\npublic class BookServiceImpl implements BookService &#123;\n    @Resource\n    private BookMapper bookMapper;\n\n    @Override\n    public boolean save(Book book) &#123;\n        System.out.println(&quot;save=====&gt;Service&quot;);\n\n        int save = bookMapper.save(book);\n        if (save&gt;0)&#123;\n            return true;\n        &#125;else &#123;\n            return false;\n        &#125;\n    &#125;\n\n    @Override\n    public boolean update(Book book) &#123;\n        System.out.println(&quot;update=====&gt;Service&quot;);\n\n        bookMapper.update(book);\n        return true;\n    &#125;\n\n    @Override\n    public boolean delete(Integer id) &#123;\n        System.out.println(&quot;delete=====&gt;Service&quot;);\n\n        bookMapper.delete(id);\n        return true;\n    &#125;\n\n    @Override\n    public Book getBookById(Integer id) &#123;\n        System.out.println(&quot;getBookById=====&gt;Service&quot;);\n\n        Book book = bookMapper.getBookById(id);\n        return book;\n    &#125;\n\n    @Override\n    public List&lt;Book&gt; getBookList() &#123;\n        System.out.println(&quot;getBookList=====&gt;Service&quot;);\n        return bookMapper.getBookList();\n    &#125;\n&#125;\n\n\n Spring—Mybatis通路检测\n编写测试类，观察输出\nimport com.KarryCode.bean.Book;\nimport com.KarryCode.config.Spring_MybatisConfig;\nimport com.KarryCode.service.BookService;\nimport org.junit.Test;\nimport org.springframework.context.annotation.AnnotationConfigApplicationContext;\n\nimport java.util.List;\n\n/**\n * @Author KarryLiu\n * @Creed may all the beauty be blessed\n * @PackageName PACKAGE_NAME\n * @ClassName mybatisTest\n * @Description TODO\n * @Version 1.0\n */\npublic class mybatisSpringTest &#123;\n    @Test\n    public void testMybatisSpring()&#123;\n        AnnotationConfigApplicationContext annotationConfigApplicationContext = new AnnotationConfigApplicationContext(Spring_MybatisConfig.class);\n        BookService bookService = annotationConfigApplicationContext.getBean(BookService.class);\n        List&lt;Book&gt; bookList = bookService.getBookList();\n        System.out.println(bookList);\n    &#125;\n&#125;\n\n\n+++info  输出\n;;;id1\n信息: {dataSource-1} inited\ngetBookList=====&gt;Service\n[Book{id=1, type=‘计算机’, name=‘软件工程’, description=‘吼吼吼吼’}, Book{id=2, type=‘文学’, name=‘儒家经典’, description=‘儒家好书啊！！！！！’}, Book{id=3, type=‘科技’, name=‘百科全书’, description=‘真不错！！！！’}, Book{id=4, type=‘美术’, name=‘当代顶流美术’, description=‘好书啊好书！！！！’}, Book{id=6, type=‘科技’, name=‘百科全书’, description=‘真不错！！！！’}, Book{id=7, type=‘科技’, name=‘百科全书’, description=‘真不错！！！！’}]\n进程已结束,退出代码0\n;;;\n+++\n至此，Spring—Mbatis整合完成\n 构建Controller\n在controller包下创建BookController\npackage com.KarryCode.controller;\n\nimport com.KarryCode.bean.Book;\nimport com.KarryCode.service.BookService;\nimport org.springframework.transaction.annotation.Transactional;\nimport org.springframework.web.bind.annotation.*;\n\nimport javax.annotation.Resource;\nimport java.util.List;\n\n/**\n * @Author KarryLiu\n * @Creed may all the beauty be blessed\n * @PackageName com.KarryCode.controller\n * @ClassName BookController\n * @Description TODO\n * @Version 1.0\n */\n@RestController\n@Transactional\n@RequestMapping(&quot;/books&quot;)\npublic class BookController &#123;\n    @Resource\n    private BookService bookService;\n    @PostMapping\n    public boolean save(@RequestBody Book book) &#123;\n        return bookService.save(book);\n    &#125;\n    @PutMapping\n    public boolean update(@RequestBody Book book) &#123;\n        return bookService.update(book);\n    &#125;\n    @DeleteMapping(&quot;/&#123;id&#125;&quot;)\n    public boolean delete(@PathVariable Integer id) &#123;\n        return bookService.delete(id);\n    &#125;\n    @GetMapping(&quot;/&#123;id&#125;&quot;)\n    public Book getBookById(@PathVariable Integer id) &#123;\n        return bookService.getBookById(id);\n    &#125;\n    @GetMapping\n    public List&lt;Book&gt; getBookList() &#123;\n        return bookService.getBookList();\n    &#125;\n&#125;\n\n\n符合Restful设计建议。\n 启动Tomcat\n\n Postman接口测试\n 存书测试\n\n\n[success]{.label .success}\n\n 查书测试\n 查询全部\n\n 指定id查书\n\n 更新书\n\n\n 删除测试\n\n\n所有接口测试调通，至此SSM整合完毕\n 总结\n其实SSM整合还是比较简单的，通过三个框架的配置就可以完成了，后面的工作都是在进行业务创建。\n好了！本篇文章到这里就结束了，有什么不理解的地方或者有疑问的地方请在评论区留言，或者添加我的联系方式，欢迎各位大佬积极批评讨论！谢谢！\n","categories":["Java","SSM框架整合"],"tags":["Java","SSM"]},{"title":"SpringCloud学习记录","url":"/Java/SpringCloudSTU/","content":" SpringCloud\n 从单体到集群再到分布式\n早期阶段，单体架构是主流选择，所有功能模块打包在一个应用中，开发简单直接，但是随着业务增长，代码变得臃肿，难以扩展特定功能模块，技术栈单一，难以采用新技术。\n为了应对单体架构的性能瓶颈和高可用需求，集群架构应运而生。\n实现方式：\n\n水平扩展：部署多个相同的单体应用实例\n通过负载均衡器(Nginx、F5等)分配请求\n共享数据库或数据库主从复制\n\n但是仍然有缺陷，比如应用本身仍然是单体，业务复杂时扩展不灵活。\n此时分布式架构与微服务应运而生，分布式架构通过将系统拆分为多个服务来解决上述问题。\n本次学习使用尚硅谷b站开放课堂：https://www.bilibili.com/video/BV1UJc2ezEFU\n框架（组件）学习与本套课程高度重合，但并不是课程资料的再复写。\n相关技术：\n\nNacos（注册中心、配置中心）来自Spring Cloud Alibaba\nOpenFegin（远程调用）来自Spring Cloud官方\nSentinel（异常处理、流控规则、熔断规则）来自Spring Cloud Alibaba\nGateway（路由、断言、过滤）来自Spring Cloud官方\nSeata（分布式事务）来自Spring Cloud Alibaba\n\n Nacos\n\n 注册中心\n 服务注册\n首先进行依赖导入\n&lt;!--    nacos 配置中心、注册中心    --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n使用docker或者直接运行的方式启动Nacos\n在Windows平台直接运行下使用命令：startup.cmd -m standalone（standalone为使用单机模式）\n在不同的服务下编写配置，如订单服务和产品服务：\nspring:\n  cloud:\n    nacos:\n      server-addr: 127.0.0.1:8848\n  application:\n    name: service-order\nserver:\n  port: 8080\n\nspring:\n  cloud:\n    nacos:\n      server-addr: 127.0.0.1:8848\n  application:\n    name: service-product\nserver:\n  port: 9000\n\n其中nacos.server-addr为nacos服务的地址为127.0.0.1:8848（本地测试）\n访问http://localhost:8848/nacos，在服务管理-服务列表可以看到现在已经注册上的服务\n\n\n\n服务名\n分组名称\n集群数目\n实例数\n健康实例数\n触发保护阈值\n操作\n\n\n\n\nservice-order\nDEFAULT_GROUP\n1\n2\n2\nfalse\n详情|示例代码|订阅者|删除\n\n\nservice-product\nDEFAULT_GROUP\n1\n3\n3\nfalse\n详情|示例代码|订阅者|删除\n\n\n\n 服务发现\n由于使用了Nacos，服务发现方法的调用存在两套标准，分别是Spring Cloud的DiscoveryClient和Nacos的NacosServiceDiscovery\n@Resource\nDiscoveryClient discoveryClient;\n\n@Resource\nNacosServiceDiscovery nacosServiceDiscovery;\n\n下面为测试代码：\n/**\n * spring标准discovery使用DiscoveryClient\n */\n@Test\npublic void testDiscoveryClient() &#123;\n    for (String service : discoveryClient.getServices()) &#123;\n        /*\n            循环输出服务列表（服务名）\n                service-order\n                service-product\n        */\n        System.out.println(service);\n\n        // 获取所有实例、输出IP与端口号\n        List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(service);\n        for (ServiceInstance instance : instances) &#123;\n            System.out.println(instance.getHost() + &quot;:&quot; + instance.getPort());\n        &#125;\n    &#125;\n&#125;\n/**\n * nacos标准discovery使用NacosServiceDiscovery\n */\n@Test\npublic void testNacosServiceDiscovery() throws NacosException &#123;\n    for (String service : nacosServiceDiscovery.getServices()) &#123;\n        // 输出服务列表\n        System.out.println(service);\n        // 获取所有实例、输出IP与端口号\n        List&lt;ServiceInstance&gt; instances = nacosServiceDiscovery.getInstances(service);\n        for (ServiceInstance instance : instances) &#123;\n            System.out.println(instance.getHost() + &quot;:&quot; + instance.getPort());\n        &#125;\n    &#125;\n&#125;\n\n输出：\nservice-order\n192.168.25.1:8080\n192.168.25.1:8001\nservice-product\n192.168.25.1:9000\n192.168.25.1:9002\n192.168.25.1:9001\n\n如果并没有如此多的输出可能是只启动了两个后端服务，还需要多启动几个来模拟分布式。\n其次，在实际应用中，这个一般会被进一步封装，其发现的过程是自动进行的。\n 初见，远程调用\n现在有一个实例，我们需要一个下单功能，当用户下单后对其商品进行结算这里我们对一些数据做出模拟。\n订单实体：\n@Data\npublic class Order &#123;\n    private Long id;\n    private BigDecimal totalAmount;\n    private Long userId;\n    private String nickName;\n    private String address;\n    private List&lt;Object&gt; product;\n&#125;\n\n商品实体：\n@Data\npublic class Product &#123;\n    private Long id;\n    private BigDecimal price;\n    private String productName;\n    private int num;\n&#125;\n\n其次就是相应和Controller与Service代码，其较为简单不在此处详细展开，不过我想说一下订单部分的Service：\n@Service\npublic class OrderServiceImpl implements OrderService &#123;\n    @Override\n    public Order createOrder(Long productId, Long userId) &#123;\n        Order order = new Order();\n        order.setId(1L);\n        // TODO 需要计算\n        order.setTotalAmount(new BigDecimal(&quot;0&quot;));\n        order.setUserId(userId);\n        order.setNickName(&quot;Karry.Liu&quot;);\n        order.setAddress(&quot;北极&quot;);\n        // TODO 需要远程查询\n        order.setProduct(null);\n        return order;\n    &#125;\n&#125;\n\n由于Order与Product分别位于两个服务之中，其详细的金额Amount与产品详情列表Product List我们目前似乎无法获取，那我们应该怎么办呢？这个我们暂时按下不表，我们现在需要解决一个更加棘手的问题。\n现在我们有如下项目结构（简略版）\n- cloud-demo(基座项目)\n|\n| - services(服务层)\n| | \n| |  - service-order(订单服务)(包含订单实体bean、服务service和控制controller)\n| | \n| |  - service-product(商品服务)(包含商品实体bean、服务service和控制controller)\n\n订单服务无法使用商品bean，反之商品服务无法使用订单bean，因为其每个服务均为独立的项目。\n\n当Order服务需要Product服务时，其在Order的代码内一定会存在与Product相关的关键字，特别地，由于两个服务之间项链紧密，在Product的代码内也许也会出现Order相关的关键字。可是两套服务分别维护着自己的bean（实体对象/实体类），在不同的服务之间甚至没有办法使用对方的实体类。\n解决方案也很简单，将商品与订单的Bean剥离出来，形成一个独立的项目，与services等价地位，并在services添加model依赖。\n&lt;!--    模型依赖    --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.KarryCode&lt;/groupId&gt;\n    &lt;artifactId&gt;model&lt;/artifactId&gt;\n    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;\n&lt;/dependency&gt;\n\n现在的结构为：\n- cloud-demo(基座项目)\n|\n| - model(模型层)(包含订单实体bean与商品实体bean)\n|\n| - services(服务层)\n| | \n| |  - service-order(订单服务)(包含服务service和控制controller)\n| | \n| |  - service-product(商品服务)(包含服务service和控制controller)\n\n至此，服务之间的实体使用已经被打通。\n其次编写远程访问请求模板类RestTemplate，由于RestTemplate是线程安全的，我们可以这样写：\n@Configuration\npublic class ProductServiceConfig &#123;\n    @Bean\n    public RestTemplate restTemplate() &#123;\n        return new RestTemplate();\n    &#125;\n&#125;\n// 上拉使他成为一个Bean\n\n其次编写远程访问方法：\nprivate Product getProductFromRemote(Long productId) &#123;\n    // 获取商品服务所在的所有机器IP+端口\n    List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(&quot;service-product&quot;);\n    // 获取第一个机器（简单版）\n    ServiceInstance serviceInstance = instances.get(0);\n    // 拼接请求地址 http://192.168.25.1:9000/product/100\n    String url = &quot;http://&quot; + serviceInstance.getHost() + &quot;:&quot; + serviceInstance.getPort() + &quot;/product/&quot; + productId;\n    log.info(&quot;远程请求: &#123;&#125;&quot;, url);\n    // 发送请求（远程）\n    return restTemplate.getForObject(url, Product.class);\n&#125;\n\n补充createOrder方法\npublic Order createOrder(Long productId, Long userId) &#123;\n    // 此处提前远程查询\n    Product productFromRemote = getProductFromRemote(productId);\n    Order order = new Order();\n    order.setId(1L);\n    // 计算\n    order.setTotalAmount(productFromRemote.getPrice().multiply(new BigDecimal(productFromRemote.getNum())));\n    order.setUserId(userId);\n    order.setNickName(&quot;Karry.Liu&quot;);\n    order.setAddress(&quot;北极&quot;);\n    // 远程查询的结果\n    order.setProduct(List.of(productFromRemote));\n    return order;\n&#125;\n\n此时双端服务已经打通了，注意到日志：2025-06-24T22:21:11.646+08:00  INFO 3496 — [service-order] [nio-8080-exec-1] c.K.service.impl.OrderServiceImpl        : 远程请求: http://192.168.25.1:9000/product/100\n&#123;\n  &quot;id&quot;: 1,\n  &quot;totalAmount&quot;: 198,\n  &quot;userId&quot;: 2,\n  &quot;nickName&quot;: &quot;Karry.Liu&quot;,\n  &quot;address&quot;: &quot;北极&quot;,\n  &quot;product&quot;: [\n    &#123;\n      &quot;id&quot;: 100,\n      &quot;price&quot;: 99,\n      &quot;productName&quot;: &quot;IPhone-100&quot;,\n      &quot;num&quot;: 2\n    &#125;\n  ]\n&#125;\n\n此时我们还有两个问题，一是我们每次只取了第一个服务器，二是这样写太复杂，没有实现负载均衡。\n针对此第一个问题将在下一小节中解决，第二个问题将在下一个组件中解决。\n 实现负载均衡APIs\n首先是使用复杂一点的方式，后面将会介绍使用注解的形式。\n先引入负载均衡环境依赖。\n&lt;!--    负载均衡    --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-loadbalancer&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n 基于方法的负载均衡\n将getProductFromRemote方法改造为getProductFromRemoteLoadBalancing：\nprivate Product getProductFromRemoteLoadBalancing(Long productId) &#123;\n    // 获取商品服务所在机器(负载均衡)\n    ServiceInstance chooseLoadBalancing = loadBalancerClient.choose(&quot;service-product&quot;);\n    // 拼接请求地址 http://192.168.25.1:9000/product/100\n    log.info(&quot;服务地址（uri）: &#123;&#125;&quot;, chooseLoadBalancing.getUri());//http://192.168.25.1:9000\n    String url = &quot;http://&quot; + chooseLoadBalancing.getHost() + &quot;:&quot; + chooseLoadBalancing.getPort() + &quot;/product/&quot; + productId;\n    log.info(&quot;远程请求: &#123;&#125;&quot;, url);\n    // 发送请求（远程）\n    return restTemplate.getForObject(url, Product.class);\n&#125;\n\n多次请求 http://localhost:8080/create 后观察日志：\n服务地址（uri）: http://192.168.25.1:9001\n远程请求: http://192.168.25.1:9001/product/100\n服务地址（uri）: http://192.168.25.1:9002\n远程请求: http://192.168.25.1:9002/product/100\n服务地址（uri）: http://192.168.25.1:9000\n远程请求: http://192.168.25.1:9000/product/100\n\n可以观察到其负载均衡的使用了不同的端口，下面将介绍注解的形式。\n 基于注解的负载均衡\n还记得我们之前使用的RestTemplate嘛？\n@Resource\nprivate RestTemplate restTemplate;\n\n我们可以观察到，无论哪种方法，最终都会是去使用restTemplate.getForObject(…)这个方法，如果这个方法自己就可以进行负载均衡呢？我们是不是可以少些一点代码？\n改造ProductServiceConfig配置类：\n@Configuration\npublic class ProductServiceConfig &#123;\n\n    @LoadBalanced// 使用负载均衡\n    @Bean\n    public RestTemplate restTemplate() &#123;\n        return new RestTemplate();\n    &#125;\n&#125;\n\n将getProductFromRemoteLoadBalancing方法改造为getProductFromRemoteLoadBalancingWithAnnotation：\nprivate Product getProductFromRemoteLoadBalancingWithAnnotation(Long productId) &#123;\n    String url = &quot;http://service-product/product/&quot; + productId;\n    // 发送请求（远程）\n    return restTemplate.getForObject(url, Product.class);\n&#125;\n\n注意到我们的url中出现了service-product，在由于restTemplate被追加了@LoadBalanced注解，使得整个restTemplate自带有负载均衡的能力，url传过去的时候service-product会被自动替换为IP+HOST的形式，替换的结果符合负载均衡。\n 配置中心\n 基本用法\n引入依赖\n&lt;!--    配置中心    --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n书写导入配置：nacos:service-order.yml\nspring:\n  config:\n    import: nacos:service-order.yml\n  cloud:\n    nacos:\n      server-addr: 127.0.0.1:8848\n  application:\n    name: service-order\nserver:\n  port: 8080\n\n在nacos中设置名为service-order.yml\norder:\n  timeout: 120s\n  autoConfirm: 7d\n\n在controller加入@RefreshScope自动刷新注解\n@Value(&quot;$&#123;order.timeout&#125;&quot;)\nprivate String orderTimeout;\n@Value(&quot;$&#123;order.auto-confirm&#125;&quot;)\nprivate String orderAutoConfirm;\n\n// 获取配置信息\n@GetMapping(&quot;/getConfig&quot;)\npublic String getConfig() &#123;\n    return &quot;order.timeout:&quot; + orderTimeout + &quot; order.autoConfirm:&quot; + orderAutoConfirm;\n&#125;\n\n访问后得到回应：order.timeout:120s order.autoConfirm:7d\n但是配置中心的依赖导入方法具有广播性，有可能出现其他服务的无法启动问题，因此可以在yml中加入：\nspring:\n  cloud:\n    nacos:\n      config:\n        import-check:\n          enabled: false\n\n来禁用检查。\n 无感动态刷新\n手动配置很麻烦，通常使用统一导入的方法来实现，创建OrderProperties：\n@Data\n@Component\n// 批量获取配置，无需使用@RefreshScope即可自动刷新\n@ConfigurationProperties(prefix = &quot;order&quot;)\npublic class OrderProperties &#123;\n    String timeout;\n    String autoConfirm;\n&#125;\n\n@ConfigurationProperties(prefix = “order”)中prefix = &quot;order&quot;获取前缀为order的配置，有了ConfigurationProperties无需使用@RefreshScope即可自动刷新。\n改造OrderController：\n@Autowired\nprivate OrderProperties orderProperties;\n\n\n// 获取配置信息\n@GetMapping(&quot;/getConfig&quot;)\npublic String getConfig() &#123;\n    return &quot;order.timeout:&quot; + orderProperties.getTimeout() + &quot; order.autoConfirm:&quot; + orderProperties.getAutoConfirm();\n&#125;\n\n 本地配置与Nacos配置冲突时\n当本地配置与Nacos配置冲突时，优先以Nacos中的配置中心为准。\n即先导入优先，外部优先。\n当：import: nacos:service-order.yml,nacos:common.yml 出现时，仍然是优先以第一次出现的nacos:service-order.yml为准。\n 数据隔离\n当出现不同环境需要不同配置时，比如dev环境、test环境和prod环境，分别需要不同的配置，我们该如何组织？\n首先Nacos提供了：命名空间-组织-配置单元的模式，命名空间可以对应到dev环境、test环境和prod环境等，组开源对应到不同的微服务比如商品微服务、用户微服务等，配置单元即为具体的详细配置。\n首先在Nacos命名空间、组织与配置。\n\n这里已经创建了dev环境、test环境和prod环境，下面创建详细的组与配置\n\n然后改造yml配置文件\nspring:\n  profiles:\n    active: dev\n  cloud:\n    nacos:\n      server-addr: 127.0.0.1:8848\n      config:\n        namespace: $&#123;spring.profiles.active:dev&#125;\n  application:\n    name: service-order\nserver:\n  port: 8080\n\n\n---\nspring:\n  config:\n    activate:\n      on-profile: dev\n    import:\n      - nacos:common.yml?group=order\n      - nacos:database.yml?group=order\n---\nspring:\n  config:\n    activate:\n      on-profile: test\n    import:\n      - nacos:common.yml?group=order\n      - nacos:database.yml?group=order\n---\nspring:\n  config:\n    activate:\n      on-profile: prod\n    import:\n      - nacos:common.yml?group=order\n      - nacos:database.yml?group=order\n\n\n具体地：\nnamespace: $&#123;spring.profiles.active:dev&#125;\n\n这里主要负责的时从项目到Nacos时我们应该选择哪套命名空间，是Nacos的命名空间\n而对于：\nprofiles:\n  active: dev\n\n主要负责的是要激活哪套分片配置，是on-profile: dev还是on-profile: test，还是on-profile: prod，这里指的是项目的配置分片\n此时只需要切换不同的active: dev，即可完成不同环境的配置切换\n Nacos总结\n\n来自尚硅谷课堂：https://www.bilibili.com/video/BV1UJc2ezEFU\n OpenFeign\nOpenFeign是Spring Cloud生态系统中的一个重要组件。Spring Cloud是一个基于Spring Boot实现的分布式系统开发工具，它提供了一系列的工具来简化分布式系统开发，包括服务注册与发现、配置中心、断路器等功能，OpenFeign默认集成了Ribbon负载均衡器。\n 远程调用\n 导入注解\n&lt;!--    openfeign 远程调用    --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n 创建feign包与XXXClient接口\n@FeignClient(value = &quot;service-product&quot;)\npublic interface ProductFeignClient &#123;\n    // mvc注解使用的两套逻辑，放在Controller上是接收请求，放在FeignClient上是发送请求\n    @GetMapping(&quot;/product/&#123;productId&#125;&quot;)\n    Product getProductById(@PathVariable String productId);\n&#125;\n\n这里的value = &quot;service-product&quot;指的是给那个微服务发送请求，这里还标注了GetMapping，指的是给那个微服务发送请求的路径是什么，其明确了接口。\n 改造订单创建方法\n @Override\n    public Order createOrder(Long productId, Long userId) &#123;\n//        Product productFromRemote = getProductFromRemoteLoadBalancingWithAnnotation(productId);\n        Product productFromRemote = productFeignClient.getProductById(productId);\n        Order order = new Order();\n        order.setId(1L);\n        // TODO 需要计算\n        order.setTotalAmount(productFromRemote.getPrice().multiply(new BigDecimal(productFromRemote.getNum())));\n        order.setUserId(userId);\n        order.setNickName(&quot;Karry.Liu&quot;);\n        order.setAddress(&quot;北极&quot;);\n        // TODO 需要远程查询\n        order.setProduct(List.of(productFromRemote));\n        return order;\n    &#125;\n\n 也可以向外部（第三方）做出请求\n比如下面是一个针对天气数据接口的代码实现\n@FeignClient(value = &quot;weather-client&quot;, url = &quot;http://apis.juhe.cn&quot;)\npublic interface WeatherFeignClient &#123;\n    @GetMapping(&quot;/simpleWeather/query&quot;)\n    String getWeatherByCityId(@RequestParam(&quot;city&quot;) String city,\n                              @RequestParam(&quot;key&quot;) String key);\n\n    default String getEncodedCity(String city) throws Exception &#123;\n        return URLEncoder.encode(city, StandardCharsets.UTF_8);\n    &#125;\n&#125;\n\n 小技巧（懒狗模式）\n如果是要访问自己的设计的接口，着通常是业务接口，比如一个订单服务要访问商品服务，最简单的方式就是将，商品的Controller下的接口找到，然后直接复制方法名（以接口的方式进行复制就可以），然后把他放在地点服务的OpenFegin接口下就可以了，值得注意的是，OpenFegin已经帮我们做好了负载均衡，相比上述Nacos中那种请求模板方便多了。\n具体地如下面的操作所示：\n\n\n找到商品服务接口的代码\n@Slf4j\n@RestController\npublic class ProductController &#123;\n\n    @Autowired\n    private ProductService productService;\n\n    @GetMapping(&quot;/product/&#123;productId&#125;&quot;)\n    public Product getProduct(@PathVariable Long productId) &#123;\n        log.info(&quot;查询商品信息: &#123;&#125;&quot;, productId);\n        return productService.getProductById(productId);\n    &#125;\n&#125;\n\n\n\n就像接口的方式去复制代码\n@GetMapping(&quot;/product/&#123;productId&#125;&quot;)\npublic Product getProduct(@PathVariable Long productId)\n\n\n\n粘贴到订单服务之中\n@FeignClient(value = &quot;service-product&quot;)\npublic interface ProductFeignClient &#123;\n    // mvc注解使用的两套逻辑，放在Controller上是接收请求，放在FeignClient上是发送请求\n    @GetMapping(&quot;/product/&#123;productId&#125;&quot;)\n    Product getProductById(@PathVariable Long productId);\n    \n    @GetMapping(&quot;/product/&#123;productId&#125;&quot;)\n    public Product getProduct(@PathVariable Long productId);\n&#125;\n\n可以观察到，粘贴到ProductFeignClient中的代码与我们之前自己定义的代码片段基本是完全一致，所以这是一个非常好用的小技巧。\n\n\n 面试题\n问：客户端的负载均衡和服务端负载均衡有不用？\n答：首先对于客户端的负载均衡来说，客户端服务会先去访问注册中心，首先获取到一些地址，然后选择一个地址，最后发起调用，这个过程完全是发生在客户端方面的，但是对于服务端的负载均衡来说，这个服务端只对外部暴露一个服务接口，那么所有的请求都需要通过这个唯一的接口来访问。然而，在这个接口背后有着一套负载均衡的逻辑，接口背后运行着许多的服务，而具体使用哪个，由不同的负载均衡算法决定，这一过程放生在服务端。\n 日志\n可以在Configuration类下加入以下代码：\n@Bean\nLogger.Level feignLoggerLevel() &#123;\n    return Logger.Level.FULL;\n&#125;\n\n在yaml下加入远程调用包的日志配置：\nlogging:\n  level:\n    com.KarryCode.feign: debug\n\n再次请求即可观察到日志输出，可以看到是怎么请求的。\n 超时控制\n分别有连接超时和读取超时，通过其源码实现可以观察到，连接超时是10秒，而读取超时是60秒。我们仍然可以通过配置来修改，这个具体的时间。\n\n这里说一个题外话，我们可以使用多个配置文件来让其生效，比如我这里有一个application-feign.yml，我们还有一个主文件application.yml，如果我们想让application-feign.yml生效的话，我们可以在主配置文件加上这样的一句话：include: feign\nspring:\n  profiles:\n    active: dev\n    include: feign\n\n\n其配置方式如下所示：\nspring:\n  cloud:\n    openfeign:\n      client:\n        config:\n          default:\n            connectTimeout: 3000\n            readTimeout: 5000\n            logger-level: full\n\n 重试机制\n默认情况下，OpenFeign的重试策略是从不重试（是的没错），我们可以手动启动这个重试策略，其具体的有：我们可以设计间隔100毫秒，最大间隔1秒。最大尝试5次。类似计网里面的退避算法\n同样在相应的配置下面加入这样的代码：\n@Bean\nRetryer feignRetryer() &#123;\n    return new Retryer.Default();\n&#125;\n\n这是一个默认的重试器，其重试规则正如我上面所说那样：我们可以设计间隔100毫秒，最大间隔1秒。最大尝试5次。\n 拦截器\n我们可以创建一个请求拦截器：\n@Component\npublic class XTokenRequestInterceptor implements RequestInterceptor &#123;\n    @Override\n    public void apply(RequestTemplate requestTemplate) &#123;\n        System.out.println(&quot;拦截器启动&quot;);\n        requestTemplate.header(&quot;X-Token&quot;, UUID.randomUUID().toString());\n    &#125;\n&#125;\n\n然后再接收请求时，我们可以解析一下：\n@GetMapping(&quot;/product/&#123;productId&#125;&quot;)\npublic Product getProduct(@PathVariable Long productId, HttpServletRequest request) &#123;\n    System.out.println(&quot;token:&quot; + request.getHeader(&quot;X-Token&quot;));\n    log.info(&quot;查询商品信息: &#123;&#125;&quot;, productId);\n    return productService.getProductById(productId);\n&#125;\n\n可以看到Token的输出：token:ebf2c253-7e63-483a-92bc-fc9d1819a418。\n Fallback兜底返回\n兜底返回机制需要配合我们还没有学到的sentinel框架，首先引入依赖：\n&lt;!--    sentinel    --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n加入配置：\nfeign:\n  sentinel:\n    enabled: true\n\n在服务发起端加入兜底策略：\n@Component\npublic class ProductFeignClientFallback implements ProductFeignClient &#123;\n\n    @Override\n    public Product getProductById(Long productId) &#123;\n        Product product = new Product();\n        product.setId(productId);\n        product.setPrice(new BigDecimal(&quot;0&quot;));\n        product.setProductName(&quot;不到啊-&quot; + productId);\n        product.setNum(2);\n        return product;\n    &#125;\n&#125;\n\n关闭Product服务，只保留Order服务，可以看到返回了兜底数据：\n&#123;\n  &quot;id&quot;: 1,\n  &quot;totalAmount&quot;: 0,\n  &quot;userId&quot;: 777,\n  &quot;nickName&quot;: &quot;Karry.Liu&quot;,\n  &quot;address&quot;: &quot;北极&quot;,\n  &quot;product&quot;: [\n    &#123;\n      &quot;id&quot;: 888,\n      &quot;price&quot;: 0,\n      &quot;productName&quot;: &quot;不到啊-888&quot;,\n      &quot;num&quot;: 2\n    &#125;\n  ]\n&#125;\n\n Sentinel\n\n\n 基础场景\n@SentinelResource(value = &quot;createOrder&quot;)\n\n添加上此注解，流量即被监控。\n\n添加流控规则：\n\n每秒只放行一次请求，如果你请求多了就会被打回。\nFail to send:http://localhost:8081/create?productId=777&amp;userId=1\n\nBlocked by Sentinel (flow limiting)\n\n这是一个默认错误页面，我们能不能返回一个高度自定义的数据呢？\n但是可以的！这涉及到Sentinal的异常处理机制。\n 异常处理\n\n\nFlow Exception：流控异常\nParam Flow Exception：热点参数异常\nDegrade Exception：熔断降级异常\nAuthority Exception：权限控制类异常\nSystem Block Exception：系统阻塞异常\n\n上述说的问题，我们需要自定义一个异常。\n@Component\npublic class MyBlockException implements BlockExceptionHandler &#123;\n\n    private ObjectMapper objectMapper = new ObjectMapper();\n\n    @Override\n    public void handle(HttpServletRequest httpServletRequest,\n                       HttpServletResponse httpServletResponse,\n                       String resourceName, BlockException e) throws Exception &#123;\n        R error = R.error(500, resourceName + &quot;被限流了&quot;, e.getMessage());\n        httpServletResponse.setContentType(&quot;application/json;charset=utf-8&quot;);\n        PrintWriter writer = httpServletResponse.getWriter();\n        String json = objectMapper.writeValueAsString(error);\n        writer.write(json);\n\n    &#125;\n&#125;\n\n\n这个时候，如果你给createOrder方法添加流控规则的话，你会发现这个我们之前定义的json不生效了。\nFail to send:http://localhost:8081/create?productId=777&amp;userId=1\n\n&lt;html&gt;&lt;body&gt;&lt;h1&gt;Whitelabel Error Page&lt;/h1&gt;&lt;p&gt;This application has no explicit mapping for /error, so you are seeing this as a fallback.&lt;/p&gt;&lt;div id=&#x27;created&#x27;&gt;Wed Nov 05 16:15:00 CST 2025&lt;/div&gt;&lt;div&gt;There was an unexpected error (type=Internal Server Error, status=500).&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\n\n这是因为它的异常类型不同，才导致的这样的结果。\n 流控规则\n阈值类型：\n\nQPS：每秒请求数\n并发线程数\n\n我们优先推荐使用QPS。\n是否集群：\n\n单机均摊\n总体阈值\n\n流控模式：\n\n直接模式\n关联模式\n链路模式\n\n流控效果\n\n快速失败\nWarm Up\n排队等待\n\n\n\n 熔断降级\n\n Gateway\n 参考文献\n\n【尚硅谷SpringCloud速成】https://www.bilibili.com/video/BV1UJc2ezEFU\n\n","categories":["Java","微服务相关"],"tags":["java","微服务"]},{"title":"Dubbo快速入门","url":"/Java/DubboStart/","content":" RPC协议远程调用的几种实现\n Dubbo简介\nDubbo是阿里巴巴公司开源的一个高性能、轻量级的Java RPC框架。\n致力于高性能透明化的透明化的RPC原创服务调用方案，以及SOA服务治理方案。\n\n面向接口的远程方法调用（RPC）：像调用本地方法一样调用远程服务，对开发者透明，降低了分布式服务调用的复杂度。\n智能容错和负载均衡：提供了多种容错策略（如失败自动切换、失败安全等）和内置的负载均衡算法（如随机、轮询、最少活跃调用数等），保证了服务的高可用性。\n服务自动注册与发现：服务提供者向注册中心注册自己的服务，消费者从注册中心发现服务地址列表，并能感知服务的上下线，实现软负载。\n高度可扩展性：几乎所有组件（如协议、序列化、传输、注册中心等）都是可插拔的，允许用户根据自身需求进行定制和扩展。\n运行期流量调度：可以在后台进行路由规则、配置规则的调整，实现灰度发布、权重路由等精细化的流量控制。\n可视化服务治理：提供丰富的服务治理功能，如服务查询、服务测试、服务 Mock、依赖分析、健康度检查等，通常通过 Dubbo Admin 控制台进行操作。\n\nDubbo 的架构主要包含以下几个角色：\n\nProvider：服务提供者。发布服务到注册中心。\nConsumer：服务消费者。从注册中心订阅服务，并调用提供者。\nRegistry：注册中心。负责服务的注册与发现。\nMonitor：监控中心。统计服务的调用次数和调用时间等监控信息。\nContainer：服务运行容器。负责启动、加载、运行服务提供者。\n\n调用流程：\n\n启动阶段：服务容器启动、加载并运行 Provider。Provider 在启动时，会向 Registry 注册自己提供的服务。\n订阅阶段：Consumer 在启动时，向 Registry 订阅自己所需要服务的地址列表。Registry 会将提供者地址列表返回给 Consumer，同时 Consumer 会与 Registry 建立动态监听，以便感知服务变化。\n调用阶段：\n\nConsumer 根据负载均衡算法，从本地缓存的服务地址列表中，选择一个 Provider 进行调用。\n调用时，Consumer 的接口代理会将调用信息（接口名、方法名、参数等）进行序列化，通过网络传输给 Provider。\nProvider 收到请求后，反序列化数据，通过反射调用本地实现，并将结果返回给 Consumer。\n\n\n监控：Consumer 和 Provider 在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到 Monitor。\n\n","categories":["Java","微服务相关"],"tags":["java","微服务"]},{"title":"GitFlow分支管理策略","url":"/Java/gitflow/","content":" GitFlow\n 什么是GitFlow？\n\nGitflow 是一种替代的 Git 分支模型，涉及功能分支和多个主分支的使用。该书最初由文森特·德里森在nvie出版并推广开来。与基于trunk的开发相比，Gitflow拥有众多且寿命更长的分支和更大的提交。在该模型下，开发者创建一个功能分支，并延迟将其合并到主干分支，直到功能完成。这些寿命较长的特征分支需要更多的协作来合并，且偏离主干分支的风险更高。它们也可能引入冲突更新。\nGitflow 可用于有预定发布周期的项目，也可用于持续交付的 DevOps 最佳实践。除了功能分支工作流程所需的概念或命令外，此工作流程不会添加任何新概念或命令。相反，它为不同的分支分配了非常具体的角色，并定义了它们应该如何以及何时进行交互。除了 feature 分支外，它还使用单独分支来准备、维护和录制版本。当然，您还可以利用功能分支工作流程的所有优势：拉取请求、隔离实验和更高效的协作。\n\n 五大分支\n\n主分支：main /  master\n开发分支：develop\n功能分支：feature\n发布分支：release\n热修复分支hotfix\n\n 主分支：main / master\n永远保持可发布状态\n每一次提交都应该是已经测试、可上线的版本\n通常只允许：\n\nrelease 合并进来\nhotfix 合并进来\n\n一般会在这里 打 tag\nv1.0.0\nv1.1.0\n\n不在 main 上直接开发代码\n 开发分支：develop\n作用：日常开发的集成分支\n\n所有新功能最终都会合并到这里\n包含“下一个版本”的所有开发内容\n相对不如 main 稳定，但应该是可运行的\n\n特点：\n\nfeature → develop\ndevelop → release\n\n 功能分支：feature/*\n作用：开发具体功能\n\n从 develop 拉出来\n一个功能一个分支\n功能完成后合并回 develop\n\n 发布分支：release/*\n作用：版本发布前的“冻结分支”\n\n从 develop 拉出来\n只允许：\n\nbug 修复\n文档/版本号修改\n\n\n不再添加新功能\n\n 热修复分支：hotfix/*\n作用：线上紧急 Bug 修复\n\n从 main 拉出来\n修完立刻发布\n同时要合并回 develop\n\n 基本图\n\n 前置流程\n\n\n\n 开发新需求A\n\n提交\n\n观察到dev并未受影响。\n\n\n 开发新需求B与C\n省略，同样提交并合并到dev之中\n 合并的结果\n\n为了整洁，新特性开发完毕之后需可以删除分支。\n\n 发布分支\n\n 生产环境出bug了怎么办\n使用hotfix分支\n\n结束之后合并到main与dev\n GitFlow的优缺点\n优点：分支职责清晰，结构规范，支持并行开发，发布流程可控、可追溯，对线上问题响应快，适合传统软件工程流程\n缺点：分支较多，流程偏重，不太适合高频发布，合并冲突概率较高\n 版本回退、撤销、重置\n查看当前状态：\ngit status\n\n 未add的代码行发生改动怎么回退？\n\ngit checkout -- F2\n\n\n\n 已add未commit的代码行发生改动怎么回退？\n\ngit reset HEAD XXXX\n\n\n\n\n 已add已commit未push的代码行发生改动怎么回退？\n两个异常改动\n\n两个^就是回退两步\n\n\n 已add已commit已push的代码行发生改动怎么回退？\n\ngit revert --no-edit cxxxxx\n\n其中cxxxx是提交流水号。\n更方便地使用还原提交。\n\n\n\nrevert后是否可以再revert？答案是可以的。\n git stash\ngit stash是临时保存当前未提交的修改，把工作区恢复到“干净状态”\n写了一半代码，还不想 commit，又必须马上切分支 / 拉代码 / 修 Bug， git stash 就是把现场“打包放抽屉里”\n\n\n\n git cherry-pick\n把“某一个（或几个）提交”从别的分支，复制一份，应用到当前分支\n关键词：只要这个提交，不要整个分支\n或者说，提交了100行，有60还可以，40行我有点后悔了，怎么只撤回这40行？\n\n先revert，push到远程\n\n基于现有分支，创建新分支\n\n撤销修改\n\n冻结\n\n\n\n 变基rebase和合并merge\n合并（git merge）是什么，就是把两个分支的历史合并到一起。\n变基（git rebase）是什么，把一个分支的提交 “挪到”另一个分支的末尾\nmerge通常是弯弯曲曲的路线，而rebase把对方的线路合并到你这里，但是这个合并更像是重新播放了一遍，改写了历史，使之成为了更加线性的流程。\nrebase推荐在本地上使用，更加整洁，团队协作推荐使用merge。\n 拉取pull和提取fetch\n提取是看看哪些变了（不修代码），拉取真正到本地（修代码）\n最佳实践是，先fetch看看那些变了，再pull去更新我们的代码。\n","categories":["Java","git相关"],"tags":["java","gitflow"]},{"title":"MongoDB的安装与初步使用（Windows平台）","url":"/MongoDB/MongoDB01/","content":" 初见 MongoDB\n 什么是 MongoDB？\nMongoDB 是一个开源的、面向文档的 NoSQL 数据库管理系统。它与传统的关系型数据库（如 MySQL、PostgreSQL）不同，因为它不使用表格来存储数据，而是使用一种称为&quot;文档&quot;的数据结构来组织和存储数据。每个文档是一个包含键值对的数据结构，类似于 JSON 格式，这使得 MongoDB 非常适合存储具有不同结构的数据。\n\n MongoDB 的安装与服务启动\n 下载 MongoDB 安装包\n访问==https://www.mongodb.com/download-center#community==\n或者点击下方连接\n\n          \n          MogoDB\n          官方下载地址\n          \n下载载最新版本的 MongoDB 数据库。\n MongoDB 安装\n\n双击刚刚下载的安装文件(mongodb-XXXX-XXXX-signed.msi)启动安装程序。\n单击【Next】按钮，进入“End-User License Agreement”界面\n勾选“I accept the terms in the License Agreement”选项，单击【Next】按钮进入“Choose Setup Type”界面，该界面中可选择安装类型\n\n[Complete。此类型将安装所有程序功能，需占用较多的磁盘空间，建议大多数用户使用。]{.label .primary}\n[Custom。此类型允许用户自行选择要安装的程序功能及安装位置，建议高级用户使用。]{.label .primary}\n\nMongoDB Compass 是 MongoDB 数据库的 GUI 管理系统，默认会选择安装，但是安装速度非常慢。\n\n值得一提的是，MongoDB 默认会将创建的数据库文件存储在 db 目录下，但是这个目录不会被主动创建，用户需要在 MongoDB 安装完成后手动创建 db 目录。在“C:\\Program Files\\MongoDB\\Server\\4.0\\data\\”目录下创建一个文件夹 db\n 配置\n同样的我们也可以将 bin 目录配置到环境变量的 Path 中\n 启动 MongoDB 服务（启动与停止）\n进入如图所示的目录输入net start MongoDB启动服务，相应的输入net stop MongoDB则为停止服务\n\n MongoDB 基本命令\n在 cmd 中输入 Mongo 进入 Mongo 的交互界面\n创建数据库:\nuse mydb\n\n展示数据库:\nshow dbs\n\n删除数据库:\ndb.dropDatabase()\n\n创建集合：\ndb.createCollection(&quot;myCollection&quot;)\n\n插入数据：\ndb.myCollection.insert(&#123;&quot;_id&quot;:1,&quot;name&quot;:&quot;诗岸梦行舟&quot;&#125;)\n\n删除：\ndb.myCollection.remove(&#123;&quot;_id&quot;:1&#125;)\n\n更新：\ndb.myCollection.update(&#123;&quot;_id&quot;:1&#125;,&#123;$set&#123;&quot;name&quot;:&quot;Karry.Liu&quot;&#125;&#125;)\n\n查询所有：\ndb.myCollection.find()\n\n指定查询：\ndb.myCollection.find(&#123;&quot;_id&quot;:1&#125;)\n\n","categories":["数据库","MongoDB"],"tags":["MongoDB"]},{"title":"初见Linux","url":"/Linux/Linux01/","content":" 初见Linux\n首先安装什么的就不讲了，这里先讲一下基础命令\n 简单命令\n who命令\n[karry@localhost ~]$ who am i\n\n\n[karry@localhost ~]$ who am i\nkarry    pts/0        2023-09-01 10:26 (laptop-karry1107)\n\n echo命令\n这个命令是将内容输出到屏幕上\n[karry@localhost ~]$ echo Hello Karry.Liu\n\n\n[karry@localhost ~]$ echo Hello Karry.Liu\nHello Karry.Liu\n\n date命令\n[karry@localhost ~]$ date\n\n\n[karry@localhost ~]$ date\n2023年 09月 01日 星期五 10:32:23 PDT\n\n cal命令\n[karry@localhost ~]$ cal 9 2023\n\n\n[karry@localhost ~]$ cal 9 2023\n九月 2023\n日 一 二 三 四 五 六\n1  2\n3  4  5  6  7  8  9\n10 11 12 13 14 15 16\n17 18 19 20 21 22 23\n24 25 26 27 28 29 30\n\n 基础命令\n 基础文件操作命令\n 展示文件夹中的内容\n[karry@localhost ~]$ ls\n\n\n[karry@localhost ~]$ ls\nDesktop  Documents  Downloads  Music  Pictures  Public  Templates  Videos\n\n 进入/退出文件夹\n[karry@localhost ~]$ cd Desktop\n\n\n[karry@localhost ~]$ cd Desktop\n[karry@localhost Desktop]$\n\n 创建文件夹\n[karry@localhost Desktop]$ mkdir LinuxHello\n\n\n[karry@localhost Desktop]$ ls\nfirefox.desktop  myFile  test\n[karry@localhost Desktop]$ mkdir LinuxHello\n[karry@localhost Desktop]$ ls\nfirefox.desktop  LiunxHello  myFile  test\n\n 创建一个文件\n[karry@localhost LinuxHello]$ touch fistText.txt\n\n\n[karry@localhost Desktop]$ ls\nfirefox.desktop  LinuxHello  myFile  test\n[karry@localhost Desktop]$ cd LinuxHello\n[karry@localhost LinuxHello]$ touch fistText.txt\n[karry@localhost LinuxHello]$ ls\nfistText.txt\n\n 编辑文件\n[karry@localhost LinuxHello]$ touch fistText.txt\n\n\n~\n~\n~\n“fistText.txt” 0L, 0C\n\n按   [i]{.kbd}        进入插入模式\n\n~\n~\n~\n– 插入 –\n\n现在可以编辑文件了！\n\nhi hi，这是我第一次学习Linux！！！\n我的名字是诗岸梦行舟\n或者是Karry.Liu\n让我们共同努力吧！！\n~\n~                                                                                                                                                                                                                                                                                                                                                                       – 插入 –\n\n按   [Esc]{.kbd}        退出插入模式\n按   [:]{.kbd}+  [w]{.kbd}       保存刚才所编辑的文件\n\nhi hi，这是我第一次学习Linux！！！\n我的名字是诗岸梦行舟\n或者是Karry.Liu\n让我们共同努力吧！！\n~\n“fistText.txt” 4L, 128C 已写入\n\n最后按 [:]{.kbd}+ [q]{.kbd}   退出vi编辑器\n\n~\n~\n:q\n\n 读文件内容\n[karry@localhost LinuxHello]$ cat fistText.txt\n\n\n[karry@localhost LinuxHello]$ cat fistText.txt\nhi hi，这是我第一次学习Linux！！！\n我的名字是诗岸梦行舟\n或者是Karry.Liu\n让我们共同努力吧！！\n[karry@localhost LinuxHello]$\n\n 使用g++编译程序\n\n\n如果你还没有安装g++编译器，请先安装\nsudo yum install gcc-c++ make\n\n安装需要一定时间，请耐心等待！\n\n\n进入指定目录，创建cpp文件。\n[karry@localhost C++]$ touch firstApp.cpp\n\n\n\n使用vi指令编辑文件\n[karry@localhost C++]$ vi firstApp.cpp\n\n\n\n按 [i]{.kbd} 进入插入模式，编辑程序\n#include&lt;iostream&gt;\nusing namespace std;\nint main()&#123;\n        int a=1;\n        int b=2;\n        int c=a+b;\n        cout&lt;&lt;&quot;计算结果为：&quot;&lt;&lt;a&lt;&lt;&quot;+&quot;&lt;&lt;b&lt;&lt;&quot;=&quot;&lt;&lt;c&lt;&lt;endl;\n        return 0;\n&#125;\n\n\n\n按   [Esc]{.kbd}        退出插入模式，按   [:]{.kbd}+  [w]{.kbd}       保存刚才所编辑的文件，最后按 [:]{.kbd}+ [q]{.kbd}   退出vi编辑器\n\n\n执行命令编译程序\n[karry@localhost C++]$ g++ -o firstAppCompile firstApp.cpp\n\n\n\n运行输出结果\n[karry@localhost C++]$ ./firstAppCompile\n\n\n[karry@localhost C++]$ ./firstAppCompile\n计算结果为：1+2=3\n[karry@localhost C++]$\n\n\n\n","categories":["Java","Linux相关"],"tags":["Linux"]},{"title":"Java高级特性1","url":"/Java/javaAd01/","content":" List 与 Set\n本篇内容意在总结课上所学的知识并加以巩固，如有错误请立即联系我，谢谢！\n\n\n关于 Collection\n掌握 List 部分内容\n掌握 Set 部分内容\n迭代器\nWeChat：a735690757\ngmail：735690757carry@gmail.com\n\n\n 1，概述\n在 Java 中，List 和 Set 接口都是继承自 Collection 接口。Collection 接口定义了一组通用的集合操作方法，而 List 和 Set 则分别扩展了 Collection 接口，以提供特定的集合行为。\n 2，Collection\n在 Java 中，Collection 接口是一个集合框架的根接口，它定义了一组通用的集合操作方法，可以用于处理各种类型的集合。Collection 接口提供了一些常用的方法，如添加元素、删除元素、检查元素是否存在、迭代集合中的元素等。它是 Java 集合框架中最基本的接口，所有其他的集合接口都是扩展自 Collection 接口。\nCollection 接口定义了如下方法（常用、不完全）：\n\n\nboolean add(E e): 将元素添加到集合中，并返回是否添加成功。\nboolean addAll(Collection&lt;? extends E&gt; c): 将指定集合中的所有元素添加到该集合中，并返回是否添加成功。\nboolean remove(Object o): 从集合中删除指定元素，并返回是否删除成功。\nvoid clear(): 清空集合中的所有元素。\nboolean isEmpty(): 检查集合是否为空。\nint size(): 返回集合中元素的数量。\n\n\n 3，List\nJava 中的 List 是一种常见的集合类，用于存储一组元素。List 是一个接口，它有多个实现类，如 ArrayList、LinkedList 等。\n！重要！\n\n\n允许存储重复的元素。\n\n\n元素按照插入顺序有序存储。\n\n\n可以通过索引访问 List 中的元素，索引从 0 开始。\n\n\nList 可以动态调整大小，即添加或删除元素。\n有序，可重复\n\n\n 3.1 ArrayList\nList 的常用方法（可能不完全）：（类似数组）\n\nadd(element)：向 List 末尾添加元素。\nadd(index, element)：在指定索引位置插入元素。\nremove(index)：删除指定索引位置的元素。\nget(index)：返回指定索引位置的元素。\nset(index, element)：替换指定索引位置的元素。\nsize()：返回 List 的大小。\ncontains(element)：判断 List 是否包含指定元素。\nindexOf(element)：返回指定元素在 List 中第一次出现的索引位置。\nclear()：清空 List 中的所有元素。\n\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class ListExample &#123;\n    public static void main(String[] args) &#123;\n        List&lt;String&gt; list = new ArrayList&lt;&gt;();\n        list.add(&quot;apple&quot;);\n        list.add(&quot;banana&quot;);\n        list.add(&quot;orange&quot;);\n\n        System.out.println(list); // 输出：[apple, banana, orange]\n\n        list.remove(1);\n        System.out.println(list); // 输出：[apple, orange]\n\n        String fruit = list.get(1);\n        System.out.println(fruit); // 输出：orange\n\n        list.set(1, &quot;grape&quot;);\n        System.out.println(list); // 输出：[apple, grape]\n    &#125;\n&#125;\n\n\n以上代码由[ChatGPT Mar 14 Version]生成。\npackage edu.beihua.KarryCode.listEX001.test;\n\nimport edu.beihua.KarryCode.listEX001.entity.news;\n\nimport java.util.ArrayList;\n\npublic class test_for_ArrayList &#123;\n    public static void main(String[] args) &#123;\n        /*\n        * add()     ----增加对象\n        * get()     ----获取对象\n        * 非泛型返回Object\n        * isEmpty() ----判空\n        * clear()   ----清除\n        * iterator()----迭代器\n        * toArray() ----转为数组\n        * */\n        news newslkr = new news(1,&quot;timu1&quot;,&quot;lkr&quot;);\n        news newsffl = new news(2,&quot;timu2&quot;,&quot;ffl&quot;);\n        news newslqr = new news(3,&quot;timu3&quot;,&quot;lqr&quot;);\n\n        news imnews = new news(4,&quot;hexin&quot;,&quot;!!!!!&quot;);\n\n        ArrayList&lt;news&gt; arrayList = new ArrayList&lt;news&gt;();\n\n        arrayList.add(newslkr);                                     //普通追加\n        arrayList.add(newsffl);\n        arrayList.add(newslqr);\n         arrayList.add(0,imnews);                              //标记追加\n        System.out.println(&quot;______________标题数目输出______________&quot;);\n        System.out.println(&quot;新闻有&quot;+arrayList.size()+&quot;条&quot;);\n        System.out.println(&quot;______________标题输出______________&quot;);\n        for (int i=0;i&lt;arrayList.size();i++)&#123;\n            System.out.println(arrayList.get(i).getTitle());          //不适用非泛型，可以使用（Object）强制转换，（news）进行匹配\n        &#125;\n        System.out.println(&quot;______________标题输出____________&quot;);\n        for(Object obj:arrayList)&#123;                                  //**增强形态的For**\n            news newst = (news) obj;\n            System.out.println(newst.getTitle());\n        &#125;\n        System.out.println(&quot;______________标题输出____________&quot;);\n        for(news news:arrayList)&#123;                                  //**增强形态的For**||最终版：面向对象\n            System.out.println(news.getTitle());\n        &#125;\n        System.out.println(&quot;________________标题判断__________&quot;);\n        System.out.println(arrayList.contains(newslqr));\n\n        arrayList.remove(3);\n        System.out.println(&quot;______________标题输出____________&quot;);\n        for(news news:arrayList)&#123;                                  //**增强形态的For**||最终版：面向对象\n            System.out.println(news.getTitle());\n        &#125;\n        System.out.println(&quot;______________标题判断__________&quot;);\n        System.out.println(arrayList.contains(newslqr));\n        System.out.println(&quot;________________清空_______________&quot;);\n        arrayList.clear();\n        System.out.println(arrayList.isEmpty());\n\n\n    &#125;\n&#125;\n\n\n以上代码为上课所写（丑丑的 www）。\n 3.2 LinkedList\nLinkedList 具有以下特点：\n\n随机访问元素效率较低，因为需要遍历链表。\n在链表的开头或结尾插入、删除元素的效率较高。\n占用的内存空间相对较小。\n\n有趣的方法：\n\n\ngetFirst()：返回链表的第一个元素。\ngetLast()：返回链表的最后一个元素。\nremoveFirst()：删除链表的第一个元素。\nremoveLast()：删除链表的最后一个元素。\n\n\nimport java.util.LinkedList;\nimport java.util.List;\n\npublic class LinkedListExample &#123;\n    public static void main(String[] args) &#123;\n        List&lt;String&gt; list = new LinkedList&lt;&gt;();\n        list.add(&quot;apple&quot;);\n        list.add(&quot;banana&quot;);\n        list.add(&quot;orange&quot;);\n\n        System.out.println(list); // 输出：[apple, banana, orange]\n\n        list.remove(1);\n        System.out.println(list); // 输出：[apple, orange]\n\n        String fruit = list.get(1);\n        System.out.println(fruit); // 输出：orange\n\n        list.set(1, &quot;grape&quot;);\n        System.out.println(list); // 输出：[apple, grape]\n    &#125;\n&#125;\n\n\n以上代码由[ChatGPT Mar 14 Version]生成。\npackage edu.beihua.KarryCode.listEX001.test;\n\nimport edu.beihua.KarryCode.listEX001.entity.news;\n\nimport java.util.Iterator;\nimport java.util.LinkedList;\n\npublic class test_for_LinkList &#123;\n    public static void main(String[] args) &#123;\n        news newslkr = new news(1,&quot;timu1&quot;,&quot;lkr&quot;);\n        news newsffl = new news(2,&quot;timu2&quot;,&quot;ffl&quot;);\n        news newslqr = new news(3,&quot;timu3&quot;,&quot;lqr&quot;);\n        news imnews = new news(4,&quot;hexin&quot;,&quot;!!!!!&quot;);\n\n        LinkedList list = new LinkedList();         //父父new子这种只能使用父子公用的方法\n\n        list.add(newslkr);                                     //普通追加\n        list.add(newsffl);\n        list.add(newslqr);\n        list.remove(0);\n\n        list.add(0,imnews);\n        System.out.println(&quot;______________内容输出____________&quot;);\n        for(Object obj:list)&#123;\n            news s = (news) obj;\n            System.out.println(s.getTitle());\n        &#125;\n        news tyu = (news)list.getFirst();\n        System.out.println(tyu.getNo());\n        news imnews666 = new news(10,&quot;hexin66666&quot;,&quot;!!66666!!!&quot;);\n        list.addFirst(imnews666);\n        System.out.println(&quot;______________内容输出____________&quot;);\n        for(Object obj:list)&#123;\n            news s = (news) obj;\n            System.out.println(s.getTitle());\n        &#125;\n\n        news re = (news)list.removeLast();\n        System.out.println(re.getData());\n        System.out.println(list.size());\n\n        Iterator iterator = list.iterator();\n        System.out.println(&quot;_________________________&quot;);\n        while (iterator.hasNext())&#123;\n            news news = (news) iterator.next();\n            System.out.println(news.getTitle());\n        &#125;\n    &#125;\n&#125;\n\n\n 4，Set\n在 Java 中，Set 是一种集合（Collection）类型，它是一组不允许包含重复元素的对象。Set 是通过哈希表（hash table）实现的，因此它没有顺序，也不能通过索引访问其中的元素。Set 接口继承自 Collection 接口，并添加了一些独有的方法。\n无序，唯一\nJava 中常用的 Set 类有以下几种：\n\nHashSet：基于哈希表实现，具有良好的插入和查询性能，但不保证元素的顺序。\nTreeSet：基于红黑树实现，元素按照自然顺序排序或指定的 Comparator 顺序排序。\nLinkedHashSet：基于哈希表和链表实现，保证元素按照插入顺序排列。\n\nSet 的常用方法包括：\n\nadd(element)：向集合中添加元素。\nremove(element)：从集合中删除指定元素。\ncontains(element)：判断集合中是否包含指定元素。\nsize()：返回集合中元素的数量。\nisEmpty()：判断集合是否为空。\nclear()：清空集合中的所有元素。\n\n例如，以下是使用 HashSet 和 TreeSet 实现的 Set 的示例：\nimport java.util.HashSet;\nimport java.util.Set;\nimport java.util.TreeSet;\n\npublic class SetExample &#123;\n    public static void main(String[] args) &#123;\n        Set&lt;String&gt; hashSet = new HashSet&lt;&gt;();\n        hashSet.add(&quot;apple&quot;);\n        hashSet.add(&quot;banana&quot;);\n        hashSet.add(&quot;orange&quot;);\n\n        System.out.println(hashSet); // 输出：[orange, banana, apple]\n\n        hashSet.remove(&quot;banana&quot;);\n        System.out.println(hashSet); // 输出：[orange, apple]\n\n        System.out.println(hashSet.contains(&quot;orange&quot;)); // 输出：true\n\n        Set&lt;String&gt; treeSet = new TreeSet&lt;&gt;();\n        treeSet.add(&quot;apple&quot;);\n        treeSet.add(&quot;banana&quot;);\n        treeSet.add(&quot;orange&quot;);\n\n        System.out.println(treeSet); // 输出：[apple, banana, orange]\n\n        treeSet.remove(&quot;banana&quot;);\n        System.out.println(treeSet); // 输出：[apple, orange]\n\n        System.out.println(treeSet.contains(&quot;orange&quot;)); // 输出：true\n    &#125;\n&#125;\n\n\n以上代码由[ChatGPT Mar 14 Version]生成。\n 4.1 HashSet\n在 Java 中，HashSet 是一种基于哈希表实现的 Set 集合，它不保证元素的顺序，但是可以快速地插入和查找元素。HashSet 使用哈希函数将元素映射到哈希表中的桶（bucket）中，桶是一个链表或树结构，用于解决哈希冲突（即不同元素映射到同一个桶中的情况）。\nHashSet 的特点包括：\n\n不保证元素的顺序。\n不允许集合中存在重复元素。\n允许 null 元素。\n\npackage edu.beihua.KarryCode.listEX001.test;\n\nimport edu.beihua.KarryCode.listEX001.entity.news;\n\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.Objects;\nimport java.util.Set;\n\npublic class test_foe_HashSet &#123;\n    public static void main(String[] args) &#123;\n        /*\n        * 无论什么List还是Set均为Cll...的继承类，其均具有父类的方法\n        * Set中存放的是对象的引用，相同的引用是互斥的只能添加一次\n        * equals()方法可以被重写\n        * Set是无序的，不存在Function（index，XXX）类似如此的有序号的的方法，且只能使用增强型的for进行循环输出*/\n        Set set = new HashSet();\n        news newslkr = new news(1,&quot;timu1&quot;,&quot;lkr&quot;);\n        news newsffl = new news(2,&quot;timu2&quot;,&quot;ffl&quot;);\n        news newslqr = new news(3,&quot;timu3&quot;,&quot;lqr&quot;);\n\n        set.add(newslkr);                                     //普通追加\n        set.add(newsffl);\n        set.add(newslqr);\n\n        for(Object obj:set)&#123;\n            news test = (news) obj;\n            System.out.println(test.getData());\n        &#125;\n        Iterator iterator = set.iterator();\n\n        while (iterator.hasNext())&#123;\n            news test = (news) iterator.next();\n            System.out.println(test.getData());\n        &#125;\n    &#125;\n&#125;\n\n\n 5，Iterator 迭代器\n在 Java 中，Iterator 是一个用于遍历集合（Collection）元素的接口，它提供了一种统一的访问集合中元素的方式。通过 Iterator，我们可以遍历集合中的每个元素，而不需要知道集合的具体实现方式。\nIterator 接口包含了以下方法：\n\nhasNext()：判断集合中是否还有下一个元素。\nnext()：返回集合中的下一个元素。\nremove()：从集合中移除通过 next()方法返回的最后一个元素。\n\nIterator 的工作原理是，首先通过集合的 iterator()方法获得一个 Iterator 对象，然后使用 hasNext()和 next()方法遍历集合中的元素，最后使用 remove()方法从集合中移除元素。\n在 4.1HashSet 中用到了本迭代器，以下为关键语句：\nIterator iterator = set.iterator();\n\n        while (iterator.hasNext())&#123;\n            news test = (news) iterator.next();\n            System.out.println(test.getData());\n        &#125;\n\n","categories":["Java"],"tags":["java"]},{"title":"将单体项目拆解为微服务项目","url":"/Java/springcloud021/","content":" 将单体项目拆解为微服务项目\n Demo1_再会MyBatisPlus——将MyBatis工程改造为MyBatisPlus工程\n 引入MyBatisPlus依赖\n&lt;dependency&gt;\n    &lt;groupId&gt;com.baomidou&lt;/groupId&gt;\n    &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt;\n    &lt;version&gt;3.5.5&lt;/version&gt;\n&lt;/dependency&gt;\n\n UserMapper继承BaseMapper\npublic interface UserMapper extends BaseMapper&lt;User&gt; &#123;\n    void saveUser(User user);\n    void deleteUser(Long id);\n    void updateUser(User user);\n    User queryUserById(@Param(&quot;id&quot;) Long id);\n    List&lt;User&gt; queryUserByIds(@Param(&quot;ids&quot;) List&lt;Long&gt; ids);\n&#125;\n\n 去除UserMapper.java与UserMapper.xml中的基础方法\npublic interface UserMapper extends BaseMapper&lt;User&gt; &#123;\n\n&#125;\n\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;\n&lt;mapper namespace=&quot;com.itheima.mp.mapper.UserMapper&quot;&gt;\n&lt;/mapper&gt;\n\n 验证\n\n 为什么MyBatisPlus这么快？\nMyBatisPlus通过扫描实体类，基于反射获取到实体类信息座位数据库表基本信息\n一下是一些提示\n\n类名驼峰转下划线作为表名\n名为id的字段作为主键\n变量名驼峰转下划线作为表的字段名\n\n 常用注解\n@TableName：用来指定表名\n@TableId：用来指定表中的主键字段信息\n@TableField：用来指定表中的普通字段信息\n 示例1——表名与类名有明显差异使用@TableName\n\n@TableName(&quot;tb_user&quot;)\npublic class User &#123;\n    private Long id;\n    private String username;\n    private String isMarried;\n    private String order;\n&#125;\n\n 示例2——主键字段名有明显差异使用@TableId\n@TableName(&quot;tb_user&quot;)\npublic class User &#123;\n    @TableId(value = &quot;id&quot;, type = IdType.AUTO)\n    private Long uuid;\n    private String username;\n    private String isMarried;\n    private String order;\n&#125;\n\n type策略\nAUTO：数据库自增长\nINPUT：通过set方法自行输入\nASSIGN_ID：分配 ID，接口IdentifierGenerator的方法nextId来生成id，默认实现类为DefaultIdentifierGenerator雪花算法（20位）\n 示例3——字段名不符合其约定@TableField\n@TableName(&quot;tb_user&quot;)\npublic class User &#123;\n    @TableId(value = &quot;id&quot;, type = IdType.AUTO)\n    private Long uuid;\n    @TableField(&quot;username&quot;)\n    private String username;\n    @TableField(&quot;is_married&quot;)\n    private String isMarried;\n    @TableField(&quot;`order`&quot;)\n    private String order;\n     @TableField(exist = false)\n    private String address;\n&#125;\n\n 使用@TableField的常见场景\n\n成员变量名与数据库字段名不一致\n成员变量名以is开头，且是布尔值\n成员变量名与数据库关键字冲突\n成员变量不是数据库字段\n\n 常见配置\nmybatis-plus: \n\ttype-aliases-package: com.itheima.mp.domain.po # 别名扫描包\n\tmapper-locations: &quot;classpath*:/mapper/**/*.xml&quot; # Mapper.xml文件地址，默认值\n\tconfiguration: \n\t\tmap-underscore-to-camel-case: true # 是否开启下划线和驼峰的映射\n\t\tcache-enabled: false # 是否开启二级缓存\n  global-config:\n  \tdb-config:\n  \t\tid-type: assign_id # id为雪花算法生成\n  \t\tupdate-strategy: not_null # 更新策略：只更新非空字段\n\n具体可参考官方文档：MyBatisPlus官方文档\n 条件构造器Wrapper\n\n QueryWrapper与LambdaQueryWrapper\n@Test\npublic void testWrapper() &#123;\n    QueryWrapper&lt;User&gt; userQueryWrapper = new QueryWrapper&lt;User&gt;()\n            .select(&quot;id&quot;, &quot;username&quot;, &quot;info&quot;, &quot;balance&quot;)\n            .like(&quot;username&quot;, &quot;o&quot;)\n            .ge(&quot;balance&quot;, 1000);\n    List&lt;User&gt; users = userMapper.selectList(userQueryWrapper);\n    users.forEach(System.out::println);\n\n    System.out.println(userMapper.selectList(new LambdaQueryWrapper&lt;User&gt;()\n            .ge(User::getBalance, 1000)\n            .like(User::getUsername, &quot;o&quot;)\n            .select(User::getId, User::getUsername, User::getInfo, User::getBalance)));\n&#125;\n\n UpdateWrapper与LambdaUpdateWrapper\n@Test\npublic void testWrapper() &#123;\n    UpdateWrapper&lt;User&gt; userUpdateWrapper = new UpdateWrapper&lt;User&gt;()\n            .eq(&quot;id&quot;, 4L)\n            .setSql(&quot;balance = balance - 10&quot;);\n    userMapper.update(null, userUpdateWrapper);\n\n    userMapper.update(null, new LambdaUpdateWrapper&lt;User&gt;()\n            .eq(User::getId, 4L)\n            .setSql(&quot;balance = balance - 10&quot;));\n\n&#125;\n\n推荐使用 LQW / LUW 因为其类型安全、较高可维护性、对重构友好。用 Lambda 引用替代字符串字段名，在编译期发现错误，提高代码在重构和长期维护中的安全性。\n 自定义SQL\n@Test\npublic void testZZSQL()&#123;\n    List&lt;Long&gt; ids = List.of(2L, 3L, 4L);\n    int amount = 2;\n    LambdaUpdateWrapper&lt;User&gt; userLambdaUpdateWrapper = new LambdaUpdateWrapper&lt;User&gt;()\n            .in(User::getId, ids);\n    userMapper.updateByIds(userLambdaUpdateWrapper, amount);\n&#125;\n\npublic interface UserMapper extends BaseMapper&lt;User&gt; &#123;\n    /**\n     * 根据id批量更新余额\n     *\n     * @param userLambdaUpdateWrapper 条件\n     * @param amount 金额\n     */\n//    @Update(&quot;update tb_user set balance = balance - #&#123;amount&#125; $&#123;ew.customSqlSegment&#125;&quot;)\n    void updateByIds(@Param(Constants.WRAPPER) LambdaUpdateWrapper&lt;User&gt; userLambdaUpdateWrapper, @Param(&quot;amount&quot;) int amount);\n&#125;\n\n&lt;mapper namespace=&quot;com.itheima.mp.mapper.UserMapper&quot;&gt;\n    &lt;update id=&quot;updateByIds&quot;&gt;\n        update tb_user set balance = balance - #&#123;amount&#125; $&#123;ew.customSqlSegment&#125;\n    &lt;/update&gt;\n&lt;/mapper&gt;\n\nConstants.WRAPPER为MP常量就是ew在SQL中也可以看到有关ew的操作：$&#123;ew.customSqlSegment&#125;\n Service接口\n\npublic interface IUserService extends IService&lt;User&gt; &#123;\n&#125;\n\npublic class UserServiceImpl extends ServiceImpl&lt;UserMapper, User&gt; implements IUserService  &#123;\n&#125;\n\n这样就可以使用了\n@SpringBootTest\nclass UserServiceImplTest &#123;\n    @Autowired\n    private IUserService userService;\n\n    @Test\n    public void testInsert() throws JsonProcessingException &#123;\n        HashMap&lt;Object, Object&gt; objectObjectHashMap = new HashMap&lt;&gt;();\n        objectObjectHashMap.put(&quot;age&quot;, 22);\n        objectObjectHashMap.put(&quot;intro&quot;, &quot;程序员&quot;);\n        objectObjectHashMap.put(&quot;gender&quot;, &quot;male&quot;);\n\n        ObjectMapper objectMapper = new ObjectMapper();\n        String json = objectMapper.writeValueAsString(objectObjectHashMap);\n        User lkr = User.builder().username(&quot;lkr&quot;)\n                .password(&quot;123123&quot;)\n                .phone(&quot;666666&quot;)\n                .info(json).status(1).balance(45454).build();\n        userService.save(lkr);\n    &#125;\n\n    @Test\n    public void testQuery()&#123;\n        System.out.println(userService.listByIds(List.of(1L, 2L, 3L, 4L)));\n    &#125;\n&#125;\n\n Controller技巧\n通常我们在Controller注入Service我们会这样写：\n@Api(tags = &quot;用户管理接口&quot;)\n@RestController\n@RequestMapping(&quot;/users&quot;)\npublic class UserController &#123;\n    @Autowired\n    private IUserService userService;\n\n&#125;\n\n尽管可以，但是Spring官方反对的字段注入，因为Service层是不会变的，使用final更为合适，但字段注入 无法使用 final，并且容易造成循环依赖\n在IDEA的自动修复中推荐道：\n@Api(tags = &quot;用户管理接口&quot;)\n@RestController\n@RequestMapping(&quot;/users&quot;)\npublic class UserController &#123;\n\n    private final IUserService userService;\n\n    public UserController(IUserService userService) &#123;\n        this.userService = userService;\n    &#125;\n&#125;\n\n尽管快捷键两部就能生成，但是不够优雅，如果你的项目中可以使用Lombok，我推荐这样写：\n@Api(tags = &quot;用户管理接口&quot;)\n@RestController\n@RequestMapping(&quot;/users&quot;)\n@RequiredArgsConstructor\npublic class UserController &#123;\n\n    private final IUserService userService;\n\n&#125;\n\n这样不仅保持了@Autowired的简介模式，还避免了字段注入。@RequiredArgsConstructor是自动生成“必要参数”的构造方法，“必要参数”指的是：\n\nfinal 修饰的字段\n被 @NonNull 修饰的字段\n\n使用@RequiredArgsConstructor注解，会在编译期生成必要参数的构造方法。\n Bean拷贝技巧\n前端或者客户端通常会给我们传一个在我们后端称之为DTO的数据，这是一个表单信息，包含了相应实体的核心信息，但是核心始终是核心，并不是完整的源实体，使用MP的IService方法时，如果你给我传了一个DTO，这会报错，Bean拷贝就是为了解决这个问题而诞生的。\n Hutool-BeanUtil一句话解决问题\nBeanUtil.copyProperties(userFormDTO, User.class)\n\n他会读取userFormDTO的信息，然后根据你指定的的类，这里就是User.class来生成源实体。\n@Api(tags = &quot;用户管理接口&quot;)\n@RestController\n@RequestMapping(&quot;/users&quot;)\n@RequiredArgsConstructor\npublic class UserController &#123;\n\n    private final IUserService userService;\n\n\n    @ApiOperation(&quot;新增用户信息&quot;)\n    @PostMapping\n    public void saveUser(@RequestBody UserFormDTO userFormDTO) &#123;\n        userService.save(BeanUtil.copyProperties(userFormDTO, User.class));\n    &#125;\n&#125;\n\n Hutool-BeanUtil批量拷贝\nBeanUtil.copyToList(userService.listByIds(ids),UserVO.class);\n\n@ApiOperation(&quot;根据id批量查询&quot;)\n@GetMapping\npublic List&lt;UserVO&gt; listUser(@RequestParam @ApiParam(&quot;用户id列表&quot;) List&lt;Long&gt; ids) &#123;\n    return BeanUtil.copyToList(userService.listByIds(ids),UserVO.class);\n&#125;\n\n IService的Lambda查询\nlambdaQuery中有很多很方便的东西。\n比如：\n@Override\npublic List&lt;User&gt; queryList(String name, Integer status, Integer minBalance, Integer maxBalance) &#123;\n    return this.lambdaQuery()\n            .like(name != null, User::getUsername, name)\n            .eq(status != null, User::getStatus, status)\n            .ge(minBalance != null, User::getBalance, minBalance)\n            .le(maxBalance != null, User::getBalance, maxBalance)\n            .list();\n&#125;\n\n看这两个参数：\nboolean condition, R column, Object val\n\ncondition就是如果是true就放行这个条件（使之生效），xxx != null那就是存在，存在的话就要生效就行。\n他很好的解决了xml中繁杂的if判断：\n\n同样的LambdaUpdate也可以这样：\nboolean update = this.lambdaUpdate()\n        .set(User::getBalance, remainBalance)\n        .set(remainBalance == 0, User::getStatus, 2)\n        .eq(User::getId, id)\n        .update();\n\n 顺便一提乐观锁与悲观锁\n@Override\n@Transactional\npublic void deductBalance(Long id, Integer money) &#123;\n    User user = baseMapper.selectById(id);\n    if (user == null || user.getStatus() == 2) &#123;\n        throw new RuntimeException(&quot;用户不存在或者已冻结&quot;);\n    &#125;\n\n    if (user.getBalance() &lt; money) &#123;\n        throw new RuntimeException(&quot;余额不足&quot;);\n    &#125;\n\n    int remainBalance = user.getBalance() - money;\n    boolean update = this.lambdaUpdate()\n            .set(User::getBalance, remainBalance)\n            .set(remainBalance == 0, User::getStatus, 2)\n            .eq(User::getId, id)\n            .eq(User::getBalance, user.getBalance())\n            .update();\n    if (!update) &#123;\n        throw new RuntimeException(&quot;更新用户余额失败&quot;);\n    &#125;\n    System.out.println(&quot;更新用户余额成功，用户id：&quot; + id + &quot;，用户余额：&quot; + remainBalance);\n&#125;\n\n这是乐观锁的一种实现方式，通过在 update 时校验旧的 balance 值来保证并发安全，属于基于 CAS 思想的条件更新，但不是 MyBatisPlus 内置的 @Version 标准乐观锁。\n乐观锁和悲观锁都是并发控制手段。乐观锁与悲观锁都是基于一种假设，乐观锁假设冲突很少，更新时才检查数据有没有被别人改过，悲观锁假设冲突一定会发生，操作数据前先把它锁住。\n悲观锁假设并发冲突一定发生，在操作数据前先加锁，其他线程必须等待。在高并发、读多写少的场景下一般使用乐观锁，在强一致、冲突频繁的场景才使用悲观锁。\n 批量新增\n不要使用for循环来添加，请使用：\nthis.saveBatch()\n\n推荐通过MySQL开启：rewriteBatchedStatements=true参数，这样会更快。\n 静态工具\n静态工具适合用于多表查询问题，如果我们在AService需要用到BService中的内容时，我们往往会想要在AService中注入一个BService，然而问题在于，如果BService也需要AService的内容呢？要在BService也注入一个AService吗，那就产生了循环依赖问题。诚然我们也可以注入一个BaseMapper，但是毕竟Service更加强大，此时静态工具应运而生。\n@Service\npublic class UserServiceImpl extends ServiceImpl&lt;UserMapper, User&gt; implements IUserService &#123;\n    @Override\n    public UserVO queryUserAndAddressById(Long id) &#123;\n        User user = this.getById(id);\n        if (user == null || user.getStatus() == 2)\n            throw new RuntimeException(&quot;用户不存在或者已冻结&quot;);\n        List&lt;Address&gt; addresses = Db.lambdaQuery(Address.class)\n                .eq(Address::getUserId, id)\n                .list();\n        UserVO userVO = BeanUtil.copyProperties(user, UserVO.class);\n        if (addresses != null)&#123;\n            userVO.setAddressList(BeanUtil.copyToList(addresses, AddressVO.class));\n        &#125;\n        return userVO;\n    &#125;\n&#125;\n\n这是一个在UserService中需要查AddressService的场景，其实就是要把用户信息和用户地址拼在一起，场景很常见。使用Db.lambdaQuery(Address.class)来实现，Db中时Service的方法，只不过是静态的，通过指定类来实现反射。\n 善用stream流\n@Override\npublic List&lt;UserVO&gt; listUserByIds(List&lt;Long&gt; ids) &#123;\n    List&lt;User&gt; users = this.listByIds(ids);\n    if (users == null) &#123;\n        return Collections.emptyList();\n    &#125;\n    List&lt;Long&gt; collect = users.stream().map(User::getId).collect(Collectors.toList());\n    List&lt;Address&gt; addresses = Db.lambdaQuery(Address.class)\n            .in(Address::getUserId, collect)\n            .list();\n    List&lt;AddressVO&gt; addressVOS = BeanUtil.copyToList(addresses, AddressVO.class);\n    Map&lt;Long, List&lt;AddressVO&gt;&gt; addressMap = new HashMap&lt;&gt;();\n    if (CollUtil.isNotEmpty(addressVOS)) &#123;\n        addressMap = addressVOS.stream().collect(Collectors.groupingBy(AddressVO::getUserId));\n    &#125;\n    ArrayList&lt;UserVO&gt; userVOS = new ArrayList&lt;&gt;(users.size());\n    for (User user : users) &#123;\n        UserVO userVO = BeanUtil.copyProperties(user, UserVO.class);\n        userVOS.add(userVO);\n        userVO.setAddressList(addressMap.get(user.getId()));\n    &#125;\n    return userVOS;\n&#125;\n\n 逻辑删除\nmybatis-plus:\n global-config:\n  db-config:\n   logic-delete-field: flag # 全局逻辑删除的实体字段名，字段类型可以是boolean、integer\n   logic-delete-value: 1 # 逻辑已删除值(默认为 1)\n   logic-not-delete-value: 0 # 逻辑未删除值(默认为 0)\n\n\n 枚举处理器 @EnumValue\n@Getter\npublic enum UserStatus &#123;\n    NORMAL(1, &quot;正常&quot;),\n    FROZEN(2, &quot;冻结&quot;);\n\n    @EnumValue\n    @JsonValue\n    private final int value;\n    private final String desc;\n\n    UserStatus(int value, String desc) &#123;\n        this.value = value;\n        this.desc = desc;\n    &#125;\n\n&#125;\n\n@EnumValue指定的是在数据库中实际的存储类型，一般是bool类或者int类，主要取决你数据库的定义，而@EnumValue只需要加载数据库需要的类型上就行，然后返回到前端之前我们一般传一个JSON，这个时候@JsonValue就起作用了，@JsonValue加在那个数据上面，JSON就返回哪个数据。当然不要忘记，User、UserVO中的字段替换成相应的枚举类型。\n JSON处理器\nJSON处理器的目的是把存储在数据库的JSON串转换为Java对象。\n@TableName(value = &quot;tb_user&quot;, autoResultMap = true)\npublic class User &#123;\n\n\n    @TableId(type = IdType.AUTO)\n    private Long id;\n\n    private String username;\n\n    private String password;\n\n    private String phone;\n\n    @TableField(typeHandler = JacksonTypeHandler.class)\n    private UserInfo info;\n\n    private UserStatus status;\n\n    private Integer balance;\n\n    private LocalDateTime createTime;\n\n    private LocalDateTime updateTime;\n&#125;\n\n注意：@TableName(value = &quot;tb_user&quot;, autoResultMap = true)和@TableField(typeHandler = JacksonTypeHandler.class)\n 插件——分页插件\nMyBatisPlus给我们提供了插件功能这个插件功能首先通过一个总的插件注册器来实现，后续如果我们需要什么插件的话，可以直接在这个总的插件器上进行挂载。\n@Configuration\npublic class MyBatisConfig &#123;\n\n    @Bean\n    public MybatisPlusInterceptor mybatisPlusInterceptor() &#123;\n        MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor();\n        PaginationInnerInterceptor paginationInnerInterceptor = new PaginationInnerInterceptor(DbType.MYSQL);\n        paginationInnerInterceptor.setMaxLimit(1000L);\n        interceptor.addInnerInterceptor(\n                paginationInnerInterceptor\n        );\n        return interceptor;\n    &#125;\n&#125;\n\n 通用分页实体\n@Data\n@ApiModel(description = &quot;分页查询条件实体&quot;)\npublic class PageQuery &#123;\n    @ApiModelProperty(&quot;页码&quot;)\n    private Integer pageNo;\n    @ApiModelProperty(&quot;每页数量&quot;)\n    private Integer pageSize;\n    @ApiModelProperty(&quot;排序字段&quot;)\n    private String sortBy;\n    @ApiModelProperty(&quot;是否升序&quot;)\n    private Boolean isAsc;\n&#125;\n\n其他条件可以继承page：\n@EqualsAndHashCode(callSuper = true)\n@Data\n@ApiModel(description = &quot;用户查询条件实体&quot;)\npublic class UserQuery extends PageQuery &#123;\n    @ApiModelProperty(&quot;用户名关键字&quot;)\n    private String name;\n    @ApiModelProperty(&quot;用户状态：1-正常，2-冻结&quot;)\n    private Integer status;\n    @ApiModelProperty(&quot;余额最小值&quot;)\n    private Integer minBalance;\n    @ApiModelProperty(&quot;余额最大值&quot;)\n    private Integer maxBalance;\n&#125;\n\n 通用分页返回实体\n@Data\n@ApiModel(description = &quot;分页结果&quot;)\npublic class PageDTO&lt;T&gt; &#123;\n    @ApiModelProperty(&quot;总记录数&quot;)\n    private Long total;\n    @ApiModelProperty(&quot;总页数&quot;)\n    private Long pages;\n    @ApiModelProperty(&quot;当前页数据&quot;)\n    private List&lt;T&gt; list;\n&#125;\n\n 实现案例\n@Override\npublic PageDTO&lt;UserVO&gt; queryListPage(UserQuery condition) &#123;\n    Page&lt;User&gt; userPageCondition = Page.of(condition.getPageNo(), condition.getPageSize());\n    if (condition.getSortBy() != null &amp;&amp; !condition.getSortBy().isEmpty()) &#123;\n        userPageCondition.addOrder(new OrderItem().setColumn(condition.getSortBy()).setAsc(condition.getIsAsc()));\n    &#125; else &#123;\n        userPageCondition.addOrder(new OrderItem().setColumn(&quot;update_time&quot;).setAsc(true));\n    &#125;\n    Page&lt;User&gt; userPage = lambdaQuery()\n            .like(condition.getName() != null, User::getUsername, condition.getName())\n            .eq(condition.getStatus() != null, User::getStatus, condition.getStatus())\n            .ge(condition.getMinBalance() != null, User::getBalance, condition.getMinBalance())\n            .le(condition.getMaxBalance() != null, User::getBalance, condition.getMaxBalance())\n            .page(userPageCondition);\n    PageDTO&lt;UserVO&gt; userPageDTO = new PageDTO&lt;&gt;();\n    userPageDTO.setTotal(userPage.getTotal());\n    userPageDTO.setPages(userPage.getPages());\n    List&lt;User&gt; records = userPage.getRecords();\n    if (records == null) &#123;\n        userPageDTO.setList(Collections.emptyList());\n        return userPageDTO;\n    &#125;\n    userPageDTO.setList(BeanUtil.copyToList(records, UserVO.class));\n    return userPageDTO;\n&#125;\n\n 兄弟，还能更方便！\n原来这种实现，实际上业务逻辑只占到很小一部分，我来说一下！\n代码一开始：\nPage&lt;User&gt; userPageCondition = Page.of(condition.getPageNo(), condition.getPageSize());\nif (condition.getSortBy() != null &amp;&amp; !condition.getSortBy().isEmpty()) &#123;\n    userPageCondition.addOrder(new OrderItem().setColumn(condition.getSortBy()).setAsc(condition.getIsAsc()));\n&#125; else &#123;\n    userPageCondition.addOrder(new OrderItem().setColumn(&quot;update_time&quot;).setAsc(true));\n&#125;\n\n构造了一个Page，然后对需要做排序的条件进行了设置。\n这里是我们的核心查询语句：\nPage&lt;User&gt; userPage = lambdaQuery()\n        .like(condition.getName() != null, User::getUsername, condition.getName())\n        .eq(condition.getStatus() != null, User::getStatus, condition.getStatus())\n        .ge(condition.getMinBalance() != null, User::getBalance, condition.getMinBalance())\n        .le(condition.getMaxBalance() != null, User::getBalance, condition.getMaxBalance())\n        .page(userPageCondition);\n\n紧接着，就是又一次参数的设置和类型转换：\nPageDTO&lt;UserVO&gt; userPageDTO = new PageDTO&lt;&gt;();\nuserPageDTO.setTotal(userPage.getTotal());\nuserPageDTO.setPages(userPage.getPages());\nList&lt;User&gt; records = userPage.getRecords();\nif (records == null) &#123;\n    userPageDTO.setList(Collections.emptyList());\n    return userPageDTO;\n&#125;\nuserPageDTO.setList(BeanUtil.copyToList(records, UserVO.class));\nreturn userPageDTO;\n\n我们能不能把创建Page和后面的整理DTO再进行一次封装呢？\n我们可以这样：\n\n在PageQuery中定义方法，将PageQuery对象转为MyBatisPlus中的Page对象\n在PageDTO中定义方法，将MyBatisPlus中的Page结果转为PageDTO结果\n\n@Data\n@ApiModel(description = &quot;分页查询条件实体&quot;)\npublic class PageQuery &#123;\n    @ApiModelProperty(&quot;页码&quot;)\n    private Integer pageNo = 1;\n    @ApiModelProperty(&quot;每页数量&quot;)\n    private Integer pageSize = 5;\n    @ApiModelProperty(&quot;排序字段&quot;)\n    private String sortBy;\n    @ApiModelProperty(&quot;是否升序&quot;)\n    private Boolean isAsc = true;\n\n\n    public &lt;T&gt; Page&lt;T&gt; toMpPage() &#123;\n        Page&lt;T&gt; userPageCondition = Page.of(pageNo, pageSize);\n        if (sortBy != null &amp;&amp; !sortBy.isEmpty()) &#123;\n            userPageCondition.addOrder(new OrderItem().setColumn(sortBy).setAsc(isAsc));\n        &#125; else &#123;\n            userPageCondition.addOrder(new OrderItem().setColumn(&quot;update_time&quot;).setAsc(false));\n        &#125;\n        return userPageCondition;\n    &#125;\n\n    public &lt;T&gt; Page&lt;T&gt; toMpPage(OrderItem... items) &#123;\n        Page&lt;T&gt; userPageCondition = Page.of(pageNo, pageSize);\n        if (sortBy != null &amp;&amp; !sortBy.isEmpty()) &#123;\n            userPageCondition.addOrder(new OrderItem().setColumn(sortBy).setAsc(isAsc));\n        &#125; else if (items != null) &#123;\n            userPageCondition.addOrder(items);\n        &#125;\n        return userPageCondition;\n    &#125;\n&#125;\n\n@Data\n@ApiModel(description = &quot;分页结果&quot;)\npublic class PageDTO&lt;T&gt; &#123;\n    @ApiModelProperty(&quot;总记录数&quot;)\n    private Long total;\n    @ApiModelProperty(&quot;总页数&quot;)\n    private Long pages;\n    @ApiModelProperty(&quot;当前页数据&quot;)\n    private List&lt;T&gt; list;\n\n    public static &lt;PO, VO&gt; PageDTO&lt;VO&gt; of(Page&lt;PO&gt; pageT, Class&lt;VO&gt; clazz) &#123;\n        PageDTO&lt;VO&gt; userPageDTO = new PageDTO&lt;&gt;();\n        userPageDTO.setTotal(pageT.getTotal());\n        userPageDTO.setPages(pageT.getPages());\n        List&lt;PO&gt; records = pageT.getRecords();\n        if (records == null || records.isEmpty()) &#123;\n            userPageDTO.setList(Collections.emptyList());\n            return userPageDTO;\n        &#125;\n        userPageDTO.setList(BeanUtil.copyToList(records, clazz));\n        return userPageDTO;\n    &#125;\n    \n    public static &lt;PO, VO&gt; PageDTO&lt;VO&gt; of(Page&lt;PO&gt; pageT, Class&lt;VO&gt; clazz, Function&lt;PO, VO&gt; convertor) &#123;\n        PageDTO&lt;VO&gt; userPageDTO = new PageDTO&lt;&gt;();\n        userPageDTO.setTotal(pageT.getTotal());\n        userPageDTO.setPages(pageT.getPages());\n        List&lt;PO&gt; records = pageT.getRecords();\n        if (records == null || records.isEmpty()) &#123;\n            userPageDTO.setList(Collections.emptyList());\n            return userPageDTO;\n        &#125;\n        userPageDTO.setList(records.stream().map(convertor).collect(Collectors.toList()));\n        return userPageDTO;\n    &#125;\n&#125;\n\n","categories":["Java","微服务相关"],"tags":["java","微服务","RabbitMQ","SpringBoot","MyBatisPlus","SpringCloud","Elasticsearch","Redis","笔记"]},{"title":"文章翻译","url":"/English/shEnglish/","content":" 英语文章翻译\n C1P1\nIt’s hard to imagine meeting someone for the first time and not exchanging any personal information.\n\n很难想象，与某人第一次见面却没有交换任何个人信息。\n\nAt the very least, you offer your name and a few important facts - perhaps age, occupation,reason for joining a certain organization, or reason for attending a certain class.\n\n至少你会提供你的名字和一些重要的个人情况比如年龄、职业、加入某个组织或者参加某个课程的原因。\n\nAs friendships develop, however, the answer to the question “Who are you?” becomes more complex.\n\n然而随着友谊的发展，“你是谁”这个问题的答案会变得越来越复杂\n\nOur identities start to form when we are children and continue to grow,solidify, and even change as we mature.\n\n我们的身份认同从孩童时代开始形成，并随着我们的成长而不断发展和巩固，甚至会随着我们的成熟而发生变化。\n\nA person’s identity is actually made up of many different aspects, some broad and some narrow.\n\n一个人的身份实际上由许多不同的方面构成，有些方面宽泛，有些方面则具体。\n\nFor instance, you might identify with the broad categories of “German,”“male,” and “student” as well as the narrower ones of&quot;violinist,&quot;“left-handed person,” and “brother of Anna.”\n\n例如，你可能会认同“德国人”、“男性”和“学生”这些宽泛的类别，以及“小提琴手”、“左撇子”和“安娜的哥哥”这些更具体的类别\n\nIdentity traits can be ascribed, achieved or chosen.\n\n身份特质可以是赋予的、获得的或选择的。\n\nAn ascribed trait is one that you are born with; examples include your ethnicity, your birthplace, and being the child and possibly the sibling of certain people.\n\n被赋予的是你与生俱来的特质；例如，你的种族、出生地，以及作为某些人的子女甚至可能是兄弟姐妹。\n\nAn achieved trait is one you work for, such as being a university graduate or the employee of a certain company. An identity such as a club membership or affiliation with a political party is chosen.\n\n获得性特质是你努力追求的，比如大学毕业或成为某公司的员工。而身份则是你选择的，比如加入某个俱乐部或成为某个政党的成员。\n\nHowever, traits are not always so easy to categorize.\n\n然而，特征并不总是那么容易归类。\n\nIs speaking your native language, for example, ascribed (because you were born into the family and country where that language was spoken), achieved (because you studied the language and became more proficient), or even chosen (if you grew up in a multilingual country, but preferred one language over another)?\n\n例如，说你的母语，是因为你出生在讲这种语言的家庭和国家，所以被赋予了这种语言？还是因为你学习了这种语言并变得更加熟练，所以掌握了这种语言？或者，如果你在一个多语言国家长大，但更喜欢一种语言而不是另一种语言，那么你是主动选择了这种语言？\n\nOur identities are important not only because they shape our belief in who we are, but also because they impact how others treat us.\n\n我们的身份之所以重要，不仅是因为它们塑造了我们对自我身份的认知，更是因为它们影响着他人对我们的态度。\n\nAlthough traits can be positive (intelligent; loyal) or negative (stubborn;criminal), people are more affected by how similar or different their traits are compared to those of other people.\n\n尽管特质可以是积极的（如聪明、忠诚）或消极的（如固执、犯罪），但人们更容易受到自己特质与他人特质相似或相异程度的影响。\n\nFor example, if you are a fan of the Falcons sports team, you have something in common with other Falcons fans.\n\n例如，如果你是猎鹰队的球迷，那么你和其他猎鹰队的球迷就有共同之处。\n\nThe next time you go to an event or social gathering, watch how people who are strangers at first try to find something in common with the people they meet-perhaps a shared hometown,a similar occupation or hobby, or even the same opinion about the weather that day or a current event\n\n下次你去参加活动或社交聚会时，观察一下那些起初是陌生人的人是如何试图找到与他们遇到的人的共同点的——也许是共同的家乡、相似的职业或爱好，甚至是当天对天气或当前事件的相同看法\n\nPeople don’t just define themselves as who they are, however; theyalso define themselves as who they are not.\n\n然而，人们不仅仅定义自己是谁，他们还定义自己不是谁。\n\nThat is to say, they aren’t just fans of the Springfield High School basketball team; they are also not fans of the Pleasant Valley High School basketball team.\n\n也就是说，他们不仅是斯普林菲尔德高中篮球队的粉丝，同时也不是普莱森特谷高中篮球队的粉丝\n\nA friendly rivalry between two sports teams isn’t necessarily a bad thing, but when rivalries are taken too far or tensions arise over differences about larger social issues, the consequences can be more serious.\n\n两支运动队之间友好的竞争并不一定是坏事，但当竞争过度或因更大的社会问题上的分歧而引发紧张局势时，后果可能会更加严重。\n\nInterestingly, groups that have a lot in common sometimes form the most intense separate identities.\n\n有趣的是，有很多共同点的群体有时会形成最强烈的独立身份。\n\nTo someone who doesn’t use a computer at all, they might all seem very similar.\n\n对于一个根本不使用电脑的人来说，它们可能看起来都很相似。\n\nHowever, debates over the best brands of laptop can become quite heated.\n\n然而，关于笔记本电脑最佳品牌的争论可能会变得相当激烈。\n\nPeople form different groups over whether they preferred a book or movie adaptation; which brand of cell phone they prefer; which leader in the same political party they support.\n\n人们会因为喜欢某本书或某个电影的改编、某个手机品牌、或因支持同一党内领导人而形成不同群体。\n\nStates or cities that are near each other can be stronger rivals than those separated by greater distances.\n\n相距较近的州或城市可能比相距较远的州或市更强大。\n\nRather than confirming the positive effects of social identity, these rivalries can make people feel insecure, threatened, angry, or even fearful.\n\n这些竞争非但没有证实社会认同的积极影响，反而会让人们感到不安全、威胁、愤怒，甚至恐惧。\n\nThe challenge, then, for both leaders and all of us in society is to foster the positive effects of group membership while avoiding the negative ones.\n\n因此，对于领导者和我们社会中的所有人来说，挑战在于培养群体成员的积极影响，同时避免消极影响。\n\n C1P2\nYou know the old saying: You can’t teach an old dog new tricks.\n\n你知道一句老话：老狗学不了新把戏。\n\nIt’s no surprise that we tend to believe that a person’s personality is stable.\n\n我们倾向于相信一个人的性格是稳定的，这并不奇怪。\n\nPeople might disagree about whether someone is born with a certain personality or develops a personality while growing up,but it’s commonly accepted that someone’s personality will be much the same at age 50 as it was at age 20.\n\n人们可能不同意一个人是天生具有某种性格，还是在成长过程中发展出某种性格，但人们普遍认为，一个人的性格在50岁时与20岁时基本相同。\n\nBoth in our personal lives and our work lives, we’re told that we need to accept people the way they are and to learn to get along with other people even when they’re difficult.\n\n在我们的个人生活和工作生活中，我们被告知，我们需要接受人们的现状，学会与他人相处，即使他们很难相处。\n\nAfter all, they’re never going to change.\n\n毕竟，他们永远不会改变。\n\nNew evidence,however, suggests that this isn’t true.\n\n然而，新的证据表明，事实并非如此。\n\nPublished in the journal Psychology and Aging, a comprehensive study by four psychologists examined a group of Scottish volunteers over a period of 63years, making it the longest study of its type ever done.\n\n发表在《心理学与衰老》杂志上的一项由四位心理学家进行的综合研究对一组苏格兰志愿者进行了为期63年的调查，使其成为有史以来最长的同类研究。\n\nAnd what they found was unexpected: namely, no correlation at all between the participants’ scores on personality tests when they were 14 years old and the same tests when they were 77 years old.\n\n他们的发现出乎意料：即参与者14岁时的性格测试得分与77岁时的相同测试得分之间根本没有相关性。\n\nThe test examined six areas: self-confidence,perseverance, stability of moods, conscientiousness,originality, and desire to learn.\n\n该测试考察了六个方面：自信、毅力、情绪稳定性、责任心、原创性和学习欲望。\n\nThe original study involved 1,208 children, and 174 of them were available for the follow-up study six decades later.\n\n最初的研究涉及1208名儿童，其中174名儿童在60年后可用于后续研究。\n\nBecause it’s not reliable to have people rate themselves, the participants were evaluated in these categories by other people-by teachers when they were 14, and by friends or relatives when they were 77.\n\n因为让人们对自己进行评分是不可靠的，所以参与者在14岁时由其他人进行评估，在77岁时由朋友或亲戚进行评估。\n\nThey were also tested for intelligence and general well-being.\n\n他们还接受了智力和总体小方格状况的测试。\n\nThe researchers were surprised to find that none of the ratings matched up with each other over the years.\n\n研究人员惊讶地发现，多年来，这些评级都不匹配。\n\nEarlier studies and tests produced somewhat different outcomes.\n\n早期的研究和测试产生了一些不同的结果。\n\nResearch suggested a few character traits had a low correlation over time and others had a modest correlation.\n\n研究表明，随着时间的推移，一些性格特征的相关性较低，而另一些则相关性适中。\n\nThe Scottish study, although smaller in scope because it involved fewer participants, measured them over a much longer period of time.\n\n苏格兰的这项研究尽管规模不大，但其时间跨度更长。\n\nThis led there searchers to conclude that personality shifts are more likely to occur over long periods of time.\n\n这导致研究人员得出结论，人格转变更有可能在很长一段时间内发生。\n\nNow, it’s not a perfect study,of course; such a thing is rare, if not impossible, with human beings and personality.\n\n当然，这并非一项完美的研究；对于人类和性格而言，这样的研究实属罕见，甚至可以说是不可能的。\n\nFor instance, the people who did the ratings in 1950 were not the same people who did the ratings in 2012, and this could have caused some difference.\n\n例如，1950 年进行评级的人与 2012 年进行评级的人并非同一拨人，这可能会造成一些差异。\n\nIt’s difficult for a study on something as broad as identity and personality to take all the variables into consideration.\n\n对于像身份和性格这样广泛的研究课题来说，要将所有变量都考虑在内是很困难的。\n\nHowever, the results are still significant, and they have interesting implications.\n\n然而，结果仍然是重要的，他们有有趣的启示。\n\nLet’s consider some of those implications for a moment. What does it all mean? And is it only of academic interest, or can you yourself apply this knowledge to your own life?\n\n让我们考虑一下其中的一些含义。这一切意味着什么？这仅仅是学术上的兴趣，还是你能把这些知识应用到你自己的生活中？\n\nFor one thing,it should give you a new way to think about other people.\n\n首先，它应该给你一种思考他人的新方式。\n\nFor example, say you’re contacted on social media by someone you knew in school years ago. If you didn’t like the person at that time, you might be tempted to refuse the connection.\n\n例如，假设你在社交媒体上被你多年前在学校认识的人联系。如果你当时不喜欢这个人，你可能会想要拒绝这段关系。\n\nIf you didn’t like each other then, after all, why would you like each other now?\n\n毕竟，如果你们当时不喜欢对方，为什么现在会喜欢对方呢？\n\nBut if it’s true that personality can change, then there’s a reason to give that person another chance.\n\n但如果性格真的可以改变，那就有理由再给那个人一次机会。\n\nHe or she might be very different now -and you might beto0.\n\n他或她现在可能和以前大不相同了——你也可能是。\n\nYou might also have more reasonable expectations of old childhood friends who reconnect after many years.\n\n你也可能对多年后重新联系的童年老朋友有更合理的期望。\n\nIf you know their personalities (and yours) could have changed over the years, you’ll be less disappointed if your friendship isn’t as deep now as it was before.\n\n如果你知道他们（和你自己）的性格可能会随着时间的推移而改变，那么如果你们的友谊没有以前那么深厚，你就不会那么失望了。\n\nRather than feel frustrated with yourselves, the two of you can accept that you have changed.\n\n与其对自己感到沮丧，不如接受自己已经改变的事实。\n\nThe study has implications for the workplace too.\n\n这项研究对工作场所也有影响。\n\nPersonality forms a large part of a worker’s suitability for a job, both in dealing with co-workers and in dealing with clients.\n\n性格在很大程度上决定了一个人是否适合一份工作，无论是在与同事打交道还是与客户打交道时。\n\nIf a person has a personality trait that interferes with work-say he argues with customers or she misses deadlines-it’s important for managers to know that these traits can change.\n\n如果一个人的性格特征会干扰工作，比如他会和客户争吵，或者她错过了截止日期，那么管理者就应该知道这些性格特征是可以改变的。\n\nIt’s usually cheaper to train a current employee than to let that person go and hire a replacement.\n\n通常来说，培训一名现有员工要比让他离开并雇佣一个替代者便宜得多。\n\nEven employees who aren’t experiencing problems can be trained to be even better and more effective in terms of personality.\n\n即使没有遇到问题的员工也可以通过培训，在个性方面表现得更好、更有效。\n\nThis will help ensure that people continue to get along with one another.\n\n这将有助于确保人们继续彼此相处。\n\nFinally, there are personal implications.\n\n最后，还有个人影响。\n\nIf you’re the sort of person who says things like &quot;I have a quick temper&quot;or “My problem is I can’t help procrastinating” or &quot;I’ve always been too sensitive, and I blame myself when ever something goes wrong,&quot;it should be good news to know that these personality traits are not ones you have to keep.\n\n如果你是那种会说“我脾气暴躁”或“我的问题是我总是忍不住拖延”或“我总是太敏感，一旦出了问题我就会责怪自己”的人，那么知道这些性格特征并不是你必须保留的，这应该是个好消息。\n\nAlthough some therapists do good work helping patients accept themselves as they are,to build self-esteem,wouldn’t it be more beneficial to eliminate negative personality traits than to learn to accept them?\n\n尽管一些治疗师在帮助病人接受自己的本来面目、建立自尊方面做得很好，但消除消极的人格特征不是比学会接受它们更有益吗？\n\nKnowing that you can change is the first stage in learning how to change.\n\n知道自己可以改变是学习如何改变的第一步。\n\nThen you can look forward to saying things like,“I used to be too sensitive, but I’m not anymore”;or look forward to a time when,as we might start saying, you can learn some new tricks.\n\n然后你就可以期待说这样的话：“我以前太敏感了，但现在不会了。”或者期待有一天，就像我们开始说的，你可以学到一些新技巧。\n\n C2P1\nOh, no! You dropped the cup, and it smashed! Time to throw it away and buy a new one.\n\n哦,不！你把杯子掉在地上，打碎了！是时候扔掉它，买一个新的了。\n\nUnless, perhaps, you are a fan of the Japanese art of kintsugi or kintsukuroi–roughly translated,&quot;to mend with gold.&quot;This is the practice among certain craftsmen of mending the broken pieces of pottery, such as a plate, a cup, or a bowl, with gold (or similar) lacquer.\n\n除非你是日本kintsugi或kintsukuroi艺术的粉丝。这是某些工匠用金（或类似的）漆修补破碎的陶器碎片的做法，例如盘子，杯子或碗。\n\nThe gold is used to glue the pieces back together\n\n金子是用来把碎片粘在一起的\n\nIf small pieces are missing, they can be created out of gold, or a piece from a different bowl or plate can be used instead.\n\n如果少了一小块，可以用黄金制作，或者用另一个碗或盘子里的小块代替。\n\nThe repaired product’s value is not reduced, though-it is actually enhanced.\n\n修复后的产品的价值并没有减少，反而增加了。\n\nIt is believed to become more beautiful because it was broken.\n\n人们认为它因为被打破而变得更加美丽。\n\nPieces of kintsugi pottery can be enormously expensive and are featured in museum exhibits in Japan and overseas.\n\n金杉陶器非常昂贵，在日本和海外的博物馆都有展出。\n\nThese days you can even see machine-made ceramics with gold designs on them that look as if they are kintsugi,even though the original was actually never broken.\n\n如今，你甚至可以看到机器制作的陶瓷，上面有金色的图案，看起来像是金杉，尽管原来的金杉实际上从未破损过。\n\nBut the mended patterns have become so trendy that people want to imitate them.\n\n但是缝补的图案变得如此流行，以至于人们都想模仿它们。\n\nThere’s a story or legend behind the practice - which may or may not be historically accurate, but beautifully illustrates the concept.\n\n这种做法背后有一个故事或传说——可能与历史不相符，但却很好地说明了这一概念。\n\nMore than five hundred years ago, there lived a military ruler in Japan, who owned a bowl he especially loved.\n\n五百多年前，日本有一位军事统治者，他有一只碗，他特别喜欢。\n\nOne day while he was entertaining some guests, his servant dropped the bowl, and it broke into five pieces.\n\n一天，当他招待客人时，他的仆人把碗掉在地上，摔成了五片。\n\nKnowing the leader’s bad temper, his guests worried that he would punish the servant\n\n客人们知道首领的坏脾气，担心他会惩罚仆人\n\nHowever, one of the guests made up an amusing poem about the incident\n\n然而，其中一位客人就这件事编了一首有趣的诗\n\nEverybody laughed, including the ruler.\n\n每个人都笑了，包括统治者。\n\nWhen he relaxed, he was able to see that the bowl’s beauty had not been destroyed by the accident.\n\n当他放松下来时，他能看到碗的美丽并没有被事故破坏。\n\nInstead,because the vessel could be repaired, the ruler now had a new appreciation for its strength and ability to survive.\n\n相反，因为容器可以修复，统治者现在对它的力量和生存能力有了新的认识。\n\nIn fact, according to the story, the true life of the bowl began the moment it was dropped.\n\n事实上，根据这个故事，这个碗的真正生命从它被扔下的那一刻就开始了。\n\nIf this seems a hard notion to understand, then consider it in light of another Japanese philosophy, that of wabi-sabi.\n\n如果这似乎是一个难以理解的概念，那么请根据另一种日本哲学来考虑它，那就是wabi-sabi。\n\nThis is harder to translate into English, but it refers to the combination of three beliefs that nothing is permanent, nothing is finished, and nothing is perfect.\n\n这句话很难翻译成英语，但它指的是三个信念的结合：没有什么是永恒的，没有什么是完成的，没有什么是完美的。\n\nApplied to arts and crafts, it explains why the Japanese traditionally value handmade objects.\n\n应用于工艺品，它解释了为什么日本传统上重视手工制品。\n\nEven though they look less perfect than those made by machine, it is actually this imperfection that makes them beautiful\n\n尽管它们看起来没有机器制作的那么完美，但实际上正是这种不完美让它们变得美丽\n\nIn fact, artists who value the wabi-sabi aesthetic create works that are deliberately imperfect, such as a bowl that isn’t entirely round or a vase with a thumbprint visible in the clay.\n\n事实上，看重wabi-sabi美学的艺术家会刻意创造出不完美的作品，比如一个不全圆的碗，或者一个在粘土上可以看到拇指指纹的花瓶。\n\nRough surfaces, instead of ones smoothed by machines, are common in wabi-sabi ceramics, and often the pieces are not glazed or colored.\n\n粗糙的表面，而不是机器光滑的表面，在wabi-sabi陶瓷中很常见，而且这些碎片通常没有上釉或着色。\n\nIt’s not just Japan that has such a tradition, however\n\n然而，并非只有日本有这样的传统\n\nA similar idea can be found in Iran, among the makers of Persian rugs.\n\n在伊朗的波斯地毯制造商中也可以找到类似的想法。\n\nTradition has it that those who weave carpets will deliberately include one small flaw,as recognition of the fact that nothing can be perfect.\n\n传统上，编织地毯的人会故意留下一个小瑕疵，因为他们意识到没有什么是完美的。\n\nThe intentional mistake reminds them to be modest about their work.\n\n故意的错误提醒他们对自己的工作要谦虚。\n\nSimilarly, some early American settlers known as the Puritans included a “humility square” when they sewed a quilt -one square that didn’t match the rest of the blanket.\n\n类似地，一些被称为清教徒的早期美国定居者在缝制被子时也会附上一块“谦卑的方巾”——这块方巾与毯子的其他部分不匹配。\n\nSome Native American bead workers would include an intentional “mistake bead” for the same reason.\n\n出于同样的原因，一些印第安人的头饰工人会故意加上一个“错误的头饰”。\n\nSuch practices have also been reported among Amish furniture makers in the United States and some forms of Islamic art - although careful work by sociologists and historians suggests that these stories are actually not true, but rather a romanticized version of their art or a misunderstanding of a tradition.\n\n在美国的阿米什家具制造商和某些形式的伊斯兰艺术中也有类似的报道——尽管社会学家和历史学家的仔细研究表明，这些故事实际上不是真实的，而是他们艺术的浪漫化版本，或者是对传统的误解。\n\nTrue or not,however, these cultural practices teach us not only about art but about life, and the importance of not only accepting, but actually celebrating, our imperfections.\n\n无论真假，这些文化实践不仅教会了我们艺术，也教会了我们生活，教会了我们不仅要接受，而且要赞美我们的不完美的重要性\n\nThat doesn’t mean we shouldn’t care about making mistakes; but for many people, worrying about small imperfections keeps them from finishing a project or appreciating one they have finished.\n\n这并不意味着我们不应该在意犯错误；但对很多人来说，担心小瑕疵让他们无法完成一个项目，也无法欣赏他们已经完成的项目。\n\nPeople who are “perfectionists” can feel insecure and anxious about the art they create, which makes it harder for them to enjoy what they do.\n\n“完美主义者”会对自己创造的艺术感到不安和焦虑，这使得他们更难享受自己所做的事情。\n\nThe concept can even be applied more broadly than just to art, however.\n\n然而，这个概念甚至可以应用于更广泛的领域，而不仅仅是艺术。\n\nConsider yourself, for example. Do you have any imperfections - anything from physical scars to personal habits?\n\n以你自己为例。你有什么不完美的地方吗——从身体上的伤疤到个人习惯？\n\nWhat if, instead of considering these to be flaws, you could appreciate them as part of what makes you a beautiful person?\n\n如果你不认为这些是缺点，而是把它们看作是让你成为一个美丽的人的一部分呢？\n\n","categories":["笔记","考研英语笔记"],"tags":["英语"]},{"title":"MongoDB初步使用","url":"/MongoDB/MongoDB02/","content":" MongoDB命令\n 创建用户（可读可写）\n 创建\ndb.createUser(&#123;user:&quot;lkr40&quot;,pwd:&quot;123456&quot;,roles:[&quot;readWrite&quot;]&#125;)\n\n 检验\ndb.auth(&quot;lkr40&quot;,&quot;123456&quot;)\n\n\ndb.createUser({user:“lkr40”,pwd:“123456”,roles:[“readWrite”]})\nSuccessfully added user: { “user” : “lkr40”, “roles” : [ “readWrite” ] }\ndb.auth(“lkr40”,“123456”)\n1\n\n 登录\nmongo 127.0.0.1:27017/mymongo -u lkr40 -p 123456\n\n 建立/使用数据库\nuse mymongo\n\n\nuse mymongo\nswitched to db mymongo\n\n 查询当前数据库\ndb\n\n\ndb\nmymongo\n\n 删除数据库\ndb.dropDatabase()\n\n 查询时间\nDate()\n\n\nDate()\nWed Sep 06 2023 10:21:36 GMT+0800\n\n 创建集合\n集合中可以包含子集合\n 显式创建（t）\ndb.createCollection(&quot;t&quot;)\n\n\ndb.createCollection(“t”)\n{ “ok” : 1 }\n\n 隐式创建（t1）\ndb.t1.insert(&#123;&quot;age&quot;:18&#125;)\n\n\ndb.t1.insert({“age”:18})\nWriteResult({ “nInserted” : 1 })\nshow collections\nt\nt1\n\n 展示集合\nshow collections\n\n\nshow collections\nt\nt1\n\n 删除集合\ndb.t.drop()\n\n\ndb.t.drop()\ntrue\n\n 查询文档\ndb.t1.find()\n\n\ndb.t1.find()\n{ “_id” : ObjectId(“64f7ec7bff2142d2a2445360”), “age” : 18 }\n\n 条件查询\n 等于\ndb.t.find(&#123;&quot;x&quot;:1&#125;)\n\n\ndb.t.find({“x”:1})\n{ “_id” : ObjectId(“64f7f05bff2142d2a2445361”), “x” : 1 }\n\n 小于\ndb.t.find(&#123;&quot;x&quot;:&#123;$lte : 5&#125;&#125;)\n\n 大于\ndb.t.find(&#123;&quot;x&quot;:&#123;$lte : 5&#125;&#125;)\n\n 在之中（包含）\ndb.t.find(&#123;&quot;x&quot;:&#123;$in : [1,3,5]&#125;&#125;)\n\n 不在其中（不包含）\ndb.t.find(&#123;&quot;x&quot;:&#123;$nin : [1,3,5]&#125;&#125;)\n\n 插入文档\n 单一插入\ndb.t1.insert(&#123;&quot;_id&quot;:&quot;1&quot;,&quot;age&quot;:23&#125;)\n\n\ndb.t1.insert({“_id”:“1”,“age”:23})\nWriteResult({ “nInserted” : 1 })\ndb.t1.find()\n{ “_id” : ObjectId(“64f7ec7bff2142d2a2445360”), “age” : 18 }\n{ “_id” : “1”, “age” : 23 }\n\n 批量插入（单语句）\ndb.t1.insertMany([&#123;&quot;_id&quot;:2,&quot;age&quot;:65&#125;,&#123;&quot;_id&quot;:3,&quot;age&quot;:45&#125;])\n\n\ndb.t1.insertMany([{“_id”:2,“age”:65},{“_id”:3,“age”:45}])\n{ “acknowledged” : true, “insertedIds” : [ 2, 3 ] }\ndb.t1.find()\n{ “_id” : ObjectId(“64f7ec7bff2142d2a2445360”), “age” : 18 }\n{ “_id” : “1”, “age” : 33 }\n{ “_id” : 2, “age” : 65 }\n{ “_id” : 3, “age” : 45 }\n\n 循环插入（for）\nfor(i=1;i&lt;9;i++) db.t.insert(&#123;&quot;x&quot;:i&#125;)\n\n\nfor(i=1;i&lt;9;i++) db.t.insert({“x”:i})\nWriteResult({ “nInserted” : 1 })\ndb.t.find()\n{ “_id” : ObjectId(“64f7f05bff2142d2a2445361”), “x” : 1 }\n{ “_id” : ObjectId(“64f7f05cff2142d2a2445362”), “x” : 2 }\n{ “_id” : ObjectId(“64f7f05cff2142d2a2445363”), “x” : 3 }\n{ “_id” : ObjectId(“64f7f05cff2142d2a2445364”), “x” : 4 }\n{ “_id” : ObjectId(“64f7f05cff2142d2a2445365”), “x” : 5 }\n{ “_id” : ObjectId(“64f7f05cff2142d2a2445366”), “x” : 6 }\n{ “_id” : ObjectId(“64f7f05cff2142d2a2445367”), “x” : 7 }\n{ “_id” : ObjectId(“64f7f05cff2142d2a2445368”), “x” : 8 }\n\n 同id插入异常\n\ndb.t1.insert({“_id”:“1”,“age”:23})\nWriteResult({\n“nInserted” : 0,\n“writeError” : {\n“code” : 11000,\n“errmsg” : “E11000 duplicate key error collection: mymongo.t1 index: id dup key: { _id: &quot;1&quot; }”\n}\n})\n\n 修改（输入全部属性/替换）\ndb.t1.save(&#123;&quot;_id&quot;:&quot;1&quot;,&quot;age&quot;:33&#125;)\n\n\ndb.t1.save({“_id”:“1”,“age”:33})\nWriteResult({ “nMatched” : 1, “nUpserted” : 0, “nModified” : 1 })\ndb.t1.find()\n{ “_id” : ObjectId(“64f7ec7bff2142d2a2445360”), “age” : 18 }\n{ “_id” : “1”, “age” : 33 }\n\n 更新\ndb.t1.update(&#123;&quot;age&quot;:65&#125;,&#123;$set:&#123;&quot;age&quot;:100&#125;&#125;)\n\n\ndb.t1.update({“age”:65},{$set:{“age”:100}})\nWriteResult({ “nMatched” : 1, “nUpserted” : 0, “nModified” : 1 })\ndb.t1.find()\n{ “_id” : ObjectId(“64f7ec7bff2142d2a2445360”), “age” : 18 }\n{ “_id” : “1”, “age” : 33 }\n{ “_id” : 2, “age” : 100 }\n{ “_id” : 3, “age” : 45 }\n\n 删除\n 批量删除\ndb.t1.remove(&#123;&quot;age&quot;:33&#125;)\n\n\ndb.t1.remove({“age”:33})\nWriteResult({ “nRemoved” : 1 })\ndb.t1.find()\n{ “_id” : ObjectId(“64f7ec7bff2142d2a2445360”), “age” : 18 }\n{ “_id” : 2, “age” : 100 }\n{ “_id” : 3, “age” : 45 }\n\n 单一删除\ndb.t1.remove(&#123;&quot;age&quot;:45&#125;,&#123;justOne:1&#125;)\n\n\ndb.t1.remove({“age”:45},{justOne:1})\nWriteResult({ “nRemoved” : 1 })\ndb.t1.find()\n{ “_id” : ObjectId(“64f7ec7bff2142d2a2445360”), “age” : 18 }\n{ “_id” : 2, “age” : 100 }\n{ “_id” : 4, “age” : 65 }\n{ “_id” : 5, “age” : 45 }\n\n Capped限制\n 创建限制\ndb.createCollection(&quot;t2&quot;,&#123;capped:true,size:3,max:8&#125;)\n\ndb.t2.isCapped()\n\n 检验\nfor(i=1;i&lt;9;i++) db.t2.insert(&#123;&quot;x&quot;:i&#125;)\n\n\ndb.t2.find();\n{ “_id” : ObjectId(“64f7f23fff2142d2a244536a”), “x” : 2 }\n{ “_id” : ObjectId(“64f7f23fff2142d2a244536b”), “x” : 3 }\n{ “_id” : ObjectId(“64f7f23fff2142d2a244536c”), “x” : 4 }\n{ “_id” : ObjectId(“64f7f23fff2142d2a244536d”), “x” : 5 }\n{ “_id” : ObjectId(“64f7f23fff2142d2a244536e”), “x” : 6 }\n{ “_id” : ObjectId(“64f7f23fff2142d2a244536f”), “x” : 7 }\n{ “_id” : ObjectId(“64f7f23fff2142d2a2445370”), “x” : 8 }\n\n 结论\n前面的数据被顶掉了\n pretty美化输出\ndb.t3.insert(&#123;&quot;_id&quot;:1,&quot;phone&quot;:&#123;&quot;hemophone&quot;:&quot;8617118&quot;,&quot;mobilephone&quot;:&quot;18643079329&quot;&#125;&#125;)\n\n\ndb.t3.find().pretty();\n{\n“_id” : 1,\n“phone” : {\n“hemophone” : “8617118”,\n“mobilephone” : “18643079329”\n}\n}\n\n 管道机制\n{“_id”:1,“name”:“iphone8”,“price”:3000,“type”:“电子通信”}\n","categories":["数据库","MongoDB"],"tags":["MongoDB"]},{"title":"MongoDB初步使用","url":"/MongoDB/MongoDB03/","content":" MongoDB约束命令\n 输出行数限制\ndb.t4.aggregate(&#123;$limit:4&#125;)\n\n xx升序/降序\ndb.t4.aggregate([&#123;$sort:&#123;price:-1&#125;&#125;])\n\n1：升序\n2：降序\n MapReduce\nMapReduce是一种用于分布式计算的编程模型和处理大规模数据集的方法。\nMapReduce模型的基本思想是将大规模数据集分成小块，然后并行处理这些小块数据以生成中间结果。\n\nMap阶段（映射阶段）：在这个阶段，原始数据被映射成键-值对的形式。每个映射操作都是独立的，可以在不同的计算节点上并行执行。Map操作通常用于筛选、过滤、排序和转换数据。\nReduce阶段（归约阶段）：在Map阶段之后，所有的中间结果按键分组，并将每个组的数据传递给Reduce函数进行聚合和处理。Reduce操作通常用于对数据进行汇总、计数、计算统计信息等操作。\n\n//MapReduce\n//map映射\nvar map = function()&#123;\n    emit(this.type,this.name);\n&#125;\n//reduce减少并以“，”分割\nvar reduce = function(key,values)&#123;\n    return values.join(&#x27;,&#x27;);\n&#125;\n//选项设置输出\nvar opt=&#123;out:&quot;name_list&quot;&#125;\ndb.t4.mapReduce(map,reduce,opt)\n\n\n[\n{\n“_id”: “电子设备”,\n“value”: “logi”\n},\n{\n“_id”: “服装”,\n“value”: “安踏,lining”\n},\n{\n“_id”: “电子通信”,\n“value”: “oppo,vivo,huawei,iphone8”\n},\n{\n“_id”: “饮品”,\n“value”: “康师傅”\n}\n]\n\n","categories":["数据库","MongoDB"],"tags":["MongoDB"]},{"title":"操作系统知识点阶段总结","url":"/Operate-system/01OS/","content":" 引论\n 操作系统是什么？\n操作系统是计算机系统中的核心软件之一，它是位于硬件和应用程序之间的一层软件，负责管理和控制计算机的硬件资源，并为应用程序提供一个运行环境。操作系统充当计算机系统的管理者，协调各种硬件和软件资源的分配和调度，以使计算机能够高效、可靠地运行。\n 为什么要操作系统？\n操作系统的存在是为了解决计算机资源管理和用户程序执行的复杂性。它提供了一种抽象层，使应用程序开发人员不必直接与底层硬件进行交互，从而简化了应用程序的开发和维护。此外，操作系统还负责处理多任务管理、内存管理、文件系统管理、用户接口等，为用户和应用程序提供了一个友好且高效的计算环境。\n 操作系统的特征是什么？\n操作系统具有以下几个主要特征：\n\n\n并发（Concurrency）： 能够同时处理多个任务或程序，使多个程序可以在同一台计算机上交替执行。\n（并发：同一时间段 并行：同一时刻）\n\n\n共享（Sharing）： 多个用户和应用程序可以同时访问计算机的资源，如内存、处理器、文件等。\n[操作系统的最基本特征：并发与共享]{.label .info}\n\n\n虚拟（Virtualization）： 操作系统可以为每个应用程序提供一种虚拟的环境，使其感觉自己独占了计算机资源。\n\n\n异步（asynchronous）： 异步特性指的是操作系统允许某些任务在进行的过程中，不必等待前一个任务的完成，而可以继续执行其他任务。（存在不确定性）\n[✔️操作系统的基本特征：并发、共享、虚拟、异步]{.label .success}\n\n\n抽象（Abstraction）： 操作系统通过抽象化硬件和软件资源，为应用程序提供一个更简单、一致的编程接口。\n\n\n持久性（Persistence）： 数据和程序可以被存储在持久性存储设备中，并在计算机关闭后保留下来。\n\n\n处理器管理（Processor Management）： 分配和管理处理器的时间片，以便多个任务可以轮流执行。\n\n\n内存管理（Memory Management）： 管理计算机的内存资源，包括分配、释放、虚拟内存等。\n\n\n文件系统管理（File System Management）： 管理文件的存储、组织、检索和保护。\n\n\n设备管理（Device Management）： 管理输入输出设备，使应用程序能够与设备进行交互。\n\n\n 实时操作系统和分时操作系统？\n实时操作系统（Real-time Operating System，RTOS）和分时操作系统（Time-sharing Operating System）是两种不同类型的操作系统，用于满足不同应用场景下的需求。\n 实时操作系统（RTOS）：\n实时操作系统是专门设计用于处理实时任务的操作系统。实时任务是具有严格时间要求的任务，可以分为硬实时和软实时任务。硬实时任务要求任务必须在严格的时间限制内完成，否则会导致系统错误。软实时任务也有时间要求，但对于这类任务，如果错过了截止日期，系统不会崩溃，但会影响任务的结果的实用性。\nRTOS 致力于确保任务能够按照特定的时间要求得到执行。它通常采用优先级调度策略，确保高优先级的实时任务能够在预定的时间内得到执行。\n 分时操作系统（Time-sharing Operating System）：\n分时操作系统旨在支持多用户的共享计算机系统。它允许多个用户通过终端或其他用户界面同时访问系统，每个用户似乎都在独占地使用计算机资源。分时操作系统通过分配时间片（时间片轮转）来在多个任务之间切换，每个任务在时间片结束前都能得到一段时间的执行。\n分时操作系统的目标是实现多任务的并发执行，使用户能够在几乎同时使用计算机资源，这样可以提高资源利用率和用户体验。\n[分时操作系统特征：多路性、独立性、及时性、交互性。详细解释：P10]{.label .info}\n 单道批处理系统和多道批处理系统？\n单道批处理系统（Single Batch Processing System）和多道批处理系统（Multi-Batch Processing System）是两种不同的操作系统工作方式，用于管理和执行计算机中的多个任务。\n 单道批处理系统：\n单道批处理系统是早期计算机操作系统的一种。在这种系统中，计算机只能处理一道程序，也就是一个任务，每次只有一个任务在运行。当一个任务执行完成后，才能加载并执行下一个任务。这种系统对于用户和程序员来说，操作不够灵活，因为他们必须等待当前任务执行完成才能继续工作。\n[💔单道批处理系统的缺点：系统中的的资源得不到充分利用！]{.label .danger}\n[✍️注意注意！！！单道批处理系统无并发，不能称之为 OS！！！]{.label .warning}\n 多道批处理系统：\n多道批处理系统是在单道批处理系统的基础上发展而来的。这种系统允许计算机在内存中同时加载和管理多个任务，而不需要等待前一个任务的完成。多道批处理系统将内存分割成多个区域，每个区域可以加载一个任务的代码和数据。操作系统会在任务之间进行切换，以实现多个任务的并发执行。这种方式提高了计算机的资源利用率和效率。\n OS 的类型？\n\n多道批处理系统（Multi-programming Batch System）： 多道批处理系统是一种操作系统，允许多个任务（作业）在内存中同时存在，但每个任务的执行是按照一定的调度策略轮流进行的。它旨在提高计算机的资源利用率，通过在任务之间切换以避免 CPU 空闲。每个任务通常独立运行，不需要用户干预。这种系统适用于大量任务需要批量处理，例如批量数据处理任务。\n分时系统（Time-sharing System）： 分时系统允许多个用户通过终端或用户界面同时访问计算机，每个用户似乎都在独占地使用计算机资源。系统通过快速的任务切换（时间片轮转）实现用户之间的并发执行，每个用户能够迅速交互并使用计算机。分时系统适用于多用户、交互式的环境，如图形用户界面和终端。\n实时系统（Real-time System）： 实时系统是专门设计用于处理实时任务的操作系统。实时任务具有严格的时间要求，可以分为硬实时和软实时。硬实时任务要求任务必须在严格的时间限制内完成，否则会导致系统错误。软实时任务也有时间要求，但错过截止日期不会导致系统错误。实时系统旨在确保任务能够按照特定的时间要求得到执行，适用于需要在严格时间约束下运行的应用，如飞行控制系统和医疗设备。\n\n 操作系统能干什么？\n操作系统的主要任务包括：\n\n管理计算机的硬件资源，如处理器、内存、硬盘、网络接口等。\n提供多任务管理，使多个应用程序可以同时运行。\n提供虚拟化，将物理资源抽象为多个虚拟资源，提供更好的资源利用率。\n管理文件系统，让用户能够创建、存储、组织和检索文件。\n处理输入输出，使用户和应用程序能够与外部设备进行交互。\n提供用户界面，让用户能够与计算机进行交互，如命令行界面或图形用户界面。\n\n 操作系统有哪些？\n常见的操作系统包括：\n\nWindows：微软开发的操作系统系列，如 Windows 10、Windows 11 等。\nmacOS：苹果公司开发的操作系统，用于 Mac 电脑。\nLinux：一种开源的 Unix-like 操作系统，有许多不同的发行版，如 Ubuntu、Fedora、Debian 等。\nUnix：一种经典的多用户多任务操作系统，影响了许多其他操作系统的设计。\nAndroid：基于 Linux 内核的移动设备操作系统，由谷歌开发。\niOS：苹果公司用于 iPhone 和 iPad 等移动设备的操作系统。\nOpenHarmony：分布式操作系统，由华为公司开发。\n\n 操作系统的新发展？\n操作系统领域一直在不断发展演进。一些新的趋势和发展包括：\n\n云操作系统： 针对云计算环境的操作系统，如 Google 的 Chrome OS 和微软的 Azure Sphere。\n嵌入式操作系统： 用于嵌入式系统，如物联网设备和嵌入式控制器的操作系统，如 FreeRTOS 和 Zephyr。\n容器化和微服务： 使用容器技术（如 Docker）和微服务架构的操作系统，以支持更高效的应用程序部署和管理。\n实时操作系统（RTOS）： 针对实时应用程序的操作系统，要求任务能够在严格的时间限制内得到执行。\n量子操作系统： 随着量子计算的发展，涉及管理量子资源和运行量子算法的操作系统正在探索中。\n\n这些都只是操作系统领域的一部分发展趋势，操作系统将继续适应新的硬件和应用场景，以满足不断变化的需求。\n 简单总结！\n操作系统的定义可归纳为：操作系统是控制和管理计算机系统内各种硬件和软件资源、合理组织计算机工作流程的系统软件 (或程序集合)，是用户与计算机之间的接口。\n操作系统是什么？是核心系统软件\n操作系统管什么？控制和管理系统内各资源\n操作系统有何用？扩充硬件功能，方便用户使用\n 练习\n\n\n一个作业第一次执行时用了 5 分钟，而第二次执行时用了 6 分钟，这说明了操作系统的 []{.gap} 特点。 {.quiz}\n\n并发\n共享\n虚拟\n异步 {.correct}\n{.options}\n\n+++primary 解释\n\n\n异步存在时间不确定性\n+++\n\n\n\n\n操作系统的最基本的两个特征是资源共享和 []{.gap} 。 {.quiz}\n\n多道程序设计\n程序的并发执行 {.correct}\n中断\n程序顺序执行\n{.options}\n\n\n\n单道批处理系统的主要缺点是 []{.gap} 。 {.quiz}\n\nCPU 利用率不高 {.correct}\n失去了交互性\n不具备并行性\n以上都不是\n{.options}\n\n\n\n+++primary 解释\n\n\n实际上 ABC 都是他的缺点，但是 CPU/资源利用率不高是他的主要缺点\n+++\n\n\n\n\n采用多道程序设计的系统中，系统中的程序道数越多，系统的效率越高。 {.quiz .false}\n\n\n通常将 CPU 模式分为内核态(核心态)和用户态，这样做的目的是为了提高运行速度。 {.quiz .false}\n\n\n操作系统内核能使用特权指令。 {.quiz .true}\n\n\n+++info 解析\n\n程序道数多多会导致每个程序分得的内存不够，很多程序所需的数据和代码要临时从磁盘调入内存系统会频繁的进行 I/O，使得系统效率下降！\n是为了提高安全性==（双重工作模式 P20）==\n确实对\n\n+++\n 进程\n 进程的几个基本状态\n\n就绪状态（Ready）： 进程已获得除处理器外的所需资源,等待分配处理器资源；只要分配了处理器进程就可执行.就绪进程可以按多个优先级来划分队列.例如,当一个进程由于时间片用完而进入就绪状态时,排入低优先级队列；当进程由 I/O 操作完成而进入就绪状态时,排入高优先级队列.\n运行状态(Running)： 进程占用处理器资源；处于此状态的进程的数目小于等于处理器的数目.在没有其他进程可以执行时(如所有进程都在阻塞状态),通常会自动执行系统的空闲进程.\n阻塞状态(Blocked)： 由于进程等待某种条件（如 I/O 操作或进程同步）,在条件满足之前无法继续执行.该事件发生前即使把处理机分配给该进程,也无法运行.\n\n\n 名词解释\n挂起： 从内存挂至外存\n时间片： 程序执行的一段时间\n阻塞： 发生 I/O 或者其他事件进入阻塞状态\n 扩展状态（考研）\n\n 练习\n\n\n当 []{.gap} 时，进程从执行状态转变为就绪状态。 {.quiz}\n\n进程被调度程序选中\n时间片到 {.correct}\n等待某一事件\n等待的事件发生\n{.options}\n\n\n\n在进程状态转换时，下列 []{.gap} 转换是不可能发生的。 {.quiz}\n\n就绪态 一&gt; 运行态\n运行态 一&gt; 就绪态\n运行态 一&gt; 阻塞态\n阻塞态 一&gt; 运行态 {.correct}\n{.options}\n\n\n\n进程和程序的本质区别是 []{.gap} {.quiz}\n\n前者是动态的，后者是静态的 {.correct}\n前者存储在内存，后者存储在外存\n前者在一个文件中，后者在多个文件中\n前者分时使用 CPU，后者独占 CPU\n{.options}\n\n\n\n程序运行时独占系统资源，只有程序本身能改变系统资源状态，这是指 []{.gap} {.quiz}\n\n程序顺序执行的再现性\n并发程序失去再现性\n并发程序失夫封闭性\n程序顺序执行时的封闭性 {.correct}\n{.options}\n\n\n\n不同的进程必然对应不同的程序。 {.quiz .false}\n\n\n进程状态的转换是由操作系统完成的，对用户是透明的。 {.quiz .true}\n\n\n 进程控制块（Process Control Block，PCB）\n每个正在运行或等待运行的进程都有一个对应的 PCB，它包含了进程的各种属性、状态以及与其相关的控制信息。PCB 是操作系统内部用于实现进程管理的重要数据结构之一。\n\n进程状态（Process State）：表示进程的当前状态，如运行、就绪、阻塞等。操作系统根据进程状态来进行调度和管理。\n程序计数器（Program Counter）：指向进程当前执行的指令的地址，用于恢复进程的执行状态。\n寄存器（Registers）：保存进程的寄存器值，包括通用寄存器、程序状态寄存器等。\n进程优先级（Process Priority）：用于调度器决定哪个进程将获得 CPU 执行时间。\n进程标识符（Process ID）：唯一标识一个进程的数字或字符串。\n进程所拥有的资源信息：如打开的文件列表、分配的内存空间等。\n进程的父子关系：记录进程之间的层次结构，用于实现进程间的通信和协作。\n进程的各种统计信息：如运行时间、等待时间等，用于性能分析和调优。\n\nPCB 的存在使得操作系统可以高效地进行进程的切换、调度、挂起、恢复等操作。当操作系统需要切换到另一个进程时，它可以保存当前进程的状态信息到其对应的 PCB，然后加载新进程的状态信息，从而实现进程切换。这种切换是操作系统多任务处理的基础，使得多个进程可以在单个 CPU 上共享时间，并且实现了对系统资源的合理分配和利用。\n PCB 组织方式\n\n\n\n 练习\n\n\n在 PCB 中可以直接或间接找到有关该进程的所有信息。 {.quiz .true}\n\n\n进程由 PCB 和其执行的程序、数据所组成 {.quiz .true}\n\n\n","categories":["操作系统"],"tags":["OS"]},{"title":"进程同步问题","url":"/Operate-system/03OS/","content":" 生产者-消费者问题\n在进程同步中第一个问题就是生产者-消费者问题，首先我们利用记录型信号来分析这个问题。\n好好好，我们直接伪代码解析！\n解释在行代码的上方\n//in代表下一个要写入的位置，out代表要读取的位置\nint in = 0,out = 0;\n//用于存储生产者生产的容器，可以理解为缓冲区\nitem buffer[n];\n//前两行不是重点，下面才是核心！！\n//mutex用于互斥访问共享资源，初值必须为1（可以理解为锁）\n//empty代表中转的容量，初值为最大承载容量，题里会给（可以是各种容器）\n//full是生产出的一种逻辑/实体的物质\n//总之不管是什么他是生产者生产出来的，一开始还没生产，full初值为0\nsemaphore mutex = 1,empty = n,full = 0;\n//生产者逻辑\nvoid producer()&#123;\n    //do-while死循环，不断生产，配合消费者模拟程序并发\n    do&#123;\n        //生产一个产品\n        produce an item nextp;\n        ...\n        //wait什么什么就是什么什么“--”，比如这里wait(empty)就是empty--\n        //相当于空位置减一，申请一个缓冲区\n        wait(empty);\n        //相当于加锁，申请缓冲区的使用权\n        wait(mutex);\n        //将产品放入缓冲区之中\n        buffer[in] = nextp;\n        //下一个缓冲区的地址\n        in = (in + 1) % n;\n        //signal什么什么就是什么什么“++”，比如这里signal(mutex)就是empty++\n        //解锁，相当于释放权限\n        signal(mutex);\n        //生产的东西数量加1，也就是释放缓冲区\n        signal(full);\n    &#125;while(true);\n&#125;\n//消费者逻辑\nvoid consumer()&#123;\n    //do-while死循环，不断生消费，配合生产者模拟程序并发\n    do&#123;\n        //消费者要消费一个物质，将生产者的生产的full--\n        wait(full);\n        //加锁，mutex--\n        wait(mutex);\n        //其实这里不用太深究，这里就是消费者从缓冲区拿走了一个生产者的生产的物质\n        //从缓冲区中取出产品\n        nextc = buffer[out];\n        //导向下一个缓冲区的地址\n        out = (out + 1) % n;\n        //解锁，mutex++\n        signal(mutex);\n        //消费者已经取走了，空位置empty++\n        signal(empty);\n        //消费者消费物质\n        consume the item nextc;\n        ...\n    &#125;while(true)\n&#125;\nvoid main()&#123;\n    cobegin\n    producer();consumer()\n    coend\n&#125;\n\n简单的写法：\nProducer():\n  Repeat\n    生产一个商品\n    wait(empty);\n    wait(mutex);\n    将商品送至缓冲区;\n    signal(mutex);\n    signal(full);\n  Until false\n\nComsumer():\n  Repeat\n    wait(full);\n    wait(mutex);\n    从缓冲区取走一个物品\n    singal(mutex);\n    signal(empty);\n  Until false\n\nProgram main()\n  empty,full,mutex;\n    begin\n      empty = n;\n      full = 0;\n      mutex = 1;\n      cobegin\n        producer();consumer();\n      coend\n    end\n\n 练习一下！\n桌上有个能盛得下五个水果的空子。爸爸不停地向盘中放苹果或橘子，儿子不停地从盘中取出桔子享用，女儿不停地从盘中取出苹果享用。规定三人不能同时从盘子中取放水果。使用信号量实现爸爸、儿子和女儿这三个循环进程之间的同步。\nempty = 5,orange = 0,apple = 0,mutex = 1;\nDad()&#123;\n    while(1)&#123;\n        wait(empty);\n        wait(mutex);\n        将水果放入盘中;\n        signal(mutex);\n        if(放了橘子) signal(orange);\n        else signal(apple);\n    &#125;\n&#125;\n\nSon()&#123;\n    while(1)&#123;\n        wait(orange);\n        wait(mutex);\n        拿走一个桔子;\n        signal(mutex);\n        signal(empty);\n        吃桔子;\n    &#125;\n&#125;\n\nDaughter()&#123;\n    while(1)&#123;\n        wait(apple);\n        wait(mutex);\n        拿一个苹果;\n        signal(mutex);\n        signal(empty);\n        吃苹果;\n    &#125;\n&#125;\n\n","categories":["操作系统"],"tags":["OS"]},{"title":"进程与线程","url":"/Operate-system/02OS/","content":" 什么是进程与线程\n进程（Process）和线程（Thread）是操作系统中的两个重要概念，用于管理和执行程序的执行单元。它们在多任务处理和并发执行中起着关键作用。\n 进程\n进程是计算机系统中运行的程序的实例。每个进程都有自己的内存空间、代码和数据，以及与其他进程隔离的资源。进程可以看作是一个独立的执行环境，可以执行自己的任务。每个进程都有一个唯一的进程标识符（PID），用于区分和管理不同的进程。\n一个进程可以包含多个线程，这些线程共享同一个进程的资源，如内存空间、文件句柄等。不同进程之间的通信相对复杂，通常需要使用进程间通信（IPC）机制，如管道、消息队列、共享内存等。\n 线程\n线程是进程内的执行单元，一个进程可以包含多个线程。线程共享同一个进程的代码和数据，但每个线程拥有自己的栈空间和程序计数器。因为线程共享相同的内存空间，它们之间的通信和数据共享更加方便，但也需要适当的同步控制来避免竞态条件和数据不一致问题。\n多线程的使用可以实现并发执行，提高程序的响应速度和资源利用率。常见的线程使用场景包括图形界面应用程序中的响应性、多媒体处理、网络服务器等。\n 程序并发执行的特征\n\n间断性：并发程序之间相互制约\n失去封闭性：多个程序共享全机资源，执行状态收外界因素影响\n不可在现性：程序经过多次执行后，虽然其执行时的环境和初始条件都相同，但得到的结果却各不相同\n\n 进程的特性\n\n动态性（最基本的特征）\n并发性\n独立性\n异步性：不可预知的速度\n\n 进程控制\n一般包括以下几个方面：\n\n进程创建\n进程终止\n进程阻塞与唤醒\n进程挂起与激活\n\n 进程创建\n[UNIX 下的进程创建——fork()]{.label .success}\n 消息机制\n直接通信方式一一消息缓冲队列\n这是指发送进程利用 OS 所提供的发送命令，直接把消息发送给目标进程。此时，要求发送进程和接收进程都以显式方式提供对方的标识符。通常，系统提供下述两条通信命令(原语):\nSend(Receiver, message): 发送一个消息给 Receiver\nReceive(Sender,message): 接收 Sender 发来的消息\n例如，原语 Send(P，m)表示将消息 m,发送给接收进程 P;而原语 Receive(P，m)则表示接收由 P 发来的消息 m。\n 进程通信\n进程通信实例------管道通信方式 Pipe\n:::default\nwrite(fd[1],buf,size); 将 buf 中长为 size 字符的消息送入 fd[1]口\n:::\n:::default\nread(fd[0], buf’,size); 从 fd[O]口读出 size 个字符置于 buf 中\n:::\n 进程与线程的比较\n 关于调度的基本单位\n在传统 OS 中，拥有资源、独立调度和分配的基本单位都是进程。\n在引入线程的 OS 中，线程作为调度和分派的基本单位，进程作为拥有资源的基本单位。\n在同一进程中，线程的切换不会引起进程切换，在由一个进程中的线程切换到另一个进程中的线程时，将会引起进程切换。\n 并发性\n在引入线程的操作系统中，不仅进程之间可以并发执行，而且在一个进程中的多个线程之间，也可并发执行\n 拥有资源\n进程是系统中拥有资源的一个基本单位，它可以拥有资源\n线程本身不拥有系统资源，仅有一点保证独立运行的资源\n允许多个线程共享其隶属进程所拥有的资源\n 独立性\n同一进程中的不同线程之间的独立性要比不同进程之间的独立性低得多\n 开销\n在创建或撤消进程时，OS 所付出的开销将显著大于创建或撤消线程时的开销\n线程切换的代价远低于进程切换的代价。\n同一进程中的多个线程之间的同步和通信也比进程的简单\n 支持多处理机系统\n","categories":["操作系统"],"tags":["OS"]},{"title":"英语笔记","url":"/English/xbbEnglish/","content":" 领航\n:::primary no-icon\n核心内容来自颉斌斌老师\n:::\n 阅读\n 常考的隐含假设\n\n\n\n要素\n隐含假设\n\n\n\n\n做了什么\n论述的主体有能力/条件可以做\n\n\n建议/应该去做一件事情\n这件事还没有做过，作者认为这件事合理\n\n\n为什么+观点\n观点成立\n\n\n一个措施有利，应该去做\n同时不会有更大的弊端\n\n\n一个措施不利，应该去做\n同时不会出现更大的利\n\n\n禁止做一件的事情\n这件事已经被做过\n\n\n\n 选项比错的依据\n\n答案已知且唯一\n选项具有有限性（4个）\n\n比出一个最佳选择即可，放弃求证每日一个选项的正确于错误\n但，什么是最佳？\n实际上，找对很难， 比错比比对更加容易！\n选项比错是高效的做法\n 比错的逻辑\n依据精准定位，评估一个出错率最小的选项\n比选项👉相对性👉借助其他选项\n比错👉 比较四个选项错误率的大小\n出错率最小不是完全正确，放弃追求完全正确的执念，转变为“接受最佳的选项”的新认识。\n:::primary no-icon\nWhen you have eliminated the impossible ，whatever remains，however improbable，must be the truth.\n:::\n 精准定位+选项比错\n 定位不到的解题思路\n\n反向定位（判断选项信息正误）\n取反/取非\n原命题转逆否命题\n主题主旨\n\n 写作(大作文)\n 图画作文\n 行文框架\n 第一段（2-3句）\n\n引出主题\n描述图片\n评价图片（可有可无）\n\n 第二段（5句）\n\n重申主题+分析影响\n影响1+研究调查\n影响2+名人名言\n\n 第三段\n\n升华主题\n建议措施\n展望未来（可有可无）\n\n 引出主题的句子（选一个即可）\nThe animated cartoon reminds its readers of the significance of  (主题词).\n:::primary no-icon\n看到这副生动的漫画，人们想到了（主题词）的重要性\n:::\nNothing can get us denying that (主题词), under any circumstances, tends to be of great/enormous importance.\n:::primary no-icon\n不可否认，在任何情况下，（主题词）往往很重要\n:::\nYou will not miss thinking that (主题词) is playing an indispensable role in our life and work.\n:::primary no-icon\n你一定会想到（主题词）在我们的生活和工作中的起着不可或缺的作用（背）\n:::\n 描述图片的句子\nThe picture is mainly related to the case that ( 图片主要人物/事物 ) + （doing/done…）, which can be a vivid expression of the topic.\n:::primary no-icon\n这一图片主要和…有关，该图是对主题词的生动表达（背）\n:::\n 评价图片（可有可无）\nWhat the drawer tries to convey has profound practical significance.\n:::primary no-icon\n画家试图传达的信息具有深刻的现实意义。(可以背一下)\n:::\nSimple as the picture looks, its implicit meaning should be thought–provoking.\n:::primary no-icon\n尽管图片简单，但其内涵发人深省。\n:::\nThe cartoon, at first glance, seems to be simple, but the deep meaning behind it is worth pondering.\n:::primary no-icon\n这幅漫画乍一看似乎很简单，但其深层含义值得深思。\n:::\n 重申主题+分析影响\nConsidering the significance of（主题词）, it is necessary to point out the far-reaching consequences behind it.\n:::primary no-icon\n考虑到 (主题词) 的重要性，有必要指出其背后意义深远的影响。\n:::\nThere seems to be more than one direct or indirect effect involved in ( 主 题 词 ) , which none of us can fail to notice.\n:::primary no-icon\n似乎不止一个直接或间接的影响与 (主题词) 有关，我们无法忽视这些影响。(背)\n:::\nThe effects that (主题词) can exert are obvious and within easy reach.\n:::primary no-icon\n(主题词) 产生的影响显而易见，也是不难找到的。\n:::\n 影响 1 + 调查研究\n 万能理由先行\n\n\n成长 / 成功 / 发展\n\n个人话题：the growth / success / prospect of individual\n社会话题：the development of economy / society / culture / legal system / enviroment\n\n\n\n知识 / 阅历 / 机会\n\nacquire adequate knowledge and experience\n\n\n\n效率 / 便利 / 时间\n\nefficiency / convenience\n\n\n\n健康（身体和心理） / 压力 / 愉悦\n\nphysical health / mental health\npressure / stress / stain\nbring a lot of mental and physical pleasure\n\n\n\n 分析影响\nIt is generally recognized that nothing is more important than (主题词) for (四大万能理由) .\n:::primary no-icon\n人们普遍认为 (主题词) 对 (四大万能理由) 最重要。（背）\n:::\n 调查研究\nDepending upon a rough estimate of the relevant department, nearly 86.2% of the respondents hold the same view.\n:::primary no-icon\n根据有关部门的粗略估计，近86.2%的受访者持相同观点。\nrelevant department可以替换为具体部门\n:::\nA recent Internet questionnaire of the relevant department indicates that approximately 86.2% of the respondents hold the same view.\n:::primary no-icon\n有关部门最近的一份互联网问卷显示，约 86.2%的受访者持相同观点。（背）\n:::\n 影响 2 + 名人名言\n 分析影响\n(主题词) contributes to/is conducive to/ is responsible for (四大万能理由) .\n:::primary no-icon\n(主题词) 有助于 (四大万能理由) 。\n:::\n 名人名言\nAs Envduct Karryguess, director of community service at LA College in LitLingo, puts it, “ (万能理由) , needless to say,  enormous importance for us.”\n 升华主题\nAll these factors support a rational conclusion that great importance should be attached to(主题词)\n:::primary no-icon\n所有这些因素都支持一个合理的结论，即应该对(主题词)给予重视。\n:::\n 建议措施\nEveryone should cultivate a better awareness of it and take practical actions.\n:::primary no-icon\n每个人都应更好地意识到这一点，并采取实际行动。\n:::\nStrict laws and disciplines can guarantee the smooth running of the society\n:::primary no-icon\n严格的法律和纪律可以保证社会的平稳运行。\n:::\n 展望未来（可有可无）\nOnly in this way would our life become more real and meaningful\n:::primary no-icon\n只有这样，我们的生活才能变得更加真实和富有意义。\n:::\n 原型\n\tYou will not miss thinking that (主题词) is playing an indispensable role in our life   and work.The picture is mainly related to the case that ( 图片主要人物/事物 ) + （doing/done....）, which can be a vivid expression of the topic.\n\tThere seems to be more than one direct or indirect effect involved in ( 主 题 词 ) , which none of us can fail to notice.It is generally recognized that nothing is more important than (主题词) for (四大万能理由) .A recent Internet questionnaire of the relevant department indicates that approximately 86.2% of the respondents hold the same view. (主题词) contributes to (四大万能理由) .As Envduct Karryguess, director of community service at LA College in LitLingo, puts it, “ (万能理由) , needless to say,  enormous importance for us.”\n\tAll these factors support a rational conclusion that great importance should be attached to(主题词). Everyone should cultivate a better awareness of it and take practical actions.\n\n原型净约136词\n 试写（2005. PartB. 52.）\n\tYou will not miss thinking that concern elderly is palying an indispensable role in our life and work. The picture is mainly related to the case that four children would not like to take care their father.\n\tThere seems to be more than one direct or indirect effect involed in concern elderly, which none of us can fail to notice, It is generally recognized that nothing is more important than concern elderly for bring a lot of mental and physical pleasure. A recent Internet questionnaire of the relevant department indicates that approximately 82.6% of the respondents hold the same view. concern elderly contributes to a happy old age. As Envduc Karry, director of community service at LA College in LitLingo, put it,&quot;A happy old age ,needless to say, enormous importance for elderly.&quot;\n\tAll these factors support a rational conclusion that great importance should be attached to concern elderly. Everyone should cultivate a better awareness of it and take pratical actions.\n\n试写仅为作者的练习，不是范文！不是范文！不是范文！可能不会拿到很高的分数，仅供参考。\n共161词，虽然针对2005年的要求有点捉襟见肘，但对于新标准来说还是游刃有余的。\n 图表作文\n 行文框架\n 第一段（2-3句）\n\n引出主题\n描述图表\n评价图表（可有可无）\n\n 第二段（5句）\n\n重申主题+分析原因\n原因1+研究调查\n原因2+名人名言\n\n 第三段\n\n升华主题\n建议措施\n展望未来（可有可无）\n\n 引出主题\n（主题）is clearly shown in the picture.\n:::primary no-icon\n在图表中清楚的表示了（主题）\n:::\n（主题）is what the chart tries to convey.\n:::primary no-icon\n(主题)是图表想要传达的信息\n:::\nThe chart,in an obvious way, speaks volumes about the phenomenon that (主题).   （背）\n:::primary no-icon\n这张图表显然表明了一个现象，即（主题）\n:::\nThe chart above demonstrates clearly that some changes have taken place in terms of（数据标题）\n:::primary no-icon\n上图清楚地表明，在（数据标题）方面已经发生了一些变化。\n:::\n 描述图表\n 饼状图/表格\nX accounts for the highest percentage at x% of （主题词）,followed by B at y% and C ,z%. At the bottom of the chart, k% is regarded as D.\n:::primary no-icon\nX占了（主题词）的最高比例即x%，紧接着是B占了y%，C占了z%。D占了图的最少的一部分，是k%。\n:::\n 柱状图/折线图\nThere was a （dramatic increase/a steep drop） in the number of (一个东西) from （数据）in （时间）to（数据） in （时间）.\n:::primary no-icon\n(一个东西的)数据出现了急剧的上升/严重的的下降，从（时间）的（数据）到（时间）的（数据）。\n:::\n 重申主题+分析原因\nThe reasons that caused the phenomenon are obvious and within easy reach.\n:::primary no-icon\n导致这一现象的原因是显而易见的。\n:::\nAfter observation and consideration, some reasons of （主题词） can be highlight as follows.\n:::primary no-icon\n经过观察和考虑，将一些原因列举如下。（背）\n:::\n 原因1+研究调查\n同图画作文\n 原因2+名人名言\n同图画作文\n 升华主题\n同图画作文\n 建议措施\n同图画作文\n 原型\n\tThe chart,in an obvious way, speaks volumes about the phenomenon that (主题).X accounts for the highest percentage at x% of （主题词）,followed by B at y% and C ,z%. At the bottom of the chart, k% is regarded as D.\n\tAfter observation and consideration, some reasons of （主题词） can be highlight as follows.It is generally recognized that nothing is more important than (主题词) for (四大万能理由) .A recent Internet questionnaire of the relevant department indicates that approximately 86.2% of the respondents hold the same view. (主题词) contributes to (四大万能理由) .As Envduct Karryguess, director of community service at LA College in LitLingo, puts it, “ (万能理由) , needless to say,  enormous importance for us.”\n\tAll these factors support a rational conclusion that great importance should be attached to(主题词). Everyone should cultivate a better awareness of it and take practical actions.\n\n模板净含约120词\n 文字作文\n 行文框架\n 第一段\n\n引出主题\n陈述作者观点和态度\n表明自己的观点\n\n 第二段\n\n重申主题 + 分析原因/影响\n原因1/影响1 + 例子（调查研究）\n原因2/影响2 + 例子（名人名言）\n\n 第三段\n\n升华主题\n建议措施\n展望未来（可有可无）\n\n 引出主题\nThe phenomenon mentioned in the above excerpt that （主题）is receiving more and more attention and hot discussion.\n:::primary no-icon\n上述摘录中提到的现象（主题）正受到越来越多的关注并成为热点讨论话题。\n:::\n 陈述作者观点和态度\nAs is presented in the excerpt , the author believes that… is of graet significance for …\n:::primary no-icon\n如摘录所示，作者认为…对…具有重要意义\n:::\n 表明自己的观点\nAs far as this question is concerned, I support the author’s view.\n:::primary no-icon\n关于这个问题，我支持作者的观点。\n:::\n 原因1+研究调查\n同图画作文\n 原因2+名人名言\n同图画作文\n 升华主题\n同图画作文\n 建议措施\n同图画作文\n 写作(小作文)\n 书信\n 格式\nDear_______,（&lt;-逗号）\n\t\n\t第一段_____________________________________________\n\t第二段_____________________________________________\n\t第三段_____________________________________________\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t（逗号-&gt;）\tyour sincerely，\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tLi Ming\n\n 行文结构\n 前置\n称呼\n 第一段\n\n表明身份（可有可无）+表明主题（重要）\n\n 第二段\n\n展开主题+两点论\n\n 第三段\n\n重申主题+寒暄（可有可无）\n\n 尾部\n落款\n 万能身份\nI am president of the Students’ Union\n 表明主题\nI am writing the letter to you for the purpose of (目的，从”directions“中汲取。doing sth)\n 展开主题+两点论\n参考万能主题\n 万能结尾\nPlease accept my sincere gratitude to you for reading the content above. If you need to know more about it, please contact me.\n 模板\nDear_______,\n\tI am president of the Students' Union)(此处视情况变换身份). I am writing the letter to you for the purpose of (目的，从”directions“中汲取。doing sth).\n\tOf all the points concerned. I'd like to name the most significant ones as follows. First and foremost,______(要点一).Additionally,_______(要点二).(此处可再加一个要点&quot;Not but not least&quot;)\n\tplease accept my sincere gratitude to you for reading the content above. If you need to know more about it, please contact me.\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tYours sincerely,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(name)\n\n 通知\n 备忘录\n 翻译\n 文章一\n 原文\n\tWe tend to think that friends and family members are our biggest source of connection, laughter and warmth. While that may well be true, researchers have also recently found that interacting with strangers actually brings a boost in mood and feelings of belonging that we didn’t expect.\n\tIn one series of studies, researchers instructed Chicago-area commuters using public transportation to strike up a conversation with someone near them. On average, participants who followed this instruction felt better than those who had been told to stand or sit in silence. The researchers also argued that when we shy away from casual interactions with strangers, it is often due to a misplaced anxiety that they might not want to talk to us. Much of that time, however, this belief is false. As it turns out, many people are actually perfectly willing to talk and may even be flattered to receive your attention.\n\n 逐句翻译\nWe tend to think that friends and family members are our biggest source of connection, laughter and warmth.\n:::info no-icon\n我们往往会认为朋友和家庭成员是我们情感联系、欢声笑语以及幸福温暖的最大来源。\n:::\nWhile that may well be true, researchers have also recently found that interacting with strangers actually brings a boost in mood and feelings of belonging that we didn’t expect.\n:::info no-icon\n尽管那很肯能是真的，但最近研究者们发现与陌生人交往实际上会带来我们没有想到的情绪上的增长以及感觉上的促进（让人心情愉悦、有归属感）\n:::\nIn one series of studies, researchers instructed Chicago-area commuters using public transportation to strike up a conversation with someone near them\n:::info no-icon\n在一系列研究中，研究者们让Chicago区域使用公共交通的通勤人员去努力和他们身边的人产生交流\n:::\nOn average, participants who followed this instruction felt better than those who had been told to stand or sit in silence.\n:::info no-icon\n通常来讲，相较于被告知在安静中站着或者坐着的来说，参与者们跟随着这个指示来做会感觉到更好\n:::\nThe researchers also argued that when we shy away from casual interactions with strangers, it is often due to a misplaced anxiety that they might not want to talk to us.\n:::info no-icon\n研究者们也认为，当我们在与陌生人随意的交谈而感到害羞跑开时，那通常是因为不合时宜的焦虑，那就是他们也许不想和我们讲话\n:::\nMuch of that time, however, this belief is false.\n:::info no-icon\n然而，大多数情况下，这种想法是错误的\n:::\nAs it turns out, many people are actually perfectly willing to talk and may even be flattered to receive your attention.\n:::info no-icon\n结果是，许多人实际上非常愿意交谈，甚至会因为得到你的关注而感到荣幸。\n:::\n 词汇\n\n\n\n单词\n释义\n\n\n\n\nflattered\nadj.（因受重视而）感到满意的，觉得荣幸的；（在正式场合表示感谢）不胜荣幸的，深感荣幸的 v. 奉承；自命不凡；使显得漂亮，使显得出色（flattered 的过去式和过去分词）\n\n\nmisplaced\nadj. 不合时宜的；（情感）寄托错的；放错位置的；暂时丢失的v. 乱放（而找不到），暂时丢失；错放（misplace 的过去式和过去分词）\n\n\nargued\nv. 争论；争吵；争辩；论证；说理；显示出，表明（argue 的过去式和过去分词）\n\n\ncommuters\nn. 通勤者；每日往返上班者（commuter 的复数）\n\n\nreceive\nv. 得到，收到；遭受，经受（特定待遇）；对……作出反应；接待，招待；接收（某人为成员）；接收，收听（信号）；（通过无线电）听到；购买，窝藏（赃物）；接（球）；领受（圣餐面包或葡萄酒）；接受（治疗）；形成（看法，印象）；容纳，承接\n\n\n\n 文章二\n 原文\n\tIn the late 18th century, William Wordsworth became famous for his poems about nature.And he was one of the founders of a movement called Romanticism, which celebrated the wonders of the natural world.\n\tPoetry is powerful. Its energy and rhythm can capture a reader, transport them to another world and make then see things differently. Through carefully selected words and phrases, poems can be dramatic, funny, beautiful, moving and inspiring.\n\tNo one knows for sure when poetry began but it has been around for thousands of years,even before people could write. It was a way to tell stories and pass down history. It is closely related to song and even when written it is usually created to be performed out loud. Poems really come to life when they are recited. This can also help with understanding them too, because the rhythm and sounds of the words become clearer\n\n 逐句翻译\nIn the late 18th century, William Wordsworth became famous for his poems about nature.\n:::info no-icon\n在18世纪后期，William Wordsworth因他的有关自然的诗集而远近闻名。\n:::\nAnd he was one of the founders of a movement called Romanticism, which celebrated the wonders of the natural world.\n:::info no-icon\n他是万千浪漫主义运动奠基者的一员，专为自然世界而歌颂。\n:::\nPoetry is powerful.\n:::info no-icon\n诗歌满含力量。\n:::\nIts energy and rhythm can capture a reader, transport them to another world and make then see things differently.\n:::info no-icon\n这种能量和神话能够俘获读者芳心，带领他们进入另外一个世界，使之他们以不同的眼光看待世界。\n:::\nThrough carefully selected words and phrases, poems can be dramatic, funny, beautiful, moving and inspiring.\n:::info no-icon\n通过在词汇和语段上的精挑细选，诗歌可以表现出戏剧的，有趣的、华丽的、感动的以及鼓舞人心的情感。\n:::\nNo one knows for sure when poetry began but it has been around for thousands of years,even before people could write.\n:::info no-icon\n没有人确切地知道诗歌始于何时，但其确实萦绕千年之久，甚至在人们学会书写之前就已经出现了。\n:::\nIt was a way to tell stories and pass down history.\n:::info no-icon\n这曾经是讲述故事和传承过去历史的一种方式\n:::\nIt is closely related to song and even when written it is usually created to be performed out loud.\n:::info no-icon\n他与歌曲密切相关，即使写成它，它通常也是为了大声表演而创作的。\n:::\nPoems really come to life when they are recited.\n:::info no-icon\n诗歌吟诵时才正真背赋予了生命。\n:::\nThis can also help with understanding them too, because the rhythm and sounds of the words become clearer\n:::info no-icon\n这也有助于理解它们，因为单词的节奏和声音变得更加清晰\n:::\n 文章三\n 原文\n\tWith the smell of coffee and fresh bread floating in the air, stalls bursting with colorful vegetables and tempting cheeses, and the buzz of friendly chats, farmers’ markets are a feast for the senses. They also provide an opportunity to talk to the people responsible for growing or raising your food, support your local economy and pick up fresh seasonal produce —— all at the same time.\n\n\tFarmers’ markets are usually weekly or monthly events, most often with outdoor stalls, which allow farmers or producers to sell their food directly to customers. The size or regularity of markets can vary from season to season, depending on the area's agricultural calendar, and you’re likely to find different produce on sale at different times of the year. By cutting out the middlemen, the farmers secure more profit for their produce. Shoppers also benefit from seeing exactly where-and to who- their money is going.\n\n 逐句翻译\nWith the smell of coffee and fresh bread floating in the air, stalls bursting with colorful vegetables and tempting cheeses, and the buzz of friendly chats, farmers’ markets are a feast for the senses.\n:::info no-icon\n伴随着漂浮在空气中的咖啡和新鲜面包香气，摊位上摆放着五颜六色的水果和楚楚可人的奶酪，以及友好的聊天嘈杂而又喧闹着，农贸市场上显现出一幅美好的景象（感官盛宴）。\n:::\nThey also provide an opportunity to talk to the people responsible for growing or raising your food, support your local economy and pick up fresh seasonal produce— all at the same time.\n:::info no-icon\n这也为你提供了一种既能去人直面交流的机会，而这个人正是负责你食物的播种与采集的人，也能去支持本地经济，与此同时挑选新鲜的季节性产品的——这一切同时发生。\n:::\nFarmers’ markets are usually weekly or monthly events, most often with outdoor stalls, which allow farmers or producers to sell their food directly to customers.\n:::info no-icon\n农贸市场通常是每周或者每月举行，最常见的是户外摊位，届时允许农民或者生产者去直接面向顾客售卖他们的商品。\n:::\nThe size or regularity of markets can vary from season to season, depending on the area’s agricultural calendar, and you’re likely to find different produce on sale at different times of the year.\n:::info no-icon\n市场的规模或者是规律很大程度上是适时而异，依托于当地农历，你很可能会发现与众不同的产品售卖于一年的不同时间。\n:::\nBy cutting out the middlemen, the farmers secure more profit for their produce.\n:::info no-icon\n鉴于没有中间商赚差价，因此农民们可以获得来自于他们产品的更多利润。\n:::\nShoppers also benefit from seeing exactly where — and to who — their money is going.\n:::info no-icon\n购买者也受益于他们可以更加准确的认识到，他们的钱支付给谁并流向何方。\n:::\n 词汇\n\n\n\n单词\n释义\n\n\n\n\nstalls\nn. 正厅前排座位（stall 的复数形式） v. 使（汽车等）抛锚（stall 的第三人称单数形式）\n\n\nbursting\nadj. 充满的；渴望的v. 爆炸；猛冲；涨满；充满感情；突然爆发（感情）；猛然打开；决堤；突然开始活跃（burst 的现在分词）\n\n\ntempting\nadj. 诱人的，吸引人的v. 引诱，诱惑；怂恿，利诱；冒……的风险（tempt 的现在分词）\n\n\nbuzz\nv. 发嗡嗡声，发蜂鸣声；匆忙走动；充满嘈杂声；充满想法；繁忙，充满活力；&lt;非正式&gt;给（某人）打电话；&lt;非正式&gt;（飞机）低空飞过；（用蜂鸣器）呼叫n. 嗡嗡声，蜂鸣声；叽叽喳喳声，嘈杂声；&lt;非正式&gt;兴奋，快乐；热闹有趣的气氛，时尚氛围；&lt;非正式&gt;电话；&lt;非正式&gt;流言，传闻adj. （词语、想法或活动）时髦的\n\n\nfeast\nn. 宴会，筵席；盛会，特别的享受；宗教节日v. 饱餐，尽情享用；宴请，设宴招待（某人）\n\n\nregularity\nn. 规律性，经常性；匀称，端正；有规则的东西，有规律的事物\n\n\nmiddlemen\nn. 中间商；经纪商\n\n\nsecure\nadj. 稳固的，可靠的；严密把守的，牢固的；安全的，稳妥的；（对自己和自己的能力）有自信的；感到有保障的，没有顾虑的；固定住的，系牢的；秘密的v. （尤指经过努力而）获得，得到；使安全，保护；缚牢，将（某物）固定；确保，保证；为（债务或贷款）作抵押，作保；（外科）压迫（血管）止血；停止工作；船抛锚\n\n\n\n 完型\n 完型理论\n\n其本质上仍然是比错\n亟需打破传统思维，摒弃完美理解\n追求文章内部的一致性\n主旨一致\n\n 局部一致性+全局一致性\n\n上下文语义一致\n注意逻辑合理（逻辑改变方向还是保持方向？）（二分法）\n\n","categories":["笔记","考研英语笔记"],"tags":["英语"]},{"title":"操作系统细碎知识点及公式速记","url":"/Operate-system/KnowledgeSummary/","content":" 操作系统细碎知识点总结\n\n现代操作系统两个最基本特征：[并发]{.label .primary}  与  [共享]{.label .primary}\nOS主要特征是：[并发]{.label .primary}、[共享]{.label .primary}、[虚拟]{.label .primary} 以及 [异步]{.label .primary}\n操作系统出现的标志：[多道程序设计]{.label .primary} 以及 [分时操作系统]{.label .primary} 的出现\n高级调度：[作业调度]{.label .primary}\n中级调度：[内存调度]{.label .primary}\n低级调度：[进程调度]{.label .primary}\n多级存储分为三部分：[高速缓存]{.label .primary}、[内存/主存]{.label .primary} 以及 [外存]{.label .primary}\n进程的调度时机：[时间片完]{.label .primary}、[正常结束]{.label .primary}、[P操作]{.label .primary}、[IO请求]{.label .primary} 以及 [优先级抢占]{.label .primary}\n文件系统的功能：[按名存取]{.label .primary}、[存储空间管理]{.label .primary}、[文件共享与保护]{.label .primary} 以及 [文件操作]{.label .primary}\nSPOOLING系统的组成：[输入进程]{.label .primary}、[输入缓冲区]{.label .primary}、[输入井]{.label .primary}、[输出进程]{.label .primary}、[输出缓冲区]{.label .primary} 以及 [输出井]{.label .primary}\n进程的结构特征（有争议/不严谨）：[独立性]{.label .primary}、[动态性]{.label .primary}、[异步性]{.label .primary} 以及 [并发性]{.label .primary}\n虚拟存储器的主要特征：[多次性]{.label .primary}、[对换性]{.label .primary} 以及 [虚拟性]{.label .primary}\n所学的进程调度算法中，最中庸、最能让长作业和短作业都满意的调度算法是：[多级反馈队列调度算法]{.label .primary}\n进程是[资源分配]{.label .primary}的基本单位，线程是[调度]{.label .primary}的基本单位\n程序的几种装入方式：[静态装入]{.label .primary}、[可重定位装入]{.label .primary} 以及 [动态运行时装入]{.label .primary}\n程序的几种链接方式：[静态链接]{.label .primary}、[装入时动态链接]{.label .primary} 以及 [运行时动态链接]{.label .primary}\n程序的顺序执行的特征：[顺序性]{.label .primary}、[封闭性]{.label .primary} 以及 [结果可再现性]{.label .primary}\n请求分页式虚拟存储系统必须至少具有三种硬件支持，即[页表机制]{.label .primary}、[缺页中断机构]{.label .primary} 以及 [地址变换机构]{.label .primary}\n程序并发执行的特性有：[间断性]{.label .primary}、[失去封闭性]{.label .primary}、[失去结果可再现性]{.label .primary}\n进程控制块的信息有：[进程标识符]{.label .primary}、[处理机状态]{.label .primary}、[进程控制信息]{.label .primary}、[进程调度信息]{.label .primary}\n扇区是磁盘空间管理的最基本单位，其物理地址是由：[柱面号]{.label .primary}、[扇区号]{.label .primary}、[磁头号]{.label .primary}三部分组成的\n火车站售票系统属于[实时系统]{.label .primary}\nI/O管制程序的主要功能是管理[设备、控制器和通道]{.label .primary}的状态信息\n依据信号量的发展过程，可将信号量分为四种：[and型信号量]{.label .primary}、[信号量集]{.label .primary}、[互斥信号量]{.label .primary} 以及 [整型信号量]{.label .primary}\n进程通信类型有：[共享存储器系统]{.label .primary}、[消息传递系统]{.label .primary}、[管道通信]{.label .primary} 以及 [异步阻塞通信]{.label .primary}\n分段存储管理的优点有：[方便编程]{.label .primary}、[信息共享]{.label .primary}、[动态增长]{.label .primary}、[动态链接]{.label .primary}以及 [信息保护]{.label .primary}\n[成批性]{.label .primary}是批处理操作系统的主要特征，不是分时系统的特性\n推动操作系统发展的主要动力有：[不断提高计算机资源利用率]{.label .primary}、[方便用户]{.label .primary}、[器件的不断更新换代]{.label .primary} 以及 [计算机体系结构的不断发展]{.label .primary}\n同步机制应当遵循的规则有：[空闲让进]{.label .primary}、[让权等待]{.label .primary}、[忙则等待]{.label .primary} 以及 [有限等待]{.label .primary}\n系统中各个进程相互制约的关系称为[同步]{.label .primary}\n对于死锁，一般应考虑死锁的[预防]{.label .primary}、[检测]{.label .primary}、[避免]{.label .primary} 以及 [解除]{.label .primary}\n典型的银行家算法是属于[死锁避免]{.label .primary}，破坏环路等待条件是属于[死锁预防]{.label .primary}，剥夺资源是属于[死锁解除]{.label .primary}\n操作系统向用户提供了两种接口分别是[用户接口]{.label .primary}和[系统接口]{.label .primary}\n设别分别配应保证设备有[高利用率]{.label .primary}和避免[死锁]{.label .primary}\n对于操作系统而言，打开文件广义指令的主要作用是装入[文件目录项]{.label .primary}\n为了实现多道程序设计，计算机在硬件方面必须提供两种支持，他们分别是[中断]{.label .primary}和[通道]{.label .primary}\n多道程序设计给存储管理提出了新课题，应考虑的三个问题是[存储分配]{.label .primary}、[虚存管理]{.label .primary}、[存储保护]{.label .primary}\n操作系统的内核应提供[终端管理]{.label .primary}、[短程管理]{.label .primary}、[原语管理]{.label .primary}三方面的功能\n死锁产生的主要原因为[资源竞争]{.label .primary}和[进程非法推进]{.label .primary}\n设备分配外部设备时。先分配[设备]{.label .primary}，再分配[控制器]{.label .primary}，再分配[通道]{.label .primary}\n可用于文件系统管理空闲磁盘块的数据结构是[位示图]{.label .primary}、[空闲盘块链]{.label .primary}、[文件分配表FAT]{.label .primary}\n[索引文件]{.label .primary}既利于文件的动态增长，也适合随机访问。\n[Peterson算法]{.label .primary}实现互斥访问，[swap指令]{.label .primary}与[TestAndSet指令]{.label .primary}实现忙则等待，[信号量算法]{.label .primary}实现让权等待，[自旋锁]{.label .primary}实现空闲让进\n银行家算法破坏了死锁四个必要条件的[循环等待条件]{.label .primary}\n死锁的四个条件为：[互斥条件]{.label .primary}、[请求与保持条件]{.label .primary}、[不可剥夺条件]{.label .primary} 以及 [循环等待条件]{.label .primary}\n\n\n 常用公式速记\n 进程调度有关公式\nT周转时间=T结束时间−T提交时间\n\\begin {array}{c}\nT_{\\text{周转时间}}=T_{\\text{结束时间}}-T_{\\text{提交时间}}\n\\end {array}\nT周转时间​=T结束时间​−T提交时间​​\nT带权周转时间=T周转时间T运行时间\n\\begin {array}{c}\nT_{\\text{带权周转时间}}=\\frac{T_{\\text{周转时间}}}{T_{\\text{运行时间}}}\n\\end {array}\nT带权周转时间​=T运行时间​T周转时间​​​\nT平均周转时间=∑i=1nTi周转时间n\n\\begin {array}{c}\nT_{\\text{平均周转时间}}=\\frac{\\sum_{i=1}^{n}  {T_{i\\text{周转时间}}}}{n}\n\\end {array}\nT平均周转时间​=n∑i=1n​Ti周转时间​​​\nT平均带权周转时间=∑i=1nTi带权周转时间n\n\\begin {array}{c}\nT_{\\text{平均带权周转时间}}=\\frac{\\sum_{i=1}^{n}  {T_{i\\text{带权周转时间}}}}{n}\n\\end {array}\nT平均带权周转时间​=n∑i=1n​Ti带权周转时间​​​\nR响应比=T当前周转时间T运行时间=T当前等待时间T运行时间+1\n\\begin {array}{c}\nR_{\\text{响应比}}=\\frac{T_{\\text{当前周转时间}}}{T_{\\text{运行时间}}}=\\frac{T_{\\text{当前等待时间}}}{T_{\\text{运行时间}}}+1\n\\end {array}\nR响应比​=T运行时间​T当前周转时间​​=T运行时间​T当前等待时间​​+1​\n 典型例题\n 逻辑地址转物理地址\n已知某分页系统，内存容量为64KB，页面大小为1KB，对一个4页大的作业，其0、1、2、3页分别被分配到内存的 2、4、6、7 块中。\n将十进制的逻辑地址 1023变换为物理地址。\n内存容量64KB——216\n\\begin {array}{c}\n内存容量64KB ——2^{16}\n\\end {array}\n内存容量64KB——216​\n页面大小为1KB——210\n\\begin {array}{c}\n页面大小为1KB——2^{10}\n\\end {array}\n页面大小为1KB——210​\n共有216210=26页\n\\begin {array}{c}\n共有\\frac{2^{16}}{2^{10}} =2^{6}页\n\\end {array}\n共有210216​=26页​\n\n\n\n页\n块\n\n\n\n\n0\n2\n\n\n1\n4\n\n\n2\n6\n\n\n3\n7\n\n\n\n1023(10)=001111111111(2)\n\\begin {array}{c}\n1023_{(10)} =0011 1111 1111_{(2)}\n\\end {array}\n1023(10)​=001111111111(2)​​\n转换后101111111111(2)=3,071(10)\n\\begin {array}{c}\n转换后 1011 1111 1111_{(2)}=3,071_{(10)}\n\\end {array}\n转换后101111111111(2)​=3,071(10)​​\n","categories":["操作系统"],"tags":["OS"]},{"title":"操作系统PV代码题速记","url":"/Operate-system/OSPVCode/","content":" 操作系统PV代码题速记\n进程P向m个进程Q1、Q2、Q3…Qm发送消息，进程P发消息到缓冲区，只有所有的Q进程都接收到消息后，进程P才能继续向缓冲区放消息，请写出PV操作逻辑。\nsemaphore mutex=1, T[i]=0, notHave=1;\n//mutex是缓冲区互斥锁，T[i]是Qi进程完成数组，是一个自阻塞数组\n//notHave是缓冲区是否为空，1就是空，初始没有信息\nint R=0;\n//R是一个计数器\nProcess_P()&#123;\n    while(1)&#123;\n        P(notHave);\n        P(mutex);\n        放入缓冲区;\n        for(int i=1;i&lt;=m;i++)&#123;\n            P(T[i]);\n        &#125;\n        R=0;\n        V(mutex);\n    &#125;\n&#125;\nProcess_Qi()&#123;\n    while(1)&#123;\n        P(T[i]);\n        P(mutex);\n        取消息;\n        R=R+1;\n        if(R==m)&#123;\n            V(notHave);\n        &#125;\n        V(mutex);\n    &#125;\n&#125;\n\n 单生产者-单消费者\n一组生产者进程和一组消费者进程共享一个初始为空、大小为n的缓冲区:只有缓冲区没满时，生产者才能把消息放入缓冲区，否则必须等待:只有缓冲区不空时，消费者才能从中取出消息，否则必须等待。由于缓冲区是临界资源，它只允许一个生产者放入消息，或一个消费者从中取出消息。\nsemaphore full = 0;\t//已生产\nsemaphore empty = n;//总可用空间\nsemaphore mutex = 1;//互斥锁\nprocuder()&#123;\n    while(1)&#123;\n        生产商品;\n        P(empty);\t//假取空间\n        P(mutex);\t//加锁\n        放入缓冲区;\t\n        V(mutex);\t//解锁\n        V(full);\t//告知以生产\n    &#125;\n&#125;\nconsumer()&#123;\n    while(1)&#123;\n        P(full);\n        P(mutex);\n        从缓冲区取;\n        V(mutex);\n        V(empty);\n        消费商品;\n    &#125;\n&#125;\n\n 多生产者-多消费者\n桌子上有一个盘子，每次只能向其中放入一个水果;爸爸专向盘子中放革果，妈妈专向盘子中放橘子;儿子专等吃盘子中的橘子，女儿专等吃盘子中的革果，只有盘子为空时，爸爸或妈妈才可向盘子中放一个水果，仅当盘子中有自己需要的水果时，儿子或女儿可以从盘子中取出。\nsemaphore mutex = 1;\t//盘子锁\nsemaphore apple = 0;\nsemaphore orange = 0;\n\ndad()&#123;\n    while(1)&#123;\n        准备苹果;\n        P(mutex);\n        放苹果;\n        V(apple);\n    &#125;\n&#125;\n\nmom()&#123;\n    while(1)&#123;\n        准备橘子;\n        P(mutex);\n        放橘子;\n        V(orange);\n    &#125;\n&#125;\n\nson()&#123;\n    while(1)&#123;\n        P(orange);\n        拿苹果;\n        V(mutex);\n        吃苹果;\n    &#125;\n&#125;\n\ndaughter()&#123;\n    while(1)&#123;\n        P(apple);\n        拿苹果;\n        V(mutex);\n        吃苹果;\n    &#125;\n&#125;\n\n\n 读者写者问题（读优先）\n有读者和写者两组并发进程，共享一个文件，当两个或以上的读进程同时访问共享数据时不会产生副作用。但若某个写进程和其他进程（读进程或写进程）同时访问共享数据时则可能导致数据不一致的错误。\n因此要求:\n\n允许多个读者可以同时对文件执行读操作。\n只中许一个写者往文件中写信息。\n任一写者在完成写操作之前不允许其他读者或写者工作。\n写者执行写操作前，应让已有的读者和写者全部退出。\n\nsemaphore rw = 1;\t//r-w 读者在读时，不容许其他写者加入。写者在写时，不容许其他读者写者加入。\nint count = 0;\t\t//记录已加入的读者的数量，如果不为0，那么其中必有读者，那么其他读者随便加入。\nsemaphore count_mutex = 1;\nwriter()&#123;\n    while(1)&#123;\n        P(rw);\n        写文件;\n        V(rw);\n    &#125;\n&#125;\n\nreader()&#123;\n    while(1)&#123;\n        P(count_mutex);\n        if(count == 0)&#123;\t//如果不为0，那么其中必有读者，那么其他读者随便加入。\n            P(rw);\t//读者为排除其他写者\n        &#125;\n        count++;\t//在线加1\n        V(count_mutex);\n        读文件;\n        P(count_mutex);\n        count--;\t//在线减1\n        if(count == 0)&#123;\n            V(rw);\n        &#125;\n        V(count_mutex);\n        \n    &#125;\n&#125;\n\n有可能造成写饥饿\n 读者写者问题（写优先）\nsemaphore rw = 1;\nint count = 0;\nsemaphore w = 1;\t//写进程霸占\n\nsemaphore count_mutex = 1;\nwriter()&#123;\n    while(1)&#123;\n        P(w);\t//写进程启用霸占\n        P(rw);\n        写文件;\n        V(rw);\n        V(w);\n    &#125;\n&#125;\n\nreader()&#123;\n    while(1)&#123;\n        P(w);\t//写进程阻碍读进程进入\n        P(count_mutex);\n        if(count == 0)&#123;\n            P(rw);\n        &#125;\n        count++;\n        V(count_mutex);\n        V(w);\n        读文件;\n        P(count_mutex);\n        count--;\n        if(count == 0)&#123;\n            V(rw);\n        &#125;\n        V(count_mutex);\n        \n    &#125;\n&#125;\n\n 吸烟者问题\n假设一个系统有三个抽烟者进程和一个供应者进程。每个抽烟者不停地卷烟并抽掉它，但要卷起并抽掉一支烟，抽烟者需要有三种材料:烟草、纸和胶水。三个抽烟者中，第一个拥有烟草，第二个拥有纸，第三个拥有胶水。供应者进程无限地提供三种材料，供应者每次将两种材料放到桌子上，拥有剩下那种材料的抽烟者卷一根烟并抽掉它，并给供应者一个信号告诉已完成，此时供应者就会将另外两种材料放到桌上，如此重复(让三个抽烟者轮流地抽烟)。\nsemaphore offer1 = 0;\nsemaphore offer2 = 0;\nsemaphore offer3 = 0;\nsemaphore isFinish = 1;\nint i = 0;\nprovider()&#123;\n    while(1)&#123;\n        P(isFinish);\n    \tif(i%3 == 0)&#123;\n        \tV(offer1);\n    \t&#125;else if(i%3 == 1)&#123;\n        \tV(offer2);\n    \t&#125;else&#123;\n        \tV(offer3);\n    \t&#125;\n    &#125;\n    \n&#125;\n\nsomker1()&#123;\n    while(1)&#123;\n        P(offer1);\n    \t制作香烟;\n    \t吸烟;\n        V(isFinish);\n    &#125; \n&#125;\n\nsomker2()&#123;\n    while(1)&#123;\n        P(offer2);\n    \t制作香烟;\n    \t吸烟;\n        V(isFinish);\n    &#125; \n&#125;\n\nsomker3()&#123;\n    while(1)&#123;\n        P(offer3);\n    \t制作香烟;\n    \t吸烟;\n        V(isFinish);\n    &#125; \n&#125;\n\n PV审题流程\n\n看有几个进程\n进程内部的步骤\n是否需要while（1）\n哪里有P？有P必有V\n连续P是否会死锁\n定义信号量，写注释\n\n 题目实操\n 2020 统考真题\n【2020 统考真题】现有5个操作A、B、C、D和E，操作C必须在A和B完成后执行操作E必须在C和D完成后执行，请使用信号量的 wait（）、signal（）操作（P、V 操作）描述上述操作之间的同步关系，并说明所用信号量及其初值。\nsemaphore ac = 0;\nsemaphore bc = 0;\nsemaphore ce = 0;\nsemaphore ce = 0;\n\nA()&#123;\n    执行任务;\n    V(ac);\n&#125;\n\nB()&#123;\n    执行任务;\n    V(bc);\n&#125;\n\nC()&#123;\n    P(ac);\n    P(bc);\n    执行任务;\n    V(ce);\n&#125;\n\nD()&#123;\n    执行任务;\n    V(de);\n&#125;\n\nC()&#123;\n    P(ce);\n    P(de);\n    执行任务;\n&#125;\n\n【2009 统考真题】三个进程P1,P2,P3,互斥使用一个包含N(N&gt;0)个单元的缓冲区。\n\nP1每次用 produce()生成一个正整数并用 put()送入缓冲区某一空单元;\nP2每次用 getodd()从该缓冲区中取出一个奇数并用 countodd()统计奇数个数;\nP3每次用geteven()从该缓冲区中取出一个偶数并用 counteven()统计偶数个数。\n\n请用信号量机制实现这三个进程的同步与互斥活动，并说明所定义的信号量的含义(要求用伪代码描述)。\nsemaphore mutex = 1;\nsemaphore odd = 0;\nsemaphore even = 0;\n\nP1()&#123;\n    while(1)&#123;\n        int num = produce();\n        if(num%2 == 0)&#123;\n            P(mutex);\n            put();\n            V(mutex);\n            V(even);\n        &#125;else&#123;\n            P(mutex);\n            put();\n            V(mutex);\n            V(odd);\n        &#125;\n    &#125;\n&#125;\n\nP2()&#123;\n    while(1)&#123;\n        P(odd);\n        P(mutex);\n        getodd();\n        V(mutex);\n        countodd()\n    &#125;\n&#125;\n\nP2()&#123;\n    while(1)&#123;\n        P(even);\n        P(mutex);\n        geteven();\n        V(mutex);\n        counteven()\n    &#125;\n&#125;\n\n 2011 统考真题\n【2011统考真题】某银行提供1个服务窗口和10个供顾客等待的座位。顾客到达银行时若有空座位，则到取号机上领取一个号，等待叫号。取号机每次仅允许一位顾客使用。当营业员空闲时，通过叫号选取一位顾客，并为其服务。\nsemaphore count = 10;\nsemaphore mutex = 1;\nsemaphore server = 1;\nsemaphore full = 0;\ncustomer()&#123;\n    while(1)&#123;\n        P(count);\n        P(mutex);\n    \t从取号机取号;\n    \tV(mutex);\n    \tV(full);\n    \t等待叫号;\n    \tP(server);\n    \t获取服务;\n    &#125;\n\n    \n&#125;\nclerk()&#123;\n    while(1)&#123;\n        P(full);\n        叫号;\n        V(server);\n        服务;\n        V(count);\n    &#125;    \n&#125;\n\n\n 809-24年 真题\n崂山有一景点称作上清宫，游客在上清宫游玩后可以在宫门口免费搭乘轿车游览其他景区，游览后再返回宫门口。已知风景区游览轿车总量有M量，游客总数为N，约定:\n\n每辆轿车限乘一位游客;\n如果有空闲的轿车，应当允许想游览的游客乘坐;\n无空闲轿车时,游客只能排队等待;\n若没有想游览的游客，空闲的轿车也要等待。\n\n试利用P、V操作实现在上清宫门口乘车点:N个游客进程和M辆轿车进程的同步操作过程。\nsemaphore mutex = 1;\nsemaphore car = 0; \t\t// 空闲车辆数量\nsemaphore customer = 0; // 等待乘车的游客数量\n\n//M个车辆进程\ncar()&#123;\n    while(1)&#123;\n        等待游客;\n        P(customer);\t// 等待有游客请求乘车\n        P(mutex);\t\t// 进入临界区\n        V(car);\t\t\t// 提供一辆空闲轿车\n        V(mutex);\t\t// 离开临界区\n        接待游览;\n        释放游客;\n\n    &#125;\n&#125;\n\n//N个游客进程\ncustomer()&#123;\n    while(1)&#123;\n        等待车辆;\n        P(car);\t\t\t// 等待空闲轿车\n       \tP(mutex);\t\t// 进入临界区\n        V(customer)\t\t// 通知有游客请求乘车\n        V(mutex);\t\t// 离开临界区\n        上车游览;\n        离开;\n    &#125;\n&#125;\n\n\n 典例 和尚打水\n某寺庙有小和尚、老和尚若干，有一水缸，由小和尚提水入缸供老和尚饮用。水缸可容10桶水，水取自同一井中。水井径窄，每次只能容一个桶取水。水桶总数为3个。每次入缸取水仅为1桶水，且不可同时进行，试给出有关从缸取水、入水的算法描述。\nsemaphore total_empty = 10;\nsemaphore total_full = 0;\nsemaphore mutex_jing = 1;\nsemaphore mutex_gang = 1;\ndemaphore bucket = 3;\nOld ()&#123;\n    P(total_full);\n    P(bucket);\n    P(mutex_gang);\n    从缸里取水;\n    V(total_empty);\n    V(mutex_gang);\n\t喝水;\n    V(bucket);\n&#125;\n\nYoung()&#123;\n    P(total_empty);\n    P(bucket)\n    P(mutex_jing);\n    从水井取水;\n    V(mutex_jing);\n    P(mutex_gang);\n\t倒进缸里;\n    V(total_full);\n    V(mutex_gang);\n    V(bucket);\n    \n&#125;\n\n 典例 公交车售票\n设公共汽车上，司机和售票员的活动分别是：\n\n司机的活动：启动车辆；正常行车；到站停车；\n售票员的活动：关车门；售票；开车门；\n\n请用记录型信号量机制实现上述问题的同步。\nsemaphore door_mutex = 1;\nsemaphore start_mutex = 0;\n\ndriver()&#123;\n    while(1)&#123;\n        P(start_mutex);\n        启动车辆;\n        正常行车;\n        到站停车;\n        V(door_mutex);\n    &#125;\n&#125;\nbusman()&#123;\n    while(1)&#123;\n        P(door_mutex);\n        开车门;\n        售票;\n        关车门;\n        V(start_mutex);\n    &#125;\n&#125;\n\n\n 典例 独木桥问题\n请用信号量解决以下的“过独木桥”问题：同一方向的行人可连续过桥，当某一方向有人过桥时，另一方向的行人必须等待；当某一方向无人过桥时，另一方向的行人可以过桥。\nsemaphore isOK = 1;\nint toACount = 0;\nint toBCount = 0;\nsemaphore toACount_mutex = 1;\nsemaphore toBCount_mutex = 1;\n\n\ntoA()&#123;\n    P(toACount_mutex);\n    if(toACount == 0)&#123;\n        P(isOK);\n    &#125;\n    toACount++;\n    V(toACount_mutex);\n    上桥走向A;\n    P(toACount_mutex);\n    toACount--;\n    if(toACount == 0)&#123;\n        V(isOK);\n    &#125;\n    V(toACount_mutex);\n&#125;\n\ntoB()&#123;\n    P(toBCount_mutex);\n    if(toBCount == 0)&#123;\n        P(isOK);\n    &#125;\n    toBCount++;\n\tV(toBCount_mutex);\n    上桥走向B;\n    P(toBCount_mutex);\n    toBCount--;\n    if(toBCount == 0)&#123;\n        V(isOK);\n    &#125;\n\tV(toBCount_mutex);\n&#125;\n\n\n 典例 阅览室问题\n有一阅览室，共有100个座位。为了很好利用它，读者进入时必须先在登记表上进行登记。该表表目设有座位号和读者姓名；离开时再将其登记项摈除。试问：\n\n为描述读者的动作，应设哪几个进程？它们之间的关系是什么？\n试用P、V操作描述进程之间的同步或算法。\n\n答：要设读者进入进程和离开进程，总计两个进程。他们是互斥关系。\nsemaphore seats = 100;\nsemaphore readers = 0;\nsemaphore table_mutex = 1;\n\ngetIn()&#123;\n    while(1)&#123;\n        P(seats);\n        P(table_mutex);\n        登记;\n        V(table_mutex);\n        V(readers);\n    &#125;\n&#125;\ngetOut()&#123;\n    while(1)&#123;\n        P(readers);\n        P(table_mutex);\n        移除登记;\n        V(table_mutex);\n        V(seats);\n    &#125;\n&#125;\n\n 典例 考试问题\n《操作系统》课程的期末考试即将举行，假设把学生和监考老师都看作进程，学生有N人，教师1人。考场门口每次只能进出一个人，进考场的原则是先来先进。当N个学生都进入了考场后，教师才能发卷子。学生交卷后即可离开考场，而教师要等收上来全部卷子并封装卷子后才能离开考场。\n(1)问共需设置几个进程?\n(2)请用P、V操作解决上述问题中的同步和互斥关系。\n答：共设置N+1个进程，一个教师进程，N个学生进程\nint studentCount = 0;\nint onPageCount = 0;\nsemaphore door_mutex = 1;\nsemaphore startTest = 0;\nsemaphore endTest = 0;\nsemaphore readyToTest = 0;\nsemaphore onPageCount_mutex = 1;\nsemaphore allStudentsOK = 0;\n\nteacher()&#123;\n    P(door_mutex);\n    进门;\n    V(door_mutex);\n    P(readyToTest);\n    发卷子;\n    V(startTest);\n    监考;\n    P(allStudentsOK);\n    整理试卷;\n    P(door_mutex);\n    出门;\n    V(door_mutex);\n&#125;\n\nstudents()&#123;\n    P(door_mutex);\n    进门;\n    studentCount++;\n    if(studentCont == N)&#123;\n        V(readyToTest);\n    &#125;\n    V(door_mutex);\n    P(startTest);\n    开始答题;\n    P(onPageCount_mutex);\n    交卷;\n    onPageCount++;\n    if(onPageCount == N)&#123;\n        V(allStudentsOK);\n    &#125;\n    V(onPageCount_mutex);\n    P(door_mutex);\n    出门;\n    studentCount--;\n    V(door_mutex);\n&#125;\n\n\n          \n          hunter_wyh CSDN 博客\n          部分题目出自此博客\n          ","categories":["操作系统"],"tags":["OS"]},{"title":"操作系统课程设计","url":"/Operate-system/kcsj/","content":" 虚拟存储器管理\n \n 什么是虚拟存储器？\n​ 虚拟存储器是一种计算机内存管理技术，它通过将计算机的硬盘空间作为辅助存储器，允许程序使用比物理内存更大的地址空间。虚拟存储器的目标是提供更大的可用内存空间，以便同时运行更多的程序，而不受物理内存的限制。\n​ 虚拟存储的实现是基于局部存储原理的，要理解虚拟存储我们就要去理解局部存储的是实现，即：分页存储、分段存储、段页式存储。\n（这里我们重点掌握具有块表的分页存储）\n 课程设计核心目的？\n将逻辑地址转为物理地址{.wavy}\n 页表？\n\n\n\n页号\n块号\n\n\n\n\n0\n0\n\n\n1\n2\n\n\n2\n4\n\n\n3\n6\n\n\n\n针对这个课程设计，我们简单的理解为应用页表就是通过页号，来去找到块号\n\n由虚拟地址（逻辑地址）转换为物理地址，这就是页表最核心的作用！\n 为什么要用虚拟内存\n但是，我们的内存很小，还想运行多个应用程序，这必然无法满足所有的有的应用程序一次性全部加载到内存之中。根据局部性理论基础，我们又引申出页面的对换算法（页面置换算法）。\n本次课程设计采用的是 Clock 算法\n将页表扩充，引入访问位、修改位、有效位（本次仅用于算法实现，它其实用其他\n更有用的功能，在此不做介绍）\n\n\n\n修改位\n访问位\n重要排名\n\n\n\n\n0\n0\n4\n\n\n0\n1\n3\n\n\n1\n0\n2\n\n\n1\n1\n1\n\n\n\n在置换时，优先考虑不那么重要的（排名越低越不重要），对于不重要的页优先换出\n 状态转换？\n这里稍后同步，大家可以先看视频里的有关状态转换的图。\n 代码实现\n/**\n * @Author KarryLiu\n * @Creed may all the beauty be blessed\n * @Date 2023/12/4 上午 10:04\n * @Description TODO 诗岸梦行舟\n * @Version 1.0\n */\n#include &lt;limits&gt;\n#include &quot;iostream&quot;\n#include &quot;windows.h&quot;\n#include &quot;unistd.h&quot;\n\n// 颜色枚举，为了好看\nenum ConsoleColor &#123;\n    Black = 0,\n    Blue = 1,\n    Green = 2,\n    Cyan = 3,\n    Red = 4,\n    Magenta = 5,\n    Yellow = 6,\n    White = 7,\n    Gray = 8\n&#125;;\n\n// 设置文本颜色函数\nvoid setConsoleColor(ConsoleColor text, ConsoleColor background) &#123;\n    int color = text + background * 16;\n    SetConsoleTextAttribute(GetStdHandle(STD_OUTPUT_HANDLE), color);\n&#125;\n// 块大小\n#define blockSize 4\n// 页大小\n#define pageSize 2\n// 快表尺寸\n#define fastTableDimensions 4\n// 页表尺寸\n#define pageTableDimensions 6\n// 程序最大逻辑地址\nconst int logicalMaxAddress = 100;\n// 逻辑地址数据结构 -&gt; 页\nstruct logicalAddressDataStruct &#123;\n    int pageNumber;\n    int inPageAddress;\n&#125;;\n// 页逻辑地址实现机构\nlogicalAddressDataStruct logicalAddress;\n// 模拟外存中单条数据的数据结构\nstruct externalMemoryDataStruct &#123;\n    int externalPageNumber;\n    int externalBlockNumber;\n    // 访问位\n    bool accessBit;\n    // 修改位\n    bool modifyBit;\n    // 有效位\n    bool validBit;\n&#125;;\n\n// 外存所有存储结构,也就是最大逻辑地址100的一半，50页\nexternalMemoryDataStruct externalMemory[logicalMaxAddress / pageSize];\n// 模拟内存中单条数据的数据结构(页表)\nstruct internalStorageDataStruct &#123;\n    int internalPageNumber;\n    int internalBlockNumber;\n    // 访问位\n    bool accessBit;\n    // 修改位\n    bool modifyBit;\n    // 有效位\n    bool validBit;\n&#125;;\n// 页表实体\ninternalStorageDataStruct pageTable[pageTableDimensions];\n// 快表数据结构\nstruct fastTableDataStruct &#123;\n    int fastTablePageNumber;\n    int fastTableBlockNumber;\n    // 访问位\n    bool accessBit;\n    // 修改位\n    bool modifyBit;\n    // 有效位\n    bool validBit;\n&#125;;\n// 快表实体\nfastTableDataStruct fastTable[fastTableDimensions];\n// 快表置换排行榜数据结构\nstruct rankForFastTableDataStruct &#123;\n    int pageNumber;\n    int fastTableIndex;\n&#125;;\n// 页表置换排行榜数据结构\nstruct rankForPageTableDataStruct &#123;\n    int pageNumber;\n    int pageTableIndex;\n&#125;;\n// 物理地址数据结构\nstruct physicalAddressDataStruct &#123;\n    int blockNumber;\n    int internalBlockAddress;\n&#125;;\n// 转换后的物理地址实体\nphysicalAddressDataStruct physicalAddress;\nusing namespace std;\n\n/**\n * @Describe 初始化外存数据\n */\nvoid InitializeExternalData() &#123;\n    for (int i = 0; i &lt; logicalMaxAddress / pageSize; i++) &#123;\n        // 计算页号与块号\n        externalMemory[i].externalPageNumber = i;\n        externalMemory[i].externalBlockNumber = i / blockSize;\n    &#125;\n//    for (int i = 0; i &lt; logicalMaxAddress / pageSize; ++i) &#123;\n//        cout&lt;&lt;externalMemory[i].externalPageNumber&lt;&lt;&quot;   &quot;&lt;&lt;externalMemory[i].externalBlockNumber&lt;&lt;endl;\n//    &#125;\n&#125;\n\n/**\n * @Describe 从外存中寻找所缺失的页\n * @return externalMemoryDataStruct 返回找到的外存数据页\n */\nexternalMemoryDataStruct LookMissingPageFromExternalMemory(int logicalAddressPageNumber) &#123;\n    for (int i = 0; i &lt; logicalMaxAddress / pageSize; i++) &#123;\n        // 计算页号与块号\n        if (externalMemory[i].externalPageNumber == logicalAddressPageNumber) &#123;\n            return externalMemory[i];\n        &#125;\n    &#125;\n&#125;\n\nvoid PageTableVisualization() &#123;\n    setConsoleColor(Magenta, Black);\n\n    cout &lt;&lt; &quot;-------------------页表（内存）-------------------+&quot; &lt;&lt; endl;\n    for (int i = 0; i &lt; pageTableDimensions; i++) &#123;\n        cout &lt;&lt; &quot;页号：&quot; &lt;&lt; pageTable[i].internalPageNumber &lt;&lt; &quot;  块号：&quot; &lt;&lt; pageTable[i].internalBlockNumber &lt;&lt; &quot;  访问位：&quot;\n             &lt;&lt; pageTable[i].accessBit &lt;&lt; &quot;  修改位：&quot; &lt;&lt; pageTable[i].modifyBit &lt;&lt; &quot;  有效位：&quot; &lt;&lt; pageTable[i].validBit\n             &lt;&lt; endl;\n    &#125;\n    cout &lt;&lt; &quot;--------------------------------------------------+&quot; &lt;&lt; endl;\n\n&#125;\n\nvoid FastTableVisualization() &#123;\n    setConsoleColor(Cyan, Black);\n//    sleep(1);\n    cout &lt;&lt; &quot;----------------------快表-----------------------+&quot; &lt;&lt; endl;\n    for (int i = 0; i &lt; fastTableDimensions; i++) &#123;\n        cout &lt;&lt; &quot;页号：&quot; &lt;&lt; fastTable[i].fastTablePageNumber &lt;&lt; &quot;  块号：&quot; &lt;&lt; fastTable[i].fastTableBlockNumber\n             &lt;&lt; &quot;  访问位：&quot; &lt;&lt; fastTable[i].accessBit &lt;&lt; &quot;  修改位：&quot; &lt;&lt; fastTable[i].modifyBit &lt;&lt; &quot;  有效位：&quot;\n             &lt;&lt; fastTable[i].validBit &lt;&lt; endl;\n    &#125;\n    cout &lt;&lt; &quot;-------------------------------------------------+&quot; &lt;&lt; endl;\n&#125;\n\n/**\n * @Describe 初始化页表\n */\nvoid InitializePageTable() &#123;\n&#125;\n\n\nint main() &#123;\n    setConsoleColor(Cyan, Black);\n    cout&lt;&lt;&quot;诗岸梦行舟&quot;&lt;&lt;endl;\n    cout&lt;&lt;&quot;操作系统课程设计：虚拟存储器管理&quot;&lt;&lt;endl;\n    sleep(2);\n    // 系统请求的逻辑地址\n    int requestLogicalAddressByOS = 0;\n    // 初始化外存数据结构\n    InitializeExternalData();\n    // 快表发现位，真就是发现了，假就是没发现\n    bool fastTableFind = false;\n    // 在快表的哪一位发现的？\n    int fastTableFindPoint;\n    // 页表发现位，真就是发现了，假就是没发现\n    bool pageTableFind = false;\n    // 在页表的哪一位发现的？\n    int pageTableFindPoint;\n    // 当前内存容量（页表剩余容量）\n    int remainingCapacityOfPageTable = 6;\n    // 当前快表容量（快表剩余容量）\n    int remainingCapacityOfFastTable = 4;\n\n\n    while (true) &#123;\n        // 初始化物理地址数据暂存\n        physicalAddress.blockNumber = -1;\n        physicalAddress.internalBlockAddress = -1;\n\n        setConsoleColor(Blue, Black);\n//        sleep(2);\n        cout &lt;&lt; &quot;系统基本信息：&quot; &lt;&lt; endl;\n        cout &lt;&lt; &quot; +------------------------+&quot; &lt;&lt;&quot;----------------------+&quot;&lt;&lt; endl;\n        cout &lt;&lt; &quot; |  块大小：&quot; &lt;&lt; blockSize &lt;&lt; &quot;             |&quot; &lt;&lt;&quot;       xx大学       |&quot;&lt;&lt; endl;\n        cout &lt;&lt; &quot; |  页大小：&quot; &lt;&lt; pageSize &lt;&lt; &quot;             |&quot; &lt;&lt;&quot;     专业     |&quot;&lt;&lt; endl;\n        cout &lt;&lt; &quot; |  最大逻辑地址：&quot; &lt;&lt; logicalMaxAddress &lt;&lt; &quot;     |&quot; &lt;&lt;&quot;     OS  课程设计     |&quot;&lt;&lt; endl;\n        cout &lt;&lt; &quot; |  快表尺寸：&quot; &lt;&lt; fastTableDimensions &lt;&lt; &quot;           |&quot; &lt;&lt;&quot;   诗岸梦行舟  |&quot;&lt;&lt; endl;\n        cout &lt;&lt; &quot; |  页表尺寸：&quot; &lt;&lt; pageTableDimensions &lt;&lt; &quot;           |&quot; &lt;&lt;&quot; 虚 拟 存 储 器 管 理 |&quot;&lt;&lt; endl;\n        cout &lt;&lt; &quot; +------------------------+&quot; &lt;&lt;&quot;----------------------+&quot;&lt;&lt; endl;\n        FastTableVisualization();\n        PageTableVisualization();\n        setConsoleColor(White, Black);\n        cout &lt;&lt; &quot;请输入应用程序请求的逻辑地址：&quot; &lt;&lt; endl;\n        cin &gt;&gt; requestLogicalAddressByOS;\n        // 结束程序\n        if (requestLogicalAddressByOS &lt; 0) &#123; break; &#125;\n        // 逻辑地址越界\n        if (requestLogicalAddressByOS &gt; logicalMaxAddress) &#123;\n            setConsoleColor(Red, Black);\n\n            cout &lt;&lt; &quot;您请求的地址超出最大逻辑地址！产生越界中断！&quot; &lt;&lt; endl;\n            setConsoleColor(White, Black);\n            sleep(2);\n            continue;\n        &#125;\n        setConsoleColor(Cyan, Black);\n        cout &lt;&lt; &quot;您请求的十进制地址：&quot; &lt;&lt; requestLogicalAddressByOS &lt;&lt; endl;\n        setConsoleColor(White, Black);\n\n        /**\n         * 计算逻辑地址数据\n         * P = [ A / L ]\n         * d = A % L\n         */\n        logicalAddress.pageNumber = requestLogicalAddressByOS / pageSize;\n        logicalAddress.inPageAddress = requestLogicalAddressByOS % pageSize;\n        cout &lt;&lt; &quot;计算后形成逻辑地址：&quot; &lt;&lt; endl;\n        setConsoleColor(Magenta, Black);\n        cout &lt;&lt; &quot;+---页号---+---页内地址----+&quot; &lt;&lt; endl;\n        cout &lt;&lt; &quot;|    &quot; &lt;&lt; logicalAddress.pageNumber &lt;&lt; &quot;    |       &quot; &lt;&lt; logicalAddress.inPageAddress &lt;&lt; &quot;       |&quot;\n             &lt;&lt; endl;\n        cout &lt;&lt; &quot;+----------+---------------+&quot; &lt;&lt; endl;\n//        cout&lt;&lt;&quot;按回车继续执行...&quot;;\n//        cin.get();cin.get();\n\n        setConsoleColor(White, Black);\n        // CPU检索块表\n        cout &lt;&lt; &quot;操作系统检索快表...&quot; &lt;&lt; endl;\n        for (int i = 0; i &lt; fastTableDimensions; i++) &#123;\n            if (fastTable[i].fastTablePageNumber == logicalAddress.pageNumber &amp;&amp; fastTable[i].validBit) &#123;\n                // 在快表中发现了与之对应的页号的数据页\n                setConsoleColor(Green, Black);\n                cout &lt;&lt; &quot;操作系统在快表中发现了相应的数据页!&quot; &lt;&lt; endl;\n                fastTableFindPoint = i;\n                // 将发现检查位置真\n                fastTableFind = true;\n                setConsoleColor(White, Black);\n                break;\n            &#125;\n        &#125;\n\n        if (!fastTableFind) &#123;\n            // 快表中没有，访问页表\n            setConsoleColor(Yellow, Black);\n            cout &lt;&lt; &quot;快表中没有发现相应的数据页!&quot; &lt;&lt; endl;\n            setConsoleColor(White, Black);\n            cout &lt;&lt; &quot;操作系统检索页表...&quot; &lt;&lt; endl;\n            // 从内存中（页表）寻找所缺失的页\n            for (int i = 0; i &lt; pageTableDimensions; i++) &#123;\n                if (pageTable[i].internalPageNumber == logicalAddress.pageNumber &amp;&amp; pageTable[i].validBit) &#123;\n                    // 在内存中（页表）找到了数据页\n                    setConsoleColor(Green, Black);\n                    cout &lt;&lt; &quot;操作系统在页表中发现了相应的数据页!&quot; &lt;&lt; endl;\n                    pageTableFind = true;\n                    pageTableFindPoint = i;\n                    setConsoleColor(White, Black);\n                    break;\n                &#125;\n            &#125;\n\n            if (!pageTableFind) &#123;\n                // 页表（内存）中没有\n                setConsoleColor(Yellow, Black);\n                cout &lt;&lt; &quot;页中没有发现相应的数据页!&quot; &lt;&lt; endl;\n                setConsoleColor(White, Black);\n                cout &lt;&lt; &quot;操作系统会正在从外存中抽取页...&quot; &lt;&lt; endl;\n                // 从外存中抽取页\n                externalMemoryDataStruct externalMemoryFindPage = LookMissingPageFromExternalMemory(\n                        logicalAddress.pageNumber);\n                /*cout&lt;&lt;externalMemoryFindPage.externalPageNumber&lt;&lt;&quot;  &quot;&lt;&lt;externalMemoryFindPage.externalBlockNumber;*/\n                cout &lt;&lt; &quot;成功抽取到了，现在正在检查内存是否已满...&quot; &lt;&lt; endl;\n                sleep(1);\n\n                if (!(remainingCapacityOfPageTable &gt; 0)) &#123;\n                    // 内存（页表）已满\n                    // 这里得选择一个最没用的一页对换出去\n                    setConsoleColor(Red, Black);\n                    cout &lt;&lt; &quot;内存已满，操作系统需要进行页面置换，正在选择一个最没用的一页进行换出...&quot; &lt;&lt; endl;;\n                    setConsoleColor(White, Black);\n                    // 开始选择一个最没用的一页\n                    // 页表置换排行榜\n                    rankForPageTableDataStruct rankForPageTable[pageTableDimensions];\n                    // 初始化排行榜\n                    for (int i = 0; i &lt; pageTableDimensions; i++) &#123;\n                        rankForPageTable[i].pageNumber - 1;\n                        rankForPageTable[i].pageTableIndex = -1;\n                    &#125;\n                    // 模拟态桶排序\n                    for (int i = 0; i &lt; pageTableDimensions; i++) &#123;\n                        if (pageTable[i].validBit\n                            &amp;&amp; !pageTable[i].accessBit\n                            &amp;&amp; !pageTable[i].modifyBit) &#123;\n                            rankForPageTable[0].pageTableIndex = i;\n                            rankForPageTable[0].pageNumber = pageTable[i].internalPageNumber;\n                        &#125; else if (pageTable[i].validBit\n                                   &amp;&amp; pageTable[i].accessBit\n                                   &amp;&amp; !pageTable[i].modifyBit) &#123;\n                            rankForPageTable[1].pageTableIndex = i;\n                            rankForPageTable[1].pageNumber = pageTable[i].internalPageNumber;\n                        &#125; else if (pageTable[i].validBit\n                                   &amp;&amp; !pageTable[i].accessBit\n                                   &amp;&amp; pageTable[i].modifyBit) &#123;\n                            rankForPageTable[2].pageTableIndex = i;\n                            rankForPageTable[2].pageNumber = pageTable[i].internalPageNumber;\n                        &#125; else &#123;\n                            rankForPageTable[3].pageTableIndex = i;\n                            rankForPageTable[3].pageNumber = pageTable[i].internalPageNumber;\n                        &#125;\n                    &#125;\n                    for (int i = 0; i &lt; pageTableDimensions; i++) &#123;\n                        if (rankForPageTable[i].pageNumber != -1) &#123;\n                            // 找到一个没用的页\n                            setConsoleColor(Green, Black);\n                            cout &lt;&lt; &quot;操作系统页表表选择了一个页号为&quot;\n                                 &lt;&lt; pageTable[rankForPageTable[i].pageTableIndex].internalPageNumber\n                                 &lt;&lt; &quot;的页进行换出...&quot;\n                                 &lt;&lt; endl;;\n                            setConsoleColor(White, Black);\n                            sleep(1);\n                            // 把页表中对应的页号置为无效\n                            pageTable[rankForPageTable[i].pageTableIndex].validBit = false;\n                            // 页表页面置换\n                            pageTable[rankForPageTable[i].pageTableIndex].internalPageNumber = externalMemoryFindPage.externalPageNumber;\n                            pageTable[rankForPageTable[i].pageTableIndex].internalBlockNumber = externalMemoryFindPage.externalBlockNumber;\n                            pageTable[rankForPageTable[i].pageTableIndex].accessBit = false;\n                            pageTable[rankForPageTable[i].pageTableIndex].modifyBit = false;\n                            pageTable[rankForPageTable[i].pageTableIndex].validBit = true;\n\n                            break;\n                        &#125;\n                    &#125;\n\n\n                &#125; else &#123;\n                    // 内存（页表）仍有空间\n                    // 有空间就可以塞进去\n                    setConsoleColor(Blue, Black);\n                    cout &lt;&lt; &quot;内存还有空间，操作系统正在读取缺页并回写内存中...&quot; &lt;&lt; endl;;\n                    setConsoleColor(White, Black);\n                    for (int i = 0; i &lt; pageTableDimensions; i++) &#123;\n                        if (pageTable[i].validBit == false) &#123;\n                            pageTable[i].validBit = true;\n                            pageTable[i].internalPageNumber = externalMemoryFindPage.externalPageNumber;\n                            pageTable[i].internalBlockNumber = externalMemoryFindPage.externalBlockNumber;\n                            break;\n                        &#125;\n                    &#125;\n                    setConsoleColor(Green, Black);\n                    cout &lt;&lt; &quot;页表回写成功！&quot; &lt;&lt; endl;;\n                    setConsoleColor(White, Black);\n                    // 塞完就减减\n                    remainingCapacityOfPageTable--;\n                &#125;\n                if (!(remainingCapacityOfFastTable &gt; 0)) &#123;\n                    //快表没有空间了\n                    setConsoleColor(Red, Black);\n                    cout &lt;&lt; &quot;快表已满，操作系统需要进行页面置换，正在选择一个最没用的一页进行换出...&quot; &lt;&lt; endl;;\n                    setConsoleColor(White, Black);\n                    // 选择一个最没用的一页进行换出！！！！！！！！！！\n                    bool fastTableFindCheckIsTrue = false;\n                    int fastTableFindCheckIndex;\n                    // 快表置换排行榜\n                    rankForFastTableDataStruct rankForFastTable[fastTableDimensions];\n                    // 初始化排行榜\n                    for (int i = 0; i &lt; fastTableDimensions; i++) &#123;\n                        rankForFastTable[i].pageNumber = -1;\n                        rankForFastTable[i].fastTableIndex = -1;\n                    &#125;\n                    // 模拟态桶排序\n                    for (int i = 0; i &lt; fastTableDimensions; i++) &#123;\n                        if (fastTable[i].validBit\n                            &amp;&amp; !fastTable[i].accessBit\n                            &amp;&amp; !fastTable[i].modifyBit) &#123;\n                            rankForFastTable[0].fastTableIndex = i;\n                            rankForFastTable[0].pageNumber = fastTable[i].fastTablePageNumber;\n                        &#125; else if (fastTable[i].validBit\n                                   &amp;&amp; fastTable[i].accessBit\n                                   &amp;&amp; !fastTable[i].modifyBit) &#123;\n                            rankForFastTable[1].fastTableIndex = i;\n                            rankForFastTable[1].pageNumber = fastTable[i].fastTablePageNumber;\n                        &#125; else if (fastTable[i].validBit\n                                   &amp;&amp; !fastTable[i].accessBit\n                                   &amp;&amp; fastTable[i].modifyBit) &#123;\n                            rankForFastTable[2].fastTableIndex = i;\n                            rankForFastTable[2].pageNumber = fastTable[i].fastTablePageNumber;\n                        &#125; else &#123;\n                            rankForFastTable[3].fastTableIndex = i;\n                            rankForFastTable[3].pageNumber = fastTable[i].fastTablePageNumber;\n                        &#125;\n                    &#125;\n                    for (int i = 0; i &lt; fastTableDimensions; i++) &#123;\n                        if (rankForFastTable[i].pageNumber != -1) &#123;\n                            // 找到一个没用的页\n                            setConsoleColor(Green, Black);\n                            cout &lt;&lt; &quot;操作系统在快表选择了一个页号为&quot;\n                                 &lt;&lt; fastTable[rankForFastTable[i].fastTableIndex].fastTablePageNumber\n                                 &lt;&lt; &quot;的页进行换出...&quot;\n                                 &lt;&lt; endl;\n                            setConsoleColor(White, Black);\n                            sleep(1);\n                            // 把快表中对应的页号置为无效\n                            fastTable[rankForFastTable[i].fastTableIndex].validBit = false;\n                            // 快表页面置换\n                            fastTable[rankForFastTable[i].fastTableIndex].fastTablePageNumber = externalMemoryFindPage.externalPageNumber;\n                            fastTable[rankForFastTable[i].fastTableIndex].fastTableBlockNumber = externalMemoryFindPage.externalBlockNumber;\n                            fastTable[rankForFastTable[i].fastTableIndex].accessBit = false;\n                            fastTable[rankForFastTable[i].fastTableIndex].modifyBit = false;\n                            fastTable[rankForFastTable[i].fastTableIndex].validBit = true;\n\n                            // 把页表中对应的页号置为无效？\n                            fastTableFindCheckIsTrue = true;\n                            break;\n                        &#125;\n\n                    &#125;\n                    if (fastTableFindCheckIsTrue) &#123;\n\n                    &#125; else &#123;\n                        setConsoleColor(Red, Black);\n                        cout &lt;&lt; &quot;一般不存在这种情况，以防万一留着DeBug用&quot; &lt;&lt; endl;\n                        setConsoleColor(White, Black);\n                    &#125;\n\n                &#125; else &#123;\n                    // 快表仍然有空间\n                    // 有空间就塞\n                    setConsoleColor(Blue, Black);\n                    cout &lt;&lt; &quot;快表还有空间，操作系统正在读取缺页并回写内存中...&quot; &lt;&lt; endl;;\n                    setConsoleColor(White, Black);\n                    for (int i = 0; i &lt; fastTableDimensions; i++) &#123;\n                        if (fastTable[i].validBit == false) &#123;\n                            fastTable[i].validBit = true;\n                            fastTable[i].fastTablePageNumber = externalMemoryFindPage.externalPageNumber;\n                            fastTable[i].fastTableBlockNumber = externalMemoryFindPage.externalBlockNumber;\n                            break;\n                        &#125;\n                    &#125;\n                    setConsoleColor(Green, Black);\n                    cout &lt;&lt; &quot;快表回写成功！&quot; &lt;&lt; endl;;\n                    setConsoleColor(White, Black);\n                    //塞完就减减\n                    remainingCapacityOfFastTable--;\n                &#125;\n                //!!!!!!!!!!!!!!\n\n                physicalAddress.blockNumber = externalMemoryFindPage.externalBlockNumber;\n                physicalAddress.internalBlockAddress = logicalAddress.inPageAddress;\n                cout &lt;&lt; &quot;计算后得到物理地址：&quot; &lt;&lt; endl;\n                setConsoleColor(Magenta, Black);\n                cout &lt;&lt; &quot;+---块号---+---块内地址----+&quot; &lt;&lt; endl;\n                cout &lt;&lt; &quot;|    &quot; &lt;&lt; physicalAddress.blockNumber &lt;&lt; &quot;    |       &quot; &lt;&lt; physicalAddress.internalBlockAddress\n                     &lt;&lt; &quot;       |&quot;\n                     &lt;&lt; endl;\n                cout &lt;&lt; &quot;+----------+---------------+&quot; &lt;&lt; endl;\n                sleep(2);\n\n            &#125; else &#123;\n                // 页表中有\n                // 页表中的话，修改页表后还需要进行快表置换\n                /**\n                 * 如果页表里面有，那就更改他的访问位和修改位，\n                 * 为了编写简单，在这里我做出一个规定：\n                 * 当应用程序第一次访问时将访问位置真，\n                 * 当应用程序第二次访问时将修改为置真。\n                 * 这么做也是相对合理的，原因如下：\n                 * 访问位与修改位联合置换逻辑（修改，访问）即对换优先级：\n                 * * * 1.（0，0）\n                 * * * 2.（0，1）\n                 * * * 3.（1，0）\n                 * * * 4.（1，1）\n                 * 排名越高，置换优先级越高，\n                 * 相对来讲，访问位与修改位置真后，访问位优先级高于修改位\n                 * 其实我懒了 但我不说（qwq）\n                 */\n                // 快表置换，基于页表\n                // 快表置换排行榜\n                rankForFastTableDataStruct rankForFastTable[fastTableDimensions];\n                // 初始化排行榜\n                for (int i = 0; i &lt; fastTableDimensions; i++) &#123;\n                    rankForFastTable[i].pageNumber = -1;\n                    rankForFastTable[i].fastTableIndex = -1;\n                &#125;\n                // 模拟态桶排序\n                for (int i = 0; i &lt; fastTableDimensions; i++) &#123;\n                    if (fastTable[i].validBit\n                        &amp;&amp; !fastTable[i].accessBit\n                        &amp;&amp; !fastTable[i].modifyBit) &#123;\n                        rankForFastTable[0].fastTableIndex = i;\n                        rankForFastTable[0].pageNumber = fastTable[i].fastTablePageNumber;\n                    &#125; else if (fastTable[i].validBit\n                               &amp;&amp; fastTable[i].accessBit\n                               &amp;&amp; !fastTable[i].modifyBit) &#123;\n                        rankForFastTable[1].fastTableIndex = i;\n                        rankForFastTable[1].pageNumber = fastTable[i].fastTablePageNumber;\n                    &#125; else if (fastTable[i].validBit\n                               &amp;&amp; !fastTable[i].accessBit\n                               &amp;&amp; fastTable[i].modifyBit) &#123;\n                        rankForFastTable[2].fastTableIndex = i;\n                        rankForFastTable[2].pageNumber = fastTable[i].fastTablePageNumber;\n                    &#125; else &#123;\n                        rankForFastTable[3].fastTableIndex = i;\n                        rankForFastTable[3].pageNumber = fastTable[i].fastTablePageNumber;\n                    &#125;\n                &#125;\n\n                for (int i = 0; i &lt; fastTableDimensions; i++) &#123;\n                    if (rankForFastTable[i].pageNumber != -1) &#123;\n                        // 找到一个没用的页\n                        setConsoleColor(Green, Black);\n                        cout &lt;&lt; &quot;操作系统在快表选择了一个页号为&quot;\n                             &lt;&lt; fastTable[rankForFastTable[i].fastTableIndex].fastTablePageNumber\n                             &lt;&lt; &quot;的页进行换出...&quot;\n                             &lt;&lt; endl;;\n                        setConsoleColor(White, Black);\n                        sleep(1);\n                        // 把快表中对应的页号置为无效\n                        fastTable[rankForFastTable[i].fastTableIndex].validBit = false;\n                        // 快表页面置换\n                        fastTable[rankForFastTable[i].fastTableIndex].fastTablePageNumber = pageTable[pageTableFindPoint].internalPageNumber;\n                        fastTable[rankForFastTable[i].fastTableIndex].fastTableBlockNumber = pageTable[pageTableFindPoint].internalBlockNumber;\n                        fastTable[rankForFastTable[i].fastTableIndex].accessBit = false;\n                        fastTable[rankForFastTable[i].fastTableIndex].modifyBit = false;\n                        fastTable[rankForFastTable[i].fastTableIndex].validBit = true;\n                        // 物理地址存储\n                        physicalAddress.blockNumber = pageTable[pageTableFindPoint].internalBlockNumber;\n                        physicalAddress.internalBlockAddress = logicalAddress.inPageAddress;\n                        break;\n                    &#125;\n\n                &#125;\n\n                cout &lt;&lt; &quot;计算后得到物理地址：&quot; &lt;&lt; endl;\n                setConsoleColor(Magenta, Black);\n                cout &lt;&lt; &quot;+---块号---+---块内地址----+&quot; &lt;&lt; endl;\n                cout &lt;&lt; &quot;|    &quot; &lt;&lt; physicalAddress.blockNumber &lt;&lt; &quot;    |       &quot; &lt;&lt; physicalAddress.internalBlockAddress\n                     &lt;&lt; &quot;       |&quot;\n                     &lt;&lt; endl;\n                cout &lt;&lt; &quot;+----------+---------------+&quot; &lt;&lt; endl;\n\n\n                // 修改页表\n                if (pageTable[pageTableFindPoint].validBit\n                    &amp;&amp; !pageTable[pageTableFindPoint].accessBit\n                    &amp;&amp; !pageTable[pageTableFindPoint].modifyBit) &#123;\n                    pageTable[pageTableFindPoint].accessBit = true;\n                &#125; else if (pageTable[pageTableFindPoint].validBit\n                           &amp;&amp; pageTable[pageTableFindPoint].accessBit\n                           &amp;&amp; !pageTable[pageTableFindPoint].modifyBit) &#123;\n                    pageTable[pageTableFindPoint].accessBit = false;\n                    pageTable[pageTableFindPoint].modifyBit = true;\n                &#125; else if (pageTable[pageTableFindPoint].validBit\n                           &amp;&amp; !pageTable[pageTableFindPoint].accessBit\n                           &amp;&amp; pageTable[pageTableFindPoint].modifyBit) &#123;\n                    pageTable[pageTableFindPoint].accessBit = true;\n                    pageTable[pageTableFindPoint].modifyBit = true;\n                &#125;\n                // 同步快表\n                for (int i = 0; i &lt; fastTableDimensions; i++) &#123;\n                    if (fastTable[i].fastTablePageNumber == pageTable[pageTableFindPoint].internalPageNumber) &#123;\n                        if (fastTable[i].validBit\n                            &amp;&amp; !fastTable[i].accessBit\n                            &amp;&amp; !fastTable[i].modifyBit) &#123;\n                            fastTable[i].accessBit = true;\n                        &#125; else if (fastTable[i].validBit\n                                   &amp;&amp; fastTable[i].accessBit\n                                   &amp;&amp; !fastTable[i].modifyBit) &#123;\n                            fastTable[i].accessBit = false;\n                            fastTable[i].modifyBit = true;\n                        &#125; else if (fastTable[i].validBit\n                                   &amp;&amp; !fastTable[i].accessBit\n                                   &amp;&amp; fastTable[i].modifyBit) &#123;\n                            fastTable[i].accessBit = true;\n                            fastTable[i].modifyBit = true;\n                        &#125;\n                        break;\n                    &#125;\n                &#125;\n\n            &#125;\n        &#125; else &#123;\n            // 快表中有\n            // 修改快表\n            fastTable[fastTableFindPoint];\n            if (fastTable[fastTableFindPoint].validBit\n                &amp;&amp; !fastTable[fastTableFindPoint].accessBit\n                &amp;&amp; !fastTable[fastTableFindPoint].modifyBit) &#123;\n                fastTable[fastTableFindPoint].accessBit = true;\n            &#125; else if (fastTable[fastTableFindPoint].validBit\n                       &amp;&amp; fastTable[fastTableFindPoint].accessBit\n                       &amp;&amp; !fastTable[fastTableFindPoint].modifyBit) &#123;\n                fastTable[fastTableFindPoint].accessBit = false;\n                fastTable[fastTableFindPoint].modifyBit = true;\n            &#125; else if (fastTable[fastTableFindPoint].validBit\n                       &amp;&amp; !fastTable[fastTableFindPoint].accessBit\n                       &amp;&amp; fastTable[fastTableFindPoint].modifyBit) &#123;\n                fastTable[fastTableFindPoint].accessBit = true;\n                fastTable[fastTableFindPoint].modifyBit = true;\n            &#125;\n            // 同步页表\n            for (int i = 0; i &lt; pageTableDimensions; i++) &#123;\n                if (pageTable[i].internalPageNumber == fastTable[fastTableFindPoint].fastTablePageNumber) &#123;\n                    if (pageTable[i].validBit\n                        &amp;&amp; !pageTable[i].accessBit\n                        &amp;&amp; !pageTable[i].modifyBit) &#123;\n                        pageTable[i].accessBit = true;\n                    &#125; else if (pageTable[i].validBit\n                               &amp;&amp; pageTable[i].accessBit\n                               &amp;&amp; !pageTable[i].modifyBit) &#123;\n                        pageTable[i].accessBit = false;\n                        pageTable[i].modifyBit = true;\n                    &#125; else if (pageTable[i].validBit\n                               &amp;&amp; !pageTable[i].accessBit\n                               &amp;&amp; pageTable[i].modifyBit) &#123;\n                        pageTable[i].accessBit = true;\n                        pageTable[i].modifyBit = true;\n                    &#125;\n                    break;\n\n                &#125;\n            &#125;\n\n            // 直接从快表中提取\n            physicalAddress.blockNumber = fastTable[fastTableFindPoint].fastTableBlockNumber;\n            physicalAddress.internalBlockAddress = logicalAddress.inPageAddress;\n            cout &lt;&lt; &quot;计算后得到物理地址：&quot; &lt;&lt; endl;\n            setConsoleColor(Magenta, Black);\n            cout &lt;&lt; &quot;+---块号---+---块内地址----+&quot; &lt;&lt; endl;\n            cout &lt;&lt; &quot;|    &quot; &lt;&lt; physicalAddress.blockNumber &lt;&lt; &quot;    |       &quot; &lt;&lt; physicalAddress.internalBlockAddress\n                 &lt;&lt; &quot;       |&quot;\n                 &lt;&lt; endl;\n            cout &lt;&lt; &quot;+----------+---------------+&quot; &lt;&lt; endl;\n            sleep(2);\n        &#125;\n        // 恢复初始状态\n        fastTableFind = false;\n        pageTableFind = false;\n        setConsoleColor(Green, Black);\n        sleep(1);\n        cout &lt;&lt; &quot;---------------地址变换结束---------------&quot;;\n        sleep(1);\n        cout &lt;&lt; endl;\n        setConsoleColor(White, Black);\n    &#125;\n    cout &lt;&lt; &quot;下次再见！&quot;;\n&#125;\n\n","categories":["操作系统"],"tags":["OS"]},{"title":"计算机操作系统复试速览","url":"/Operate-system/os_retest/","content":" 计算机操作系统复试\n 操作系统的作用\n\n是用户与计算机硬件之间的接口\n是计算机系统资源的管理者\n实现了对计算机资源的抽象\n\n 操作系统基本特征\n\n并发\n共享\n虚拟\n异步\n\n 操作系统最基本调特征\n\n并发\n共享\n\n 处理机的双重工作模式\n用户态、核心态\n特权指令在核心态执行，非特权指令在用户态下执行\n 操作系统主要功能\n\n处理机调度\n\n进程控制\n进程同步\n进程通信\n调度\n\n作业调度（高级调度）\n进程调度（低级调度）\n\n\n\n\n存储器管理\n\n内存的分配与回收\n内存保护\n地址映射\n内存扩充\n\n\n设备管理\n\n缓冲管理\n设备分配\n设备处理\n\n\n文件管理\n\n文件存储空间管理\n目录管理\n文件的读写管理和保护\n\n\n接口管理\n\n用户接口（脱机用户接口、联机用户接口、图形用户接口）\n程序接口\n\n\n\n 程序并发执行特性\n\n间断性\n失去封闭性\n不可再现性\n\n 进程的特征\n动态性、并发性、独立性、异步性\n 进程的3状态模型、5状态模型、7状态模型\n 单CPU中的状态\n\n\n\n状态\n最多\n最少\n\n\n\n\n运行\n1\n0\n\n\n就绪\nN-1\n0\n\n\n阻塞\nN\n0\n\n\n\n PCB中的信息\n\n进程标识符\n处理机状态\n进程调度信息\n进程控制信息\n\n 拥有资源？调度？\n\n进程是拥有资源的\n线程是独立调度的\n\n 高级调度（作业调度）\n决定哪些作业从外存调入内存准备执行，控制系统的并发度。\n 中级调度（内存调度）\n管理内存中的进程，通过挂起和激活来调整内存使用。\n 低级调度（进程调度）\n决定哪个就绪进程获得CPU执行。\n 先来先服务FCFS\n按作业或进程到达的顺序分配CPU。\n\n简单易实现。\n非抢占式，可能导致“护航效应”（短作业等待长作业完成）。\n平均等待时间较长，适合长作业。\n\n 短作业优先SJF\n优先调度运行时间最短的作业。\n\n可抢占或非抢占。\n最小化平均等待时间。\n可能导致长作业“饥饿”。\n\n 优先级调度Priority Scheduling\n按优先级分配CPU，优先级高的先执行。\n\n可抢占或非抢占。\n可能导致低优先级进程“饥饿”。\n优先级可以是静态或动态调整。\n\n 时间片轮转调度RR\n每个进程分配一个固定时间片（Time Quantum），轮流执行。\n\n抢占式，适合分时系统。\n时间片大小影响性能：太小增加上下文切换开销，太大退化为FCFS。\n公平性好，响应时间短。\n\n 多级队列调度Multilevel Queue Scheduling\n将进程分为多个队列，每个队列采用不同的调度算法。\n\n队列间可设置优先级。\n适合混合型任务（如前台交互任务和后台批处理任务）。\n\n 多级反馈队列调度Multilevel Feedback Queue Scheduling\n进程可在多个队列间移动，根据行为动态调整优先级。\n\n结合了优先级调度和轮转调度的优点。\n动态适应进程行为，避免“饥饿”。\n实现复杂。\n\n 算法总结\n\n\n\n算法\n特点\n适用场景\n\n\n\n\nFCFS\n简单，非抢占，护航效应\n批处理系统\n\n\nSJF\n最小化等待时间，可能导致长作业饥饿\n批处理系统\n\n\n优先级调度\n按优先级执行，可能导致低优先级饥饿\n实时系统\n\n\n轮转调度\n公平，响应时间短，时间片大小影响性能\n交互式系统\n\n\n多级队列调度\n多队列，不同策略\n混合任务系统\n\n\n多级反馈队列调度\n动态调整优先级，避免饥饿\n通用操作系统\n\n\n\n 死锁\n死锁是操作系统中的一种资源竞争现象，指多个进程或线程因争夺资源而陷入无限等待的状态，导致它们都无法继续执行。死锁的发生通常需要满足四个必要条件，称为死锁的四个必要条件。\n 死锁的四个必要条件\n\n互斥条件（Mutual Exclusion）：\n\n资源一次只能被一个进程占用，其他进程必须等待。\n\n\n请求与保持推荐（Hold and Wait）：\n\n进程已经占有一些资源，同时请求其他被占用的资源。\n\n\n不剥夺条件（No Preemption）：\n\n进程已获得的资源不能被强制剥夺，只能由进程自行释放。\n\n\n循环等待条件（Circular Wait）：\n\n存在一个进程等待的循环链，每个进程都在等待下一个进程占用的资源。\n\n\n\n 死锁的处理方法\n\n死锁预防\n死锁避免\n死锁检测\n死锁解除\n\n 银行家算法\n\n\n检查资源分配后系统是否处于安全状态（即是否存在一个执行序列使所有进程完成）。\n\n\n如果不安全，则拒绝分配请求。\n\n\n 两种制约关系\n\n互斥关系\n同步关系\n\n 解决临界区同步问题须遵循以下准则\n\n空闲让进\n忙则等待\n有限等待\n让权等待\n\n 程序的装入方式\n\n绝对装入\n\n程序中的地址是固定的，装入时不需要进行地址转换。\n适用于单道程序环境，或者内存地址固定的系统。\n灵活性差，程序必须加载到指定的内存位置，无法适应多道程序环境。\n\n\n可重定位装入\n\n程序可以在内存的不同位置加载，适应多道程序设计。\n装入时需要进行地址重定位，通常由操作系统或装入程序完成。\n适用于多道程序环境，支持内存的动态分配。\n\n\n动态运行时装入\n\n程序可以部分加载，只有在需要时才将相关部分加载到内存。\n地址转换由硬件和操作系统共同完成。\n支持虚拟内存技术，允许程序的大小超过物理内存的容量。\n\n\n\n\n绝对装入：适用于单道程序环境，地址固定，装入简单但缺乏灵活性。\n可重定位装入：适用于多道程序环境，支持地址重定位，灵活性高。\n动态运行时装入：支持虚拟内存，程序可以动态加载，适合现代操作系统。\n\n 连续分配存储器管理方式\n\n单一连续分配\n固定分区分配\n动态分区分配\n\n 动态分区分配的算法\n\n首次适应算法\n循环首次适应算法\n最佳适配算法\n最坏适配算法\n\n 分页存储\n 分段存储\n 页面置换算法\n\n先进先出算法\n最佳页面置换算法\n最近最久未使用算法\n最少使用算法\nClock算法\n改进型Clock算法\n\n 抖动\n**抖动（Thrashing）**是操作系统中与虚拟内存管理相关的一种现象，通常发生在系统内存资源不足时。当系统频繁地进行页面置换（即频繁地将内存中的页面换出到外存，再从外存换入新的页面），导致大部分时间都花在页面调度上，而不是执行实际任务时，就发生了抖动。\n I/O设别控制方式\n\n使用轮询的可编程I/O方式（落后）\n使用中断的可编程I/O方式（用的比较多）\n使用DMA（减少了CPU对I/O的干预）\n使用I/O通道\n\n 假脱机技术-SPOOLing\n假脱机技术（SPOOLing，Simultaneous Peripheral Operations On-Line） 是一种用于管理输入/输出（I/O）操作的技术，特别是在处理低速外设（如打印机）时。SPOOLing 的核心思想是通过缓冲和排队机制，将外设的操作与主机的计算任务分离，从而提高系统的效率和资源利用率。\nSPOOLing 的基本原理\n\n输入井和输出井：\n\nSPOOLing 系统在磁盘上创建两个专门的存储区域：输入井和输出井。\n输入井用于暂存从输入设备（如读卡器）读取的数据。\n输出井用于暂存需要发送到输出设备（如打印机）的数据。\n\n\n缓冲和排队：\n\n当用户提交任务时，数据首先被写入磁盘的输入井或输出井，而不是直接发送到外设。\n外设（如打印机）从输出井中按顺序读取数据并执行操作，而主机可以继续执行其他任务。\n\n\n并行操作：\n\n主机和外设可以并行工作。主机不需要等待外设完成操作，而是将数据交给 SPOOLing 系统后继续执行其他任务。\n\n\n\nSPOOLing 的应用场景\n\n打印任务管理：\n\n打印机是典型的低速外设，SPOOLing 技术可以有效地管理多个打印任务，避免用户等待。\n\n\n批处理系统：\n\n在批处理系统中，SPOOLing 技术可以用于管理输入作业和输出结果。\n\n\n网络打印：\n\n现代网络打印机通常使用 SPOOLing 技术来管理来自多个用户的打印任务。\n\n\n\nSPOOLing技术就是将独占设备转变为逻辑上的共享设备，提高了I/O速度，实现了虚拟设备的功能。\n 文件在外存的组织方式\n\n连续组织方式\n链接组织方式（显示连接FAT、隐式链接）\n索引组织方式（一级索引、二级索引…）\n\n 显示链接\n显示链接通过一个专门的链接表来记录文件中各个块的物理地址。这个表通常称为文件分配表（FAT, File Allocation Table）。\n 隐式链接\n隐式链接将每个块的指针存储在块本身中，而不是使用一个全局的链接表。每个块中除了存储数据外，还包含下一个块的地址。\n 显示链接 vs 隐式链接\n\n\n\n特性\n显示链接（FAT）\n隐式链接\n\n\n\n\n存储结构\n使用全局的 FAT 表存储链接信息\n链接信息存储在块内部\n\n\n访问方式\n支持随机访问\n仅支持顺序访问\n\n\n空间开销\nFAT 表占用额外空间\n无额外空间开销\n\n\n实现复杂度\n较复杂\n较简单\n\n\n可靠性\nFAT 损坏可能导致文件系统失效\n单个块损坏可能影响文件完整性\n\n\n适用场景\n需要随机访问的场景\n顺序访问为主的场景\n\n\n\n 小问题\n 进程和程序有什么区别\n\n程序\n\n程序是存储在磁盘或其他存储介质上的静态实体，由一系列指令和数据组成。\n它是一个被动的实体，只有在被加载到内存并执行时才会发挥作用。\n\n\n进程\n\n进程是程序在内存中的动态执行实例。\n它是一个主动的实体，包含了程序的执行状态（如程序计数器、寄存器、堆栈等）以及系统分配的资源（如内存、文件句柄等）。\n\n\n\n\n\n\n特性\n程序（Program）\n进程（Process）\n\n\n\n\n定义\n静态的指令和数据集合\n程序的动态执行实例\n\n\n生命周期\n永久存在\n临时存在，从创建到终止\n\n\n状态\n无状态\n有状态（运行、就绪、阻塞等）\n\n\n资源占用\n不占用系统资源\n占用系统资源（CPU、内存等）\n\n\n并发性\n不支持并发\n支持并发\n\n\n独立性\n独立存在\n进程间独立，可通过 IPC 交互\n\n\n创建方式\n开发者编写，编译生成\n操作系统创建\n\n\n示例\n可执行文件（如 a.exe）\n运行中的程序实例\n\n\n\n 死锁是什么？死锁是怎么产生的？如何处理死锁？\n死锁是操作系统中的一种资源竞争问题，指的是多个进程或线程因为竞争资源而相互等待，导致它们都无法继续执行的状态。死锁是一种严重的系统问题，会导致系统资源浪费和程序无法正常运行。\n\n死锁的产生条件\n\n互斥条件\n不剥夺条件\n请求与保持条件\n循环等待条件\n\n\n死锁的处理方法\n\n死锁的预防\n死锁的检测\n死锁的避免\n死锁的解除\n\n\n\n 饥饿是什么？和死锁有什么区别？\n饥饿是指某个进程或线程因为资源总是被其他进程抢占，导致它长期得不到所需的资源，从而无法继续执行。\n产生原因\n\n资源分配策略不公平：\n\n某些进程总是优先获得资源，而其他进程被忽略。\n\n\n优先级调度问题：\n\n高优先级进程不断抢占资源，低优先级进程始终得不到资源。\n\n\n资源竞争激烈：\n\n资源数量有限，且竞争资源的进程过多。\n\n\n\n\n\n\n特性\n饥饿（Starvation）\n死锁（Deadlock）\n\n\n\n\n定义\n某个进程长期得不到资源\n多个进程相互等待，无法继续执行\n\n\n成因\n资源分配不公平或优先级调度问题\n四个必要条件同时满足\n\n\n涉及进程数量\n可能只有一个进程被“饿死”\n至少有两个或多个进程相互等待\n\n\n资源状态\n资源被其他进程占用，但未被阻塞\n资源被占用且进程相互阻塞\n\n\n解决方法\n公平调度、动态优先级调整、资源预留\n预防、避免、检测与恢复、忽略\n\n\n影响范围\n只影响被“饿死”的进程\n影响所有参与死锁的进程\n\n\n是否可恢复\n可以通过调整资源分配恢复\n需要外部干预（如终止进程）才能恢复\n\n\n\n","categories":["操作系统"],"tags":["OS"]},{"title":"CPU与存储器的连接","url":"/Principles-of-computer-composition/Connection_between_CPU_and_memory/","content":" 前言\n针对这一部分内容我还是推荐大家去看看书本上和其他教学视频里的内容，本篇文章只针对计算机组成原理（第 3 版）P91 页之后的内容进行简单整理汇总。\n接下来我将会按照书上的顺序进行叙述。\n CPU 与存储器的连接\n 存储器容量扩展\n 字扩展\n在某些情况下，字扩展可以被视为一种扩容操作。字扩展通常指的是将一个字符或字符串转换为具有更多位数或更高精度的数据类型，以便进行更复杂的计算或存储更大的值。这个过程可能涉及到内存分配和重新分配，因此可以看作是一种扩容操作。但是，在其他情况下，字扩展可能只是简单地将一个字符或字符串进行格式化或解码，而不涉及任何内存操作。因此，是否将字扩展视为扩容取决于具体的上下文和实现方式。\n但是我们可以简单地把字扩展理解为扩容。\n比如，两片 1K*8 位通过字扩展扩容位逻辑上的一片 2K*8 位\n 位扩展\n位扩展是一种将一个二进制数值的位数增加的操作。在位扩展中，如果原始二进制数值的最高位为 0，则在其左侧添加 0 以增加位数；如果最高位为 1，则在其左侧添加 1 以保持符号不变。例如，将 8 位二进制数值&quot;00110110&quot;进行位扩展为 16 位，则结果为&quot;00000000 00110110&quot;。位扩展通常用于将低精度数据类型转换为高精度数据类型，或者在进行算术运算时对操作数进行对齐。\n咳咳咳，简单理解就是二进制加长\n 字位扩展\n字位扩展是一种将一个数据类型的位数增加的操作，其中“字”指的是计算机中的一个固定大小的数据单元。在字位扩展中，如果原始数据类型的最高位为 0，则在其左侧添加 0 以增加位数；如果最高位为 1，则在其左侧添加 1 以保持符号不变。例如，将 16 位有符号整数进行字位扩展为 32 位，则结果为在最高位之前添加 16 个相同的符号位。字位扩展通常用于将低精度数据类型转换为高精度数据类型，或者在进行算术运算时对操作数进行对齐。\n简单理解就是…结合版\n 例题\n怎么出题呢？一般都像这样：\nCPU 有 16 根地址线、8 根数据线，并用 MREQ 非作为访存控制信号（低电频有效），用 WR 作为读/写控制信号。现在有如下存储芯片：1K X 4 位 RAM、4K X 8 位 RAM、8K X 8 位 RAM、2K X 8 位 ROM、4K X 8 位 ROM、8K X 8 位 ROM及 74138 译码器和各种门电路。现要求如下：\n\n主存地址空间分配：\n\n6000H ~ 67FFH 为系统程序区\n6800H ~ 6BFFH 为用户程序区\n\n\n合理选用上述存储芯片，说明各选几片\n详细画出芯片片选逻辑图\n\n 解！！\n第一步：写出地址空间分配范围二进制码\n6000H ~ 67FFH 为系统程序区\n6800H ~ 6BFFH 为用户程序区\n6000H、67FFH、6800H、6BFFH 化为二进制：\n\n\n\nA15\nA14\nA13\nA12\nA11\nA10\nA9\nA8\nA7\nA6\nA5\nA4\nA3\nA2\nA1\nA0\n\n\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n首先，先确定芯片，全零到全一就是一个芯片\n先看 ROM，也就是系统区：\n\n\n\nA15\nA14\nA13\nA12\nA11\nA10\nA9\nA8\nA7\nA6\nA5\nA4\nA3\nA2\nA1\nA0\n\n\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\nA0 ~ A10 为一个 ROM，一共 11 位，理想是要一个规格为2K X 8 位 ROM\n再来看 RAM ，也就是用户区：\n\n\n\nA15\nA14\nA13\nA12\nA11\nA10\nA9\nA8\nA7\nA6\nA5\nA4\nA3\nA2\nA1\nA0\n\n\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\nA0 ~ A9 为一个 RAM，一共 10 位，理想是要一个规格为1K X 8 位 RAM\n然后呢，我们需要对 ROM 和 RAM 进行读写，这就涉及到切换 ROM 和 RAM 芯片的问题，我们需要弄一个片选信号：\n\n\n\nA15\nA14\nA13\nA12\nA11\nA10\nA9\nA8\nA7\nA6\nA5\nA4\nA3\nA2\nA1\nA0\n\n\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n上两行是 ROM，下两行是 RAM，可以看到 A11 及其关键的决定了到底选择哪一个芯片，是 ROM 还是 RAM。当 A11 为 0 时 ROM 被选中，当 A11 为 1 时 RAM 被选中。\n然而 74138 译码器需要三个数据输入端，这时候我们就需要再拉来两个垫背的 A12 与 A13。\n\n\n\nA15\nA14\nA13\nA12\nA11\nA10\nA9\nA8\nA7\nA6\nA5\nA4\nA3\nA2\nA1\nA0\n\n\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\nA11 A12 A13一次排列连接至 74138 的数据输入端A B C，这一点非常重要！！低位是起始点 A，高位时终点 C。\n数据为100时 ROM 被选中，74138Y4 非端被点亮，因为 100 正是数字 4。\n数据为101时 RAM 被选中，74138Y4 非端被点亮，因为 101 正是数字 5。\n接下来再讨论讨论芯片的选择：\nA0 ~ A10 为一个 ROM，一共 11 位，理想是要一个规格为2K X 8 位 ROM\nA0 ~ A9 为一个 RAM，一共 10 位，理想是要一个规格为1K X 8 位 RAM\n对于 ROM，题中刚好有一个规格为2K X 8 位的 ROM 芯片，拿来便是！\n对于 RAM，题中没有那个规格的我们这里就需要进行扩展，这里我们选择两片 1K X 4 位的 ROM 芯片对他进行位扩展就好了。位扩展 位扩展 位扩展，说明它位数不够，数据一次无法写进一个芯片，但是一次可以写进两个芯片。这样就解决了！\n 总结\nCPU 与存储器的连接是计算机计算机组成原理中的重要考题，更是期末考试中的重点，看着很复杂的连接图，但其实内在的原理还是很简单的，无非就是：写二进制、选芯片、扩展外带画图连接，本质还是比较容易作的\n","categories":["计算机组成原理"],"tags":["计算机组成原理"]},{"title":"DB中的范式","url":"/database/DataBase01/","content":" 数据库范式\n 什么是数据库范式\n数据库范式（Normalization）是一种设计关系型数据库的方法，它旨在减少数据冗余，提高数据的一致性和可靠性，避免数据修改时出现异常。数据库范式通常分为一到五个级别，每个级别对应一组设计规则。\n范式越高，数据冗余越少，数据一致性和可靠性越高。但是，范式过高也可能会带来一些问题，如增加了数据库的复杂性和查询的开销，需要根据实际应用场景来确定使用哪个范式。\n 层级分类\n 1NF（第一范式）\n关系模式中的所有属性都是原子性的，即不可再分。\n 2NF（第二范式）\n关系模式必须满足第一范式，且不存在非关键字属性对主键的部分依赖。\n 3NF（第三范式）\n关系模式必须满足第二范式，且不存在非关键字属性对其他非关键字属性的传递依赖。\n BCNF（Boyce-Codd 范式）\n关系模式必须满足第一范式，且不存在非主属性对主属性的非平凡依赖关系。\n 4NF（第四范式）\n关系模式必须满足第三范式，且不存在多值依赖。\n 5NF（第五范式）\n关系模式必须满足第四范式，且不存在连接依赖。\n 小题目\n让我们通过一些题目来感受一下吧！！\n Ex1\n\n设有关系 R（工号，姓名，工种，定额），则 R 是属于第 2 范式，将其转化为第三范式\n\n根据关系 R（工号，姓名，工种，定额），我们可以看出工号和姓名是候选键，因为它们可以唯一地标识每个元组。同时，我们也可以看出工种和定额这两个属性完全依赖于候选键，因此关系 R 符合 2NF 的要求。\n现在我们来考虑将 R 转化为 3NF 的过程。根据 3NF 的要求，我们需要消除任何非主属性对主键的传递依赖。因此，我们需要检查每个非主属性是否直接依赖于主键，或者是直接依赖于其他非主属性。\n在关系 R 中，我们可以发现定额属性直接依赖于工种属性，而工种属性并不是主键的一部分。因此，我们可以将定额和工种拆分到一个新的关系中，以消除定额对于工种的传递依赖，从而使得关系 R 满足 3NF 的要求。\n拆分后的两个关系如下：\n\nR1（工号，姓名，工种）：该关系包含了工号、姓名和工种三个属性，其中工号和姓名作为候选键，工种是一个非主属性，但它直接依赖于主键。因此，关系 R1 符合 2NF 和 3NF 的要求。\nR2（工种，定额）：该关系包含了工种和定额两个属性，其中工种作为主键，定额是一个非主属性，但它直接依赖于主键。因此，关系 R2 符合 2NF 和 3NF 的要求。\n\n最终，我们将关系 R 转化为两个符合 3NF 的关系 R1 和 R2，符合 3NF 的要求。\n Ex2\n\n设有关系 STUDENT(S#,SNAME,SDEPT,MNAME,CNAME,GRADE)，S#,CNAME 为候选码，设关系中有如下函数依赖：\nS#,CNAME→SNAME,SDEPT,MNAME\nS#→SNAME,SDEPT,MNAME\nS#,CNAME→GRADE\nSDEPT→MNAME\n试求下列问题：\n（1）关系 STUDENT 属于第几范式？\n（2）如果关系 STUDENT 不属于 BCNF，请将关系 STUDENT 逐步分解为 BCNF。\n要求：写出达到每一级范式的分解过程，并指明消除什么类型的函数依赖。\n\n（1）1NF\n（2）首先消除非主属性对候选码的部分依赖，即消除部分函数依赖\n（S#，CNAME）-&gt;（SNAME,SDEPT,MNAME）\nR 分解为：\nR1(S#,SNAME,SDEPT,MNAME)\nR2(S#,CNAME,GRADE)\nR 至此分解为 2NF\n此时 R1 中存在非主属性对候选码的传递依赖，即消除传递依赖\n（S#）-&gt;（SDEPT）\n（SDEPT）-&gt;（MNAME）\nR1 分解为：\nR11(S#,SNAME,SDEPT)\nR12(SDEPT,MNAME)\nR1 至此分解为 3NF\n此时 R11，R12，R2 可以得出的函数依赖：\n（S#,CNAME）-&gt;（GRADE）\n（S#）-&gt;（SNAME）,（S#）-&gt;（SDEPT）\n（SDEPT）-&gt;（MNAME）\n​ 由此可以看出，数据表中的每个非主属性都完全依赖于候选键，即每个非主属性都不只依赖于候选键的一部分，且非主属性无法决定另一个非主属性，即不存在非平凡函数依赖。\n​ 上述关系表 R11，R12，R2 满足 BCNF\n Ex3\n\n一个关系模式不属于第二范式可能会产生( )、( )和 ( )等几个问题，解决的办法是( )。\n\n\n\n第一空：\n插入异常\n\n\n第二空：\n删除异常\n\n\n第三空：\n修改异常\n\n\n第四空：\n投影分解\n\n\n 关键词\n 候选码和非主属性\n候选码和非主属性是关系数据库中的两个概念，它们之间有一定的关系。\n候选码（Candidate Key）指的是能够唯一标识关系模式中每一条记录的属性集合。一个关系模式可能有多个候选码，但是其中只有一个会被选择作为主键（Primary Key）来标识每一条记录。通常情况下，主键会被用来建立关系模式之间的联系，因此选择一个恰当的候选码作为主键是关系数据库设计中的重要步骤。\n非主属性指的是关系模式中除了主键之外的属性。一个关系模式可能包含多个非主属性，它们之间可以存在依赖关系。例如，一个关系模式 R（A，B，C）中，如果存在函数依赖 A→B，则 B 是非主属性。\n候选码和非主属性之间的关系在于，一个候选码包含了所有的主属性，而非主属性可能依赖于主属性，也可能存在依赖于其他非主属性的情况。在进行关系数据库设计时，我们需要通过确定候选码和依赖关系来规范化关系模式，从而消除数据冗余和不一致性的问题。通常情况下，我们会选择最小的候选码作为主键，并确保所有非主属性都完全依赖于主键，以保证数据的完整性和一致性。\n 平凡依赖关系\n平凡依赖关系是指对于一个属性集合 X，在一个关系 R 中如果 X 的任意超集都可以唯一地确定 X，那么 X 对于 R 就是平凡的依赖。例如，在一个关系 R（A, B, C），如果存在函数依赖 A→A，那么 A 对于 R 来说是一个平凡依赖。因为 A 的任意超集都可以唯一地确定 A 本身，即 A 已经包含了它所依赖的属性。\n可以这样理解，一个属性集合如果能够唯一地确定自己，那么它就是平凡的依赖。例如，在一个关系 R（A, B, C），如果存在函数依赖 A→A，那么 A 对于 R 来说是一个平凡依赖，因为属性集合 A 已经包含了它所依赖的属性 A 本身，即 A 已经能够唯一地确定自己。而非平凡的依赖是指一个属性集合不能唯一地确定自己，需要依赖于其他属性才能确定自己，例如在关系 R（A, B, C）中，存在函数依赖 A→B，B 就是一个非平凡依赖，因为 B 不能唯一地确定自己，需要依赖于 A 才能确定。\n 传递依赖\n传递依赖（Transitive Dependency）指的是一个非主属性依赖于另一个非主属性的非主属性。例如，一个关系模式 R（A，B，C）中，如果存在函数依赖 A→B 和 B→C，则 C 对 A 存在传递依赖。这种情况下，如果我们要更新 A 的值，就会导致 C 的值也随之改变，从而引起数据不一致的问题。\n","categories":["数据库","数据库理论"],"tags":["数据库"]},{"title":"政治知识点速记（持续更新）","url":"/PoliticalKnowledge/politicalNote/","content":" 领航\n:::primary no-icon\n部分口诀来自李丽双老师\n主要内容来自《肖秀荣1000题》、《精讲精炼》、《30天70分》的答案解析\n以及其他同学们的笔记\n版权问题请联系作者QQ：735690757\n:::\n 史纲\n 马原理\n 习思想\n 琐碎\n在习近平新时代中国特色社会主义思想的指导下，中国共产党团结带领中国人民，自信自强、守正创新，统揽伟大斗争、伟大工程、伟大事业、伟大梦想，创造了新时代中国特色社会主义的 伟大成就，为实现中华民族伟大复兴提供了更为完善的制度保证、更为坚实的物质基础、更为主动的精神力量。\n 六个坚持\n“六个必须坚持”:必须坚持人民至上、必须坚持自信自立、必须坚持守正创新、必须坚持问题导向、必须坚持系统观念、必须坚持胸怀天下。\n:::primary\n人心正道系天下\n:::\n 党的根本宗旨\n党的根本宗旨是全心全意为人民服务\n 实现途径、行动指南、根本保障、精神力量\n改革开放以来我们取得一切成绩和进步的根本原因，归结起来就是；开辟了中国特色社会主义道路，形成了中国特色社会主义理论体系，确立了中国特色社会主义制度，发展了中国特色社会主义文化。其中，道路是实现途径，理论体系是行动指南，制度是根本保障，文化是精神力量，四者统一于中国特色社会主义伟大实践。\n 四个自信理论支撑和根本依据\n马克思主义深刻揭示了社会主义必然代替资本主义的客观规律，这是我们坚定“四个自信”的理论支撑和根本依据。\n 四个自信的来源\n中国特色社会主义道路自信、理论自信、制度自信、文化自信，来源于实践、来源于人民、来源于真理。\n 四个全面处于主导地位的是\n“四个全面”战略布局既有战略目标又有战略举措，每个“全面”之间具有紧密的内在逻辑是一个整体战略部署的有序展开。全面建设社会主义现代化国家是战略目标，在“四个全面”中居于引领地位;全面深化改革、全面依法治国、全面从严治党是三大战略举措，为全面建设社会主义现代化国家提供重要保障。\n 党的基本路线\n党的基本路线是国家的生命线、人民的幸福线。\n 科学社会主义的主张\n科学社会主义的主张受到中国人民热烈欢迎，并最终扎根中国大地、开花结果，是因为：第一，它回答了近代以来中华民族面临的历史性课题(求得民族独立和人民解放；实现国家富强和人民幸福)、指明了实现中华民族伟大复兴的正确道路(中国特色社会主义是实现中华民族复兴的正确道路);第二，它同我国传承了几千年的优秀历史文化和广大人民日用而不觉的价值观念具有高度的契合性\n 百年奋斗目标-不是顺序的而是交错的100年\n第一个一百年（共产党成立100周年），是到中国共产党成立100年时全面建成小康社会（1921-2021）\n第二个一百年（新中国成立100周年），是到新中国成立100年时建成富强民主文明和谐美丽的社会主义现代化强国（1949-2049）\n 新形势下做好新闻舆论工作的关键\n互联网是意识形态工作的主阵地、主战场、最前沿，管好用好互联网是新形势下做好新闻舆论工作的关键。习近平指出：“过不了互联网这一关，就过不了长期执政这一关。\n 文化是…\n面建成社会主义现代化强国必然要求建设社会主义文化强国。统筹推进“五位一体”总体布局、协调推进“四个全面”战略布局，文化是重要内容；推动高质量发展，文化是重要支点；满足人民日益增长的美好生活需要，文化是重要因素；战胜前进道路上各种风险挑战，文化是重要力量源泉。只有推动文化繁荣兴盛，才能为推进中国式现代化建设、实现第二个百年奋斗目标提供思想保证、舆论支持、精神动力和文化条件。\n 中国共产党的精神之源\n伟大建党精神是中国共产党的精神之源\n 关乎旗帜、关乎道路、关乎国家政治安全，决定文化前进方向和发展道路\n意识形态关乎旗帜、关乎道路、关乎国家政治安全，决定文化前进方向和发展道路。\n 坚持百花齐放、百家争鸣(“双百”)\n坚持百花齐放、百家争鸣(“双百”)是繁荣发展社会主义文化的重要方针。\n 坚持为人民服务、为社会主义服务(“二为”)\n坚持为人民服务、为社会主义服务(“二为”)的根本方向，是决定社会主义文化事业前途命运的关键。\n 区别于其他国家和民族的根本特征\n中华民族创造的优秀传统文化是民族的根脉，根植在中国人内心，形成了中国人看待世界、看待社会、看待人生的独特价值体系、文化内涵和精神品质，这是我们区别于其他国家和民族的根本特征。\n 文化的生命所在、本质特征\n创新创造是文化的生命所在，是文化的本质特征。因此\n 共同富裕的基础性制度\n分配制度\n 初次分配\n第一次分配的主体是市场参与的各要素主体。\n:::primary\n初次分配，即初次收入分配。在社会分配中，初次分配注重效率，是按贡献分配。该贡献包括对创造利润有益的各种因素，如资金、技术、管理、生产资料、劳动力、信息、市场、营销等。谁能利用这些要素作出贡献，就能分到一杯羹。这样群策群力，效率便得以提高。\n参考\n:::\n 再分配\n第二次分配的主体是政府。\n:::primary\n再分配（二次分配）是指政府根据法律法规，在初次分配的基础上通过征收税收和政府非税收入，在各收入主体之间以现金或实物进行的收入再次分配过程。与初次分配不同，再分配中起主导作用的是政府，强调公平的原则，具有通过国家权力强制进行的特征。除了公平的目标外，再分配也通过教育、健康等基本公共服务的提供，创造机会平等的养教环境，以提升社会经济发展的可持续性。\n参考\n:::\n 三次分配\n而第三次分配的主体是民间社会力量，包括企业、社会组织和个人等。\n:::primary\n三次分配有别于前两者，主要是企业、社会组织、家族、家庭和个人等基于自愿原则和道德准则，以募集、捐赠、资助、义工等慈善、公益方式对所属资源和财富进行分配。社会组织和社会力量是三次分配的中坚力量 。\n参考\n:::\n 三次分配的变化\n市场👉政府👉社会\n 军民融合发展\n习近平强军思想明确军民融合发展是兴国之举、强军之策；明确党对人民军队的绝对领导是人民军队建军之本、强军之魂；明确依法治军是我们党建军治军基本方式；明确作风优良是我军鲜明特色和政治优势。\n 政治建军\n政治建军是人民军队的立军之本，抓军队建设首先要从政治上看。\n 依法治军\n依法治军是我们党建军治军基本方式。\n 一流军队要求一流科技\n一流军队要求一流科技，必须全面实施科技强军战略。\n科技是核心战斗力，是军事发展中最活跃、最具革命性的因素。\n 坚持党对人民军队的绝对领导\n军委主席负责制是坚持党对人民军队绝对领导的根本制度和根本实现形式，在党领导军队的一整套制度体系中处于最高层次，居于统领地位。\n 听党指挥——首要、核心\n听党指挥是灵魂，决定军队建设的政治方向；能打胜仗是核心，反映军队的根本职能和军队建设的根本指向。\n 作风优良——保证、性质、宗旨、本质\n作风优良是保证，关系军队的性质、宗旨、本色，是人民军队的鲜明特色和政治优势，也是人民军队战无不胜、攻无不克的重要保证。\n 作风优良——核心\n 关键一招\n改革是决定人民军队发展壮大、制胜未来的关键一招。\n 四个战略支撑\n‌我军新时代使命任务的四个战略支撑是‌：‌\n\n\n为巩固‌中国共产党领导和社会主义制度提供战略支撑‌：确保党的领导和社会主义制度的安全和稳定。\n\n\n为捍卫‌国家主权、统一、‌领土完整提供战略支撑‌：维护国家的领土完整和主权不受侵犯。\n\n\n‌为维护国家海外利益提供战略支撑‌：保护国家在海外的利益和公民的安全。\n\n\n为促进‌世界和平与发展提供战略支撑‌：积极参与国际事务，推动世界和平与发展。\n\n\n 一国两制\n“一国两制”的根本宗旨是维护国家主权、安全、发展利益，保持香港、澳门长期繁荣稳定\n维护国家安全是“一国两制”的核心要义\n维护国家主权、安全、发展利益是“一国两制”方针的最高原则，在这个前提下，香港、澳门保持原有的资本主义制度长期不变，享有高度自治权。\n坚持“爱国者治港”“爱国者治澳”原则是事关国家主权、安全、发展利益，事关香港、澳门长期繁荣稳定的根本原则，是保证香港、澳门长治久安的必然要求\n 全面贯彻新时代党解决台湾问题的总体方略\n\n根本保证坚持党中央对对台工作的集中统一领导\n历史方位在中华民族伟大复兴进程中推进祖国统一\n战略思路在祖国大陆发展进步基础上解决台湾问题\n大政方针“和平统一、一国两制”\n政治基础一个中国原则和“九二共识”\n实践途径推动两岸关系和平发展、融合发展\n根本动力团结台湾同胞、争取台湾民心\n必然要求粉碎“台独”分裂图谋\n外部条件反对外部势力干涉\n战略支撑决不承诺放弃使用武力\n\n 外交工作主线\n服务民族复兴，促进人类进步\n 经济建设主线\n供给侧结构性改革\n 全面深化改革开放主线\n制度建设\n 一带一路\n互联互通——————————主线\n共商共建共享————————原则\n坚持开放、绿色、廉洁————理念\n高标准、可持续、惠民生———目标\n 党建\n加强党的长期执政能力建设，先进性和纯洁性建设\n 构建新型国际关系要秉持原则\n\n相互尊重\n公平正义\n合作共赢原则\n\n没有和平共处\n 区分新型国际关系和大国关系\n国际关系不用和平，大国关系不用共赢。\n 新型国际关系\n尊重—合作—公平\n 新型国际关系又分为不同的具体要求\na、【大国关系格局】——和平共处、总体稳定、均衡发展\nb、周边外交方针\nc、周边外交理念（外交全局的首要位置）\nd、发展中国家——真实亲诚\ne、新型政党关系——求同存异、相互尊重、互学互鉴\n 党的政治建设首要任务\n保证全党服从党中央，维护党中央权威和集中统一领导\n 毛中特\n 琐碎\n“工农武装割据”和农村包围城市、武装夺取政权的思想是毛泽东在土地革命战争时期提出的\n实事求是，就是一切从实际出发，理论联系实际，坚持在实践中检验真理和发展真理，实事求是是A. 中国共产党的思想路线，B. 中国共产党人认识世界、改造世界的根本要求，C. 中国共产党的基本思想方法、工作方法、领导方法，D. 马克思主义的根本观点\n:::primary\n群众路线是党的生命线和根本工作路线\n:::\n 南方谈话的内容\n\n计划和市场都是经济手段;\n阐明了社会主义本质;\n提出了“发展才是硬道理”的重要论断\n提出判断改革开放和各项工作成败得失的“三个有利于”标准;\n强调加强党的建设;\n关于社会主义初级阶段的长期性和前途\n\n 速记口诀一二三四\n一大立党，二大立纲，三大对国共合作的方针和办法做了正式决定，即“党内合作”，四大提出无产阶级领导校和工农同盟军思想，五大批评陈独秀右倾错误，六大提出中国革命的性质是资产阶级民主革命，七大将毛理论命名为毛思想，并将其作为党的指导思想写入党章，八大提出主要矛盾，十二大提出建设中国特色社会主义社会，十三大系统地论述了关于社会主义初级阶段的理论，十四大概括了中国特色社会主义理论的主要内容，系统释了这一理论的历史地位和指导意义，十五大邓理论被确立为中国共产党的指导思想，并写入了党章。\n\n一党\n二纲\n三合作\n四权\n五批\n六性\n七入\n八矛\n十二大建中特\n十三大论阶段 （邓小平轮廓）\n十四大系阐释\n十五大邓入党章\n十八大进入新时代\n\n毛思想-&gt;七大（毛七七）\n邓理论-&gt;十五大\n三个代表-&gt;十六大\n科学发展观-&gt;十七大\n习思想-&gt;十九大\n 一个神奇的口诀\n三线五纲，四体六康\n 十三大\n党的十一届三中全会的胜利召开，结束了粉碎“四人帮”后党和国家工作在徘徊中前进的局面，标志着中国共产党重新确立了马克思主义的思想路线、政治路线和组织路线，开启了我国改革开放和社会主义现代化建设历史新时期，实现了历史性的伟大转折。\n 十八大\n中共十八大的召开，标志着中国已经进入全面建成小康社会的决定性阶段，开启了中国特 色社会主义新时代。\n 毛泽东思想的地位\n毛泽东思想是马克思主义中国化的第一次历史性飞跃的理论成 果；是中国革命和建设的科学指南；是党和人民的宝贵精神财富。\n 有关中心问题\n①无产阶级的领导权是中国革命的中心问题。\n②在中国民主革命中，无产阶级领导权的中心问题是农民问题。\n③毛泽东在《国民革命与农民运动》中还提到“农民问题乃国民革命的中心问题”。\n回答问题的时候要注意题干具体考查的角度。\n 革命的性质（1948毛泽东指出…）\n中国是一个封建经济占明显优势的半殖民地半封建社会，革命是为了终结这个半殖民地半封建的社会形态；中国革命的对象主要是帝国主义和封建主义势力；革命的性质是为了推翻这两个主要敌人的民族革命和民主革命\n 谁?\n中国革命的中心问题、新民主主义苹命理论的核心问题：无产阶级的领导权问题\n中国革命的实质：党领导下的农民革命\n中国革命的最基本动力：无产阶级;\n主力军：农民阶级\n 打谁?\n中国革命的首要问题：分清敌友\n中国革命的对象：帝国主义(首要对象)、封建主义和官僚资本主义\n 为什么打?\n中国革命的根本问题：国家政权问题\n中国革命的基本问题：农民问题\n 怎么打?\n中国革命的主要内容：没收封建地主阶级的士地归农民所有\n 中国民主革命的基本内容\n中国革命走农村包围城市、武装夺取政权的道路，必须处理好土地革命、武装斗争、农村革命根据地建设三者之间的关系。土地革命是民主革命的基本内容；武装斗争是中国革命的主要形式，是农村革命根据地建设和土地革命的强有力保证；农村革命根据地是中国革命的战略阵地，是进行武装斗争和开展土地革命的依托。\n 统一战线的经验\n新民主主义革命时期，中国共产党领导的统一战线，先后经过了第一次国共合作的统一战线、工农民主统一战线、抗日民族统一战线、人民民主统一战线等几个时期，积累了丰富的经验。其中，最根本的经验就是正确处理好与资产阶级的关系当党能够正确处理与资产阶级建立统一战线或被迫分裂统一战线的问题时，党的发展和巩固就会前进；反之，党的发展和巩固就会后退。\n 建立统一战线的必要性\n建立最广泛的统一战线，是由中国半殖民地半封建社会的阶级状况所决定的。毛泽东指出：“中国社会是一个两头小中间大的社会，无产阶级和地主大资产阶级都只占少数，最广大的人民是农民、城市小资产阶级以及其他的中间阶级。”作为无产阶级先锋队的中国共产党所领导的革命力量，要战胜作为地主阶级和官僚资产阶级集中代表的国民党所领导的强大的反革命力量，就必须把农民、城市小资产阶级以及其他的中间阶级都团结在自己的周围，结成最广泛的统一战线。\n 新民主主义革命时期的统一战线\n党领导的统一战线，先后经过了第一次国共合作的统一战线、工农民主统一战线、抗日民族统一战线、人民民主统一战线等几个时期。\n以国共合作为基础的是国民革命联合战线(第一次国共合作的统一战线)与抗日民族统一战线。\n国共就合作了2次，一次是推翻封建主义制度，一次是抗日合作\n 新民主主义革命理论的基本纲领\n新民主主义文化是科学的，是反对一切封建思想和迷信思想主张实事求是、客观真理及理论和实践的一致性。对于封建时代创造的文化，应剔除其封建糟粕，吸收其民主性精华。同时，要尊重中国的历史，反对民族虚无主义，以历史唯物主义的态度对待古今中外文化，发展民族新文化，提高民族自信心。\n ”两步走“\n由于中国革命的反帝反封建的双重任务，决定了革命分两步走：第一步是先进行推翻三座大山的新民主主义革命，第二步是进行社会主义革命。\n “一次革命论”和“二次革命论”都是错误的\n“左”倾教条主义的“一次革命论”(如“无间断”革命论)的错误在于，只看到了民主革命与社会主义革命的联系，而混淆了民主革命与社会主义革命的区别，在反帝反封建的同时，也反对民族资产阶级，否定了中国革命的阶段性。而右的“二次革命论”的错误在于，只看到了民主革命与社会主义革命的区别，而没有看到两个革命阶段的联系，放弃了党对民主革命的领导权。\n ”两头小中间大”的社会\n毛泽东指出：“中国社会是一个两头小中间大的社会，无产阶级和地主大资产阶级都只占少数，最广大的人民是农民、城市小资产阶级以及其他的中间阶级。”\n 三件“法宝”\n统一战线、武装斗争、党的建设\n:::primary\n战斗党\n:::\n 战胜敌人的两个基本武器\n统一战线、武装斗争\n:::primary\n战斗\n:::\n 中国革命道路\n土地革命、武装斗争、农村革命根据地建设\n:::primary\n土斗根\n:::\n 活的灵魂\n实事求是、群众路线、独立自主\n:::primary\n实众独\n:::\n 优良作风\n理论联系实际、密切联系群众、批评与自我批评\n 新民主主义革命纲领最具特色一项\n保护民族工商业，是新民主主义经济纲领中极具特色的一项内容。在新民主主义条件下保护民族工商业，发展资本主义，是由中国落后的生产力和新民主主义革命的性质所决定的。新民主主义革命的对象是帝国主义、封建主义和官僚资本主义，而不是一般地消灭资本主义和资产阶级。\n 新民主主义革命胜利的历史意义\n新民主主义革命的伟大成就，为实现中华民族伟大复兴创造了根本社会条件。\n 各个阶段中国社会的主要矛盾\n1840—1949，中华民族与帝国主义的矛盾。\n1949—1952，与帝国主义、封建主义、国民党残余势力之间的矛盾。\n1952—1956，工人阶级和资产阶级之间的矛盾。（土改基本完成）\n1956—1978，人民对于建立先进的工业国的要求同落后的农业国的现实之间的矛盾，人民对于经济文化迅速发展的需要同当前经济文化不能满足人民需要的状况之间的矛盾。\n1978，人民日益增长的物质文化需要同落后的社会生产之间的矛盾。\n2018，人民日益增长的美好生活需要和不平衡不充分的发展之间的矛盾。\n 互助组、常年互助组、初级合作社、高级合作社\n互助组(包括临时互助组和常年互助组两种形式)不涉及生产资料问题，只是在生产方面组织起来、互帮互助，是农业合作化的最初过渡形式，具有社会主义萌芽性质。初级农业生产合作社以土地入股和统一经营为特点，实行集体劳动，产品分配采取按劳分配和土地入股分红相结合，具有半社会主义性质。高级农业生产合作社实行生产资料农民集体所有，实行按劳分配，具有完全的社会主义性质。\n 民族资产阶级的两面性\n既有剥削工人取得利润的一面，又有拥护中国共产党的领导、拥护宪法、愿意接受社会主义改造的一面\n 民族资产阶级与工人阶级的矛盾的两重性\n既有剥削者与被剥削者的阶级利益相互对立的对抗性的一面，又有相互合作、具有相同利益的非对抗性的一面\n 资本主义工商业的社会主义改造经历了三个步骤\n第一步(初级形式的国家资本主义)和第二步(个别企业的公私合营)的企业利润分配都是四马分肥。第三步(全行业的公私合营)在分配上实行定股定息。\n:::primary\n第一步实行初级形式的国家资本主义。国家在私营工业中实行委托加工、计划订货、统购包销，这主要是同资本家在企业外部的合作。\n第二步实行个别企业的公私合营。这是社会主义成分同资本主义成分在企业内部的合作。\n第三步实行全行业的公私合营。全行业公私合营后，国家对合营企业进行清产核资、定股定息，委派人员负责企业的生产经营管理，统一调配企业的人、财、物，生产资料为国家所有。全行业公私合营后，企业的生产关系已经发生了根本的变化，基本上成为社会主义国营性质的企业。\n:::\n 推进手工业合作化的方针\n在推进手工业合作化的过程中，中国共产党采取的是积极领导、稳步前进的方针。\n 推进农业社会主义改造的原则\n这条道路遵循自愿互利、典型示范和国家帮助的原则，以互助合作的优越性吸引农民走互 助合作道路。\n:::primary\n士力架\n:::\n 资本主义工商业社会主义改造的方针\n和平赎买\n 经济成分构成及阶级构成\n三种主要的经济成分：社会主义经济、个体经济、资本主义经济\n阶级构成：工人、农民、民族资产阶级、其他小资产阶级\n其中，半社会主义性质的合作社经济是个体经济向社会主义集体经济过渡的形式，国家资本主义经济是私人资本主义经济向社会主义国营经济过渡的形式。\n 萌芽性质、半社会主义性质、完全社会主义性质\n社会主义萌芽：农业互助组，手工业供销小组，资本主义工商业初级国家资本主义经济\n半社会主义：农业初级生产合作社，手工业供销合作社，工商业个别企业公私合营\n完全社会主义：农业高级生产合作社，手工业生产合作社，工商业全行业公私合营\n:::primary\n工商业:高级国家资本主义=个别企业公私合营+全行业公私合营\n:::\n 1956年年底社会主义改造的基本完成\n\n标志着中国历史上长达数千年的阶级剥削制度的结束\n实现了由新民主主义向社会主义的转变\n标志着社会主义基本制度在我国确立\n标志着我国的社会主义初级阶段从此开始\n\n 两个结合和“两次结合”\n毛泽东的两次结合：马+革命、马+建设道路\n习近平的两个结合：马+中国国情、马+传统文化\n 《论十大关系》——探索中国社会主义建设道路的良好开端\n毛泽东在1956年4月25日中央政治局扩大会议和5月2日最高国务会议上作《论十大关系》的报告。强调以苏为鉴、独立自主地探索适合中国情况的社会主义建设道路。其中，《论十大关系》中的第一大关系，即重工业和轻工业、农业的关系。\n在《论十大关系》的报告中，初步总结了我国社会主义建设的经验，明确提出要以苏为鉴，独立自主地探索适合中国情况的社会主义建设道路。《论十大关系》标志着党探索中国社会主义建设道路的良好开端。\n:::primary\n十全十美良好开端\n:::\n 《关于正确处理人民内部矛盾的问题》\n《关于正确处理人民内部矛盾的问题》一文，是一篇重要的马克思主义文献。它创造性地阐述了社会主义社会矛盾学说，是对科学社会主义理论的重要发展，对中国社会主义事业具有长远的指导意义。\n《关于正确处理人民内部矛盾的问题》指出，对于政治思想领域的人民内部矛盾，实行“团结—批评—团结”的方针，坚持说服教育、讨论的方法。\n 中国特色社会主义理论体系形成发展的帽子题\n中国特色社会主义理论体系形成发展的社会历史条件：【国际背景+历史条件+实践基础】\n而如果光问历史条件就不一样，社会历史条件范围要大一些\n:::primary\n国际形势的深刻变化和世界发展新趋势——国际背景;\n建设社会主义正反两方面经验和我国发展的历史方位——历史条件;\n改革开放和社会主义现代化建设实践——实践基础\n:::\n 中国特色社会主义理论体系形成发展的历史条件\n党的十一届三中全会以后，中国共产党人鲜明指出建设社会主义没有固定的模式，**必须结合中国实际，在实践中不懈探索和回答什么是社会主义、怎样建设社会主义这一基本问题。**正是在探索和回答这一首要的基本的理论问题的过程中，我们党开创了中国特色社会主义的伟大事业。\n 马克思主义中国化的第一次飞跃与第二次飞跃\n==马克思主义中国化的第一次飞跃，是党在探索中国革命道路的过程中完成的。==在毛泽东思想的指导下，党领导人民走以农村包围城市的道路，取得了新民主主义革命的胜利；走具有中国特色的社会主义改造的道路，积极探索社会主义建设道路，取得社会主义革命和建设的伟大成就。\n马克思主义中国化时代化的第二次飞跃是中国特色社会主义理论体系。\n 毛泽东思想的精髓\n实事求是\n 邓小平理论的精髓\n解放思想，实事求是\n 十三大提出社会主义初级阶段基本路线\n基本途径：一个中心，两个基本点（以经济建设为中心，坚持改革开放，坚持四项基本原则）\n跟本立足点：自力更生，艰苦创业\n领导力量和依靠力量：领导和团结各族人民\n奋斗目标：建设富强民主文明的社会主义现代化国家\n 几个帽子\n改革——是社会主义社会发展的直接动力;\n阶级斗争——是阶级社会发展的直接动力;\n科技——是第一生产力；\n科技革命–是社会发展的重要动力。\n解放生产力，发展生产力，是社会主义的根本任务。\n 三个有利于\n\n是否有利于发展社会主义生产力\n是否有利于增强社会主义国家的综合国力\n是否有利于提高人民的生活水平\n\n:::primary\n生国人\n:::\n 正确认识和处理改革、发展、稳定的关系\n改革是动力，发展是目的，稳定是前提。要把改革的力度、发展的速度和社会可承受的程度统一起来，把不断改善人民生活作为处理改革、发展、稳定关系的重要结合点，在社会稳定中推进改革发展，通过改革发展促进社会稳定。\n “三个代表”重要思想的”花边“\n“三个代表”重要思想是在对冷战结束后国际局势科学判断的基础上形成的，是在科学判断党的历史方位和总结历史经验的基础上提出来的，是在建设中国特色社会主义伟大实践的基础上形成的。\n “三个代表”重要思想\n\n中国共产党始终代表中国先进生产力的发展要求\n始终代表中国先进文化的前进方向\n始终代表中国最广大人民的根本利益\n\n 新民主主义革命时期 1921-1949\n\n\n\n子时期\n时间\n\n\n\n\n党的创立和大革命时期\n1921-1927\n\n\n土地革命战争时期\n1927-1937\n\n\n全民族抗日战争时期\n1937-1945\n\n\n解放战争时期\n1945-1949\n\n\n\n 新民主主义社会时期 1949-1956\n 社会主义建设时期 1956-1978\n 1978之后\n\n\n\n大事件\n时间\n\n\n\n\n十一届三中全会提出改革开放\n1978\n\n\n提出命题“建设有中国特色的社会主义\n1982\n\n\n苏联解体，冷战结束\n1991\n\n\n南方谈话\n1992\n\n\n中共十四大，确立社会主义市场经济体制的改革目标\n1992\n\n\n邓小平理论提出\n1997\n\n\n三个代表指导思想\n2002\n\n\n\n 科学发展观\n第一要义：发展\n核心立场：以人为本\n基本要求：全面可持续发展\n根本方法：统筹兼顾\n保持先进性是党自身建设的根本任务和永恒课题\n 科学发展观的形成条件\n\n是在深刻把握我国基本国情和新的阶段性特征的基础 上形成和发展的。\n是在深入总结改革开放以来实践经验的基础上形成和发展的。\n是在深刻分析国际形势及借鉴国外发展经验基础上形成和发展的。\n\n 领导人的思想精髓\n毛泽东：实事求是\n邓小平：解放思想，实事求是\n江泽民：解放思想，实事求是，与时俱进\n胡锦涛：解放思想，实事求是，与时俱进，求真务实\n 思修\n 琐碎\n中国特色社会主义建设实践是社会主义核心价值观的现实基础\n先进性、人民性和真实性是社会主义核心价值观的道义力量\n 道德作用\n认识功能—反映揭示（行为前）\n规范功能—规范引导（行为中）\n调节功能—指导纠正（行为后）\n 爱国主义\n爱国主义是 道德要求 政治原则 法律规范 精神纽带\n以爱国主义为核心的民族精神，以改革创新为核心的时代精神\n 信念\n执着性、支撑性、多样性\n 理想\n超越性、实践性、时代性\n 价值观\n先进性、人民性、真实性\n 区分创造精神，梦想精神，团结精神，和奋斗精神\n神话就是梦想，发明一类就是创造，开垦治理河流之类的就是奋斗，剩下的那个就是团结\n 文化\n文化是涵养民族心理、民族个性、民族精神的摇篮，而不是祖国大好河山\n 祖国河山\n祖国的河山在人们的心中占据着至高无上的地位，祖国的山山水水滋养哺育着她的子子孙孙。“禾苗离土即死，国家无土难存”,祖国的大好河山，不只是自然风光，更是主权、财富、民族发 展和进步的基本载体。因此，每一个爱国者都会把“保我国土” “爱我家乡”、维护祖国领土的完整和统一，作为自己的神圣使命和义不容辞的责任。\n 中华传统美德的根本要求\n中华传统美德的根本要求是公义胜私欲\n 道德的作用\n\n道德为经济基础的形成、巩固和发展服务，是一种重要的精神力量。\n道德对其他社会意识形态的存在有着重大的影响\n道德通过调整人们之间的关系维护社会秩序和稳定\n道德是提高人的精神境界、促进人的自我完善，推动人的全面发展的内在动力\n在阶级社会中，道德是阶级斗争的重要工具\n\n 公共生活的四个特征\n\n一是活动范围的广泛性\n二是活动内容的开放性\n三是交往对象的复杂性\n四是活动方式的多样性\n\n 全心全意为人民服务是贯穿中国革命道德的一根红线\n\n是坚持历史唯物主义必然要求\n是中国共产党践行根本宗旨\n也是社会主义道德观集中体现\n是全体中国人民共同遵循道德要求\n是社会主义经济基础和人际关系的客观要求\n是社会主义市 场经济健康发展的要求\n是先进性要求和广泛性要求的统一\n为人民服务是社会主义道德的核心\n集体主义是社会主义道德的原则\n\n 法律的作用\n\n普遍适用（遵守）\n优先适用\n不可违返（遵守+制裁）\n\n 时政（2024年）\n 二十届三中全会\n\n通过了《中共中央关于进一步全面深化改革、推进中国式现代化的决定》（最重要成果）\n当前和今后一个时期是以中国式现代化全面推进强国建设、民族复兴伟业的关键时期\n高水平社会主体市场经济体制是中国式现代化的重要保障\n教育、科技、人才是中国式现代化的基础性、战略性支撑\n城乡融合发展是中国式现代化的必然要求\n开放是中国式现代化的鲜明标识\n发展全过程人民民主是中国式现代化的本质要求\n法治是中国式现代化的重要保障\n在发展中保障和改善民生是中国式现代化的重大任务\n国家安全是中国式现代化行稳致远的重要基础\n国防和军队现代化是中国式现代化的重要组成部分\n党的领导是进一步全面深化改革、推进中国式现代化的根本保证\n\n 2024年中非合作论坛北京峰会\n2024年中非合作论坛北京峰会的主题为“携手推进现代化，共筑高水平中非命运共同体”。\n治国理政、工业化和农业现代化、和平安全、高质量共建“一带一路”分别是中非合作论坛北京峰会四场高级别会议的议题。\n 培养社会主义法治思维\n宪法保障、立法保障、行政保障和司法保障。\n宪法保障是权利保障的前提和基础，\n立法保障是权利保障的重要条件，\n行政保障是权利保障的关键环节，\n司法保障是公民权利保障的最后防线。\n 建设中国特色社会主义法治体系\n建设中国特色社会主义法治体系，就是要形成完备的法律规范体系、高效的法治实施体系、严密的法治监督体系、有力的法治保障体系，形成完善的党内法规体系。\n形成完备的法律规范体系，是中国特色社会主义法治体系的前提，是法治国家、法治政府、法治社会的制度基础；\n建设高效的法治实施体系，是建设中国特色社会主义法治体系的重点；\n形成严密的法治监督体系，是宪法法律有效实施的重要保障；\n建设有力的法治保障体系，是全面依法治国的重要依托；\n形成完善的党内法规体系，是中国特色社会主义法治体系的本质要求和重要内容。\n 六个必须坚持\n\n\n\n坚持\n是…\n体现了\n\n\n\n\n人民至上\n根本价值立场\n唯物主义群众史观\n\n\n自立自信\n内在精神特质\n客观规律性和主观能动性的有机结合\n\n\n守正创新\n鲜明理论品格\n变与不变，继承与发展的内在联系\n\n\n问题导向\n重要实践要求\n矛盾的普遍性与客观性\n\n\n系统观念\n基本思想和工作方法\n辩证唯物主义普遍联系的原理\n\n\n胸怀天下\n中国共产党人的境界格局\n马克思主义追求人类进步和解放的崇高理想\n\n\n\n\n\n\n\n\n 新质生产力口诀\n关谷悠悠业务劳累咳嗽，立志全心做点心。\n\n关键在于质优（关谷悠悠）\n重点任务是新产业（业务）\n内涵——劳动者、劳动对象、劳动资料（劳累）\n核心要素——科技创新（咳嗽）\n本质——先进生产力（立志）\n核心标志——全要素生产率（全心）\n显著特点——创新（创新）\n\n 多民族国家…\n血脉相融——历史根基\n信念相同——内生动力\n文化相通——文化基因\n经济相依——强大力量\n 科技创新和产业创新深度融合\n基础——高质量科技供给\n关键——企业科技创新主体地位\n途径——促进科技成果转化应用\n 主观题解题\n 问题分类\n\n为什么\n是什么…如何理解？\n意义是？\n怎么做？未来怎么做？\n\n四大类型+主语\n 万能三步走\n观点【出发点】+落脚点+论据\n 中国声音关键词\n携手共建人类命运共同体、和平、公道正义、共同发展、发展与安全、公正\n一切的核心是共同发展，构建人类命运共同体。\n和平共处五项原则发表70周年，引出中非合作论坛峰会，引出上海合作组织、引出国际友好大会、引出金砖国家领导人第十六次会晤、引出二十国集团领导人第十九次峰会。\n抓住主旋律：\n\n观点：世界和平发展、安全、公道正义、合作、多边主义\n落脚点：人类命运共同体、全人类共同价值\n\n 小小模板\n 38时政，第一问\n当前，世界百年变局加速推进，一些西方国家为维护自身霸权，大搞单边主义、保护主义，建立封闭排他的“小圈子”，危害世界和平与共同发展。\n①（主语【材料】金砖....）坚持开放包容、合作共赢的初心使命，顺应全球南方崛起大势。\n②（主语【材料】中非....）走到一起，顺应世界和平和发展发出和平之声，倡导互惠包容的经济全球化和世界多极化，坚守共同发展的大道。\n③（主语【材料】和平共处五项原则....）树立了历史标杆，为不同社会制度国家建立和发展关系提供正确指导，为发展中国家团结合汇聚强大合力，为推动国际秩序朝着更加公正合理的方向贡献了中国智慧。\n\n 38时政，第二问\n历史和现实告诉我们，各国必须共担维护和平责任，同走和平发展道路，要秉持公道正义的理念，展现开放包容的胸襟。\n①我们要建设“和平金砖”、“创新金砖”、“绿色金砖”、“公正金砖”，“人文金砖”。\n②我们要坚持和平的原则，夯实相互尊重的基础，实现和平安全的愿景，汇聚共筑繁荣的动力。\n③提出“坚持和平，实现共同安全” “重振发展，实现普遍繁荣” “共兴文明，实现多元和谐发展”三点主张。\n在世界变局乱局中开辟长治久安、共同繁荣的人间正道，奋力开创中国特色大国外交更有作为的新局面，为进一步全面深化改革、推进中国式现化、推为构建人类命运共同体作出新的贡献。\n\n有金砖写一二，无金砖选二三\n 答题逻辑\n\n抛出考点\n解释考点\n阐明关系\n方法论\n结合材料\n\n 肖四25——大题逻辑练习\n 马原部分\n:::primary\n（1）结合“该改的坚决改，不该改的不改”，说明为什么必须坚持守正创新。\n:::\n“该改的坚决改，不该的不改”，体现的正是守正与创新的辩证统一。“不该改的不改”，守正才能不迷失方向，不犯颠覆性错误，“该改的坚决改”，创新才能把握时代，引领时代。\n坚持守正不动摇，所谓守正，就是坚持实事求是，坚持真理性认识，坚持正确的政治方向。\n坚持创新不停步，所谓创新，就是坚持解放思想，秉承科学的思想观念，发现和运用事物的新联系、新观点、新规律，更有效的认识世界改造世界。\n守正是创新的前提和基础，创新是守正的目的和路径。守正创新深刻的揭示了“变”与“不变”、继承与发展的辩证统一。\n守正创新为党和人民事业提供了科学的观点、立场、方法。\n:::primary\n（2）为什么要“坚持破和立的辩证统一”？\n:::\n唯物辩证法认为，事物发发展是通过其自身的矛盾运动以自我否定的方式来实现的，着要求我们掌握破与立的辩证统一关系。\n所谓破，就是要破除与事物发展进程不相符合的旧观点、旧理论、旧方法。所谓立，就是秉承科学的理念，寻找并发现事物的新联系、新方法、理论。\n破与立是矛盾着的对立面，即相区别，又相联系，二者对立统一，在事物的发展中起到重要的作用。该原理要求我们掌握破与立的辩证统一关系。\n:::primary\n（1）运用唯物史观原理，说明为什么“发展新质生产力，必须进一步全面深化改革，形成与之相适应的新型生产关系”。\n:::\n唯物史观认为，生产力与生产关系是对立统一的有机体。\n生产力时生产的物质内容，生产关系是生产的社会形式。\n发展新质生产力就要形成与之相匹配的生产关系这是因为，生产力决定生产关系，而生产关系对生产力具有能动的反作用。生产关系对生产力能动的反作用主要体现在两个方面：当生产关系适应生产力的发展时就生产力起到促进和推动的作用，当生产关系不适应生产力的发展时就对成产力起到阻碍的作用。\n生产力与生产关系的相互作用，构成了生产关系要适应生产力的客观规律，这是社会形态发展的普遍规律。\n:::primary\n（2）善用因地制宜的“土办法”体现了怎样的辩证思维？（或：为什么不能“一哄而上”“简单 套用单一发展模式”，而要“因地制宜”发展新质生产力？）\n:::\n辩证法认为，矛盾具有普遍性和特殊性，二者是辩证统一的整体。\n矛盾的普遍性就是，矛盾无处不在，无时不有，矛盾具有普遍存在的特点。矛盾的特殊性在于，事物有着不同的矛盾以及同一矛盾在不同时期体现出不同的特点，只有具体分析矛盾的特殊性，才能认清事物的本质与发展规律。\n矛盾的普遍性和特殊性辩证统一也是共性与个性的辩证统一，共性寓于个性之中，个性也离不开共性，任何事物都是个性与共性的统一体。\n这要求我们掌握矛盾的普遍性与特殊性的其二者的关系，更好的理解事物的本质。善用“因地制宜”的办法正是体现了具体问题具体分析的辩证思维。\n:::primary\n（3）从理论创新和实践创新的辩证关系角度，分析为什么要将已经在实践中形成的新质生产力 “从理论上进行总结、概括，用以指导新的发展实践”。\n:::\n人类的创新活动主要体现在理论创新和实践创新两个方面，实践创新为理论创新提供了不竭动力，理论创新为实践创新提供了科学的行动指南，理论创新与实践创新二者构成了有机的统一体，我们要坚持将理论创新与实践创新相联系。\n理论与实践相统一，理论创新不仅要以实践创新为基础，还要以科学的知道反哺实践创新。\n","categories":["笔记","考研政治笔记"],"tags":["政治"]},{"title":"数据库设计","url":"/database/DataBase02/","content":" 数据库设计\n 什么是数据库设计\n数据库设计是指根据需求分析和业务流程，设计出符合应用需求的数据库结构和表结构的过程。通常包括以下几个步骤：\n\n需求分析：对应用的功能和数据进行分析，确定应用所需的数据和数据间的关系，明确数据的属性和约束条件。\n概念设计：将需求分析的结果转化为概念模型，包括实体-关系模型和 E-R 图等，以及各个实体之间的联系、属性及约束等。\n逻辑设计：将概念模型转化为逻辑模型，包括表的定义、属性定义、关键字、完整性规则等，确定表与表之间的联系，构建数据模型。\n物理设计：在逻辑设计的基础上，结合具体的数据库管理系统（如 MySQL、Oracle、SQL Server 等），考虑实现方式和优化策略，设计出物理存储结构，包括数据表的存储方式、索引、分区等等。\n\n在数据库设计的过程中，需要遵循以下原则：\n\n数据库的设计应该符合应用需求和业务流程，具有可扩展性、可维护性和可靠性。\n数据库应该遵循数据规范化的原则，减少数据冗余和数据异常，提高数据存储效率。\n数据库设计应该考虑数据的完整性和安全性，采取适当的权限管理和数据备份策略，保证数据的安全性和可靠性。\n数据库设计应该遵循一定的命名规范，使数据库的结构清晰易懂，便于后期的维护和管理。\n\n综上所述，数据库设计是一个综合性较强的任务，需要综合考虑多方面的因素，才能设计出符合应用需求和业务流程，高效可靠的数据库结构。\n 数据库设计过程\n\n需求分析\n概念结构设计\n逻辑结构设计\n数据库物理设计\n数据库实施\n数据库运行与维护\n\n 数据库设计过程各个阶段上的设计描述\n数据库设计过程一般包括以下几个阶段：\n\n需求分析阶段：在需求分析阶段，需要进行数据需求的调研和收集，明确系统需要存储的数据、数据之间的关系以及对数据的操作等，设计描述的主要内容包括数据流图、数据字典等。\n概念设计阶段：在概念设计阶段，需要将需求分析得到的数据需求转化为概念模型，设计描述的主要内容包括实体关系图（ER 图）。\n逻辑设计阶段：在逻辑设计阶段，需要将概念模型转化为具体的逻辑模型，设计描述的主要内容包括关系模式、属性、主键、外键、索引等。\n物理设计阶段：在物理设计阶段，需要将逻辑模型转化为数据库实现模型，设计描述的主要内容包括表结构、存储方式、物理索引等。\n\n在每个阶段中，设计描述的内容都不同，但都是在将需求转化为具体的数据库结构的过程中所必需的，而且在设计描述的过程中，需要对需求进行细致的分析和理解，以保证设计结构的正确性和实用性。\n","categories":["数据库","数据库理论"],"tags":["数据库"]},{"title":"计算机中的除法运算","url":"/Principles-of-computer-composition/Division_in_Computers/","content":" 计算机中的除法运算\n 简述\n计算的中的除法十分有趣。对于 6 与 4，我们可以很容易的判断出那个大，进而进行后续的除法运算，而计算机中却不是。比此更复杂的除法运算，又是怎么实现的呢？\n 方法一：恢复余数法\n题目源自《计算机组成原理（第三版）》例 6.24\n\n\n\n\n被除数（余数）\n商\n说明\n\n\n\n\n0.1011+1.0011=1.1110\n【0.0000】\n第一步无条件 - y*\n\n\n1.1110+0.1101=0.1011\n【0.0000】\n1.1110 小于零 恢复余数\n\n\n1.0110\n【0.000】0\n上 0 左移 （补位来自商）\n\n\n1.0110+1.0011=0.1001\n【0.000】0\n0.1001 大于 0 无需恢复\n\n\n1.0010\n【0.00】01\n上 1 左移 （补位来自商）\n\n\n1.0010+1.0011=0.0101\n【0.00】01\n0.0101 大于 0 无需恢复\n\n\n0.1010\n【0.0】011\n上 1 左移 （补位来自商）\n\n\n0.1010+1.0011=1.1101\n【0.0】011\n1.1101 小于 0 恢复余数\n\n\n1.1101+0.1101=0.1010\n【0.0】011\n\n\n\n1.0100\n【0】.0110\n上 0 左移（补位来自商）\n\n\n1.0100+1.0011=0.0111\n【0】.0110\n0.0111 大于零 无需恢复\n\n\n0.0111\n0.1101\n上 1\n\n\n\n值：0.1101\n符号位：\n\n恢复余数法总结：首先第一步无条件加 - y*，然后看余数大于零还是小于 0，如果大于 0 无需恢复 上 1 左移，如果小于 0 需要恢复 上 0 左移，如此循环往复，直到用完所有的商位。\n 方法二：加减交替法\n加减交替法可以说是恢复余数的改良方法，我们无需恢复 直接进行运算，详情见下题：\n题目源自《计算机组成原理（第三版）》例 6.25\n\n\n\n\n被除数（余数）\n商\n说明\n\n\n\n\n0.1011+1.0011=1.1110\n【0.0000】\n无条件加 - y*\n\n\n1.1100\n【0.000】0\n1.1110 小于0 上0 左移\n\n\n1.1100+0.1101=0.1001\n\n\n\n\n1.0010\n【0.00】01\n0.1001 大于0 上 1 左移\n\n\n1.0010+1.0011=0.0101\n\n\n\n\n0.1010\n【0.0】011\n0.0101 大于0 上 1 左移\n\n\n0.1010+1.0011=1.1101\n\n\n\n\n1.1010\n【0】.0110\n1.1101 小于0 上 0 左移\n\n\n1.1010+0.1101=0.0111\n\n\n\n\n0.0111\n0.1101\n0.0111 大于 0 上 1\n\n\n\n核心：上 0 加正，上 1 加负，此口诀为上一步上的 0 或 1 在下一步体现，如果上了0那么下一步就加正，如果上了1那么下一步就加负。那么我们怎么知道到底是上 0 还是 1 呢？这个主要看余数，如果它是正的就上 1，如果他是负的就上 0\n","categories":["计算机组成原理"],"tags":["计算机组成原理"]},{"title":"回望2025——永不熄灭之光","url":"/life/p2025/","content":" 永不熄灭之光\n2025年，是一个充满挑战也充满希望的年份。\n这一年，我结束了大学生涯，进入研究生阶段。\n这一年，我与旧时好友挥手告别，也结识了许多新朋友。\n这一年，我学会在不确定中前行，在压力与迷茫中寻找方向。\n这一年，也是自我怀疑与重塑信念彼此交织的一年。\n那些看似平凡的日子，终将在回望时，汇聚成值得珍藏的记忆。\n我曾在深夜反复审视自己的选择，也曾在沉默中质问前路的意义。\n所谓坚定，并非从未动摇，\n而是在一次次动摇之后，仍愿意选择前行。\n那些未被看见的努力，那些无人知晓的坚持，\n如暗夜中的星火，虽小，却足以照亮漫长的路。\n心中的那束光，历经风雨，依然未曾熄灭。\n2025年我开启了全新篇章，学习了很多新技术，\n这些知识不仅改变了我的学习方式，也重塑了我看待问题的视角，\n让我在未知面前，少了一分畏惧，多了一分从容。\n由此，我将其称之为：2025 永不熄灭之光。\n\n2026年我仍将持续努力，不断学习新技术，奋勇前行。\n畅游在艺术与编程的海洋中， 用代码艺术编织未来。\nSwim in the ocean of art and programming, weave the future with code art.\n","categories":["生活"],"tags":["生活"]},{"title":"基于用户的协同过滤算法","url":"/experience/CollaborativeFiltering/","content":" 基于用户的协同过滤算法\n本篇文档将会用通俗易懂的方式带你来了解并实现基于用户的协同过滤算法，不再基于特定开源库，使用原生代码完成协同过滤的构建，实际上，基于用户的协同过滤很简单，只需要弄懂那几个公式是怎么用的就好了，真正的难点是公式的提出与证明，不过呢今天我不想讲相关的证明了。\n本篇文章主要是带你打开协同过滤的大门，我只是想告诉你，作为人的你我具有不断认识的能力，且在认识中不断消除谬误，并无限趋近于真理，我们有能力也必将认识这个世界，而协同过滤正是其中之一，究其本质并不复杂。\n那么请坐好，协同过滤的列车要发车啦！\n 算法介绍\n基于用户的协同过滤，实际上就是找与目标用户最为相似的一个或多个用户，这里的目标用户实际上就是我们要为其进行信息推送的用户。假如瑞瑞喜欢A和B，鸭鸭喜欢A、B和C，他们都对喜欢的商品给予较高的评价，嘎嘎非常讨厌A和B，但是钟爱D。\n那么我们可以看到似乎鸭鸭的A和B与瑞瑞的A和B是一样的，但是鸭鸭买过C而瑞瑞没有买过C，瑞瑞和鸭鸭是高度相似的，那么我们就可以给瑞瑞推荐鸭鸭喜欢的C，或许瑞瑞也喜欢呢！\n而对于嘎嘎来说瑞瑞喜欢的A和B正是嘎嘎他所讨厌的，而嘎嘎对D情有独钟，我们可以了解到，瑞瑞或许是讨厌嘎嘎所喜欢的，那么嘎嘎和瑞瑞是低相似的，嘎嘎喜欢的D也许不应该给瑞瑞推荐！\n\n 计算用户间的相似度\n Jaccard相似系数\nJaccard相似系数主要是衡量两个集合的相似度J(A,B)是它的数学表达\nJ(A,B)=∣A∩B∣∣A∪B∣\\begin {array}{c}\nJ(A,B)=\\frac{\\left | A\\cap B \\right | }{\\left | A\\cup  B \\right |}\n\\end {array}\nJ(A,B)=∣A∪B∣∣A∩B∣​​\nJaccard相似系数特别适用于处理二值数据（即只有两个值：0或1）。例如，在文本处理中，可以通过是否包含某个词来表示文档中的词汇出现与否，这种情况下Jaccard相似系数非常有效。然而 Jaccard相似度不考虑元素的权重或频率。如果两个集合中有的元素出现频率较高，而有的频率较低，Jaccard相似度会将这些元素视为等价，不适用于需要考虑元素重要性的场景。\n 余弦相似度\n余弦相似度是n维空间中两个n维向量之间角度的余弦。它等于两个向量的点积（向量积）除以两个向量长度（或大小）的乘积。\nsim(A,B)=A⋅B∣∣A∣∣×∣∣B∣∣=∑i=1n(Ai×Bi)∑i=1nAi2×∑i=1nBi2sim(A,B)=\\frac{A·B}{\\left | \\left | A \\right |  \\right | \\times \\left | \\left | B \\right |  \\right | } =\\frac{\\sum_{i=1}^{n}(A_{i}\\times B_{}i)}{\\sqrt{\\sum_{i=1}^{n}A_{i}^{2} } \\times \\sqrt{\\sum_{i=1}^{n}B_{i}^{2} }} \nsim(A,B)=∣∣A∣∣×∣∣B∣∣A⋅B​=∑i=1n​Ai2​​×∑i=1n​Bi2​​∑i=1n​(Ai​×B​i)​\nsim(A,B)sim(A,B)sim(A,B)是有取值范围的，在[−1,1][-1,1][−1,1]之间，1是完全相似，-1是完全不相似，余弦相似度的计算公式简单，通常只涉及向量的点积和模长的运算，因此在实际应用中计算速度较快。\n 皮尔逊相关系数\n皮尔逊相关系数，也称为皮尔逊积矩相关系数，是衡量两个变量之间线性关系的强度和方向的统计量。其值范围从-1到+1，广泛应用于统计学、数据分析、机器学习等领域，特别是在衡量变量之间的相关性时。\n标准差：\nS=∑i=1n(Xi−X‾)2N−1S=\\sqrt{\\frac{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^2}{N-1}}\nS=N−1∑i=1n​(Xi​−X)2​​\nNNN是数据总量，X‾\\overline{X}X是平均值。\n协方差：\nCov(X,Y)=∑i=1n(Xi−X‾)(Yi−Y‾)N−1Cov(X,Y)=\\frac{\\sum_{i=1}^{n}(X_{i}-\\overline{X})(Y_{i}-\\overline{Y}) }{N-1}\nCov(X,Y)=N−1∑i=1n​(Xi​−X)(Yi​−Y)​\n皮尔逊相关系数\nρ=Cov(X,Y)SxSy\\rho =\\frac{Cov(X,Y)}{S_{x}S_{y}}\nρ=Sx​Sy​Cov(X,Y)​\nSxS_{x}Sx​是X的标准差。同样他是有取值范围的，在[−1,1][-1,1][−1,1]之间，1是完全相似，-1是完全不相似\n皮尔逊相关系数的计算公式相对简单，容易理解，且其值的意义非常直观，可以清楚地表示变量之间的线性关系强度和方向。 它不仅衡量了两个变量之间的协方差，还考虑了变量的标准差，因此能够归一化协方差，使得相关系数的值独立于变量的尺度。\n 应用示例\n\n\n\n用户-商品评分表\n用户1001\n用户1002\n用户1003\n用户1004\n用户1005\n\n\n\n\n商品1\n5\n4\n5\n4\n5\n\n\n商品2\n4\n3\n4\n4\n4\n\n\n商品3\n5\n2\n5\n3\n5\n\n\n商品4\n4\n2\n4\n4\n3\n\n\n\n这个时候我们要找与用户1003最相似的用户，可见1003给出了（5，4，5，4）的评分向量，这与用户1001的（5，4，5，4）评分向量完全一致，用户1003与用户1001高度相似，我们还可以看到用户1005给出了（5，4，5，3）的评分向量，这也与用户1003高度相似。\n这些都是显然的，但是计算机怎么能知道呢？本篇文章采用皮尔逊相关系数完成协同过滤。\n 计算标准差\ndouble standardDeviationCalculations(int[] ratings) &#123;\n    double std = 0;        //标准差\n    double avarage = 0;    //平均值\n    int sum = 0;           //评分和\n    double varianceUP = 0; //方差分子\n    double variance = 0; //方差\n    for (int i = 0; i &lt; ratings.length; i++) &#123;\n        sum += ratings[i];\n    &#125;\n    avarage = (double) sum / ratings.length;\n    for (int i = 0; i &lt; 4; i++) &#123; //方差分子\n        varianceUP += Math.pow(ratings[i] - avarage, 2);\n    &#125;\n    variance = varianceUP / (ratings.length - 1);\n    std = Math.sqrt(variance);\n    return std;\n&#125;\n\n 计算协方差\ndouble covarianceCalculation(int[] X, int[] Y) &#123;\n    double covariance = 0;       // 协方差\n    double avarageX = 0;         // 平均值X\n    double avarageY = 0;         // 平均值Y\n    for (int i = 0; i &lt; X.length; i++) &#123;\n        avarageX += X[i];\n    &#125;\n    avarageX = avarageX / X.length;\n    for (int i = 0; i &lt; Y.length; i++) &#123;\n        avarageY += Y[i];\n    &#125;\n    avarageY = avarageY / Y.length;\n    for (int i = 0; i &lt; X.length; i++) &#123;\n        covariance += (X[i] - avarageX) * (Y[i] - avarageY);\n    &#125;\n    covariance = covariance / (X.length - 1);\n    return covariance;\n&#125;\n\n 计算皮尔逊相关系数\ndouble pearsonCorrelationCoefficient(double covarianceXY, double stdX, double stdY) &#123;\n    return covarianceXY / (stdX * stdY);\n&#125;\n\n 调用\npublic class SuiBianTest &#123;\n    double standardDeviationCalculations(int[] ratings) &#123;\n        double std = 0;        //标准差\n        double avarage = 0;    //平均值\n        int sum = 0;           //评分和\n        double varianceUP = 0; //方差分子\n        double variance = 0; //方差\n        for (int i = 0; i &lt; ratings.length; i++) &#123;\n            sum += ratings[i];\n        &#125;\n        avarage = (double) sum / ratings.length;\n        for (int i = 0; i &lt; 4; i++) &#123; //方差分子\n            varianceUP += Math.pow(ratings[i] - avarage, 2);\n        &#125;\n        variance = varianceUP / (ratings.length - 1);\n        std = Math.sqrt(variance);\n        return std;\n    &#125;\n\n    double covarianceCalculation(int[] X, int[] Y) &#123;\n        double covariance = 0;       // 协方差\n        double avarageX = 0;         // 平均值X\n        double avarageY = 0;         // 平均值Y\n        for (int i = 0; i &lt; X.length; i++) &#123;\n            avarageX += X[i];\n        &#125;\n        avarageX = avarageX / X.length;\n        for (int i = 0; i &lt; Y.length; i++) &#123;\n            avarageY += Y[i];\n        &#125;\n        avarageY = avarageY / Y.length;\n        for (int i = 0; i &lt; X.length; i++) &#123;\n            covariance += (X[i] - avarageX) * (Y[i] - avarageY);\n        &#125;\n        covariance = covariance / (X.length - 1);\n        return covariance;\n    &#125;\n\n    double pearsonCorrelationCoefficient(double covarianceXY, double stdX, double stdY) &#123;\n        return covarianceXY / (stdX * stdY);\n    &#125;\n\n    @Test\n    public void testCollaborativeFiltering() &#123;\n        int[][] ratings_1_5 = &#123;\n                &#123;5, 4, 5, 4&#125;,       //1001对商品1、2、3、4的评分\n                &#123;4, 3, 2, 2&#125;,       //1002对商品1、2、3、4的评分\n                &#123;5, 4, 5, 4&#125;,       //1003对商品1、2、3、4的评分\n                &#123;4, 4, 2, 4&#125;,       //1004对商品1、2、3、4的评分\n                &#123;5, 4, 5, 3&#125;,       //1005对商品1、2、3、4的评分\n        &#125;;\n        // ---------------------计算方差-----------------\n        ArrayList&lt;Double&gt; stdVector = new ArrayList&lt;&gt;();    // 方差向量\n        for (int i = 0; i &lt; ratings_1_5.length; i++) &#123;\n            stdVector.add(standardDeviationCalculations(ratings_1_5[i]));\n        &#125;\n        System.out.println(&quot;方差向量：&quot; + stdVector);\n        // ----------------协方差计算cov(X,Y)------------\n        ArrayList&lt;Double&gt; covarianceVector = new ArrayList&lt;&gt;(); // 协方差向量\n        for (int i = 0; i &lt; ratings_1_5.length; i++) &#123;\n            covarianceVector.add(covarianceCalculation(ratings_1_5[2], ratings_1_5[i]));\n        &#125;\n        System.out.println(&quot;协方差向量：&quot; + covarianceVector);\n        // --------------皮尔逊相关系数----------------\n        ArrayList&lt;Double&gt; pearsonVector = new ArrayList&lt;&gt;();    // 皮尔逊相关系数向量\n        for (int i = 0; i &lt; ratings_1_5.length; i++) &#123;\n            pearsonVector.add(pearsonCorrelationCoefficient(covarianceVector.get(i), stdVector.get(2), stdVector.get(i)));\n        &#125;\n        System.out.println(&quot;皮尔逊相关系数向量：&quot; + pearsonVector);\n        System.out.println(&quot;----------------------------&quot;);\n        HashMap&lt;String, Double&gt; map = new HashMap&lt;&gt;();\n        map.put(&quot;1001&quot;, pearsonVector.get(0));\n        map.put(&quot;1002&quot;, pearsonVector.get(1));\n//        map.put(&quot;1003&quot;, pearsonVector.get(2));\n        map.put(&quot;1004&quot;, pearsonVector.get(3));\n        map.put(&quot;1005&quot;, pearsonVector.get(4));\n        // 使用Stream API获取最大的两个值及它们对应的键\n        List&lt;Map.Entry&lt;String, Double&gt;&gt; topTwoEntries = map.entrySet()\n                .stream()\n                .sorted(Map.Entry.&lt;String, Double&gt;comparingByValue().reversed()) // 按照value降序排序\n                .limit(2) // 只取前两项\n                .toList();\n\n        // 输出结果\n        topTwoEntries.forEach(entry -&gt;\n                System.out.println(&quot;最大皮尔逊相关系数之一对应的键：&quot; + entry.getKey() + &quot;，值为：&quot; + entry.getValue())\n        );\n\n        if (topTwoEntries.size() &lt; 2) &#123;\n            System.out.println(&quot;注意：Map中的元素不足两个&quot;);\n        &#125;\n    &#125;\n&#125;\n\n 输出\n\n 后续\n此时我们已经找到了，与用户1003最为相似的两个用户，分别是用户1001——1.000和用户1005——0.905，这与我们之前的”显然猜想“相一致。\n然后我们就可以根据用户1001和用户1005已经购买的商品但用户1003没有购买的商品，将他们两个加权综合计算，为用户1003推荐。\n 小结\n其实，利用皮尔逊相关系数计算用户之间的相关性并不复杂，而在计算之前，我们应该怎么设计根据用户1003的已购买商品的评分信息，抽取与他购买的相同商品的其他用户的商品评分信息的SQL或者是其他信息提取方法，我想，这才是更为重要的点。\n这可能就是简单的“数据清洗”吧！哈哈哈哈哈哈哈。\n","categories":["开发经验"],"tags":["协同过滤"]},{"title":"booth算法","url":"/Principles-of-computer-composition/booth_algorithm/","content":" Booth 算法\n这里写点什么捏？就先来点简介吧！！比较法是 Booth 夫妇先提出来的，也称 booth 算法。\n那么！！咱们怎么解题呢，不要担心，其实 booth 算法非常简单，不要被长长的算式所迷惑鸭！！\n接下来就由我来简单快速的叙述一下如何利用 booth 算法去做题！\n 准备开始！\n首先题目里一般会这样给：已知[x]补=xxxxx，[y]补=xxxxx，利用 booth 算法求解[x*y]。\nbooth 算法的符号位是成双成对的，和你不一样，因为你还没有成双成对哈哈哈哈哈哈（不是）\n另外，我们还需要额外记住的四个固定形态：\n\n\n\n尾数形态\n作法\n\n\n\n\n00\n右移一位\n\n\n01\n+[x]补 后 右移一位\n\n\n10\n+[-x]补后 右移一位\n\n\n11\n右移一位\n\n\n\n 例题一\n已知[x]补=0.0101，[y]补=1.0101，利用 booth 算法求解[x*y]\n做题前我们需要做一些准备，首先我们写出双符号位形态的[x]补、[-x]补以及[y]补（无需双符号）\n\n\n[x]补=00.0101\n\n\n[-x]补=11.1011\n\n\n[y]补=1.0101\n\n\n然后列出如下表格：\n\n\n\n部分积\n乘数\n辅助\n\n\n\n\n\n\n0\n\n\n\n我先写到这，如果有想先试试的小伙伴可以先按照自己的思路去写，还没记起来的的小伙伴们可以继续往下看，一会我还会写出两道例题供大家去练习。\n解答开始！\n辅助位置默认为 0，乘数就是[y]补=11.0101，部分积初始值也为 0.\n\n\n\n\n部分积\n乘数\n辅助\n\n\n\n\n\n\n00 0000\n1010 1\n0\n一次判断\n\n\n10还记得加什么嘛？没错就是加[-x]补 +\n11 1011\n\n\n\n\n\n=\n11 1011\n\n\n\n\n\n右移\n11 1101\n1101 0\n1\n二次判断\n\n\n01还记得加什么嘛？没错就是加[x]补 +\n00 0101\n\n\n\n\n\n=\n00 0010\n\n\n\n\n\n右移\n00 0001\n0110 1\n0\n三次判断\n\n\n10还记得加什么嘛？没错就是加[-x]补 +\n11 1011\n\n\n\n\n\n=\n11 1100\n\n\n\n\n\n右移\n11 1110\n0011 0\n1\n四次判断\n\n\n01还记得加什么嘛？没错就是加[x]补 +\n00 0101\n\n\n\n\n\n=\n00 0011\n\n\n\n\n\n右移\n00 0001\n1001 1\n0\n五次判断\n\n\n10还记得加什么嘛？没错就是加[-x]补 +\n11 1011\n\n\n\n\n\n\n11 1100\n\n\n\n\n\n\nOK 兄弟们，小数点后有四位，那么我们需要进行五次判断，现在已经完成计算过程了，答案已经呼之欲出了！！\n[x*y] = 1.1100 1001（如果要双符号为就这样去写：[x*y] = 11.1100 1001）\n总结一下，辅助位配合乘数最后一位作判断操作，00 和 11 只需右移就好，而 01 要+[x]补，11 要+[-x]补，这样我们就能得到最终的结果了。\n接下来再来一道爽题！！！\n 例题二\n已知[x]补=1.0101，[y]补=1.0011，利用 booth 算法求解[x*y]\n老规矩：\n\n[x]补=11.0101\n[-x]补=00.1011\n[y]补=1.0011\n\n列表：\n\n\n\n\n部分积\n乘数\n辅助\n\n\n\n\n\n\n00 0000\n10011\n0\n一次判断\n\n\n10 +\n00 1011\n\n\n\n\n\n=\n00 1011\n\n\n\n\n\n右移\n00 0101\n1100 1\n1\n二次判断\n\n\n11 右移\n00 0010\n1110 0\n1\n三次判断\n\n\n01 +\n11 0101\n\n\n\n\n\n=\n11 0111\n\n\n\n\n\n右移\n11 1011\n1111 0\n0\n四次判断\n\n\n00 右移\n11 1101\n1111 1\n0\n五次判断\n\n\n10 +\n00 1011\n\n\n\n\n\n\n00 1000\n\n\n\n\n\n\n第五次判断后无需移动\n答案：0.1000 1111\n怎么样，是不是非常爽，在五次判断中出现了两次数字相同的情况，这时候我们只需要进行右移，无需进行计算。\n最后我在给大家一道练习题，自己练习一下叭~~~\n 练习\n已知[x]补=0.1101，[y]补=0.1011，利用 booth 算法求解[x*y]\n\n\n\n部分积\n乘数\n辅助\n\n\n\n\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n 鸣谢\n感谢 B 站 up 主制作的视频，真的非常不错\n","categories":["计算机组成原理"],"tags":["计算机组成原理"]},{"title":"写给我这一年多的考研时光","url":"/life/shangan/","content":" 2024.01——2025.03\n昨天收到了学校的拟录取通知，很幸运我的名字出现在了拟录取名单里，一年多的努力于此刻兑现。回想一年来的旅途，我真的有很多话想说。\n 前奏\n我其实很早之前就决定考研了，并不是跟风考研党，最初的考研目的也很简单，一是想与自己和解，和十八岁的自己和解，以此来填补高考的遗憾，二是去提升一下自己自身的竞争力。\n就这样，我在刚刚进入大学的时候，就已经决定好考研了。事实上，我是一名转专业的学生，高考的时候由于成绩不理想，导致没能学上自己想要的学的专业，对此我十分懊悔。\n和大多数普通人一样，我是一个堪称绝对平凡的人。普通人究竟该怎样向上，究竟该如何一步步的提升自己？我的回答是：脚踏实地，仰望星空。\n 启程\n2024年1月，我真正意义上的开启的我的考研之路，我认为我这个时间节点不算早也不算晚。\n2024的“年”显得格外压抑，我觉得考研就是一场完全黑暗的旅途，你能“听见”同学们的声音，但是“看不见”他们在哪里。有人说，考研就是在黑夜里洗衣服，你不知道你是否洗干净了，你只能就这样在黑暗中继续的洗下去，直到天明。\n 月升\n要说科目是我投入最大的，那一定是数学。我本身数学基础较差，我我花费很长时间去学习数学，遗憾的是最终的数学成绩并不是那么理想，在这里我就不献丑去讲我是怎么学习数学的了，我一定是头号反面教材。讲一句就是少看课，多练题，多练题，多练题。\n英语是我的老大难问题，英语的问题实际上要比数学还糟糕，在大学期间，我仅仅是470分过了四级，六级考了三次没考过，最后一次六级差了一道选择题的分数。我想说，英语你要掌握足够的词汇量，一定要多背单词，只有你认识单词了，才会看懂一句话，才知道他想表达什么意思，其次是语法，语法也很重要，但我认为，语法是一个锦上添花的。单词一定是前提，认识了足够多的单词之后，再学一学语法，那会让你更加如鱼得水。其次是做题技巧，尤其是阅读的做题技巧，多和网上的老师学一学，但是不要依赖这个，在学习之后一定要形成一套自己的解题方式。作文我觉得没什么好说的，多背多写就好了。\n我是一名纯纯理工科的学生，上一次认真学政治还是在初中，我在这里讲一讲怎么用理工科的思维去学政治，我再补充一句，如果你完全用理工的思维去学政治，那我想说，那很难学好政治。这里的理工思维一定要融入一些文科思想。政治一定要去了解历史，历史是由人民群众创造的，只有你了解了历史，理解当时他们的处境，与之共情，你才能明白，为什么他们这样做，以及这样做的好处。在答题时候，加入一些理工思维的公式，如果真正想学好仅靠一些答题公式还是不行的，但是呢我们这个是一个速成，或者是面对一个完全没怎么好好学过政治的人来说的。必要时用一些模板，公式化答题，是十分有利的。\n专业课方面我就不多讲了，我认为我这方面还是比较擅长的。还是多练多理解，毕竟这是你吃饭的工具呀。\n 月落\n黎明前的日子最煎熬，网上我好像从来没看见过，有人说初试结束后，复试有多难。其实也不能说难，但是这段日子极其痛苦、折磨与难熬。\n成绩出来后，学校不公布排名，同学们自发在网上去排名，每一天你看见你的排名往下掉，整个人都down down的，这种感觉很难形容，很难受。最后在复试前，学校还是公布了排名。今年招生人数35人，进复试44人，我排在35名。这个时候我觉得真的很难保持一个良好的心态，这是我觉得考研最折磨的一个点。没办法，复试还要继续准备，此刻便是至暗。\n 微光\n复试结束，一天后公布了最终排名，我以复试笔试第一名，复试成绩第四名，总成绩第十七名拟录取了。\n我想说复试真的很重要，我们学校的复试1分，就是初试1.07分，复试和初始同等重要。拟录取的那一刻犹如做梦一样，我不敢相信，我反复确认我真的拟录取了，生怕这一切全是梦境。\n是真的。\n 回望\n回望我这一年多的旅程，有欢笑，有泪水，此刻我想对自己说，你真的很棒，你站在你曾经幻想过的未来，于此刻，不再是幻想。\n“夹岸群芳相送远帆过千障碍”\n","categories":["生活","上岸"],"tags":["生活"]},{"title":"MD5计算时间","url":"/experience/MD5CalculationTimePractice/","content":" MD5计算时间的实践\n 引言\n最近在做一个项目，里面用到了MD5算法来生成文件名，并且可以对相同文件进行合并存储，不禁让我想到了一个问题——MD5的计算很耗时吗还是MD5是一个轻量级的算法，在网上翻阅了一些资料，得到了结果，确实MD5的计算是很轻量的。\n但是，他到底需要多长时间呢？我直接开始实践一下。\n 配置\n机器采用：AMD Ryzen 7 5800H处理器 + 32GB 内存 频率为 3200 使用约 50%\n 实践\n先对小文件进行测试，现选用文件大小为2.83 MB的图片，在anaconda环境下，对比不同chunk_size下的计算时间。\nimport hashlib\nimport time\n\n\ndef file_to_md5(file_path, chunk_size=8192):\n    md5 = hashlib.md5()\n    with open(file_path, &#x27;rb&#x27;) as f:\n        while chunk := f.read(chunk_size):\n            md5.update(chunk)\n    return md5.hexdigest()\n\n\nt = time.perf_counter()\n\nfile_to_md5(&#x27;D:\\\\ZzMu\\\\Aping\\\\bz\\\\zb.png&#x27;)\nprint(f&#x27;&#123;time.perf_counter() - t:.8f&#125;s&#x27;)\n\n\n\n\nchunk_size\n时间\n\n\n\n\n1024\n0.00551090s\n\n\n2048\n0.00501470s\n\n\n4096\n0.00491630s\n\n\n8192\n0.00470980s\n\n\n16384\n0.00436570s\n\n\n\n随着chunk_size的增大，计算时间不断缩短，不过确实微乎其微，0.00几的时间无法感知，那么大一点的文件能放大影响吗？\n下面对文件大小为957 MB的4分44秒的5k视频进行计算。\n\n\n\nchunk_size\n时间\n\n\n\n\n1024\n1.68504020s\n\n\n2048\n1.61570920s\n\n\n4096\n1.57428070s\n\n\n8192\n1.55151410s\n\n\n16384\n1.35576710s\n\n\n32768\n1.28117580s\n\n\n\n大文件对chunk_size的感知更为明显，变现为0.几的波动。\n下面试试更大的文件，文件大小为9.58 GB的Linux安装文件CentOS-7-x86_64-Everything-2207-02.iso进行计算。\n\n\n\nchunk_size\n时间\n\n\n\n\n1024\n17.36447650s\n\n\n2048\n17.03117640s\n\n\n4096\n16.44976660s\n\n\n8192\n16.01030290s\n\n\n16384\n14.48009140s\n\n\n32768\n13.57228980s\n\n\n\n得出结论，越大的文件对chunk_size越敏感。到此为止，我仍然对这个执行时间有疑问，不同的变成语言对计算时间有什么影响呢，以上是使用Python语言完成的测试，接下来试试Java的语言。\n原生：\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.security.MessageDigest;\nimport java.security.NoSuchAlgorithmException;\n\npublic class Main &#123;\n\n    private static final int BUFFER_SIZE = 32768;\n\n    public static String fileToMD5(String filePath) throws IOException, NoSuchAlgorithmException &#123;\n        try (InputStream is = new FileInputStream(filePath)) &#123;\n            MessageDigest md5Digest = MessageDigest.getInstance(&quot;MD5&quot;);\n            byte[] buffer = new byte[BUFFER_SIZE];\n            int bytesRead;\n            while ((bytesRead = is.read(buffer)) != -1) &#123;\n                md5Digest.update(buffer, 0, bytesRead);\n            &#125;\n            return bytesToHex(md5Digest.digest());\n        &#125;\n    &#125;\n\n    private static String bytesToHex(byte[] bytes) &#123;\n        StringBuilder hexString = new StringBuilder();\n        for (byte b : bytes) &#123;\n            String hex = Integer.toHexString(0xff &amp; b);\n            if (hex.length() == 1) hexString.append(&#x27;0&#x27;);\n            hexString.append(hex);\n        &#125;\n        return hexString.toString();\n    &#125;\n\n    public static void main(String[] args) throws IOException, NoSuchAlgorithmException &#123;\n        long startTime = System.nanoTime();\n        fileToMD5(&quot;D:\\\\ZzMu\\\\Aping\\\\bz\\\\zb.png&quot;);\n        long endTime = System.nanoTime();\n        System.out.printf(&quot;Time taken: %.8fs%n&quot;, (endTime - startTime) / 1e9);\n    &#125;\n&#125;\n\n使用java 8：\n\n\n\nchunk_size\n时间\n\n\n\n\n8192\n22.89824750\n\n\n16384\n21.72088020\n\n\n32768\n20.98676390\n\n\n\n没想到啊，java表现的更为糟糕，是不是和java版本有关系呢？\n使用java 17：\n\n\n\nchunk_size\n时间\n\n\n\n\n1024\n27.40612860\n\n\n8192\n15.47918140\n\n\n16384\n14.53450360\n\n\n32768\n13.85291620\n\n\n\n确实java 17 的表现要比java 8 的表现好。不过还是弱于Python？难道是我写的代码太烂了？\n又引入了DigestUtils工具类，DigestUtils是apache. commons包下的的MD5计算工具，默认chunk_size大小为1024。\nimport org.apache.commons.codec.digest.DigestUtils;\nimport org.junit.jupiter.api.Test;\n\nimport java.io.FileInputStream;\nimport java.io.IOException;\n\npublic class MD5 &#123;\n    @Test\n    public void test() throws IOException &#123;\n        FileInputStream inputStream = new FileInputStream(&quot;E:\\\\LinuxIso\\\\CentOS-7-x86_64-Everything-2207-02.iso&quot;);\n        long startTime = System.nanoTime();\n        DigestUtils.md5Hex(inputStream);\n        long endTime = System.nanoTime();\n        System.out.printf(&quot;Time taken: %.8fs%n&quot;, (endTime - startTime) / 1e9);\n    &#125;\n&#125;\n\n\n\n\nchunk_size\n时间\n\n\n\n\n1024\n27.50305870\n\n\n\n彼此彼此吧，哈哈哈哈哈哈。\n下面又引入了SecureUtil，该工具是HuTool包下一个计算MD5的工具，默认chunk_size为1024。\nimport cn.hutool.crypto.SecureUtil;\nimport org.junit.jupiter.api.Test;\n\nimport java.io.FileInputStream;\nimport java.io.IOException;\n\npublic class MD5 &#123;\n    @Test\n    public void test() throws IOException &#123;\n        FileInputStream inputStream = new FileInputStream(&quot;E:\\\\LinuxIso\\\\CentOS-7-x86_64-Everything-2207-02.iso&quot;);\n        long startTime = System.nanoTime();\n        SecureUtil.md5(inputStream);\n        long endTime = System.nanoTime();\n        System.out.printf(&quot;Time taken: %.8fs%n&quot;, (endTime - startTime) / 1e9);\n    &#125;\n&#125;\n\n\n\n\nchunk_size\n时间\n\n\n\n\n1024\n15.42640900\n\n\n\nHuTool的SecureUtil确实更高效，应该是做了一些特殊优化，源码中显示的是MD5没有使用BC库，完全依赖于jdk库。\n","categories":["开发经验"],"tags":["MD5"]},{"title":"矩阵论","url":"/math/matrix_anl/","content":" 矩阵论\n 线性变换T在基底下的矩阵\n定义如下：\nT(α1,α2,α3…αn)=(α1,α2,α3…αn)AT(\\alpha_1,\\alpha_2,\\alpha_3\\dots\\alpha_n)=(\\alpha_1,\\alpha_2,\\alpha_3\\dots\\alpha_n)A\nT(α1​,α2​,α3​…αn​)=(α1​,α2​,α3​…αn​)A\n这个题目较为灵活，最复杂情况如下：\nT(α1,α2,α3…αn)=(α1,α2,α3…αn)AT(β1,β2,β3…βn)=(β1,β2,β3…βn)B(β1,β2,β3…βn)=(α1,α2,α3…αn)CT(\\alpha_1,\\alpha_2,\\alpha_3\\dots\\alpha_n)=(\\alpha_1,\\alpha_2,\\alpha_3\\dots\\alpha_n)A\n\\\\\nT(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)=(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)B\n\\\\\n(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)=(\\alpha_1,\\alpha_2,\\alpha_3\\dots\\alpha_n)C\nT(α1​,α2​,α3​…αn​)=(α1​,α2​,α3​…αn​)AT(β1​,β2​,β3​…βn​)=(β1​,β2​,β3​…βn​)B(β1​,β2​,β3​…βn​)=(α1​,α2​,α3​…αn​)C\n一般已知过度矩阵 C和基下矩阵A，C是由α\\alphaα，过渡到β\\betaβ的，核心在于求B：\nT(α1,α2,α3…αn)=(α1,α2,α3…αn)A(β1,β2,β3…βn)C−1=(α1,α2,α3…αn)T(β1,β2,β3…βn)C−1=(β1,β2,β3…βn)C−1AT(β1,β2,β3…βn)=(β1,β2,β3…βn)C−1AC(β1,β2,β3…βn)C−1AC=(β1,β2,β3…βn)BB=C−1ACT(\\alpha_1,\\alpha_2,\\alpha_3\\dots\\alpha_n)=(\\alpha_1,\\alpha_2,\\alpha_3\\dots\\alpha_n)A\n\\\\\n(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)C^{-1}=(\\alpha_1,\\alpha_2,\\alpha_3\\dots\\alpha_n)\n\\\\\nT(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)C^{-1}=(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)C^{-1}A\n\\\\\nT(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)=(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)C^{-1}AC\n\\\\\n(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)C^{-1}AC=(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)B\n\\\\\nB=C^{-1}AC\nT(α1​,α2​,α3​…αn​)=(α1​,α2​,α3​…αn​)A(β1​,β2​,β3​…βn​)C−1=(α1​,α2​,α3​…αn​)T(β1​,β2​,β3​…βn​)C−1=(β1​,β2​,β3​…βn​)C−1AT(β1​,β2​,β3​…βn​)=(β1​,β2​,β3​…βn​)C−1AC(β1​,β2​,β3​…βn​)C−1AC=(β1​,β2​,β3​…βn​)BB=C−1AC\n\n例一：已知线性变换T在基底η1=(−1,1,1)T,η2=(1,0,−1)T,η3=(0,1,1)T\\eta_1=(-1,1,1)^T,\\eta_2=(1,0,-1)^T,\\eta_3=(0,1,1)^Tη1​=(−1,1,1)T,η2​=(1,0,−1)T,η3​=(0,1,1)T下的矩阵\nA=[101110−121]A=\\begin{bmatrix}\n 1 &amp; 0 &amp; 1\\\\\n 1 &amp; 1 &amp; 0\\\\\n -1 &amp; 2 &amp; 1\n\\end{bmatrix}\nA=⎣⎢⎡​11−1​012​101​⎦⎥⎤​\n求在基底e1=(1,0,0)T,e2=(0,1,0)T,e3=(0,0,1)Te_1=(1,0,0)^T,e_2=(0,1,0)^T,e_3=(0,0,1)^Te1​=(1,0,0)T,e2​=(0,1,0)T,e3​=(0,0,1)T下的矩阵。\n解\n由题可知：\nT(η1,η2,η3)=(η1,η2,η3)AT(e1,e2,e3)=(e1,e2,e3)B(e1,e2,e3)=(η1,η2,η3)CT(\\eta_1,\\eta_2,\\eta_3)=(\\eta_1,\\eta_2,\\eta_3)A\n\\\\\nT(e_1,e_2,e_3)=(e_1,e_2,e_3)B\n\\\\\n(e_1,e_2,e_3)=(\\eta_1,\\eta_2,\\eta_3)C\nT(η1​,η2​,η3​)=(η1​,η2​,η3​)AT(e1​,e2​,e3​)=(e1​,e2​,e3​)B(e1​,e2​,e3​)=(η1​,η2​,η3​)C\nC可以轻易的求解出来：\nC=(η1,η2,η3)−1(e1,e2,e3)=[−1101011−11]−1⋅[100010001]=[−11−101−1101]C−1=[−1101011−11]C=(\\eta_1,\\eta_2,\\eta_3)^{-1}(e_1,e_2,e_3)=\\begin{bmatrix}\n -1 &amp; 1 &amp; 0\\\\\n 1 &amp; 0 &amp; 1\\\\\n 1 &amp; -1 &amp; 1\n\\end{bmatrix}^{-1} \\cdot \\begin{bmatrix}\n 1 &amp; 0 &amp; 0\\\\\n 0 &amp; 1 &amp; 0\\\\\n 0 &amp; 0 &amp; 1\n\\end{bmatrix}=\\begin{bmatrix}\n -1 &amp; 1 &amp; -1\\\\\n 0 &amp; 1 &amp; -1\\\\\n 1 &amp; 0 &amp; 1\n\\end{bmatrix}\n\\\\\nC^{-1}=\\begin{bmatrix}\n -1 &amp; 1 &amp; 0\\\\\n 1 &amp; 0 &amp; 1\\\\\n 1 &amp; -1 &amp; 1\n\\end{bmatrix}\nC=(η1​,η2​,η3​)−1(e1​,e2​,e3​)=⎣⎢⎡​−111​10−1​011​⎦⎥⎤​−1⋅⎣⎢⎡​100​010​001​⎦⎥⎤​=⎣⎢⎡​−101​110​−1−11​⎦⎥⎤​C−1=⎣⎢⎡​−111​10−1​011​⎦⎥⎤​\n求基底e下的矩阵正是矩阵B，于是有：\nB=C−1ACB=C^{-1}AC\nB=C−1AC\n\n 基下的坐标\n一般是已知坐标(y1,y2,y3)T(y_1,y_2,y_3)^{T}(y1​,y2​,y3​)T，求其在基底(η1,η2,η3)(\\eta_1,\\eta_2,\\eta_3)(η1​,η2​,η3​)，下的坐标(x1,x2,x3)T(x_1,x_2,x_3)^T(x1​,x2​,x3​)T，一般表示为：\nY=AX[y1y2y3]=[η1,η2,η3]⋅[x1x2x3]Y=AX\n\\\\\n\\begin{bmatrix}\n y_1 \\\\\n y_2 \\\\\n y_3 \n\\end{bmatrix}=\\begin{bmatrix}\\eta_1,\\eta_2,\\eta_3\\end{bmatrix}\\cdot\\begin{bmatrix}\n x_1 \\\\\n x_2 \\\\\n x_3 \n\\end{bmatrix}\nY=AX⎣⎢⎡​y1​y2​y3​​⎦⎥⎤​=[η1​,η2​,η3​​]⋅⎣⎢⎡​x1​x2​x3​​⎦⎥⎤​\n求解x：\n[x1x2x3]=[η1,η2,η3]−1⋅[y1y2y3]\\begin{bmatrix}\n x_1 \\\\\n x_2 \\\\\n x_3 \n\\end{bmatrix}=\\begin{bmatrix}\\eta_1,\\eta_2,\\eta_3\\end{bmatrix}^{-1}\\cdot\\begin{bmatrix}\n y_1 \\\\\n y_2 \\\\\n y_3 \n\\end{bmatrix}\n⎣⎢⎡​x1​x2​x3​​⎦⎥⎤​=[η1​,η2​,η3​​]−1⋅⎣⎢⎡​y1​y2​y3​​⎦⎥⎤​\n 内积的证明\n要满足如下性质：\n(α,β)=(β,α)‾(kα,β)=k(α,β)(α+β,γ)=(α,γ)+(β,γ)当α≠0时(α,α)&gt;0(\\alpha,\\beta)=\\overline{(\\beta,\\alpha)}\\\\\n(k\\alpha,\\beta)=k(\\alpha,\\beta)\\\\\n(\\alpha+\\beta,\\gamma)=(\\alpha,\\gamma)+(\\beta,\\gamma)\\\\\n当\\alpha\\ne 0时(\\alpha,\\alpha)&gt;0\n(α,β)=(β,α)​(kα,β)=k(α,β)(α+β,γ)=(α,γ)+(β,γ)当α=0时(α,α)&gt;0\n 求矩阵A的最小多项式\n\n先用λE−A\\lambda E-AλE−A求出特征多项式。\n由低次方向高次方验证A−λEA-\\lambda EA−λE，直到出现零矩阵。\n\n\n例一：求矩阵\nA=[3−32−15−2−130]A=\\begin{bmatrix}\n 3 &amp; -3 &amp; 2\\\\\n -1 &amp; 5 &amp; -2\\\\\n -1 &amp; 3 &amp; 0\n\\end{bmatrix}\nA=⎣⎢⎡​3−1−1​−353​2−20​⎦⎥⎤​\n的最小多项式。\n解\n∣λE−A∣=(λ−2)2(λ−4)|\\lambda E-A|=(\\lambda-2)^2(\\lambda-4)\n∣λE−A∣=(λ−2)2(λ−4)\n由此最小特征是可能是(λ−2)2(λ−4)(\\lambda-2)^2(\\lambda-4)(λ−2)2(λ−4)或者(λ−2)(λ−4)(\\lambda-2)(\\lambda-4)(λ−2)(λ−4)，先从低次开始验证，也就是(λ−2)(λ−4)(\\lambda-2)(\\lambda-4)(λ−2)(λ−4)。\n(A−2E)(A−4E)=[1−32−13−2−13−2]⋅[−1−32−11−2−13−4]=[000000000](A-2E)(A-4E)=\\begin{bmatrix}\n 1 &amp; -3 &amp; 2\\\\\n -1 &amp; 3 &amp; -2\\\\\n -1 &amp; 3 &amp; -2\n\\end{bmatrix}\\cdot\\begin{bmatrix}\n -1 &amp; -3 &amp; 2\\\\\n -1 &amp; 1 &amp; -2\\\\\n -1 &amp; 3 &amp; -4\n\\end{bmatrix}=\\begin{bmatrix}\n 0 &amp; 0 &amp; 0\\\\\n 0 &amp; 0 &amp; 0\\\\\n 0 &amp; 0 &amp; 0\n\\end{bmatrix}\n(A−2E)(A−4E)=⎣⎢⎡​1−1−1​−333​2−2−2​⎦⎥⎤​⋅⎣⎢⎡​−1−1−1​−313​2−2−4​⎦⎥⎤​=⎣⎢⎡​000​000​000​⎦⎥⎤​\n由此可知，由(λ−2)(λ−4)(\\lambda-2)(\\lambda-4)(λ−2)(λ−4)可构成零矩阵，(λ−2)(λ−4)(\\lambda-2)(\\lambda-4)(λ−2)(λ−4)就是最小多项式。\n\n 行列式因子、不变因子、初等因子与Jordan标准型\n推荐通过Smith标准型来求解一般这个最快且出错率较低，如果Smith标准型求解困难，再通过子式来求。\n求矩阵\nA=[−1−26−103−1−14]A =\\begin{bmatrix}\n -1 &amp; -2 &amp; 6\\\\\n -1 &amp; 0 &amp; 3\\\\\n -1 &amp; -1 &amp; 4\n\\end{bmatrix}\nA=⎣⎢⎡​−1−1−1​−20−1​634​⎦⎥⎤​\n的行列式因子、不变因子、初等因子与Jordan标准型。\n解\n构造∣λE−A∣|\\lambda E-A|∣λE−A∣：\n∣λE−A∣=[λ+12−61λ−311λ−4]|\\lambda E-A|=\\begin{bmatrix}\n \\lambda+1 &amp; 2 &amp; -6\\\\\n 1 &amp; \\lambda &amp; -3\\\\\n 1 &amp; 1 &amp; \\lambda-4\n\\end{bmatrix}\n∣λE−A∣=⎣⎢⎡​λ+111​2λ1​−6−3λ−4​⎦⎥⎤​\n观察是否有公因子，并没有，1行1列必是1：\n[1λ−311λ−4λ+126]≃[10001−λ1−λ01−λλ2−3λ+2]\\begin{bmatrix}\n 1 &amp; \\lambda &amp; -3\\\\\n 1 &amp; 1 &amp; \\lambda-4\\\\\n \\lambda+1 &amp; 2 &amp; 6\\\\\n\\end{bmatrix}\\simeq\\begin{bmatrix}\n 1 &amp; 0 &amp; 0\\\\\n 0 &amp; 1-\\lambda &amp; 1-\\lambda\\\\\n 0 &amp; 1-\\lambda &amp; \\lambda^2-3\\lambda+2\\\\\n\\end{bmatrix}\n⎣⎢⎡​11λ+1​λ12​−3λ−46​⎦⎥⎤​≃⎣⎢⎡​100​01−λ1−λ​01−λλ2−3λ+2​⎦⎥⎤​\n观察右下角2x2子式是否有公因子，有的，是(1−λ)(1-\\lambda)(1−λ)，所以2行2列必是(1−λ)(1-\\lambda)(1−λ)。\n[10001−λ1−λ01−λ−λ2+3λ−2]≃[1000λ−1000(λ−1)2]\\begin{bmatrix}\n 1 &amp; 0 &amp; 0\\\\\n 0 &amp; 1-\\lambda &amp; 1-\\lambda\\\\\n 0 &amp; 1-\\lambda &amp; -\\lambda^2+3\\lambda-2\\\\\n\\end{bmatrix}\\simeq\\begin{bmatrix}\n 1 &amp; 0 &amp; 0\\\\\n 0 &amp; \\lambda-1 &amp; 0\\\\\n 0 &amp; 0 &amp; (\\lambda-1)^2\\\\\n\\end{bmatrix}\n⎣⎢⎡​100​01−λ1−λ​01−λ−λ2+3λ−2​⎦⎥⎤​≃⎣⎢⎡​100​0λ−10​00(λ−1)2​⎦⎥⎤​\n由此可知\n不变因子：d1=1,d2=λ−1,d3=(λ−1)2d_1=1,d_2=\\lambda-1,d_3=(\\lambda-1)^2d1​=1,d2​=λ−1,d3​=(λ−1)2\n行列式因子：D1=1,D2=λ−1,D3=(λ−1)(λ−1)2D_1=1,D_2=\\lambda-1,D_3=(\\lambda-1)(\\lambda-1)^2D1​=1,D2​=λ−1,D3​=(λ−1)(λ−1)2\n初等因子：(λ−1),(λ−1)2(\\lambda-1),(\\lambda-1)^2(λ−1),(λ−1)2\n由此可得出Jordan标准型：\nJ=[100011001]J=\\begin{bmatrix}\n 1 &amp; 0 &amp; 0\\\\\n 0 &amp; 1 &amp; 1\\\\\n 0 &amp; 0 &amp; 1\\\\\n\\end{bmatrix}\nJ=⎣⎢⎡​100​010​011​⎦⎥⎤​\n 证明向量范数\n非负性\nx≠0时∣∣x∣∣&gt;0,当且仅当x=0时∣∣x∣∣=0x\\ne 0时||x||&gt;0,当且仅当x=0时||x||=0\nx=0时∣∣x∣∣&gt;0,当且仅当x=0时∣∣x∣∣=0\n齐次性\n∣∣kx∣∣=∣k∣⋅∣∣x∣∣||kx||=|k|\\cdot||x||\n∣∣kx∣∣=∣k∣⋅∣∣x∣∣\n三角不等式\n∣∣x+y∣∣≤∣∣x∣∣+∣∣y∣∣||x+y||\\le ||x||+||y||\n∣∣x+y∣∣≤∣∣x∣∣+∣∣y∣∣\n 向量范数\n一范数\n∣∣x∣∣1=∑i=1n∣xi∣||x||_1=\\sum_{i=1}^{n}|x_i|\n∣∣x∣∣1​=i=1∑n​∣xi​∣\n二范数\n∣∣x∣∣2=∑i=1nxi2||x||_2=\\sqrt{\\sum_{i=1}^{n}x_i^2}\n∣∣x∣∣2​=i=1∑n​xi2​​\n无穷范数\n∣∣x∣∣∞=max∣xi∣||x||_\\infty=max|x_i|\n∣∣x∣∣∞​=max∣xi​∣\n 证明矩阵范数\n非负性\n∣∣A∣∣&gt;0,当且仅当A=0时∣∣A∣∣=0||A||&gt;0,当且仅当A=0时||A||=0\n∣∣A∣∣&gt;0,当且仅当A=0时∣∣A∣∣=0\n齐次性\n∣∣kA∣∣=∣k∣⋅∣∣A∣∣||kA||=|k|\\cdot||A||\n∣∣kA∣∣=∣k∣⋅∣∣A∣∣\n三角不等式\n∣∣A+B∣∣≤∣A∣+∣B∣||A+B||\\le|A|+|B|\n∣∣A+B∣∣≤∣A∣+∣B∣\n相容性\n∣∣A⋅B∣∣≤∣∣A∣∣⋅∣∣B∣∣||A\\cdot B||\\le||A||\\cdot||B||\n∣∣A⋅B∣∣≤∣∣A∣∣⋅∣∣B∣∣\n 矩阵范数\n一范数\n∣∣A∣∣1=max(列模长之和)||A||_1=max(列模长之和)\n∣∣A∣∣1​=max(列模长之和)\n二范数\n∣∣A∣∣2=λmax,其中λmax是AHA的最大特征值||A||_2=\\sqrt {\\lambda_{max}},其中\\lambda_{max}是A^HA的最大特征值\n∣∣A∣∣2​=λmax​​,其中λmax​是AHA的最大特征值\n无穷范数\n∣∣A∣∣∞=max(行模长之和)||A||_\\infty=max(行模长之和)\n∣∣A∣∣∞​=max(行模长之和)\nM1范数\n∣∣AM1∣∣=∑i=1m∑j=1n∣aij∣||A_{M1}||=\\sum_{i=1}^{m}\\sum_{j=1}^{n}|a_{ij}|\n∣∣AM1​∣∣=i=1∑m​j=1∑n​∣aij​∣\nM2范数\n∣∣AM2∣∣=∑i=1m∑j=1n∣aij∣2||A_{M2}||=\\sum_{i=1}^{m}\\sum_{j=1}^{n}\\sqrt{|a_{ij}|^2}\n∣∣AM2​∣∣=i=1∑m​j=1∑n​∣aij​∣2​\nM无穷范数\n∣∣AM∞∣∣=max(aij)||A_{M\\infty}||=max(a_{ij})\n∣∣AM∞​∣∣=max(aij​)\n QR分解\n先将源矩阵写成列向量\nA=[a11a12a13a21a22a23a31a32a33]=[α1α2α3]A=\\begin{bmatrix}\n a_{11} &amp; a_{12} &amp; a_{13}\\\\\n a_{21} &amp; a_{22} &amp; a_{23}\\\\\n a_{31} &amp; a_{32} &amp; a_{33}\\\\\n\\end{bmatrix}=\\begin{bmatrix}\n \\alpha_{1} &amp; \\alpha_{2} &amp; \\alpha_{3}\\\\\n\\end{bmatrix}\nA=⎣⎢⎡​a11​a21​a31​​a12​a22​a32​​a13​a23​a33​​⎦⎥⎤​=[α1​​α2​​α3​​]\n正交化，并保留正交化过程记录\nβ1=α1β2=α2−(α2,β1)(β1,β1)β1β3=α3−(α3,β1)(β1,β1)β1−(α3,β2)(β2,β2)β2\\beta_1=\\alpha_1\\\\\n\\beta_2=\\alpha_2-\\frac{(\\alpha_2,\\beta_1)}{(\\beta_1,\\beta_1)}\\beta_1\\\\\n\\beta_3=\\alpha_3-\\frac{(\\alpha_3,\\beta_1)}{(\\beta_1,\\beta_1)}\\beta_1-\\frac{(\\alpha_3,\\beta_2)}{(\\beta_2,\\beta_2)}\\beta_2\\\\\nβ1​=α1​β2​=α2​−(β1​,β1​)(α2​,β1​)​β1​β3​=α3​−(β1​,β1​)(α3​,β1​)​β1​−(β2​,β2​)(α3​,β2​)​β2​\n反写正交化（用β\\betaβ表示α\\alphaα）\nα1=β1α2=(α2,β1)(β1,β1)β1+β2α3=(α3,β1)(β1,β1)β1+(α3,β2)(β2,β2)β2+β3\\alpha_1=\\beta_1\\\\\n\\alpha_2=\\frac{(\\alpha_2,\\beta_1)}{(\\beta_1,\\beta_1)}\\beta_1+\\beta_2\\\\\n\\alpha_3=\\frac{(\\alpha_3,\\beta_1)}{(\\beta_1,\\beta_1)}\\beta_1+\\frac{(\\alpha_3,\\beta_2)}{(\\beta_2,\\beta_2)}\\beta_2+\\beta_3\nα1​=β1​α2​=(β1​,β1​)(α2​,β1​)​β1​+β2​α3​=(β1​,β1​)(α3​,β1​)​β1​+(β2​,β2​)(α3​,β2​)​β2​+β3​\n单位化β\\betaβ\nγ1=β1∣β1∣γ2=β2∣β2∣γ3=β3∣β3∣\\gamma_1=\\frac{\\beta_1}{|\\beta_1|}\\\\\n\\gamma_2=\\frac{\\beta_2}{|\\beta_2|}\\\\\n\\gamma_3=\\frac{\\beta_3}{|\\beta_3|}\\\\\nγ1​=∣β1​∣β1​​γ2​=∣β2​∣β2​​γ3​=∣β3​∣β3​​\n把β\\betaβ用γ\\gammaγ换掉\nα1=∣β1∣γ1α2=(α2,β1)(β1,β1)∣β1∣γ1+∣β2∣γ2α3=(α3,β1)(β1,β1)∣β1∣γ1+(α3,β2)(β2,β2)∣β2∣γ2+∣β3∣γ3\\alpha_1=|\\beta_1|\\gamma_1\\\\\n\\alpha_2=\\frac{(\\alpha_2,\\beta_1)}{(\\beta_1,\\beta_1)}|\\beta_1|\\gamma_1+|\\beta_2|\\gamma_2\\\\\n\\alpha_3=\\frac{(\\alpha_3,\\beta_1)}{(\\beta_1,\\beta_1)}|\\beta_1|\\gamma_1+\\frac{(\\alpha_3,\\beta_2)}{(\\beta_2,\\beta_2)}|\\beta_2|\\gamma_2+|\\beta_3|\\gamma_3\nα1​=∣β1​∣γ1​α2​=(β1​,β1​)(α2​,β1​)​∣β1​∣γ1​+∣β2​∣γ2​α3​=(β1​,β1​)(α3​,β1​)​∣β1​∣γ1​+(β2​,β2​)(α3​,β2​)​∣β2​∣γ2​+∣β3​∣γ3​\nγ\\gammaγ系列就是Q，上述替换过的就是R\nA=[γ1γ2γ3]⋅[∣β1∣(α2,β1)(β1,β1)∣β1∣(α3,β1)(β1,β1)∣β1∣0∣β2∣(α3,β2)(β2,β2)∣β2∣00∣β3∣]A=\\begin{bmatrix}\n \\gamma_1 &amp; \\gamma_2 &amp; \\gamma_3\n\\end{bmatrix}\\cdot\\begin{bmatrix}\n |\\beta_1| &amp; \\frac{(\\alpha_2,\\beta_1)}{(\\beta_1,\\beta_1)}|\\beta_1| &amp; \\frac{(\\alpha_3,\\beta_1)}{(\\beta_1,\\beta_1)}|\\beta_1|\\\\\n 0 &amp; |\\beta_2| &amp; \\frac{(\\alpha_3,\\beta_2)}{(\\beta_2,\\beta_2)}|\\beta_2|\\\\\n 0 &amp; 0 &amp; |\\beta_3|\\\\\n\\end{bmatrix}\nA=[γ1​​γ2​​γ3​​]⋅⎣⎢⎢⎡​∣β1​∣00​(β1​,β1​)(α2​,β1​)​∣β1​∣∣β2​∣0​(β1​,β1​)(α3​,β1​)​∣β1​∣(β2​,β2​)(α3​,β2​)​∣β2​∣∣β3​∣​⎦⎥⎥⎤​\nQR分解完毕。\n 正交矩阵谱分解\n求特征值，特征向量，正交化单位化得到η1,η2,η3\\eta_1,\\eta_2,\\eta_3η1​,η2​,η3​\nA1=η1T⋅η1A2=η2T⋅η2…A_1=\\eta_1^T\\cdot\\eta_1\\\\\nA_2=\\eta_2^T\\cdot\\eta_2\\\\\n\\dots\nA1​=η1T​⋅η1​A2​=η2T​⋅η2​…\nA=λ1A1+λ2A2+…A=\\lambda_1A_1+\\lambda_2A_2+\\dots\nA=λ1​A1​+λ2​A2​+…\n 可对角化/异根谱分解\n求特征值，设特征函数，如果有重根只计算一次，这里只做对方的特征值，不管自己的特征值\nφ1(λ)=(λ−λ2)(λ−λ3)φ2(λ)=(λ−λ1)(λ−λ3)φ3(λ)=(λ−λ1)(λ−λ2)\\varphi _1(\\lambda)=(\\lambda-\\lambda_2)(\\lambda-\\lambda_3)\\\\\n\\varphi _2(\\lambda)=(\\lambda-\\lambda_1)(\\lambda-\\lambda_3)\\\\\n\\varphi _3(\\lambda)=(\\lambda-\\lambda_1)(\\lambda-\\lambda_2)\\\\\nφ1​(λ)=(λ−λ2​)(λ−λ3​)φ2​(λ)=(λ−λ1​)(λ−λ3​)φ3​(λ)=(λ−λ1​)(λ−λ2​)\n把特征值和矩阵带入\nA1=φ1(A)φ1(λ1)A2=φ2(A)φ2(λ2)A3=φ3(A)φ3(λ3)A_1=\\frac{\\varphi _1(A)}{\\varphi _1(\\lambda_1)}\\\\\nA_2=\\frac{\\varphi _2(A)}{\\varphi _2(\\lambda_2)}\\\\\nA_3=\\frac{\\varphi _3(A)}{\\varphi _3(\\lambda_3)}\nA1​=φ1​(λ1​)φ1​(A)​A2​=φ2​(λ2​)φ2​(A)​A3​=φ3​(λ3​)φ3​(A)​\nA=λ1A1+λ2A2+λ3A3…A=\\lambda_1A_1+\\lambda_2A_2+\\lambda_3A_3\\dots\nA=λ1​A1​+λ2​A2​+λ3​A3​…\n 满秩分解\n满秩分解先将矩阵化为行最简，然后选取指定的n列，指定n行，这里前后指的是化简前后，而具体指定哪列由化简后的阶梯矩阵决定。\nA=[14−1562000−14−12−40126−55−7]A=\\begin{bmatrix}\n 1 &amp; 4 &amp; -1 &amp; 5 &amp; 6\\\\\n 2 &amp; 0 &amp; 0 &amp; 0 &amp; -14 \\\\\n -1 &amp; 2 &amp; -4 &amp; 0 &amp; 1\\\\\n 2 &amp; 6 &amp; -5 &amp; 5 &amp; -7\\\\\n\\end{bmatrix}\nA=⎣⎢⎢⎢⎡​12−12​4026​−10−4−5​5005​6−141−7​⎦⎥⎥⎥⎤​\n[1000−70101072970015725700000]\\begin{bmatrix}\n 1 &amp; 0 &amp; 0 &amp; 0 &amp; -7\\\\\n 0 &amp; 1 &amp; 0 &amp; \\frac{10}{7} &amp; \\frac{29}{7} \\\\\n 0 &amp; 0 &amp; 1 &amp; \\frac{5}{7} &amp; \\frac{25}{7}\\\\\n 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\\n\\end{bmatrix}\n⎣⎢⎢⎢⎡​1000​0100​0010​0710​75​0​−7729​725​0​⎦⎥⎥⎥⎤​\n前列后行\nB=[14−1200−12−426−5]B=\\begin{bmatrix}\n 1 &amp; 4 &amp; -1\\\\\n 2 &amp; 0 &amp; 0\\\\\n -1 &amp; 2 &amp; -4\\\\\n 2 &amp; 6 &amp; -5 \\\\\n\\end{bmatrix}\nB=⎣⎢⎢⎢⎡​12−12​4026​−10−4−5​⎦⎥⎥⎥⎤​\nC=[1000−701010729700157257]C=\\begin{bmatrix}\n 1 &amp; 0 &amp; 0 &amp; 0 &amp; -7\\\\\n 0 &amp; 1 &amp; 0 &amp; \\frac{10}{7} &amp; \\frac{29}{7} \\\\\n 0 &amp; 0 &amp; 1 &amp; \\frac{5}{7} &amp; \\frac{25}{7}\\\\\n\\end{bmatrix}\nC=⎣⎢⎡​100​010​001​0710​75​​−7729​725​​⎦⎥⎤​\nA=BCA=BC\nA=BC\n具体流程请看书P74-P75\n 奇异值分解\n具体流程请看书P85\n核心在于这两个公式，下面我会给予做题提示。\nUn=AVnΔ−1A=USVHU_n=AV_n\\Delta^{-1}\\\\\nA=USV^H\nUn​=AVn​Δ−1A=USVH\n其中S一定与A同型，Δ\\DeltaΔ是非零奇异值张成的矩阵且一定是方阵，如果时间紧张我们可以通过A、S、Δ\\DeltaΔ的大小猜测出其他元素的大小。注意！奇异值要开根号！！！Δ−1\\Delta^{-1}Δ−1一定要求逆！！！\n\n求出AHAA^HAAHA的全部特征值及奇异值，由所有非零奇异值得到正线对角矩阵Δ\\DeltaΔ进而得到奇异矩阵SSS\n对于AHAA^HAAHA的每一个不同的特征根，求出与之相应的特征向量的极大无关组，经正交化、单位化得AHAA^HAAHA相应于该特征根的标准正交特征向量组。将其中与非零特征根相应的那些小组（作为一些列向量）顺序排成矩阵V1V_1V1​其次序应与Δ\\DeltaΔ中相关奇异值在对角线上的排列顺序相一致。再以AHAA^HAAHA相应于零特征根的标准正交特征向量（极大无关）组排成矩阵V2V_2V2​。于是可得酉矩阵V=(V1,V2)V=(V_1,V_2)V=(V1​,V2​)\n计算U1=AV1Δ−1U_1=AV_1\\Delta^{-1} \\kern 0.1ptU1​=AV1​Δ−1\n求出AAHAA^HAAH相应于零特征根的一个标准正交特征向量（极大无关）组,由它们排成m∗(m−r)m*(m-r)m∗(m−r)的部分酉阵U2U_2U2​，可得U=(U1,U2)U=(U_1,U_2)U=(U1​,U2​)综上，便得到奇异值分解A=USVHA=USV^HA=USVH\n\n 矩阵函数\n推荐使用待定系数法求解：\n\n写出∣λE−A∣|\\lambda E-A|∣λE−A∣，求出特征多项式\n求出最小多项式，详见最小多项式章节\n观察最小多项式次数n，设n-1次多项式\n将特征值带入右侧等于目标函数，当有重根时两侧同时求导再带入\n\n例：已知矩阵\nA=[221−261004]A=\\begin{bmatrix}\n 2 &amp; 2 &amp; 1\\\\\n -2 &amp; 6 &amp; 1\\\\\n 0 &amp; 0 &amp; 4\\\\\n\\end{bmatrix}\nA=⎣⎢⎡​2−20​260​114​⎦⎥⎤​\n求矩阵函数A\\sqrt{A} \\kern 0.1ptA​\n\n∣λE−A∣=(λ−4)3|\\lambda E-A|=(\\lambda-4)^3\n∣λE−A∣=(λ−4)3\n将矩阵A带入，先从最低次试起\n(A−4E)≠O(A−4E)2=O(A-4E)\\ne O\\\\\n(A-4E)^2= O\n(A−4E)=O(A−4E)2=O\n因此最小多项式就是(λ−4)2(\\lambda-4)^2(λ−4)2，平方项设1次式\naλ+b=λa\\lambda +b=\\sqrt{\\lambda}\naλ+b=λ​\n特征值带入，重根求导\n4a+b=2a=144a+b=2\\\\\na=\\frac{1}{4}\n4a+b=2a=41​\n解得a与b，b=1\nA=14A+E\\sqrt{A}=\\frac{1}{4}A+E\nA​=41​A+E\n 2011\n\n设ε1,ε2,ε3\\varepsilon_1,\\varepsilon_2,\\varepsilon_3ε1​,ε2​,ε3​是线性空间VVV的一组基，线性变换δ\\deltaδ使δ(ε1)=ε3,δ(ε2)=ε2,δ(ε3)=ε1\\delta(\\varepsilon_1)=\\varepsilon_3,\\delta(\\varepsilon_2)=\\varepsilon_2,\\delta(\\varepsilon_3)=\\varepsilon_1δ(ε1​)=ε3​,δ(ε2​)=ε2​,δ(ε3​)=ε1​，求δ\\deltaδ的所有特征根及全部特征向量。\nδ(ε1,ε2,ε3)=(ε1,ε2,ε3)[001010100]\\delta(\\varepsilon_1,\\varepsilon_2,\\varepsilon_3)=(\\varepsilon_1,\\varepsilon_2,\\varepsilon_3)\\begin{bmatrix}\n 0 &amp; 0 &amp; 1\\\\\n 0 &amp; 1 &amp; 0\\\\\n 1 &amp; 0 &amp; 0\\\\\n\\end{bmatrix}\nδ(ε1​,ε2​,ε3​)=(ε1​,ε2​,ε3​)⎣⎢⎡​001​010​100​⎦⎥⎤​\n∣λE−A∣=[λ0−10λ−10−10λ]|\\lambda E-A|=\\begin{bmatrix}\n \\lambda &amp; 0 &amp; -1\\\\\n 0 &amp; \\lambda-1 &amp; 0\\\\\n -1 &amp; 0 &amp; \\lambda\\\\\n\\end{bmatrix}\n∣λE−A∣=⎣⎢⎡​λ0−1​0λ−10​−10λ​⎦⎥⎤​\n得到特征值λ1=λ2=1,λ3=−1\\lambda_1=\\lambda_2=1,\\lambda_3=-1λ1​=λ2​=1,λ3​=−1\n得到特征向量\nl1=(−1,0,1)Tl2=(1,0,1)Tl3=(0,1,0)Tl_1=(-1,0,1)^T\\\\\nl_2=(1,0,1)^T\\\\\nl_3=(0,1,0)^T\nl1​=(−1,0,1)Tl2​=(1,0,1)Tl3​=(0,1,0)T\n对于特征值1k1(ε1+ε3)+k2(ε2)对于特征值−1k3(−ε1+ε3)对于特征值1\\\\\nk_1(\\varepsilon_1+\\varepsilon_3)+k_2(\\varepsilon_2)\\\\\n对于特征值-1\\\\\nk3(-\\varepsilon_1+\\varepsilon_3)\n对于特征值1k1​(ε1​+ε3​)+k2​(ε2​)对于特征值−1k3(−ε1​+ε3​)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC是nxn的笔误了。。\n\n\n\n 2013-J\n\n\n 2021\n\n\n\n\n\n\n\n\n\n\n\n 2022-?\n这里针对长春理工大学2022-2023年矩阵论期末考试最后一道题（8题）的相关内容做出个人解题见解。\n我认为PDF中的题解有问题，如果同样对这道题有疑问的同学，可在文章评论区（我可能看不到），微信（Sa9329Mxz），QQ（735690757）进一步讨论。\n\n\n基变换公式来自于p8\n坐标变换公式来自于p9\n线性变换在基下矩阵公式来自于p19\n线性变换作用在坐标下公式来自于p22\n\n\n\n\n 手写版\n这个手写版我会放在我的书里，但是可能会弄丢了，保险起见我把它扫描上来了。\n\n\n\n\n\n\n\n 特别鸣谢\n特别鸣谢我的室友：求求你别骗我，对本文详细审阅。\n","categories":["笔记","矩阵论解题笔记"],"tags":["矩阵论解题笔记"]},{"title":"计算机网络复试速览","url":"/network/computer_network_retest/","content":" 计算机网络复试\n 物理层\n传输比特流，规定机械特性、电气特性、功能特性。\n 数据链路层\n将原始比特流封装成帧，将物理层提供的可能出错的物理连接改造为逻辑上无差错的数据链路，是对物理层的一次增强。\n 网络层\n传输单位是数据报，进行路由选择，实现流量控制、拥塞控制、差错控制、网际互连。\n 传输层\n负责主机中两个进程通信，提供端到端的可靠传输。\n端到端：进程的端口号之间\n点对点：IP之间\n 应用层\nOSI最高层，是用户与网络之间的接口。\n 信道、串行与并行\n是通信的通道，即传输介质。串行适合长距离通信，并行适用于并行通信。\n 单工、半双工与全双工\n单工：只单向传输。\n半双工：双向，但是不能同时。\n全双工：双向，允许同时。\n 奈奎斯特定理\n理想信道中，为避免串码干扰极限速率。\n 香农定理\n实际信道中，传输速率上界。\n 双绞线、同轴电缆、光纤与无线传输\n双绞线、同轴电缆、光纤与无线传输这些都是物理层的传输介质。\n 物理层设备\n中继器：信号的转发、放大、整形，以防止失真，无差错控制。\n集线器：实际上集线器是一个多端口中继器。\n 帧\n是数据链路层对等实体之间进行的逻辑通信单元。\n 为网络层提供的三种服务\n\n无确认、无连接的服务，适用于误码率低的比如以太网\n有确认、无连接的服务，适用于无线通信\n有确认的面向连接的服务，适用于有高可靠要求的网络\n\n 封装成帧\n在一点数据的头和尾加一些信息，构成帧，主要是：标志、地址、控制、信息、帧检验序列、标志组成。\n 透明传输\n透明传输的作用是你无论传输了什么数据，都会保证你的传输原数据，不会被”误解“\n 流量控制\n链路两端存在工作速率差异，为以防淹没现象，使用流量控制。接收方控制发送方。\n 差错检测\n传输时可能产生错误，为修复这些错误使用差错检测机制\n\n位错：用CRC冗余校验\n帧错：这个是传输错误\n\n 字节填充法\n在开头填充和末尾填充\n 零比特填充法\n每遇五个‘1’在后面填充一个‘0’\n 差错控制\n自动重传请求ARQ / 前向纠错FEC：ARQ 是一种基于重传的错误控制机制。当接收端检测到数据包错误或丢失时，会请求发送端重新发送数据。FEC 是一种通过在发送的数据中添加冗余信息来纠正错误的机制。接收端利用这些冗余信息检测并纠正错误，而无需请求重传。\n差错控制又可分为：检错编码（奇偶校验码、循环冗余码）和纠错编码（海明码）\n\n奇偶校验码（检错）就是让1的个数是奇数个还是偶数个\n海明码（纠错）就是在信息位中加入几个就错位\n\n 停止-等待流量控制（一个一个的）\n发送方每次发送一个帧，只有接收方回复确认帧才发下一帧\n 滑动窗口流量控制（一块一块的）\n集中发送一块，收到最左侧的帧后向右滑动，如果未收确认消息到那就重发。\n ARQ协议（自动重传请求）!\n\n停止-等待Stop-and-Wait：发送端发送一个数据包后，等待接收端的确认（ACK）。如果收到 ACK，发送下一个数据包；如果超时未收到 ACK，则重传。简单但效率较低，适合低速率通信。\n后退N帧Go-Back-N：如果某个数据包出错，接收端会丢弃该包及其后续的所有包，发送端需要从出错的数据包开始重传。\n选择性重传Selective Repeat：如果某个数据包出错，只重传出错的数据包，而不是后续的所有包。\n\n1-&gt;2-&gt;3，越来越优秀。\n 介质访问控制\n数据链路层的一个子层，主要负责管理多个设备共享同一通信介质时的访问权限。它的核心目标是解决多路访问问题，即如何高效、公平地协调多个设备对共享介质的访问，以避免冲突并提高网络性能。\n\n频分复用FDM：将总带宽划分为多个子频带，每个子频带分配给一个信道（用户或设备）\n时分复用TDM：将时间划分为固定长度的时隙，每个时隙分配给一个信道。\n波分复用WDM：在光纤通信中，将不同波长的光信号复用到同一根光纤中传输。\n码分复用CDM：每个信道使用唯一的编码序列对数据进行调制，多个信道可以在同一频率和时间内传输。\n\n\n\n\n技术\n划分依据\n适用场景\n优点\n缺点\n\n\n\n\nFDM\n频率\n广播、模拟通信\n简单、适合恒定速率数据\n频率利用率低\n\n\nTDM\n时间\n数字电话、数字通信\n高效、适合数字信号\n对同步要求高\n\n\nWDM\n波长（光频率）\n光纤通信\n高容量、长距离传输\n设备成本高\n\n\nCDM\n编码\n移动通信、卫星通信\n抗干扰、多用户同时通信\n实现复杂\n\n\n\n ALOHA\n最简单的随机访问协议，设备随时发送数据。如果发生冲突，随机等待一段时间后重传。效率较低，最大信道利用率为 18.4%。\n 载波侦听多路访问（CSMA）\n设备在发送数据前先侦听介质是否空闲。如果介质空闲，则发送数据；否则等待。\n\n\nCSMA/CD（冲突检测）：用于有线以太网，检测到冲突后立即停止发送。\n\n\nCSMA/CA（冲突避免）：用于无线网络（如 Wi-Fi），通过随机退避避免冲突。\n\n\nCSMA有：1-坚持CSMA、非坚持CSMA、p-坚持CSMA\n\n“1-坚持” 表示设备以概率 1 发送数据（即一旦介质空闲，必定发送）。\n设备不会持续侦听介质，而是随机退避。\np 是一个可调参数，用于控制发送概率，结合了 1-坚持和非坚持的特点。\n\n 载波侦听多路访问/冲突检测 CSMA/CD （有线网络）\n 载波侦听（Carrier Sense）\n\n设备在发送数据前先侦听介质是否空闲。\n如果介质空闲，则开始发送数据。\n如果介质忙，则等待直到介质空闲。\n\n 多路访问（Multiple Access）\n\n多个设备共享同一通信介质，可能同时尝试发送数据。\n\n 冲突检测（Collision Detection）（CD）\n\n设备在发送数据的同时继续侦听介质。\n如果检测到冲突（即信号强度异常），立即停止发送数据。\n发送一个冲突强化信号（Jamming Signal），通知其他设备发生了冲突。\n\n 随机退避（Random Backoff）\n\n设备等待一个随机时间后重新尝试发送数据。\n随机时间通过二进制指数退避算法计算，以减少再次冲突的概率。\n\n先听再发、边发边听、冲突停发、随机重发\n 争用期2τ\nτ 是信号在网络中传播的单程最大传播时延，争用期（2τ） 是 CSMA/CD 协议中冲突检测的时间窗口。决定了网络的最大物理范围和最小帧长。通过合理设计争用期，可以确保冲突检测的有效性，从而提高网络的可靠性。\n 二进制指数退避算法\n冲突之后要等待，等？到底等多久？利用二进制指数退避算法就能知道。\n 冲突避免（CA）\n冲突避免通过预先协调和随机退避来尽量避免冲突的发生，而不是在冲突发生后再进行处理。\n 载波侦听多路访问/冲突避免 CSMA/CA （无线网络）\n是一种用于无线网络（如 Wi-Fi）的介质访问控制协议。\n CSMA/CA 与 CSMA/CD 的区别\n\n\n\n特性\nCSMA/CA（冲突避免）\nCSMA/CD（冲突检测）\n\n\n\n\n适用网络\n无线网络（如 Wi-Fi）\n有线网络（如以太网）\n\n\n冲突处理\n通过 RTS/CTS 和退避避免冲突\n通过检测冲突并重发数据\n\n\n隐藏终端问题\n有效解决\n无法解决\n\n\n开销\n较高（RTS/CTS 和退避机制）\n较低\n\n\n延迟\n较高\n较低\n\n\n\n 常见拓扑结构\n\n星型\n环型\n总线型\n\n 以太网\n逻辑拓扑结构是总线型，物理拓扑是星型结构\n以太网采用两项措施简化通信\n\n采用无连接工作方式\n使用曼彻斯特编码\n\n MAC地址\n设备身份证，绝对唯一的地址，MAC地址48位。\n 单播、多播、广播\n\n单播：一对一\n多播：一对多\n广播：一对所有\n\n 以太网的目的地址、源地址\n\n目的地址：标识帧的接收方。\n源地址：标识帧的发送方。\n类型：指示帧内数据的协议类型。（IPV4、IPV6、ARP）\n\n VLAN虚拟局域网\nVLAN（Virtual Local Area Network） 是一种将物理局域网划分为多个逻辑局域网的技术。通过VLAN，可以在同一物理网络设备（如交换机）上创建多个独立的广播域，从而实现网络的逻辑隔离和优化。\n PPP 协议（Point-to-Point Protocol）\nPPP（点对点协议） 是一种数据链路层协议，用于在两个节点之间直接传输数据。它广泛应用于拨号连接、DSL、串行通信等场景，支持多种网络层协议（如 IP、IPX 等），并提供了身份验证、错误检测和压缩等功能。\n\n只保证无差错接收（CRC校验），是不可靠的服务\n只支持全双工的点对点链路\nPPP两端可以运行不同的网络层协议\n\nPPP无连接、不可靠、速度快\n 集线器\n广播传输：数据会被发送到所有端口，无论目标设备是哪一个。\n无智能过滤：集线器无法识别数据的目标地址，因此无法优化数据传输。\n冲突域：所有设备共享同一个冲突域，容易发生数据冲突，导致网络效率低下。\n带宽共享：所有端口共享总带宽（例如，10 Mbps 的集线器连接 5 台设备，每台设备平均只能使用 2 Mbps）。\n 网桥\n网桥可以识别帧转发帧，解决冲突的问题\n 集线器与网桥的主要区别\n\n\n\n特性\n集线器（Hub）\n网桥（Bridge）\n\n\n\n\n工作层次\n物理层（Layer 1）\n数据链路层（Layer 2）\n\n\n数据传输方式\n广播到所有端口\n根据 MAC 地址转发到目标端口\n\n\n冲突域\n所有端口共享一个冲突域\n每个端口是一个独立的冲突域\n\n\n广播域\n所有端口共享一个广播域\n所有端口共享一个广播域\n\n\n带宽管理\n所有端口共享总带宽\n每个端口有独立带宽\n\n\n智能过滤\n无\n有（基于 MAC 地址）\n\n\n应用场景\n小型网络或临时连接\n连接两个局域网或扩展网络\n\n\n现代替代品\n交换机\n交换机\n\n\n\n 交换机\n是多接口的网桥，是网桥的增强版，拥有自学习算法\n记录各个端口的MAC地址（交换表），冷启动阶段通过泛洪，找到对应的MAC\n 集线器、交换机\n集线器不隔离冲突域，也不隔离广播域。\n交换机隔离冲突域，但不隔离广播域。\n 设备\n\n\n\n特性\n能否隔离冲突域\n能否隔离广播域\n\n\n\n\n集线器\n不能\n不能\n\n\n中继器\n不能\n不能\n\n\n交换机\n能\n不能\n\n\n网桥\n能\n不能\n\n\n路由器\n能\n能\n\n\n\n 网络层功能\n\n路由选择：选择数据从源设备到目标设备的最佳路径。\n分组转发：根据路由表将数据包从输入接口转发到输出接口。\n逻辑寻址：为网络中的设备分配唯一的逻辑地址（如 IP 地址）。\n分段与重组：将大数据包分割成适合传输的小数据包，并在目标设备处重新组装。\n拥塞控制：防止网络因数据流量过大而出现拥塞。\n异构网络互联：连接不同类型的网络（如以太网、Wi-Fi、帧中继等）。\n错误处理与诊断：检测和处理网络中的错误。\n服务质量：为不同类型的数据流提供优先级和带宽保障。\n安全性：保护网络层数据的安全性和隐私性。\n\n 路由器功能\n\n路由选择\n分组转发\n\n SDN软件定义网络\n软件定义网络（SDN，Software-Defined Networking） 是一种网络架构，旨在通过将网络控制平面与数据平面分离，提升网络的可编程性、灵活性和管理效率。SDN的核心思想是通过集中式的控制器来管理网络设备，简化配置和优化流量。\nSDN的优势\n\n集中控制：\n\n通过集中控制器管理网络，简化配置和故障排查。\n\n\n灵活性与可编程性：\n\n网络行为可通过软件编程动态调整，适应不同需求。\n\n\n自动化与智能化：\n\n支持自动化配置和优化，减少人工干预。\n\n\n网络虚拟化：\n\n支持多租户网络虚拟化，提升资源利用率。\n\n\n快速创新：\n\n新功能可通过软件快速部署，无需更换硬件。\n\n\n\n 拥塞控制\n因为网络中出现过多的分组而引起的网络性能下降。\n控制拥塞的两种方法：\n\n开环控制：事发前\n闭环控制：事发中/后\n\n IPv4（32位）\n控制网络数据传送的网络协议。其主要由首部（20字节）加数据部\n网络号+主机号\n全0是本机地址\n全1是广播地址\n IPv6（128位）\n 源地址字段、目的地址字段\n MTU（最大传送单元）\n OSI 五层模型 vs OSI 七层模型\n\n\n\nOSI 五层模型\nOSI 七层模型\n功能描述\n\n\n\n\n应用层\n应用层（Application）\n提供应用程序接口和服务（如 HTTP、FTP、DNS 等）。\n\n\n\n表示层（Presentation）\n数据格式化、加密和解密（如 SSL/TLS）。\n\n\n\n会话层（Session）\n建立、管理和终止会话（如 RPC、NetBIOS）。\n\n\n传输层\n传输层（Transport）\n提供端到端的可靠数据传输（如 TCP、UDP）。\n\n\n网络层\n网络层（Network）\n负责逻辑地址寻址和路由选择（如 IP、ICMP）。\n\n\n数据链路层\n数据链路层（Data Link）\n提供节点到节点的可靠数据传输（如 Ethernet、PPP）。\n\n\n物理层\n物理层（Physical）\n传输原始的比特流（如网线、光纤、Wi-Fi）。\n\n\n\n NAT网络地址转换\nNAT（Network Address Translation，网络地址转换） 是一种用于将私有网络中的 IP 地址映射到公共 IP 地址的技术。它主要用于解决 IPv4 地址不足的问题，并提高网络的安全性。\n 子网掩码\n子网掩码（Subnet Mask） 是用于划分 IP 地址的网络部分和主机部分的关键工具。它的主要作用是帮助网络设备确定一个 IP 地址属于哪个子网，从而实现高效的路由和网络管理。\n\n划分网络和主机\n支持子网划分\n路由选择\n广播域控制\n\n IP 地址的分类\n\n\n\n类别\n前缀\n地址范围\n默认子网掩码\n用途\n\n\n\n\nA 类\n0\n1.0.0.0 到 126.0.0.0\n255.0.0.0\n大型网络\n\n\nB 类\n10\n128.0.0.0 到 191.255.0.0\n255.255.0.0\n中型网络\n\n\nC 类\n110\n192.0.0.0 到 223.255.255.0\n255.255.255.0\n小型网络\n\n\nD 类\n1110\n224.0.0.0 到 239.255.255.255\n无\n多播（Multicast）\n\n\nE 类\n1111\n240.0.0.0 到 255.255.255.255\n无\n保留（实验用途）\n\n\n\n 路由聚合\n将好几个路由器逻辑上视为一个，是地址聚合。\n 最长前缀匹配\n ARP地址解析协议\n DHCP动态主机配置协议\n动态分配IP地址，允许一台主机无需手动配置即可获取到IP地址，基于UDP。\n ICMP网际控制报文协议\nICMP是传输了一个控制报文，专门用来报告报告错误和异常的情况\n RIP的优势和劣势\n好消息传得快、坏消息传得慢\n","categories":["笔记","计算机网络笔记"],"tags":["计算机网络"]},{"title":"模糊数学25习题","url":"/math/mh_math/","content":" 第一章\n\n\n\n\n\n\n\n\n 第二章\n\n\n\n\n\n\n\n \n 第三章\n\n\n\n\n\n\n\n\n 第四章\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 居卡莫特方法——哥弟姐妹公式\n居卡莫特方法是求解模糊关系方程的核心所在，对于考试来说，主要目的在于求解形如：\n\n这样的题。\n居卡莫特方法定义了如下两种运算方法：\nbεa={b,当a&gt;b时[b,1],当a=b时∅,当b&gt;a时b\\varepsilon a=\\left\\{\\begin{matrix}\n b, &amp; 当a&gt;b时\\\\\n [b,1], &amp; 当a=b时\\\\\n \\emptyset , &amp; 当b&gt;a时\n\\end{matrix}\\right.\nbεa=⎩⎪⎨⎪⎧​b,[b,1],∅,​当a&gt;b时当a=b时当b&gt;a时​\nbε^a={[0,b],当a&gt;b时[0,1],当a≤b时b \\hat{\\varepsilon} a=\\left\\{\\begin{matrix}\n [0,b], &amp; 当a&gt;b时\\\\\n [0,1], &amp; 当a\\le b时\n\\end{matrix}\\right.\nbε^a={[0,b],[0,1],​当a&gt;b时当a≤b时​\n背下来就行，问题是怎么背，我在这里提出一个原创故事来帮助你记忆——哥弟姐妹公式。\n不带帽子的称为哥弟公式，b是哥哥，a是弟弟。\nbεa={b,当a&gt;b时[b,1],当a=b时∅,当b&gt;a时b\\varepsilon a=\\left\\{\\begin{matrix}\n b, &amp; 当a&gt;b时\\\\\n [b,1], &amp; 当a=b时\\\\\n \\emptyset , &amp; 当b&gt;a时\n\\end{matrix}\\right.\nbεa=⎩⎪⎨⎪⎧​b,[b,1],∅,​当a&gt;b时当a=b时当b&gt;a时​\n故事开始了…\n哥哥让着弟弟（当a&gt;b时），我们说这是一个有担当的哥哥，我们应该褒奖哥哥（b）\n哥哥与弟弟一样时，这说明兄弟二人相处良好，哥哥爱着弟弟（这个[b, 1]可以理解为b是哥哥，1就像字母i一样，哥哥b，i弟弟，[b, 1]）\n当哥哥比弟弟大，也就是说他不爱弟弟，爱归于空寂，所以不爱了是空集。\n有帽子的称为姐妹公式，b是姐姐，a是妹妹。\nbε^a={[0,b],当a&gt;b时[0,1],当a≤b时b \\hat{\\varepsilon} a=\\left\\{\\begin{matrix}\n [0,b], &amp; 当a&gt;b时\\\\\n [0,1], &amp; 当a\\le b时\n\\end{matrix}\\right.\nbε^a={[0,b],[0,1],​当a&gt;b时当a≤b时​\n故事开始了…\n妹妹比较牛，当妹妹生气的时候（a&gt;b），这个时候姐姐也很生气，无论妹妹做什么姐姐都不想理妹妹，姐姐睁一只眼闭一只眼（睁一只眼闭一只眼，[0,b]，这个睁一只眼就像0一样，闭一只眼就是b，所以是[0,b]）。\n后来（其他情况），吵来吵去终究姐妹重归于好，他们要共同成长，共赴未来（[0,1]从零到一描绘了从小到大，共同成长）。\n 碎碎念\n模糊数学复习就到这里了，感谢你能看到这里，祝复习顺利。\n 特别鸣谢\n特别鸣谢我的室友：求求你别骗我，对本文详细审阅。\n","categories":["笔记","模糊数学解题笔记"],"tags":["模糊数学解题笔记"]},{"title":"Attention is all you need","url":"/paper/attention_is_all_your_need/","content":" Attention is all you need\n\n\n【Original Link】 Attention Is All You Need\nUpdated on Aug 2023\nAuthors\n\nAshish Vaswani Google Brain  avaswani@google.com\nNoam Shazeer Google Brain  noam@google.com\nNiki Parmar Google Research nikip@google.com\nJakob Uszkoreit Google Research usz@google.com\nLlion Jones Google Research llion@google.com\nAidan N. Gomez † University of Toronto aidan@cs.toronto.edu\nŁukasz Kaiser Google Brain  lukaszkaiser@google.com\nIllia Polosukhin ‡  illia.polosukhin@gmail.com\n\n\n\n\n Abstract\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.\n显性序列转导模型基于复杂的循环或卷积神经网络，其中包括编码器和解码器。\nThe best performing models also connect the encoder and decoder through an attention mechanism.\n性能最佳的模型还通过注意力机制连接编码器和解码器。\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n我们提出了一种新的简单网络架构，即 Transformer，它仅基于注意力机制，完全省去了重复和卷积。\nExperiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.\n对两项机器翻译任务的实验表明，这些模型在质量上表现出色，同时更具可并行化性，并且需要更少的训练时间。\nOur model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU.\n我们的模型在 WMT 2014 英德翻译任务中达到了 28.4 BLEU，比现有的最佳结果（包括合奏）提高了 2 BLEU 以上。\nOn the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n在 WMT 2014 英法翻译任务中，我们的模型在 8 个 GPU 上训练 3.5 天后，建立了新的单模型最先进的 BLEU 分数 41.8，这只是文献中最佳模型训练成本的一小部分。\nWe show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n我们证明了Transformer模型能够很好地推广到其他任务，通过将其成功应用于英语成分句法分析，无论是在大量训练数据还是有限训练数据的情况下。\n Introduction\nRecurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation.\n循环神经网络，尤其是长短期记忆网络（LSTM）和门控循环单元（GRU），已经在序列建模和转导问题（如语言建模和机器翻译）中被牢固地确立为最先进的方法。\nNumerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.\n自那以后，人们不断努力突破循环语言模型和编码器 - 解码器架构的极限。\nRecurrent models typically factor computation along the symbol positions of the input and output sequences.\n循环模型通常沿着输入和输出序列的符号位置进行计算分解。\nAligning the positions to steps in computation time, they generate a sequence of hidden states hth_tht​, as a function of the previous hidden state ht−1h_t−1ht​−1 and the input for position ttt.\n将位置与计算时间的步骤对齐，它们生成一系列隐藏状态hth_tht​，这些隐藏状态是前一个隐藏状态ht−1h_t−1ht​−1和位置ttt的输入的函数。\nThis inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.\n这种固有的顺序性使得在训练样本内部无法进行并行化处理，这在序列长度较长时变得尤为关键，因为内存限制会限制跨样本的批处理。\nRecent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation, while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n最近的研究通过分解技巧和条件计算在计算效率方面取得了显著的改进，同时也提高了后者的模型性能。然而，顺序计算的基本限制仍然存在。\n\n分解技巧可以用于优化矩阵乘法、卷积操作等。例如，通过将一个大的矩阵乘法分解为多个小的矩阵乘法，可以利用现代硬件的并行计算能力，从而提高计算效率。\n条件计算可以用于动态调整模型的计算量，例如在某些情况下跳过某些层的计算，或者根据输入数据的特征选择性地激活某些神经元。\n\nAttention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences\n注意力机制已经成为各种任务中令人信服的序列建模和转导模型的一个重要组成部分，它允许在不考虑输入或输出序列中依赖项距离的情况下对依赖关系进行建模。\nIn all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.\n然而，在几乎所有情况下，这些注意力机制都是与循环网络结合使用的。\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n在这项工作中，我们提出了Transformer模型，这是一种摒弃了循环结构，完全依赖于注意力机制来捕捉输入和输出之间全局依赖关系的架构。Transformer模型能够实现显著更高的并行化程度，并且在仅使用八块P100 GPU训练十二小时后，就能在翻译质量上达到新的最高水平。\n Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet and ConvS2S, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.\n减少顺序计算的目标也是扩展型神经GPU（Extended Neural GPU）、ByteNet和ConvS2S的基础，这些模型都使用卷积神经网络（CNN）作为基本构建块，能够并行计算所有输入和输出位置的隐藏表示。\nIn these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.\n在这些模型中，将两个任意输入或输出位置的信号关联起来所需的运算次数会随着这两个位置之间的距离增加而增长。对于ConvS2S模型，这种增长是线性的；而对于ByteNet模型，这种增长是对数的。\nit more difficult to learn dependencies between distant positions. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n这使得学习远距离位置之间的依赖关系变得更加困难。在Transformer模型中，这种困难被减少到了一个固定的运算次数，尽管这以平均注意力加权位置导致的有效分辨率降低为代价，而我们通过第3.2节中描述的多头注意力（Multi-Head Attention）机制来抵消这种影响。\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations\n自注意力（Self-attention），有时也称为内部注意力（intra-attention），是一种注意力机制，它通过关联单个序列中不同位置的信息来计算该序列的表示。自注意力已经在多种任务中成功应用，包括阅读理解、摘要生成、文本蕴含以及学习与任务无关的句子表示。\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks.\n端到端记忆网络（End-to-end Memory Networks）基于一种循环注意力机制，而不是基于序列对齐的循环，已经在简单语言问答和语言建模任务上表现良好。\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as and.\n然而，据我们所知，Transformer是第一个完全依赖自注意力来计算其输入和输出的表示的转导模型，而不使用序列对齐的循环神经网络（RNN）或卷积。在接下来的章节中，我们将介绍Transformer模型，阐述自注意力的动机，并讨论其相对于其他模型（如RNN和卷积模型）的优势。\n Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations (x1,...,xn)(x_1, ..., x_n)(x1​,...,xn​) to a sequence of continuous representations z=(z1,...,zn)z = (z_1, ..., z_n)z=(z1​,...,zn​).\n大多数具有竞争力的神经序列转导模型都采用了编码器 - 解码器结构。在这种结构中，编码器将输入符号序列的表示(x1,...,xn)(x_1, ..., x_n)(x1​,...,xn​) 映射到一个连续表示的序列 z=(z1,...,zn)z = (z_1, ..., z_n)z=(z1​,...,zn​)。\nGiven zzz, the decoder then generates an output sequence (y1,...,ym)(y_1, ..., y_m)(y1​,...,ym​) of symbols one element at a time.\n给定 zzz，解码器随后逐个生成符号的输出序列 (y1,...,ym)(y_1, ..., y_m)(y1​,...,ym​)。\nAt each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.\n在每一步中，模型都是自回归的，在生成下一个符号时，会将之前生成的符号作为额外的输入。\n\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\nTransformer模型遵循这种总体架构，使用堆叠的自注意力层和逐点全连接层来构建编码器和解码器，分别如图1的左半部分和右半部分所示。\n Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N=6N = 6N=6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x+Sublayer(x))LayerNorm(x + Sublayer(x))LayerNorm(x+Sublayer(x)), where Sublayer(x)Sublayer(x)Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel=512.d_{model} = 512.dmodel​=512.\n编码器由 N=6N = 6N=6 个相同的层堆叠而成。每一层包含两个子层。第一个子层是多头自注意力机制（Multi-Head Self-Attention），第二个子层是一个简单的逐点全连接前馈网络（Position-wise Fully Connected Feed-Forward Network）。我们在每个子层周围使用残差连接（Residual Connection），并随后进行层归一化（Layer Normalization）。具体来说，每个子层的输出是LayerNorm(x+Sublayer(x))LayerNorm(x + Sublayer(x))LayerNorm(x+Sublayer(x))，其中Sublayer(x)Sublayer(x)Sublayer(x) 是子层自身实现的函数。为了便于实现这些残差连接，模型中的所有子层以及嵌入层都产生维度为 dmodel=512d_{model} = 512dmodel​=512 的输出。\nDecoder: The decoder is also composed of a stack of N=6N = 6N=6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position iii can depend only on the known outputs at positions less than iii.\n解码器同样由N=6N = 6N=6 个相同的层堆叠而成。除了编码器层中的两个子层外，解码器插入了一个第三个子层，该子层对编码器堆栈的输出执行多头注意力操作。与编码器类似，我们在每个子层周围使用残差连接，并随后进行层归一化。我们还修改了解码器堆栈中的自注意力子层，以防止位置关注后续位置。这种掩码（masking）与输出嵌入偏移一个位置的事实相结合，确保了位置 iii 的预测只能依赖于小于 iii 的已知输出。\n Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n注意力函数可以被描述为将一个查询（query）和一组键值对（key-value pairs）映射到一个输出，其中查询、键、值和输出都是向量。输出是值的加权和，其中每个值的权重是由查询与相应键的兼容性函数计算得出的。\n\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n图 2：（左）缩放点积注意力。（右）多头注意力由并行运行的多个注意力层组成。\n Scaled Dot-Product Attention(缩放点积注意力)\nWe call our particular attention “Scaled Dot-Product Attention” (Figure 2).\n我们将我们的特殊注意力称为“尺度点积注意力”（图 2）。\nThe input consists of queries and keys of dimension dkd_kdk​, and values of dimension dvd_vdv​. We compute the dot products of the  query with all keys, divide each by dk\\sqrt{d_k}dk​​, and apply a softmax function to obtain the weights on the values.\n输入由维度 dkd_kdk​ 的查询和键以及维度 dvd_vdv​ 的值组成。我们计算所有键的查询的点积，将每个键除以 dk\\sqrt{d_k}dk​​，并应用 softmax 函数来获得值的权重。\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix QQQ. The keys and values are also packed together into matrices KKK and V . We compute the matrix of outputs as:\n在实践中，我们同时计算一组查询的注意力函数，并打包成矩阵 QQQ。键和值也打包到矩阵 KKK 和 VVV 中。我们将输出矩阵计算为：\nAttention(Q,K,V)=softmax(QKTdk)V\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\nAttention(Q,K,V)=softmax(dk​​QKT​)V\nThe two most commonly used attention functions are additive attention, and dot-product (multiplicative) attention.\n两个最常用的注意力函数是加法注意力和点积（乘法）注意力。\nDot-product attention is identical to our algorithm, except for the scaling factor of 1dk\\frac{1}{\\sqrt{d_k}}dk​​1​  .\n点积注意力与我们的算法相同，只是缩放因子为 1dk\\frac{1}{\\sqrt{d_k}}dk​​1​ 。\nAdditive attention computes the compatibility function using a feed-forward network with  a single hidden layer\n加性注意力使用具有单个隐藏层的前馈网络计算兼容性函数\nWhile the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n虽然两者在理论复杂性上相似，但点积注意力在实践中速度更快、空间效率更高，因为它可以使用高度优化的矩阵乘法代码来实现。\nWhile for small values of dkd_kdk​ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dkd_kdk​\n虽然对于较小的 dkd_kdk​ 值，这两种机制的性能相似，但对于较大的 dkd_kdk​ 值，加法注意力在不缩放的情况下优于点积注意力。\nWe suspect that for large values of dkd_kdk​, the dot products grow large in magnitude, pushing the softmax function into regions where it has  extremely small gradients\n我们怀疑，对于较大的 dkd_kdk​ 值，点积的幅度会变大，将 softmax 函数推入梯度极小的区域\nTo counteract this effect, we scale the dot products by √1dk\n为了抵消这种影响，我们将点积缩放为1dk\\frac{1}{\\sqrt{d_k}}dk​​1​\n Multi-Head Attention（多头注意力）\nInstead of performing a single attention function with dmodeld_{model}dmodel​-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dkd_kdk​, dkd_kdk​ and dvd_vdv​ dimensions, respectively.\n我们发现，与其对 dmodeld_{model}dmodel​维度的键、值和查询执行单个注意力函数，不如分别使用不同的学习线性投影将查询、键和值线性投影到 dkd_kdk​, dkd_kdk​ 和dvd_vdv​ 维度。\nOn each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dvd_vdv​-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n在这些查询、键和值的投影版本上，我们并行地执行注意力函数，得到维度为 dvd_vdv​的输出值。这些输出值被拼接在一起，并再次进行投影，从而得到最终的值，如图2所示。\n\n假设查询 QQQ、键 KKK  和值 VVV的维度分别为  dmodeld_{\\text{model}}dmodel​ ，多头注意力机制可以表示为：\n\n投影\n\nQi=QWiQ,Ki=KWiK,Vi=VWiVfor i=1,…,hQ_i = Q W_i^Q, \\quad K_i = K W_i^K, \\quad V_i = V W_i^V \\quad \\text{for } i = 1, \\ldots, h\nQi​=QWiQ​,Ki​=KWiK​,Vi​=VWiV​for i=1,…,h\n其中，WiQW_i^QWiQ​、WiKW_i^KWiK​ 和 WiVW_i^VWiV​ 是每个头的权重矩阵，hhh 是头的数量。\n\n\n并行执行注意力函数\nheadi=Attention(Qi,Ki,Vi)=softmax(QiKiTdk)Vi\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i) = \\text{softmax}\\left(\\frac{Q_i K_i^T}{\\sqrt{d_k}}\\right) V_i\nheadi​=Attention(Qi​,Ki​,Vi​)=softmax(dk​​Qi​KiT​​)Vi​\n\n\n拼接\nConcat(head1,head2,…,headh)\\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h)\nConcat(head1​,head2​,…,headh​)\n\n\n再次投影\nMultiHead(Q,K,V)=Concat(head1,head2,…,headh)WO\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h) W^O\nMultiHead(Q,K,V)=Concat(head1​,head2​,…,headh​)WO\n\n\n\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n多头注意力允许模型共同关注来自不同位置的不同表示子空间的信息。对于单个注意力头，平均会抑制这种情况。\nMultiHead(Q,K,V)=Concat(head1,head2,…,headh)where headi=Attention(QWiQ,KWiK,VWiV)\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h)\\\\\n\\text{where} \\ head_i=Attention(QW_{i}^Q,KW_{i}^K,VW_{i}^V)\nMultiHead(Q,K,V)=Concat(head1​,head2​,…,headh​)where headi​=Attention(QWiQ​,KWiK​,VWiV​)\nWhere the projections are parameter matrices WiQ∈Rdmodel×dk,WiK∈Rdmodel×dk,WiV∈Rdmodel×dvW^Q_i\\in\\mathbb{R^{d_{model}\\times d_k}}, W^K_i\\in\\mathbb{R^{d_{model}\\times d_k}}, W^V_i\\in\\mathbb{R^{d_{model}\\times d_v}}WiQ​∈Rdmodel​×dk​,WiK​∈Rdmodel​×dk​,WiV​∈Rdmodel​×dv​ and WiQ∈Rhdmodel×dkW^Q_i\\in\\mathbb{R^{hd_{model}\\times d_k}}WiQ​∈Rhdmodel​×dk​\n其中投影是参数矩阵WiQ∈Rdmodel×dk,WiK∈Rdmodel×dk,WiV∈Rdmodel×dvW^Q_i\\in\\mathbb{R^{d_{model}\\times d_k}}, W^K_i\\in\\mathbb{R^{d_{model}\\times d_k}}, W^V_i\\in\\mathbb{R^{d_{model}\\times d_v}}WiQ​∈Rdmodel​×dk​,WiK​∈Rdmodel​×dk​,WiV​∈Rdmodel​×dv​ 和 WiQ∈Rhdmodel×dkW^Q_i\\in\\mathbb{R^{hd_{model}\\times d_k}}WiQ​∈Rhdmodel​×dk​\nIn this work we employ h=8h = 8h=8 parallel attention layers, or heads. For each of these we use dk=dv=dmodelh=64d_k = d_v = \\frac{d_{\\text{model}}}{h} = 64dk​=dv​=hdmodel​​=64.Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n在这项工作中，我们使用了h=8h = 8h=8个并行的注意力层，或者称为头。对于每一个头，我们使用dk=dv=dmodelh=64d_k = d_v = \\frac{d_{\\text{model}}}{h} = 64dk​=dv​=hdmodel​​=64。由于每个头的维度减小，总计算成本与全维的单头注意力相似。\n Applications of Attention in our Model(注意力在我们模型中的应用)\nThe Transformer uses multi-head attention in three different ways:\nTransformer 以三种不同的方式使用多头注意力：\n\nIn “encoder-decoder attention” layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.在“编码器-解码器注意”层中，查询来自前一个解码器层，内存键和值来自编码器的输出。This allows every position in the decoder to attend over all positions in the input sequence.这允许解码器中的每个位置都关注输入序列中的所有位置。This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].这模仿了序列到序列模型（如[38,2,9]）中典型的编码器-解码器注意力机制。\nThe encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.编码器包含自注意力层。在自注意力层中，所有键、值和查询都来自同一个位置，在本例中，是编码器中上一层的输出。编码器中的每个位置都可以关注编码器上一层中的所有位置。\nSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.类似地，解码器中的自注意力层允许解码器中的每个位置关注解码器中的所有位置，直到该位置并包括该位置。We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.我们需要防止解码器中信息向左流动，以保留自回归属性。我们通过屏蔽（设置为 -∞）softmax 输入中对应于非法连接的所有值来实现这一点。参见图 2。\n\n Position-wise Feed-Forward Networks（位置前馈网络）\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n除了注意力子层外，我们的编码器和解码器中的每一层都包含一个完全连接的前馈网络，该网络分别且相同地应用于每个位置。这包括两个线性变换，中间有一个 ReLU 激活。\nFFN(x)=max(0,xW1+b1)W2+b2FFN(x)=max(0,xW_1+b_1)W_2+b_2\nFFN(x)=max(0,xW1​+b1​)W2​+b2​\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n虽然线性变换在不同位置上是相同的，但它们在层与层之间使用不同的参数。另一种描述这一点的方法是两个内核大小为 1 的卷积。\nThe dimensionality of input and output is dmdoel=512d_{mdoel}=512dmdoel​=512,and the inner-layer has dimensionalitydff=2048d_{ff}=2048dff​=2048\n输入和输出的维数为 dmdoel=512d_{mdoel}=512dmdoel​=512，内层的维数为 dff=2048d_{ff}=2048dff​=2048。\n Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodeld_{model}dmodel​.\n与其他序列转导模型类似，我们使用学习的嵌入将输入标记和输出标记转换为维度  dmodeld_{model}dmodel​的向量。\nWe also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.\n们还使用通常学习的线性变换和 softmax 函数将解码器输出转换为预测的下一个标记概率。\nIn our model, we share the same weight matrix between the two embedding layers and the pre-softmax  linear transformation, similar to [30].In the embedding layers, we multiply those weights by dmodel\\sqrt{d_{model}}dmodel​​\n在我们的模型中，我们在两个嵌入层和预softmax线性变换之间共享相同的权重矩阵，类似于[30]。在嵌入层中，我们将这些权重乘以dmodel\\sqrt{d_{model}}dmodel​​\n","categories":["深度学习","论文阅读"],"tags":["python","深度学习","attention","transformer"]},{"title":"3DCNN 网络结构","url":"/python/3D_CNN/","content":" 3DCNN 网络结构\n学习内容基于：https://www.bilibili.com/video/BV1kc411Q7Tj\n 图像的本质\n图像是由RGB三通道组成的，一般数值范围是0到255.\n因此像素矩阵为：H * W * 3，其中H为高，W是宽。\n 从图像到视频\n视频本质上是由连续的图片（视频帧），快速播放构成的。\n图片的像素矩阵是H * W * 3\n视频就是D * H * W * 3，其中D是深度，是视频帧的叠加。\n不同的库对与这4个维度顺序表述不同，但这四个维度诚然如此。\n 2D检测方法对比\n\n\n\n C3D网络流程\n\n这个（1，1，1），三个数字是有点手法的，第一个数是代表了深度，第二个、第三个代表了高和宽。\n我们可以看到，一般来说这个卷积核大小一般是（3，3，3），填充是（1，1，1），经过这样的卷积过后，特征图大小不变。\n\n这里是up的解释，已经是非常的直观易懂。\n 动手搭建一个C3D\nimport torch\nfrom torch import nn\nfrom torchsummary import summary\n\n\nclass C3D(nn.Module):\n    def __init__(self, num_classes, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.block1 = nn.Sequential(\n            nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.ReLU(),\n            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n        )\n        self.block2 = nn.Sequential(\n            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.ReLU(),\n            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n        )\n        self.block3 = nn.Sequential(\n            nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.ReLU(),\n            nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.ReLU(),\n            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n        )\n        self.block4 = nn.Sequential(\n            nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.ReLU(),\n            nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.ReLU(),\n            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n        )\n        self.block5 = nn.Sequential(\n            nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.ReLU(),\n            nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n            nn.ReLU(),\n            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1))\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(8192, 4096),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(4096, num_classes)\n        )\n        self._init_weights()\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n        x = x.flatten(start_dim=1)\n        x = self.fc(x)\n        return x\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight, mode=&#x27;fan_out&#x27;, nonlinearity=&#x27;relu&#x27;)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n\n\n\n\nif __name__ == &#x27;__main__&#x27;:\n    model = C3D(num_classes=101)\n    print(summary(model, (3, 16, 112, 112), device=&#x27;cpu&#x27;))\n\n\n 预训练权重\n比如，有一个猫狗分类的模型，权重和偏置已经训练好了，但是现在有一个新任务，就是狼和老虎进行分类，我们就可以用猫狗分类的权重来继续训练狼和老虎分类，这样的话会加速我们的训练速度，非常方便。\n@staticmethod\n    def __load__pretrained_model():\n        p_dict = torch.load(&#x27;./ucf101-caffe.pth&#x27;)\n        s_dict = model.state_dict()\n        corresp_name = &#123;\n            &#x27;features.0.weight&#x27;: &#x27;block1.0.weight&#x27;,\n            &#x27;features.0.bias&#x27;: &#x27;block1.0.bias&#x27;,\n\n            &#x27;features.3.weight&#x27;: &#x27;block2.0.weight&#x27;,\n            &#x27;features.3.bias&#x27;: &#x27;block2.0.bias&#x27;,\n\n            &#x27;features.6.weight&#x27;: &#x27;block3.0.weight&#x27;,\n            &#x27;features.6.bias&#x27;: &#x27;block3.0.bias&#x27;,\n\n            &#x27;features.8.weight&#x27;: &#x27;block3.2.weight&#x27;,\n            &#x27;features.8.bias&#x27;: &#x27;block3.2.bias&#x27;,\n\n            &#x27;features.11.weight&#x27;: &#x27;block4.0.weight&#x27;,\n            &#x27;features.11.bias&#x27;: &#x27;block4.0.bias&#x27;,\n\n            &#x27;features.13.weight&#x27;: &#x27;block4.2.weight&#x27;,\n            &#x27;features.13.bias&#x27;: &#x27;block4.2.bias&#x27;,\n\n            &#x27;features.16.weight&#x27;: &#x27;block5.0.weight&#x27;,\n            &#x27;features.16.bias&#x27;: &#x27;block5.0.bias&#x27;,\n\n            &#x27;features.18.weight&#x27;: &#x27;block5.2.weight&#x27;,\n            &#x27;features.18.bias&#x27;: &#x27;block5.2.bias&#x27;,\n\n            &#x27;classifier.0.weight&#x27;: &#x27;fc.0.weight&#x27;,\n            &#x27;classifier.0.bias&#x27;: &#x27;fc.0.bias&#x27;,\n\n            &#x27;classifier.3.weight&#x27;: &#x27;fc.3.weight&#x27;,\n            &#x27;classifier.3.bias&#x27;: &#x27;fc.3.bias&#x27;\n        &#125;\n        for name in p_dict:\n            if name not in corresp_name:\n                continue\n            s_dict[corresp_name[name]] = p_dict[name]\n\n像这样，我们就可以做参数移植了\n","categories":["深度学习","项目与实战"],"tags":["python","深度学习","CNN","3D_CNN"]},{"title":"Harris兴趣点检测","url":"/python/Harris/","content":" Harris兴趣点检测\n 什么是“兴趣点”\n在 Harris 兴趣点检测中，“兴趣点”通常指图像中那些在水平和垂直两个方向上都具有显著灰度变化的像素点，也就是说在局部窗口在任意方向做小幅平移都会引起较大像素差异的点。这类点往往对应于图像中的角点或局部纹理显著的区域，因此被认为是具有判别性和稳定性的特征点。\n\n一大片平坦区域，所有方向都变化小，不是兴趣点\n高楼顶部的转角处、十字路口的交叉点，在所有方向都变化大这是兴趣点\n\n\n角点会在两个方向同时产生剧烈变化，边界只会在一个方向上产生剧烈变化，平坦区则都不会。\n 数学原理\n 从上帝视角理解\n如何检测本质在于计算窗口滑动前后，其像素的变化情况。\n\n假设一个窗口包含四个像素，那么这里会采集到这四个值：\nx1,x2,x3,x4x_1,x_2,x_3,x_4\nx1​,x2​,x3​,x4​\n再次假设，白色部分数值为0，蓝色部分数值为1，那么有：\nx1=0,x2=1,x3=0,x4=1x_1=0,x_2=1,x_3=0,x_4=1\nx1​=0,x2​=1,x3​=0,x4​=1\n水平方向移动：\n\n此时有：\nx1=0,x2=0,x3=0,x4=0x_1=0,x_2=0,x_3=0,x_4=0\nx1​=0,x2​=0,x3​=0,x4​=0\n数值产生剧烈变化。\n垂直方向移动：\n\n$$\nx_1=0,x_2=1,x_3=0,x_4=1\n$$\n数值不变。\n噢~这是边界！\n如果把这个放在蓝色矩形的左上角呢？可以很轻松的想象到，无论是在水平方向还是垂直方向，数值都会产生剧烈变化。噢~这是角点！\n如果把这个放在蓝色矩形的中间呢？可以更很轻松的想象到，无论是在水平方向还是垂直方向，数值都不会产生剧烈变化。噢~这是平坦区！\n我们一看就知道，噢，数值产生剧烈变化了，再思考一下我们就知道这是角点还是边界了，那有没有一种数学方法让计算机也知道这是角点还是边界亦或是平坦区呢？\n有的兄弟有的，这就是Harris兴趣点检测法。\n Harris检测方法\n对于图像I(x,y)I(x,y)I(x,y)，我们可以判断在点(x,y)(x,y)(x,y)处平移(Δx,Δy)(\\Delta x,\\Delta y)(Δx,Δy)后的自相似性。此处I(x,y)I(x,y)I(x,y)是一个灰度图像。\n有自相似性公式：\nc(x,y,Δx,Δy)=∑(u,v)∈W(x,y)W(u,v)(I(u,v)−I(u+Δx,v+Δy))2c(x,y,\\Delta x,\\Delta y)=\\sum_{(u,v)\\in W(x,y)}W(u,v)(I(u,v)-I(u+\\Delta x,v+\\Delta y))^2\nc(x,y,Δx,Δy)=(u,v)∈W(x,y)∑​W(u,v)(I(u,v)−I(u+Δx,v+Δy))2\n其中I(u,v)I(u,v)I(u,v)是原来的灰度值，I(u+Δx,v+Δy)I(u+\\Delta x,v+\\Delta y)I(u+Δx,v+Δy)是经过平移变换后的灰度值，做减法是为了看看平移操作后其与之前的差异是多大的，有时候我们的灰度变化是上升的，而有时候我们的灰度变化是下降的，我们只关心他变化了多少，因此这里做了平方操作，经过平方后不仅化负为正，还进一步强化了前后的差异。\n而W(u,v)W(u,v)W(u,v)是一个权重设计，在整个窗口中，我们没有必要平等的看待每一个像素（ps：当然也可以平等的看待，此时W(u,v)W(u,v)W(u,v)每一项均为同一常数），通常我们使用高斯加权函数，在图像上，最常用的是二维版本：\nw(x,y)=12πσ2exp⁡(−x2+y22σ2)w(x, y) = \\frac{1}{2\\pi \\sigma^2} \\exp\\left(-\\frac{x^2 + y^2}{2\\sigma^2}\\right)\nw(x,y)=2πσ21​exp(−2σ2x2+y2​)\n其中(x,y)(x, y)(x,y)：相对于窗口中心的坐标，σ\\sigmaσ控制模糊程度或者说是影响范围，二维权重的三维表示如图所示：\n\n像图中表示的一样，他将会更加重视窗口中心的元素，一个3×33\\times 33×3的高斯矩阵形如：\n[121242121]\\begin{bmatrix}\n 1 &amp; 2 &amp; 1\\\\\n 2 &amp; 4 &amp; 2\\\\\n 1 &amp; 2 &amp; 1\n\\end{bmatrix}\n⎣⎢⎡​121​242​121​⎦⎥⎤​\n对I(u+Δx,v+Δy)I(u+\\Delta x,v+\\Delta y)I(u+Δx,v+Δy)进行泰勒展开：\nI(u+Δx,v+Δy)=I(u,v)+Ix(u,v)Δx+Iy(u,v)Δy+O(Δx2,Δy2)I(u+\\Delta x,v+\\Delta y)=I(u,v)+I_x(u,v)\\Delta x+I_y(u,v)\\Delta y+O(\\Delta x^2,\\Delta y^2)\nI(u+Δx,v+Δy)=I(u,v)+Ix​(u,v)Δx+Iy​(u,v)Δy+O(Δx2,Δy2)\n如此展开到一阶就够用了，丢弃高阶无穷小后，原式近似等于：\nI(u+Δx,v+Δy)≈I(u,v)+Ix(u,v)Δx+Iy(u,v)ΔyI(u+\\Delta x,v+\\Delta y)\\approx I(u,v)+I_x(u,v)\\Delta x+I_y(u,v)\\Delta y\nI(u+Δx,v+Δy)≈I(u,v)+Ix​(u,v)Δx+Iy​(u,v)Δy\n其中IxI_xIx​表示其对xxx的偏导，其中IyI_yIy​表示其对yyy的偏导。\n对于原式I(u,v)−I(u+Δx,v+Δy)I(u,v)-I(u+\\Delta x,v+\\Delta y)I(u,v)−I(u+Δx,v+Δy)我们可以观察到，其包含了I(u,v)I(u,v)I(u,v)，因此可以直接被消掉，于是我们重写c(x,y,Δx,Δy)c(x,y,\\Delta x,\\Delta y)c(x,y,Δx,Δy)，近似得到：\nc(x,y,Δx,Δy)≈∑(u,v)∈W(x,y)W(u,v)(Ix(u,v)Δx+Iy(u,v)Δy)2c(x,y,\\Delta x,\\Delta y)\\approx \\sum_{(u,v)\\in W(x,y)}W(u,v)(I_x(u,v)\\Delta x+I_y(u,v)\\Delta y)^2\nc(x,y,Δx,Δy)≈(u,v)∈W(x,y)∑​W(u,v)(Ix​(u,v)Δx+Iy​(u,v)Δy)2\n简单起见，我们将∑(u,v)∈W(x,y)W(u,v)\\sum_{(u,v)\\in W(x,y)}W(u,v)∑(u,v)∈W(x,y)​W(u,v)，收缩至求和符号中记作：∑w\\sum_{w}∑w​\n进一步，我们可以把上式改写成矩阵的形式，设矩阵M(x,y)M(x,y)M(x,y)为：\nM(x,y)=∑w[Ix(x,y)2Ix(x,y)Iy(x,y)Ix(x,y)Iy(x,y)Iy(x,y)2]=[∑wIx(x,y)2∑wIx(x,y)Iy(x,y)∑wIx(x,y)Iy(x,y)∑wIy(x,y)2]M(x,y)=\\sum_{w}\\begin{bmatrix}\n I_x(x,y)^2 &amp; I_x(x,y)I_y(x,y)\\\\\n I_x(x,y)I_y(x,y) &amp; I_y(x,y)^2\n\\end{bmatrix}=\\begin{bmatrix}\n\\sum_{w} I_x(x,y)^2 &amp; \\sum_{w} I_x(x,y)I_y(x,y)\\\\\n\\sum_{w} I_x(x,y)I_y(x,y) &amp;\\sum_{w} I_y(x,y)^2\n\\end{bmatrix}\nM(x,y)=w∑​[Ix​(x,y)2Ix​(x,y)Iy​(x,y)​Ix​(x,y)Iy​(x,y)Iy​(x,y)2​]=[∑w​Ix​(x,y)2∑w​Ix​(x,y)Iy​(x,y)​∑w​Ix​(x,y)Iy​(x,y)∑w​Iy​(x,y)2​]\n观察到矩阵的副对角线完全一样，为进一步简化M(x,y)M(x,y)M(x,y)，设A=Ix(x,y)2A=I_x(x,y)^2A=Ix​(x,y)2、B=Ix(x,y)2B=I_x(x,y)^2B=Ix​(x,y)2、C=Ix(x,y)Iy(x,y)C=I_x(x,y)I_y(x,y)C=Ix​(x,y)Iy​(x,y)，于是得到：\nM(x,y)=[ACCB]M(x,y)=\\begin{bmatrix}\nA &amp; C\\\\\nC &amp; B\n\\end{bmatrix}\nM(x,y)=[AC​CB​]\n显然，M(x,y)M(x,y)M(x,y)是一个对称阵，因此得到：\nc(x,y,Δx,Δy)≈[Δx,Δy]M(x,y)[ΔxΔy]=[Δx,Δy][ACCB][ΔxΔy]c(x,y,\\Delta x,\\Delta y)\\approx \\begin{bmatrix}\n\\Delta x, \n\\Delta y \n\\end{bmatrix}M(x,y)\\begin{bmatrix}\n\\Delta x \\\\\n\\Delta y \n\\end{bmatrix}=\\begin{bmatrix}\n\\Delta x, \n\\Delta y \n\\end{bmatrix}\\begin{bmatrix}\nA &amp; C\\\\\nC &amp; B\n\\end{bmatrix}\\begin{bmatrix}\n\\Delta x \\\\\n\\Delta y \n\\end{bmatrix}\nc(x,y,Δx,Δy)≈[Δx,Δy​]M(x,y)[ΔxΔy​]=[Δx,Δy​][AC​CB​][ΔxΔy​]\n化简可得：\nc(x,y,Δx,Δy)≈AΔx2+2CΔxΔy+BΔy2c(x,y,\\Delta x,\\Delta y)\\approx A\\Delta x^2+2C\\Delta x\\Delta y+B\\Delta y^2\nc(x,y,Δx,Δy)≈AΔx2+2CΔxΔy+BΔy2\n其中A=∑wIx2A=\\sum_{w}I_x^2A=∑w​Ix2​，B=∑wIy2B=\\sum_{w}I_y^2B=∑w​Iy2​，C=∑wIxIyC=\\sum_{w}I_x I_yC=∑w​Ix​Iy​。\n这里的AΔx2+2CΔxΔy+BΔy2A\\Delta x^2+2C\\Delta x\\Delta y+B\\Delta y^2AΔx2+2CΔxΔy+BΔy2，非常像椭圆方程，回忆椭圆一般二次型：Ax2+Bxy+Cy2+Dx+Ey+F=0Ax^2 + Bxy + Cy^2 + Dx + Ey + F = 0Ax2+Bxy+Cy2+Dx+Ey+F=0，出现交叉项时，其变为非标准形，形如：\n\n我们需要把整个非标准椭圆做标准化变换。\n为什么AΔx2+2CΔxΔy+BΔy2A\\Delta x^2+2C\\Delta x\\Delta y+B\\Delta y^2AΔx2+2CΔxΔy+BΔy2并不标准，因为这里存在交叉项2CΔxΔy2C\\Delta x\\Delta y2CΔxΔy，如果这一项能被消掉那就变成标准的了，最直接的方法就是让CCC等于000，此处的CCC是由M(x,y)M(x,y)M(x,y)产生的，观察到MMM矩阵是一个实对称矩阵，那么我们就可以对MMM进行相似对角化，消掉CCC，变标准椭圆。\n\n下面假设已经完成了对矩阵MMM的形似对角化：\nP−1MP∼[λ1λ2]P^{-1}MP\\sim  \\begin{bmatrix}\n\\lambda _1&amp; \\\\\n &amp; \\lambda _2\n\\end{bmatrix}\nP−1MP∼[λ1​​λ2​​]\n其中，λ1\\lambda _1λ1​，λ2\\lambda _2λ2​，是矩阵MMM的特征值，因此可进一步写做：\nc(x,y,Δx,Δy)≈[Δx,Δy][ACCB][ΔxΔy]≈[Δx′,Δy′][λ1λ2][Δx′Δy′]=λ1(Δx′)2+λ2(Δy′)2c(x,y,\\Delta x,\\Delta y)\\approx\\begin{bmatrix}\n\\Delta x, \n\\Delta y \n\\end{bmatrix}\\begin{bmatrix}\nA &amp; C\\\\\nC &amp; B\n\\end{bmatrix}\\begin{bmatrix}\n\\Delta x \\\\\n\\Delta y \n\\end{bmatrix}\\approx\n\\begin{bmatrix}\n\\Delta x&#x27; , \n\\Delta y&#x27; \n\\end{bmatrix}\n\n\\begin{bmatrix}\n\\lambda _1&amp; \\\\\n &amp; \\lambda _2\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta x&#x27; \\\\\n\\Delta y&#x27;\n\\end{bmatrix}=\\lambda_1 (\\Delta x&#x27;)^2 + \\lambda_2 (\\Delta y&#x27;)^2\nc(x,y,Δx,Δy)≈[Δx,Δy​][AC​CB​][ΔxΔy​]≈[Δx′,Δy′​][λ1​​λ2​​][Δx′Δy′​]=λ1​(Δx′)2+λ2​(Δy′)2\n于是我们可以用λ1,λ2\\lambda_1,\\lambda_2λ1​,λ2​的大小关系区分平坦、边缘亦或是角点。相似对角化不是为了方便计算，而是为了方便理解 与分类，实际实现 Harris 的时候，基本不会显式去算特征值、特征向量，对角化是为了看清λ1,λ2\\lambda_1,\\lambda_2λ1​,λ2​的几何意义。\n像这样的λ1(Δx′)2+λ2(Δy′)2\\lambda_1 (\\Delta x&#x27;)^2 + \\lambda_2 (\\Delta y&#x27;)^2λ1​(Δx′)2+λ2​(Δy′)2，我们可以把它改变一下，让其与标准式对齐，得到：\nΔx′2(1λ1)2+Δy′2(1λ2)2\\frac{\\Delta x&#x27;^2}{(\\sqrt{\\frac{1}{\\lambda _1} } )^2} +\\frac{\\Delta y&#x27;^2}{(\\sqrt{\\frac{1}{\\lambda _2} } )^2}\n(λ1​1​​)2Δx′2​+(λ2​1​​)2Δy′2​\n太复杂了，简单点写做：\nΔx′2(λ1−12)2+Δy′2(λ2−12)2\\frac{\\Delta x&#x27;^2}{(\\lambda_1^{-\\frac{1}{2} })^2} +\\frac{\\Delta y&#x27;^2}{(\\lambda_2^{-\\frac{1}{2} })^2}\n(λ1−21​​)2Δx′2​+(λ2−21​​)2Δy′2​\n\n现在有三种情况：\n\n平坦区域：λ1≈0, λ2≈0\\lambda_1 \\approx 0,\\ \\lambda_2 \\approx 0λ1​≈0, λ2​≈0（两个都很小）\n边缘区域：λ1≫0, λ2≈0\\lambda_1 \\gg 0,\\ \\lambda_2 \\approx 0λ1​≫0, λ2​≈0或者λ2≫0, λ1≈0\\lambda_2 \\gg 0,\\ \\lambda_1 \\approx 0λ2​≫0, λ1​≈0（一大一小）\n角点：λ1≫0, λ2≫0\\lambda_1 \\gg 0,\\ \\lambda_2 \\gg 0λ1​≫0, λ2​≫0（两个都很大）\n\n然后，使用det⁡(M)\\det(M)det(M)和trace(M)\\text{trace}(M)trace(M) 构造角点响应RRR值：\nR=det⁡(M)−k⋅trace2(M)R = \\det(M) - k \\cdot \\text{trace}^2(M)\nR=det(M)−k⋅trace2(M)\n其中det⁡(M)\\det(M)det(M)是矩阵MMM的行列式，trace(M)\\text{trace}(M)trace(M)是矩阵MMM的迹，det⁡(M)=λ1λ2\\det(M) = \\lambda_1\\lambda_2det(M)=λ1​λ2​，trace(M)=λ1+λ2\\text{trace}(M) = \\lambda_1 + \\lambda_2trace(M)=λ1​+λ2​，这就是Harris设计出来的一个角点评分公式。\n进一步重述上述三种情况：\n\n平坦区域：λ1≈0, λ2≈0\\lambda_1 \\approx 0,\\ \\lambda_2 \\approx 0λ1​≈0, λ2​≈0（两个都很小），det⁡=λ1λ2≈0\\det = \\lambda_1\\lambda_2 \\approx 0det=λ1​λ2​≈0，trace2≈0⇒(R≈0)\\text{trace}^2 \\approx 0 ⇒ (R \\approx 0)trace2≈0⇒(R≈0)\n边缘区域：λ1≫0, λ2≈0\\lambda_1 \\gg 0,\\ \\lambda_2 \\approx 0λ1​≫0, λ2​≈0或者λ2≫0, λ1≈0\\lambda_2 \\gg 0,\\ \\lambda_1 \\approx 0λ2​≫0, λ1​≈0（一大一小），det⁡=λ1λ2\\det = \\lambda_1\\lambda_2det=λ1​λ2​不算大，trace2=(λ1+λ2)2≈λ12\\text{trace}^2 = (\\lambda_1+\\lambda_2)^2 \\approx \\lambda_1^2trace2=(λ1​+λ2​)2≈λ12​很大，$⇒ R $往往是负的（这里Harris 故意用 $-k \\cdot \\text{trace}^2 $把边缘压下去）\n角点：λ1≫0, λ2≫0\\lambda_1 \\gg 0,\\ \\lambda_2 \\gg 0λ1​≫0, λ2​≫0（两个都很大），det⁡=λ1λ2\\det = \\lambda_1\\lambda_2det=λ1​λ2​很大，即便减去了k(λ1+λ2)2k(\\lambda_1+\\lambda_2)^2k(λ1​+λ2​)2，但整体还是会是个比较大的正数\n\n\n综上所述，RRR有如下特性：\n\nR≈0R \\approx 0R≈0 ⇒ 平坦区\nR&lt;0R &lt; 0R&lt;0 ⇒ 更像边缘\nR≫0R \\gg 0R≫0 ⇒ 很像角点（兴趣点）\n\ndet⁡(M)\\det(M)det(M)大，就说明两个方向上变化都大，trace(M)2\\text{trace}(M)^2trace(M)2大，可能只是有一个方向特别大，比如边缘，R=det⁡−k⋅trace2R = \\det - k \\cdot \\text{trace}^2R=det−k⋅trace2，用 det⁡\\detdet 奖励两个方向都大的情况，用 −ktrace2-k\\text{trace}^2−ktrace2 惩罚只有一个方向大、另一个方向小的情况。\n 在OpenCV中的使用\nimport cv2\nimg = cv2.imread(&#x27;1.jpg&#x27;)\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\ndst = cv2.cornerHarris(gray, 2, 3, 0.04)\nimg[dst &gt; 0.1 * dst.max()]=[0, 0, 255]\ncv2.imshow(&#x27;dst&#x27;, img)\ncv2.imwrite(&#x27;1_corner.png&#x27;, img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n\n\nblockSize=2是窗口大小\nksize =3是Sobel 求导算子核大小，Harris需要先算图像在x，y 方向的梯度 Ix,IyI_x, I_yIx​,Iy​，OpenCV用的是 Sobel 算子。\nk = 0.04是Harris 公式里的经验参数k，R=det⁡(M)−k⋅trace2(M)R = \\det(M) - k \\cdot \\text{trace}^2(M)R=det(M)−k⋅trace2(M)\n\n 对比\n 简单场景\n\n\n 复杂场景\n\n 参考\n[1]  Harris C G, Stephens M. A combined corner and edge detector[C] Alvey vision conference. 1988, 15(50): 10-5244.\n[2]  Harris角点检测与SIFT特征匹配全解析 https://www.bilibili.com/video/BV1Zi1eYLEiR\n[3]  Harris Corner Detection https://docs.opencv.org/4.x/dc/d0d/tutorial_py_features_harris.html\n[4]  Harris corner detector https://en.wikipedia.org/wiki/Harris_corner_detector?utm_source=chatgpt.com\n","categories":["深度学习","深度学习基础"],"tags":["python","Harris"]},{"title":"从公式角度看深度学习","url":"/python/DLFromFormula/","content":" 从公式角度看深度学习\n学习基于尚硅谷：https://www.bilibili.com/video/BV1MRJmzSEaa\n 损失函数\n 均方误差MSE / L2 Loss（回归问题较适用）\nMean Squared Error（MSE）\nL=1n∑i=1n(yi−ti)2\\begin {array}{c}\nL=\\frac{1}{n} \\sum_{i=1}^{n}(y_{i}-t_{i})^{2}  \n\\end{array}\nL=n1​∑i=1n​(yi​−ti​)2​\n这个ti是目标标签(向量)，yi是预测标签(向量)，n是总个数。\n往往我们在是实际使用的是时候，使用下面这个公式。\nL=12n∑i=1n(yi−ti)2\\begin {array}{c}\nL=\\frac{1}{2n} \\sum_{i=1}^{n}(y_{i}-t_{i})^{2}\n\\end{array}\nL=2n1​∑i=1n​(yi​−ti​)2​\nL2 Loss对异常值敏感，遇到异常值时易发生梯度爆炸。\ndef mean_squared_error(y, t):\n    return 0.5 * np.sum((y-t)**2)\n\n 交叉熵误差CEE（分类问题较适用）\nCross Entropy Error（CEE）\nL=−1n∑i=1ntilog⁡(yi)\\begin {array}{c}\nL=-\\frac{1}{n} \\sum_{i=1}^{n} t_{i} \\log (y_{i})\n\\end{array}\nL=−n1​∑i=1n​ti​log(yi​)​\nyi表示神经网络的输出，ti表示正确解标签；而且，ti中只有正确解标签对应的值为1，其它均为0（one-hot表示）。\ndef cross_entropy_error(y, t):\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n        \n    # 监督数据是one-hot向量的情况下，转换为正确解标签的索引\n    if t.size == y.size:\n        t = t.argmax(axis=1)\n             \n    batch_size = y.shape[0]\n    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n\n 二元交叉熵误差BCEE\nBinary Cross-Entropy Loss\nL=−1n∑i=1ntilog⁡(yi)+(1−ti)log⁡(1−yi)\\begin {array}{c}\nL=-\\frac{1}{n} \\sum_{i=1}^{n} t_{i} \\log (y_{i})+(1-t_{i}) \\log (1-y_{i})\n\\end{array}\nL=−n1​∑i=1n​ti​log(yi​)+(1−ti​)log(1−yi​)​\n 多类交叉熵误差CCEE\nCategorical Cross-Entropy Loss\nL=−1n∑i=1n∑j=1Ctijlog⁡(yij)\\begin {array}{c}\nL=-\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{C} t_{i j} \\log (y_{i j})\n\\end{array}\nL=−n1​∑i=1n​∑j=1C​tij​log(yij​)​\n 平均绝对误差MAE / L1 Loss\nMean Absolute Erro\nL=1n∑i=1n∣yi−ti∣\\begin {array}{c}\nL=\\frac{1}{n} \\sum_{i=1}^{n}|y_{i}-t_{i}|\n\\end{array}\nL=n1​∑i=1n​∣yi​−ti​∣​\nL1 Loss对异常值鲁棒，但在0点处不可导。\n 平滑L1误差/ Smooth L1\nSmooth L1\nSmooth L1={12(yi−y^i)2,if ∣yi−y^i∣&lt;1∣yi−y^i∣−12,if ∣yi−y^i∣≥1\\begin {array}{c}\n\\text{Smooth } L1 =\n\\begin{cases}\n\\frac{1}{2}(y_i - \\hat{y}_i)^2, &amp; \\text{if } |y_i - \\hat{y}_i| &lt; 1 \\\\\n|y_i - \\hat{y}_i| - \\frac{1}{2}, &amp; \\text{if } |y_i - \\hat{y}_i| \\geq 1\n\\end{cases}\n\\end{array}\nSmooth L1={21​(yi​−y^​i​)2,∣yi​−y^​i​∣−21​,​if ∣yi​−y^​i​∣&lt;1if ∣yi​−y^​i​∣≥1​​\n 梯度\n梯度\n∇L=∂L∂w1∂w1∂x+∂L∂w2∂w2∂x+⋯+∂L∂wn∂wn∂x\\begin {array}{c}\n\\nabla L=\\frac{\\partial L}{\\partial w_{1}} \\frac{\\partial w_{1}}{\\partial x}+\\frac{\\partial L}{\\partial w_{2}} \\frac{\\partial w_{2}}{\\partial x}+\\cdots+\\frac{\\partial L}{\\partial w_{n}} \\frac{\\partial w_{n}}{\\partial x}\n\\end{array}\n∇L=∂w1​∂L​∂x∂w1​​+∂w2​∂L​∂x∂w2​​+⋯+∂wn​∂L​∂x∂wn​​​\n梯度是关于参数的导数，即对于参数w1，w2，w3，w4，w5…wn，求关于x的导数，这些偏导数组成的向量就是梯度，如下面这个形式。\n∇L=[∂L∂w1,∂L∂w2,∂L∂w3,∂L∂w4,∂L∂w5,⋯ ,∂L∂wn]\\begin {array}{c}\n\\nabla L=\\left[\\frac{\\partial L}{\\partial w_{1}}, \\frac{\\partial L}{\\partial w_{2}}, \\frac{\\partial L}{\\partial w_{3}}, \\frac{\\partial L}{\\partial w_{4}}, \\frac{\\partial L}{\\partial w_{5}}, \\cdots, \\frac{\\partial L}{\\partial w_{n}}\\right]\n\\end{array}\n∇L=[∂w1​∂L​,∂w2​∂L​,∂w3​∂L​,∂w4​∂L​,∂w5​∂L​,⋯,∂wn​∂L​]​\n在极小值处、极大值处和鞍点处，梯度向量都为0。\n梯度代表的其实是函数值增大最快的方向；在实际应用中，我们需要寻找损失函数的最小值，所以一般选择负梯度\n向量。同样地，负梯度代表的是函数值减小最快的方向，并不一定直接指向函数图像的最低点。\n 优化函数\n 随机梯度下降 SGD\nstochastic Gradient Descent\nwt+1=wt−η∇L(wt)\\begin {array}{c}\nw_{t+1}=w_{t}-\\eta \\nabla L(w_{t})\n\\end{array}\nwt+1​=wt​−η∇L(wt​)​\nη\\etaη是学习率，一般取0.01。\nSGD有以下问题：\n\n局部最优解：陷入局部最优，尤其在非凸函数中，难以找到全局最优解。\n鞍点：陷入鞍点，梯度为0，导致训练停滞。\n收敛速度慢：高维或非凸函数中，收敛速度较慢。\n学习率选择：学习率过大导致震荡或不收敛，过小则收敛速度慢。\n\n 自适应梯度AdaGrad\nAdaGrad\nwt+1=wt−η1∑i=1n(wt,i)2+ϵ∇L(wt)\\begin {array}{c}\nw_{t+1}=w_{t}-\\eta \\frac{1}{\\sqrt{\\sum_{i=1}^{n}\\left(w_{t, i}\\right)^{2}+\\epsilon}} \\nabla L(w_{t})\n\\end{array}\nwt+1​=wt​−η∑i=1n​(wt,i​)2+ϵ​1​∇L(wt​)​\nAdaGrad的优点是：\n\n快速收敛：在非凸函数中，AdaGrad能够快速收敛到全局最优解。\n梯度加速：在凸函数中，AdaGrad能够加速梯度下降，使得训练速度更快。\n\n 动量法Momentum\nMomentum\nwt+1=wt+ηmt\\begin {array}{c}\nw_{t+1}=w_{t}+\\eta m_{t}\n\\end{array}\nwt+1​=wt​+ηmt​​\nmt=βmt−1+(1−β)∇L(wt)\\begin {array}{c}\nm_{t}=\\beta m_{t-1}+\\left(1-\\beta\\right) \\nabla L(w_{t})\n\\end{array}\nmt​=βmt−1​+(1−β)∇L(wt​)​\nβ\\betaβ是动量的超参数，一般取0.9。\n动量的作用是加速梯度下降，会保存历史梯度并给予一定的权重，使其也参与到参数更新中，使得梯度下降速度更快。\n其中，mt-1是历史梯度，mt是当前梯度。\n在当前梯度中，也包含了历史梯度，因此，mt-1中的历史梯度也会被加入到当前梯度中，从而实现梯度加速。\n Adam\nAdam\nmt=β1mt−1+(1−β1)∇L(wt)\\begin {array}{c}\nm_{t}=\\beta_{1} m_{t-1}+(1-\\beta_{1}) \\nabla L(w_{t})\n\\end{array}\nmt​=β1​mt−1​+(1−β1​)∇L(wt​)​\nvt=β2vt−1+(1−β2)∇L(wt)2\\begin {array}{c}\nv_{t}=\\beta_{2} v_{t-1}+(1-\\beta_{2}) \\nabla L(w_{t})^{2}\n\\end{array}\nvt​=β2​vt−1​+(1−β2​)∇L(wt​)2​\nwt+1=wt−ηv^t+ϵm^t\\begin {array}{c}\nw_{t+1}=w_{t}-\\frac{\\eta}{\\sqrt{\\hat{v}_{t}}+\\epsilon} \\hat{m}_{t}\n\\end{array}\nwt+1​=wt​−v^t​​+ϵη​m^t​​\nβ1\\beta_{1}β1​和β2\\beta_{2}β2​是Adam算法中的两个超参数，一般取0.9和0.999。\nϵ\\epsilonϵ是Adam算法中的超参数，一般取1e-8。\nm^t\\hat{m}_{t}m^t​和v^t\\hat{v}_{t}v^t​是Adam算法中的两个中间变量，分别表示mtm_{t}mt​和vtv_{t}vt​的估计值。\nm^t\\hat{m}_{t}m^t​和v^t\\hat{v}_{t}v^t​的计算公式如下：\nm^t=mt1−β1t\\begin {array}{c}\n\\hat{m}_{t}=\\frac{m_{t}}{1-\\beta_{1}^{t}}\n\\end{array}\nm^t​=1−β1t​mt​​​\nv^t=vt1−β2t\\begin {array}{c}\n\\hat{v}_{t}=\\frac{v_{t}}{1-\\beta_{2}^{t}}\n\\end{array}\nv^t​=1−β2t​vt​​​\n AdamW\nAdamW\nwt+1=wt−ηv^t+ϵm^t+λwt\\begin {array}{c}\nw_{t+1}=w_{t}-\\frac{\\eta}{\\sqrt{\\hat{v}_{t}}+\\epsilon} \\hat{m}_{t}+\\lambda w_{t}\n\\end{array}\nwt+1​=wt​−v^t​​+ϵη​m^t​+λwt​​\nλ\\lambdaλ是AdamW算法中的超参数，一般取0.01。\nm^t\\hat{m}_{t}m^t​和v^t\\hat{v}_{t}v^t​的计算公式如下：\nm^t=mt1−β1t\\begin {array}{c}\n\\hat{m}_{t}=\\frac{m_{t}}{1-\\beta_{1}^{t}}\n\\end{array}\nm^t​=1−β1t​mt​​​\nv^t=vt1−β2t\\begin {array}{c}\n\\hat{v}_{t}=\\frac{v_{t}}{1-\\beta_{2}^{t}}\n\\end{array}\nv^t​=1−β2t​vt​​​\n 学习率衰减机制\n 等间隔衰减\nηt=η0(1−tT)p\\begin {array}{c}\n\\eta_{t}=\\eta_{0}\\left(1-\\frac{t}{T}\\right)^{p}\n\\end{array}\nηt​=η0​(1−Tt​)p​\nη0\\eta_{0}η0​是初始学习率，TTT是总迭代次数，ppp是衰减因子，一般取0.5。\n等间隔衰减的缺点是，学习率衰减过快，导致训练效果变差。\n 指定间隔衰减\n在指定的epoch，让学习率按照一定的系数衰减，直到指定epoch结束，再按照初始学习率继续训练。\n 指数衰减\nηt=η0×(0.99)t/T\\begin {array}{c}\n\\eta_{t}=\\eta_{0} \\times \\left(0.99\\right)^{t / T}\n\\end{array}\nηt​=η0​×(0.99)t/T​\nη0\\eta_{0}η0​是初始学习率，TTT是总迭代次数。\n指数衰减的缺点是，学习率衰减过慢，导致训练效果变差。\n 余弦退火\nηt=η0×(0.5(1+cos⁡(πtT)))\\begin {array}{c}\n\\eta_{t}=\\eta_{0} \\times \\left(0.5\\left(1+\\cos\\left(\\frac{\\pi t}{T}\\right)\\right)\\right)\n\\end{array}\nηt​=η0​×(0.5(1+cos(Tπt​)))​\nη0\\eta_{0}η0​是初始学习率，TTT是总迭代次数。\n余弦退火算法的优点是，学习率衰减速度适中，训练效果较好。\n 初始化\n He初始化（Kaiming初始化）\nHe初始化的优点是，初始化后的权重分布更接近于标准正态分布，从而加速训练。\nimport torch.nn as nn\n\nlinear = nn.Linear(5, 2)\n\n# Kaiming正态分布初始化\nnn.init.kaiming_normal_(linear.weight)\nprint(linear.weight)\n\n# Kaiming均匀分布初始化\nnn.init.kaiming_uniform_(linear.weight)\nprint(linear.weight)\n\n 正则化\n机器学习的问题中过拟合是一个很常见的问题。\n过拟合指的是能较好拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据。机器学习的目标是提高泛化能力，希望即便是不包含在训练数据里的未观测数据，模型也可以进行正确的预测。因此可以通过\n正则化方法来抑制过拟合。\n常用的正则化方法有Batch Normalization、权值衰减、Dropout、早停法等。\n Batch Normalization批量标准化\nBatch Normalization\ny=x−μσ2+ϵ\\begin {array}{c}\ny=\\frac{x-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}\n\\end{array}\ny=σ2+ϵ​x−μ​​\n其中，μ\\muμ是均值，σ2\\sigma^{2}σ2是方差，ϵ\\epsilonϵ是一个很小的数，一般取0.001。\n","categories":["深度学习","深度学习基础"],"tags":["python","深度学习","CNN","RNN","LSTM"]},{"title":"CNN经典卷积神经网络与实战","url":"/python/CNN_Acc/","content":" 卷积神经网络\n学习内容基于：Pytorch框架与经典卷积神经网络与实战\n CNN卷积神经网络算法原理\n 全连接神经网络\n\n输入层是我们输入的数据，这里看到的第一列节点并不是输入层，中间为隐藏层。\n输入层就像X（自变量），模型或者说这些网络就是F（函数），我们得到的输出就是Y（因变量）。\n 为什么要使用激活函数\n在神经网络中使用激活函数的根本原因是引入非线性，从而使模型能够拟合和表达复杂的函数关系。如果没有激活函数，神经网络无论堆叠多少层，本质上都是一个线性模型，能力极其有限。多层线性变换的叠加依然是线性变换，最终的模型只能拟合直线/平面，完全无法处理复杂的数据模式\n Sigmoid激活函数\n\n优点：简单、非常适用分类任务。\n缺点：反向传播训练时有梯度消失的问题；输出值区间为(0,1)，关于0不对称；梯度更新在不同方向走得太远，使得优化难度增大，训练耗时。\n Tanh激活函数\n\n优点：解决了Sigmoid函数输出值非0对称的问题，训练比Sigmoid函数快，更容易收敛\n缺点：反向传播训练时有梯度消失的问题，Tanh函数和Sigmoid函数非常相似。\n ReLU激活函数\n\n优点：解决了梯度消失的问题；计算更为简单，没有Sigmoid函数和Tanh函数的指数运算\n缺点：训练时可能出现神经元死亡\n Leaky ReLU激活函数\n\n优点：解决了ReLU的神经元死亡问题\n缺点：无法为正负输入值提供一致的关系预测(不同区间函数不同)\n 前向传播\n前向传播是神经网络中数据从输入层依次流向输出层的过程，它的核心目标是根据当前的模型参数（权重和偏置）计算出预测结果。\n前向传播就是“把输入数据依次喂给每一层，经过线性计算 + 激活函数，逐层输出，最后得到预测值”的过程。\n 损失函数\n 均方误差\n\n\n\n\n\n\n\n\n前面有可能有出现1/2，那只是为了方便求导，都是均方误差。\n 梯度下降法\n\n\n 全连接神经网络在图片中存在的问题\n 1. 参数量巨大\n\n全连接层的每一个神经元都与上一层的所有神经元相连。\n对于图片来说，输入通常是高维的，例如一张 224×224 的 RGB 图片就是 224×224×3 = 150,528 个输入特征。\n假设第一层有 1000 个神经元，那么权重数量就是 150,528 × 1000 ≈ 1.5 亿个参数！\n问题：参数太多 → 容易过拟合 → 训练时间长 → 需要大量显存。\n\n\n 2. 忽略空间结构\n\n图片是二维或三维（RGB）的网格数据，有局部空间相关性（邻近像素往往相关）。\n全连接层把图片“拉平”成一维向量，然后再进行矩阵乘法。\n问题：丢失了图片的空间信息（如边缘、纹理、形状），无法有效捕捉局部特征。\n\n\n 3. 缺乏平移不变性\n\n图像中物体的位置可能变化。\n全连接网络对输入的每个位置都固定，物体稍微移动，输出可能完全不同。\n问题：无法自动识别图像中的平移或局部位移，泛化能力差。\n\n\n 4. 计算效率低\n\n全连接层计算复杂度高（矩阵乘法量大）。\n对高分辨率图像，训练和推理速度都很慢。\n对比卷积神经网络（CNN），后者通过卷积核共享权重大幅减少计算量。\n\n\n 5. 不适合捕捉层次特征\n\n图片的特征是有层次结构的：边缘 → 纹理 → 形状 → 对象。\n全连接层一次性处理所有像素，无法自然学习层次特征。\nCNN 则通过卷积和池化层逐步抽象特征，更符合视觉认知规律。\n\n 卷积、步幅、填充、池化\n这部分请看：\n\n          \n          PyTorch深度学习convolution 卷积操作\n          https://735690757.github.io/python/pytorch\n          \n 经过卷积后特征图大小\n\nFH：卷积核（filter）的高度（Filter Height）\nFW：卷积核的宽度（Filter Width）\nC_in：输入通道数（Input Channels）\nC_out：输出通道数（Output Channels）\nS：步幅（Stride）\nP：填充（Padding）\n如果算出来是小数，一般是向下取整。\n LeNet与AlexNet原理\n LeNet-5诞生背景\n简单来说，LeNet-5的诞生背景是为了解决手写数字识别这一实际应用问题，它是世界上首个成功商用的卷积神经网络，奠定了现代深度学习的基础。\n LeNet-5网络结构\n\n\n输入1 * 28 * 28\n5 * 5 卷积（6），填充2\n2 * 2 平均池化层，步幅2\n5 * 5 卷积（16），填充0\n2 * 2 平均池化层，步幅2\n全连接（120）\n全连接（84）\n全连接（10）\n\n\n再强调一下：\n\n这个计算公式非常重要。\n\n AlexNet诞生背景\n在算力达到临界点、大数据已经就位的环境下，一个古老但曾被忽视的算法（深度学习/卷积神经网络）迎来了证明自己的最佳时机。AlexNet不仅仅是一个优秀的模型，它更是一个时代的开创者，是人工智能发展史上的一个关键转折点。\n AlexNet网络结构\n\n5层卷积，3层全连接，共8层，激活函数使用ReLU。\n\n输入3 * 227 * 227（？这个可能有问题）\n11 * 11 卷积（96），步幅4，ReLU\n3 * 3 最大池化，步幅2\n5 * 5 卷积（256），填充2，ReLU\n3 * 3最大池化，步幅2\n3 * 3 卷积（384），填充1，ReLU\n3 * 3 卷积（384），填充1，ReLU\n3 * 3 卷积（256），填充1，ReLU\n3 * 3最大池化，步幅2\n全连接（4096）\n全连接（4096）\n全连接（10）\n\n这里在全连接层有很多的参数，参数太多容易过拟合，我们引入了Dropout操作。\n 图像增强 - 水平翻转\n\n 图像增强 - 随机裁剪\n\n 图像增强 - PCA\n\n\n\n\n方面\n描述\n\n\n\n\n核心思想\n利用PCA找到图像颜色的主要变化方向，并沿这些方向添加随机扰动来模拟光照变化。\n\n\n目的\n增强模型对颜色和光照变化的鲁棒性，是一种高效的数据增强手段。\n\n\n优点\n变化方式基于图像自身的统计特性，生成的图像颜色变化自然、合理。\n\n\n缺点\n计算成本相对较高（需要对每张图或每批图做PCA）。\n\n\n遗产\n是AlexNet的一个创新点，启发了对颜色增强的重视，但已被更简单高效的方法所取代。\n\n\n\n LRN正则化\n这是一个针对通道间的计算\n\n尽管LRN是AlexNet的一个关键创新，但在后续更深、更先进的网络（如VGG、ResNet）中，它几乎被完全弃用了。主要原因如下：\n\n效果有限且不稳定：后续研究发现，LRN带来的性能提升非常微弱，甚至有时不稳定。其正则化效果远不如Dropout和Batch Normalization（BN） 那样显著和可靠。\n被更好的技术取代：\n\nDropout：通过随机断开神经元连接来防止过拟合，更为直接有效。\nBatch Normalization（批归一化）：这是革命性的技术。BN对整个Batch的每个通道进行归一化（均值为0，方差为1），极大地改善了梯度流动，加速了训练，同时本身也具有轻微的正则化效果。BN的效果远超LRN，并且已经成为现代深度网络的标准组件。\n\n\n增加计算开销和超参数：LRN引入了额外的计算量，并且 k, α, β, n 这些超参数需要调优，增加了模型设计的复杂性。\n\n 重叠池化\n与LRN类似，重叠池化在现代深度学习架构中也已经不常用了。\n\n计算成本更高：由于存在重叠，为了得到相同尺寸的输出特征图，重叠池化需要进行更多次池化操作。例如，将 5x5 降采样到 2x2，非重叠池化 (2x2, stride=2) 需要4次操作，而重叠池化 (3x3, stride=2) 也需要4次操作，但每次操作的窗口更大，计算量稍高。\n被更有效的技术取代：如今，防止过拟合和提升性能的重任更多地由 Batch Normalization、更深的网络结构（如ResNet的残差连接）、更先进的优化器 和 Dropout 等方法来承担。\n设计趋势变化：现代网络有时甚至会完全摒弃池化层，转而使用步长大于1的卷积（Strided Convolution） 来同时实现特征提取和降采样，这被认为能提供更大的模型容量和灵活性。\n\n LeNet实战\n 模型\nclass LeNet(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2),\n            nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0),\n            nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Flatten(),\n            nn.Linear(in_features=400, out_features=120),\n            nn.Sigmoid(),\n            nn.Linear(in_features=120, out_features=84),\n            nn.Sigmoid(),\n            nn.Linear(in_features=84, out_features=10)\n        )\n\n    def forward(self, x):\n        return self.block(x)\n\n 设备、实例化与summary\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\nnet = LeNet().to(device)\nsummary(net, (1, 28, 28))\n\n\n这里的 -1 代表的是 批量大小，更具体地说：-1 是一个占位符，表示这个维度的大小是由其他维度推断出来的，而不是一个固定值。\n批量大小：在深度学习训练中，数据通常是按批次（batch）输入的。比如，你可能会一次输入 32 张图片、64 张图片等。这个数量就是批量大小。\n为什么是 -1？：PyTorch 模型在设计时，其核心计算逻辑不依赖于具体的批量大小。为了增加灵活性，在定义模型的前向传播时，我们通常将输入张量的第一个维度设为批量大小。当打印模型摘要时，库（如 torchsummary）无法预先知道你会用多大的批量大小来训练，所以它使用 -1 来代表“任何尺寸”。\n动态推断：在实际运行中，这个 -1 会被你输入数据的真实批量大小所替代。\n例如，如果你用一批 32 张图片输入到模型，那么 [-1, 1, 32, 32] 就会变成 [32, 1, 32, 32]。\n如果你用 128 张图片，它就会变成 [128, 1, 32, 32]。\n 加载 FashionMNIST\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision.transforms import transforms\nimport numpy as np\n\ntransform = transforms.Compose([\n    transforms.Resize((28, 28)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\ntrain_data = FashionMNIST(root=&quot;./data/FashionMNIST&quot;, train=True, download=True, transform=transform)\ntest_data = FashionMNIST(root=&quot;./data/FashionMNIST&quot;, train=False, download=True, transform=transform)\n\nfrom torch.utils.data import DataLoader\n\ntrain_dataLoader = DataLoader(train_data, batch_size=64, shuffle=True)\ntest_dataLoader = DataLoader(test_data, batch_size=64, shuffle=False)\n\n 展示\nimport matplotlib.pyplot as plt\n\nfor setp, (features, label) in enumerate(train_dataLoader):\n    if setp == 0:\n        x = features.squeeze().numpy()\n        y = label.numpy()\n        break\nplt.figure(figsize=(12, 5))\nfor ii in np.arange(len(y)):\n    plt.subplot(4, 16, ii+1)\n    plt.imshow(x[ii,:,:], cmap=plt.cm.gray)\n    plt.title(y[ii])\n    plt.axis(&quot;off&quot;)\nplt.show()\n\n\n 训练、验证（此处为最佳实践）\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom tqdm import tqdm\nimport copy\nimport time\nimport pandas as pd\n\n\ndef train_model_process(model, train_dataloader, val_dataloader, num_epochs):\n    device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    scheduler = CosineAnnealingLR(\n        optimizer,\n        T_max=num_epochs,\n        eta_min=1e-6\n    )\n    criterion = nn.CrossEntropyLoss().to(device)\n    model = model.to(device)\n    best_model_wts = copy.deepcopy(model.state_dict())\n\n    best_acc = 0.0\n    train_loss_all = []\n    val_loss_all = []\n    train_acc_all = []\n    val_acc_all = []\n    learning_rates = []\n    since = time.time()\n\n    for epoch in range(num_epochs):\n        print(&quot;Epoch &#123;&#125;/&#123;&#125;&quot;.format(epoch, num_epochs - 1))\n        print(&quot;-&quot; * 10)\n\n        current_lr = optimizer.param_groups[0][&#x27;lr&#x27;]\n        learning_rates.append(current_lr)\n        print(f&quot;当前学习率: &#123;current_lr:.6f&#125;&quot;)\n\n        train_loss = 0.0\n        train_corrects = 0\n        val_loss = 0.0\n        val_corrects = 0\n        train_num = 0\n        val_num = 0\n\n        for step, (b_x, b_y) in tqdm(enumerate(train_dataloader), desc=f&quot;总步骤：&#123;len(train_dataloader)&#125;&quot;,\n                                     leave=False, unit=&quot;step&quot;, total=len(train_dataloader),\n                                     bar_format=&quot;&#123;desc&#125;: |&#123;bar:30&#125;| &#123;percentage:3.0f&#125;% 唱跳Rap🏀，Music~&quot;,\n                                     ascii=&quot;🏀🥰🥰😘&quot;):\n            b_x = b_x.to(device)\n            b_y = b_y.to(device)\n            model.train()\n            output = model(b_x)\n            pre_lab = torch.argmax(output, dim=1)\n            loss = criterion(output, b_y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * b_x.size(0)\n            # 如果预测正确，则准确度train_corrects加1\n            train_corrects += torch.sum(pre_lab == b_y.data)\n            train_num += b_x.size(0)\n        for step, (b_x, b_y) in enumerate(val_dataloader):\n            b_x = b_x.to(device)\n            b_y = b_y.to(device)\n            model.eval()\n            output = model(b_x)\n            pre_lab = torch.argmax(output, dim=1)\n            loss = criterion(output, b_y)\n\n            val_loss += loss.item() * b_x.size(0)\n            val_corrects += torch.sum(pre_lab == b_y.data)\n            val_num += b_x.size(0)\n\n        train_loss_all.append(train_loss / train_num)\n        train_acc_all.append(train_corrects.double().item() / train_num)\n\n        val_loss_all.append(val_loss / val_num)\n        val_acc_all.append(val_corrects.double().item() / val_num)\n\n        print(&quot;&#123;&#125; train loss:&#123;:.4f&#125; train acc: &#123;:.4f&#125;&quot;.format(epoch, train_loss_all[-1], train_acc_all[-1]))\n        print(&quot;&#123;&#125; val loss:&#123;:.4f&#125; val acc: &#123;:.4f&#125;&quot;.format(epoch, val_loss_all[-1], val_acc_all[-1]))\n\n        if val_acc_all[-1] &gt; best_acc:\n            best_acc = val_acc_all[-1]\n            best_model_wts = copy.deepcopy(model.state_dict())\n\n        time_use = time.time() - since\n        print(&quot;训练和验证耗费的时间&#123;:.0f&#125;m&#123;:.0f&#125;s&quot;.format(time_use // 60, time_use % 60))\n        scheduler.step()\n\n    model.load_state_dict(best_model_wts)\n    torch.save(best_model_wts, &quot;models/best_model.pth&quot;)\n\n    train_process = pd.DataFrame(data=&#123;&quot;epoch&quot;: range(num_epochs),\n                                       &quot;train_loss_all&quot;: train_loss_all,\n                                       &quot;val_loss_all&quot;: val_loss_all,\n                                       &quot;train_acc_all&quot;: train_acc_all,\n                                       &quot;val_acc_all&quot;: val_acc_all,\n                                       &quot;learn_rates&quot;: learning_rates&#125;)\n\n    return train_process\n\ndef matplot_acc_loss(train_process):\n    # 显示每一次迭代后的训练集和验证集的损失函数和准确率\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 3, 1)\n    plt.plot(train_process[&#x27;epoch&#x27;], train_process.train_loss_all, &quot;ro-&quot;, label=&quot;Train loss&quot;)\n    plt.plot(train_process[&#x27;epoch&#x27;], train_process.val_loss_all, &quot;bs-&quot;, label=&quot;Val loss&quot;)\n    plt.legend()\n    plt.xlabel(&quot;epoch&quot;)\n    plt.ylabel(&quot;Loss&quot;)\n    plt.subplot(1, 3, 2)\n    plt.plot(train_process[&#x27;epoch&#x27;], train_process.train_acc_all, &quot;ro-&quot;, label=&quot;Train acc&quot;)\n    plt.plot(train_process[&#x27;epoch&#x27;], train_process.val_acc_all, &quot;bs-&quot;, label=&quot;Val acc&quot;)\n    plt.xlabel(&quot;epoch&quot;)\n    plt.ylabel(&quot;acc&quot;)\n    plt.legend()\n    plt.subplot(1, 3, 3)\n    plt.plot(train_process[&#x27;epoch&#x27;], train_process.learn_rates, &quot;go-&quot;, label=&quot;Learn rates&quot;)\n    plt.xlabel(&quot;epoch&quot;)\n    plt.ylabel(&quot;Learn rates&quot;)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n 这里是优化后的版本\nfrom torch.optim import lr_scheduler\nimport copy\nimport time\nimport pandas as pd\n\ndef train_model_process(model, train_dataloader, val_dataloader, num_epochs):\n    device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n    if not os.path.exists(&quot;models&quot;):\n        os.mkdir(&quot;models&quot;)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=0.05)\n    scheduler = lr_scheduler.SequentialLR(\n        optimizer,\n        schedulers=[\n            lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=15),\n            lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs-15, eta_min=1e-6)\n        ],\n        milestones=[15]\n    )\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)\n    model = model.to(device)\n    best_model_wts = copy.deepcopy(model.state_dict())\n\n    best_acc = 0.0\n    train_loss_all = []\n    val_loss_all = []\n    train_acc_all = []\n    val_acc_all = []\n    learning_rates = []\n    since = time.time()\n\n    for epoch in range(num_epochs):\n        print(&quot;Epoch &#123;&#125;/&#123;&#125;&quot;.format(epoch, num_epochs - 1))\n        print(&quot;-&quot; * 10)\n\n        current_lr = optimizer.param_groups[0][&#x27;lr&#x27;]\n        learning_rates.append(current_lr)\n        print(f&quot;当前学习率: &#123;current_lr:.6f&#125;&quot;)\n\n        train_loss = 0.0\n        train_corrects = 0\n        val_loss = 0.0\n        val_corrects = 0\n        train_num = 0\n        val_num = 0\n\n        for step, (b_x, b_y) in tqdm(enumerate(train_dataloader), desc=f&quot;总步骤：&#123;len(train_dataloader)&#125;&quot;,\n                                     leave=False, unit=&quot;step&quot;, total=len(train_dataloader),\n                                     bar_format=&quot;&#123;desc&#125;: |&#123;bar:30&#125;| &#123;percentage:3.0f&#125;% 我在努力训练，唱跳Rap🏀，Music~&quot;,\n                                     ascii=&quot;🏀🥰🥰😘&quot;):\n            b_x = b_x.to(device)\n            b_y = b_y.to(device)\n            model.train()\n            output = model(b_x)\n            pre_lab = torch.argmax(output, dim=1)\n            loss = criterion(output, b_y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * b_x.size(0)\n            # 如果预测正确，则准确度train_corrects加1\n            train_corrects += torch.sum(pre_lab == b_y.data)\n            train_num += b_x.size(0)\n        for step, (b_x, b_y) in tqdm(enumerate(val_dataloader), desc=f&quot;总步骤：&#123;len(val_dataloader)&#125;&quot;,\n                                     leave=False, unit=&quot;step&quot;, total=len(val_dataloader),\n                                     bar_format=&quot;&#123;desc&#125;: |&#123;bar:30&#125;| &#123;percentage:3.0f&#125;% 该我上场表演了，唱跳Rap🏀，Music~&quot;,\n                                     ascii=&quot;🏀🥰🥰😘&quot;):\n            b_x = b_x.to(device)\n            b_y = b_y.to(device)\n            model.eval()\n            output = model(b_x)\n            pre_lab = torch.argmax(output, dim=1)\n            loss = criterion(output, b_y)\n\n            val_loss += loss.item() * b_x.size(0)\n            val_corrects += torch.sum(pre_lab == b_y.data)\n            val_num += b_x.size(0)\n\n        train_loss_all.append(train_loss / train_num)\n        train_acc_all.append(train_corrects.double().item() / train_num)\n\n        val_loss_all.append(val_loss / val_num)\n        val_acc_all.append(val_corrects.double().item() / val_num)\n\n        print(&quot;&#123;&#125; train loss:&#123;:.4f&#125; train acc: &#123;:.4f&#125;&quot;.format(epoch, train_loss_all[-1], train_acc_all[-1]))\n        print(&quot;&#123;&#125; val loss:&#123;:.4f&#125; val acc: &#123;:.4f&#125;&quot;.format(epoch, val_loss_all[-1], val_acc_all[-1]))\n\n        if val_acc_all[-1] &gt; best_acc:\n            best_acc = val_acc_all[-1]\n            best_model_wts = copy.deepcopy(model.state_dict())\n            checkpoint = &#123;\n                &#x27;epoch&#x27;: epoch,\n                &#x27;model_state_dict&#x27;: model.state_dict(),\n                &#x27;optimizer_state_dict&#x27;: optimizer.state_dict(),\n                &#x27;scheduler_state_dict&#x27;: scheduler.state_dict(),\n                &#x27;best_acc&#x27;: best_acc,\n            &#125;\n            torch.save(checkpoint, &quot;models/best_checkpoint.pth&quot;)\n\n        time_use = time.time() - since\n        print(&quot;训练和验证耗费的时间&#123;:.0f&#125;m&#123;:.0f&#125;s&quot;.format(time_use // 60, time_use % 60))\n        scheduler.step()\n\n    model.load_state_dict(best_model_wts)\n    torch.save(best_model_wts, &quot;models/theBest.pth&quot;)\n\n    train_process = pd.DataFrame(data=&#123;&quot;epoch&quot;: range(num_epochs),\n                                       &quot;train_loss_all&quot;: train_loss_all,\n                                       &quot;val_loss_all&quot;: val_loss_all,\n                                       &quot;train_acc_all&quot;: train_acc_all,\n                                       &quot;val_acc_all&quot;: val_acc_all,\n                                       &quot;learn_rates&quot;: learning_rates&#125;)\n\n    return train_process\n\n这里是学习率使用0.01，可以得到更好的效果，学习率为0.001在图像上更美观。\n 测试（此处为最佳实践）\ndef test_model_process(model, test_dataloader):\n    device = &quot;cuda&quot; if torch.cuda.is_available() else &#x27;cpu&#x27;\n    model = model.to(device)\n    test_corrects = 0.0\n    test_num = 0\n    with torch.no_grad():\n        for test_data_x, test_data_y in test_dataloader:\n            test_data_x = test_data_x.to(device)\n            test_data_y = test_data_y.to(device)\n            model.eval()\n            output= model(test_data_x)\n            pre_lab = torch.argmax(output, dim=1)\n            test_corrects += torch.sum(pre_lab == test_data_y.data)\n            test_num += test_data_x.size(0)\n\n    test_acc = test_corrects.double().item() / test_num\n    print(&quot;测试的准确率为：&quot;, test_acc)\n\n AlexNet实战\nclass AlexNet(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11, stride=4),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Flatten(),\n            nn.Linear(256 * 5 * 5, 4096),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 10)\n        )\n\n    def forward(self, x):\n        x = self.block(x)\n        return x\n\n此处并不是标准是AlexNet模型，主要是为了适配FashionMNIST。\ntrain_process = train_model_process(AlexNet(), train_dataloader, val_dataloader, num_epochs=5)\nmatplot_acc_loss(train_process)\n\n由于主包的GPU实在是太慢了，就不放结果图了。\n VGG网络原理\nVGGNet有6种不同的结构，主要以VGG-16为核心拆解。\n\nvgg-block内的卷积层都是同结构的，池化层都得上一层的卷积层特征缩减一半，深度较深，参数量够大，较小的filter size/kernel size\nVGG大量使用了3 x 3 的卷积核，参数很小而且效果还不错。\n还有VGG使用了块状结构，相当于一个小单元，非常方便。\n 模型\nclass VGG16(nn.Module):\n    def __init__(self):\n        super(VGG16, self).__init__()\n        self.block1 = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.block2 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.block3 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.block4 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.block5 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.block6 = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(),\n            nn.Linear(4096, 10)\n        )\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        return x\n\n\n 最佳实践——权重初始化\n在我们训练的时候，我们的模型可能不收敛，训练出来的结果图很难看，实际上大概率可能是出现了梯度消失问题，核心原因是我们的权重初始化过于随机了。\n为什么权重初始化如此重要？\n在深度神经网络中，权重初始化直接影响：\n\n激活值的分布（前向传播）\n梯度的大小和稳定性（反向传播）\n模型是否收敛、收敛速度、最终性能\n\n如果权重初始化不当，比如：\n\n权重太小 → 激活值趋近于0 → 梯度消失\n权重太大 → 激活值饱和 → 梯度爆炸\n\n所以我们必须要引入权重初始化！\n 何凯明 - 凯明初始化法\n凯明初始化法（Kaiming Initialization），又称 He 初始化，由 何恺明（Kaiming He） 在 2015 年提出，专为 ReLU 及其变种（如 LeakyReLU） 设计的权重初始化方法。\nfor param in self.modules():\n    if isinstance(param, nn.Conv2d):\n        nn.init.kaiming_normal_(param.weight, nonlinearity=&#x27;relu&#x27;)\n        if param.bias is not None:\n            nn.init.constant_(param.bias, 0)\n\nfor param in self.modules():\n    # 卷积层初始化\n    if isinstance(param, nn.Conv2d):\n        nn.init.kaiming_normal_(param.weight, nonlinearity=&#x27;relu&#x27;)\n        if param.bias is not None:\n            nn.init.constant_(param.bias, 0)\n    # 全连接层初始化\n    elif isinstance(param, nn.Linear):\n        nn.init.normal_(param.weight, 0, 0.01)\n        if param.bias is not None:\n            nn.init.constant_(param.bias, 0)\n\n以上可以放在：\nclass VGG16_init(nn.Module):\n    def __init__(self):\n        super(VGG16, self).__init__()\n\n初始化函数之下。\n 最佳实践——调整批次\n在深度学习中，调整批次大小（Batch Size） 是最有效、最低成本的性能调优手段之一。“最佳实践”不是越大越好，也不是越小越精，而是根据硬件、任务、训练阶段动态权衡。\n GoogLeNet网络原理\n\n这里面最唬人的地方就是这个Inception块，实际上没有那么吓人。\n以前流行的网络使用小到1×1，大到7×7的卷积核。 本文的一个观点：有时使用不同大小的卷积核组合是有利的。\n\n通道合并 ：将四个路线输出的通道合并。\n 1 x 1卷积的优点\n**在不改变空间结构的前提下，高效地融合通道信息、调整通道维度、引入非线性，从而提升模型表达能力并降低计算成本。**实现跨通道的交互和信息整合，卷积核通道数的降维和升维，减少网络参数。\n 全局平均池化GAP\n优点：“无参降维+抗过拟合”——把特征图全局平均成单个数值，直接当分类 logits，省掉全连接层，大幅减少参数量且强制保留通道级语义，降低过拟合风险。\n缺点：“丢细节+强假设”——空间信息被压成一点，对细粒度特征或目标定位任务无能为力，并隐含“通道即类别”的假设，若类别间特征重叠则易混淆。\n注意区别：全局平均池化GAP 与直接Flatten平展的区别\n 如何训练自己的数据集\n在深度学习中，设计一个良好的模型需要基础知识与运气，在此基础之上，数据的预处理往往是拉开差距的关键点。\n 数据集的划分\n如何将这样的数据目录：\ndata_cat_dog\n├── cat\n└── dog\n\n变成：\ndata\n├── train\n│   ├── cat\n│   └── dog\n└── test\n    ├── cat\n    └── dog\n\n有这样的脚本：\nimport os\nfrom shutil import copy\nimport random\n\n\ndef mkfile(file):\n    if not os.path.exists(file):\n        os.makedirs(file)\n\n\n# 获取data文件夹下所有文件夹名（即需要分类的类名）\nfile_path = &#x27;data_cat_dog&#x27;\nflower_class = [cla for cla in os.listdir(file_path)]\n\n# 创建 训练集train 文件夹，并由类名在其目录下创建5个子目录\nmkfile(&#x27;data/train&#x27;)\nfor cla in flower_class:\n    mkfile(&#x27;data/train/&#x27; + cla)\n\n# 创建 验证集val 文件夹，并由类名在其目录下创建子目录\nmkfile(&#x27;data/test&#x27;)\nfor cla in flower_class:\n    mkfile(&#x27;data/test/&#x27; + cla)\n\n# 划分比例，训练集 : 测试集 = 9 : 1\nsplit_rate = 0.1\n\n# 遍历所有类别的全部图像并按比例分成训练集和验证集\nfor cla in flower_class:\n    cla_path = file_path + &#x27;/&#x27; + cla + &#x27;/&#x27;  # 某一类别的子目录\n    images = os.listdir(cla_path)  # iamges 列表存储了该目录下所有图像的名称\n    num = len(images)\n    eval_index = random.sample(images, k=int(num * split_rate))  # 从images列表中随机抽取 k 个图像名称\n    for index, image in enumerate(images):\n        # eval_index 中保存验证集val的图像名称\n        if image in eval_index:\n            image_path = cla_path + image\n            new_path = &#x27;data/test/&#x27; + cla\n            copy(image_path, new_path)  # 将选中的图像复制到新路径\n\n        # 其余的图像保存在训练集train中\n        else:\n            image_path = cla_path + image\n            new_path = &#x27;data/train/&#x27; + cla\n            copy(image_path, new_path)\n        print(&quot;\\r[&#123;&#125;] processing [&#123;&#125;/&#123;&#125;]&quot;.format(cla, index + 1, num), end=&quot;&quot;)  # processing bar\n    print()\n\nprint(&quot;processing done!&quot;)\n\n 数据的预处理\n 重调整\ntransforms.Resize(160)\n\n或\ntransforms.Resize((160,160))\n\n第一个是的比例裁剪，第二个是指定像素裁剪，他们都是重调整。\n 随机裁剪\ntransforms.RandomResizedCrop(128, scale=(0.8, 1.0))\n\n先随机在原图里裁出一块面积占 80 %–100 % 的区域，再直接 resize 成 128×128，既做了随机裁剪又做了尺度增强。\n 原始数据增强\ntransforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET),\n\n自动从 ImageNet 预训练好的 25 种增强策略里，随机挑一条子策略（含 5 种强度可变的图像变换）作用到输入图上，属于“自动数据增强”里的经典算法，无需手工设计组合。\n\n策略搜索阶段用强化学习在 ImageNet 上离线搜出 25 条子策略（每条含 5 个变换）。\n每次训练迭代时：\n\n随机选一条子策略；\n按该子策略里指定的概率、幅度依次对图像做 5 次变换；\n变换列表包括 ShearX/Y, TranslateX/Y, Rotate, Color, Posterize, Solarize, Contrast, Sharpness, Brightness, AutoContrast, Equalize 等。\n\n\n\n 标准化\ntransforms.ToTensor()\n\n本身就具有归一化的功能，他将数值转化为0-1的区间，但这只是“线性缩放”，不是真正意义上的“标准化 (normalization)”。\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n这才是真正的标准化：把数值变成均值为 0、方差为 1 的分布，加速模型收敛。\n怎么算呢？还是来一个预处理。\nfrom PIL import Image\nimport os\nimport numpy as np\n\n# 文件夹路径，包含所有图片文件\nfolder_path = &#x27;data_cat_dog&#x27;\n\n# 初始化累积变量\ntotal_pixels = 0\nsum_normalized_pixel_values = np.zeros(3)  # 如果是RGB图像，需要三个通道的均值和方差\n\n# 遍历文件夹中的图片文件\nfor root, dirs, files in os.walk(folder_path):\n    for filename in files:\n        if filename.endswith((&#x27;.jpg&#x27;, &#x27;.jpeg&#x27;, &#x27;.png&#x27;, &#x27;.bmp&#x27;)):\n            image_path = os.path.join(root, filename)\n            image = Image.open(image_path)\n            image_array = np.array(image)\n            # 归一化像素值到0-1之间\n            normalized_image_array = image_array / 255.0\n            # 累积归一化后的像素值和像素数量\n            total_pixels += normalized_image_array.size\n            sum_normalized_pixel_values += np.sum(normalized_image_array, axis=(0, 1))\n\n# 计算均值和方差\nmean = sum_normalized_pixel_values / total_pixels\n\n\nsum_squared_diff = np.zeros(3)\nfor root, dirs, files in os.walk(folder_path):\n    for filename in files:\n        if filename.endswith((&#x27;.jpg&#x27;, &#x27;.jpeg&#x27;, &#x27;.png&#x27;, &#x27;.bmp&#x27;)):\n            image_path = os.path.join(root, filename)\n            image = Image.open(image_path)\n            image_array = np.array(image)\n            # 归一化像素值到0-1之间\n            normalized_image_array = image_array / 255.0\n            try:\n                diff = (normalized_image_array - mean) ** 2\n                sum_squared_diff += np.sum(diff, axis=(0, 1))\n            except:\n                print(f&quot;捕获到自定义异常&quot;)\n\nvariance = sum_squared_diff / total_pixels\n\nprint(&quot;Mean:&quot;, mean)\nprint(&quot;Variance:&quot;, variance)\n\n 随机擦除\ntransforms.RandomErasing(p=0.3, scale=(0.02, 0.2))\n\n以 30 % 的概率在图像上随机挖掉一块矩形区域（面积占图 2 %–20 %），用随机值（灰色、白色或黑色）填平，迫使模型学会“靠局部也能猜对”，属于简单的正则化/抗遮挡增强。\n ResNet原理与实战\n\n 网络持续加深带来那些问题\n一、优化难题：网络越深越“学不动”\n二、表达难题：网络越深越“记不住”\n三、工程难题：网络越深越“养不起”\n“深”本身不是错，错的是深+ Plain 堆叠；，\n\nPlain Network = 只有“卷积–BN–ReLU”一路串行下去，不带任何跳跃连接的直筒式架构。\n\n“链式求导”本身就是根源——但要把话拆成两句说：\n\n链式求导必然导致深度网络里的梯度是“连乘”形式；\n连乘的因子一旦持续小于 1（或大于 1），层数一多就指数级衰减 / 爆炸，这就是梯度消失/爆炸的数学本质。\n\n所以 Plain Net 的退化问题虽然表现形式是“越深层训练误差越大”，但底层机制仍然绕不开链式求导带来的数值不稳定。\nResNet 的 skip connection 正是人为在链式乘积里插进一项 1，把“连乘”改写成“连乘 + 1”，从而打断指数衰减——用加法给链式法则打了一个补丁。\n 残差块\nResNet（Residual Network）的核心创新就是残差连接（Residual Connection），它解决了深层网络的梯度消失问题，使得训练非常深的网络成为可能。\n残差块的设计是深度学习领域的重大突破，它不仅在图像识别任务中表现出色，还被广泛应用于各种深度学习架构中。a = h(x) + x\n\n上图有一个错误，填充应该是0，步幅是1。\n Batch Normalization归一化\nBatch Normalization（批归一化，简称BN） 的目的是 让神经网络训练更快、更稳定、更容易收敛。\n\nBatchNorm 不是“锦上添花”，而是“深度网络能训得动”的刚需——它把每层的输入分布强行拉回 N(0,1)，切断梯度消失/爆炸与内部协变量偏移的恶性循环，让链式求导的连乘因子始终落在 1 附近，于是非常深的 Plain/ResNet 才吃得下大学习率、快速收敛且不用特别精调初始化。\nBN的解决方案非常直观有力：既然层输入的分布老在变，那我们就强制把它拉回一个稳定、标准的分布。\nBN的位置通常放在：\n全连接层/卷积层 → BatchNorm → 激活函数(ReLU等)\n\n ResNet的基本实现\n 残差块\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, use_conv1x1=False, stride=1) -&gt; None:\n        super(ResidualBlock).__init__()\n        self.RelU = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride,\n                               padding=1)\n        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=stride,\n                               padding=1)\n        self.BN1 = nn.BatchNorm2d(out_channels)\n        self.BN2 = nn.BatchNorm2d(out_channels)\n        if use_conv1x1:\n            self.conv3 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=stride,\n                                   stride=stride, padding=0)\n        else:\n            self.conv3 = None\n\n    def forward(self, x):\n        y = self.RelU(self.BN1(self.conv1(x)))\n        y = self.BN2(self.conv2(y))\n        if self.conv3 is not None:\n            x = self.conv3(x)\n        y = self.RelU(y + x)\n        return y\n\n ResNet18\nclass ResNet18(nn.Module):\n    def __init__(self, ResidualBlock) -&gt; None:\n        super(ResNet18, self).__init__()\n        self.b1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n        self.b2 = nn.Sequential(\n            ResidualBlock(64, 64, use_conv1x1=False, stride=1),\n            ResidualBlock(64, 64, use_conv1x1=False, stride=1)\n        )\n        self.b3 = nn.Sequential(\n            ResidualBlock(64, 128, use_conv1x1=True, stride=2),\n            ResidualBlock(128, 128, use_conv1x1=False, stride=1)\n        )\n        self.b4 = nn.Sequential(\n            ResidualBlock(128, 256, use_conv1x1=True, stride=2),\n            ResidualBlock(256, 256, use_conv1x1=False, stride=1)\n        )\n        self.b5 = nn.Sequential(\n            ResidualBlock(256, 512, use_conv1x1=True, stride=2),\n            ResidualBlock(512, 512, use_conv1x1=False, stride=1)\n        )\n        self.b6 = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(512, 10)\n        )\n    def forward(self, x):\n        x = self.b1(x)\n        x = self.b2(x)\n        x = self.b3(x)\n        x = self.b4(x)\n        x = self.b5(x)\n        x = self.b6(x)\n        return x\n\n","categories":["深度学习","深度学习基础"],"tags":["python","深度学习","CNN"]},{"title":"Transformer实战","url":"/python/Transformer/","content":" Transformer实战\n 背景\n为了解决在序列建模中提升并行与长距离依赖建模能力，摆脱对循环与卷积的依赖。其核心是编码器与解码器结构。\n其中有三种注意力机制：编码器多头注意力、交叉注意力、解码器多头注意力（含因果编码）\n\n 输入数据\nTransformer里计算的全是数值，在输出之后和输出之前，都要把词转换为词向量，或者将词词向量转化为词。\n 词嵌入（Word Embedding）\n\n这个词嵌入矩阵的d相当于压缩后词向量的大小（维度）。\n\n 位置编码（Positional Encoding）\n\n 自注意力机制\n自注意力机制让每个输入位置都能动态地根据与其他位置的相关性重新表示自己，从而实现了信息的全局建模与动态加权融合。\n\n 多头注意力机制\n\n 掩码注意力（推理）\n\n 交叉注意力\n\n 参考文献\n\n【Transformer算法原理与实战】https://www.bilibili.com/video/BV1ej1EBWEWu\n\n","categories":["深度学习","深度学习基础"],"tags":["python","Transformer"]},{"title":"FastAPI","url":"/python/fastapi/","content":" FastAPI\nFastAPI 是一个用于构建 Web 应用的 Python 框架。\n 天生非阻塞\n\n\n 类型提示与验证\n\n 自带交互式文档\n\n 快速开始\nfrom fastapi import FastAPI\n\n# 创建一个 FastAPI 实例\napp = FastAPI()\n\n\n@app.get(&quot;/&quot;)\nasync def root():\n    return &#123;&quot;message&quot;: &quot;Hello World&quot;&#125;\n\n\n@app.get(&quot;/hello/&#123;name&#125;&quot;)\nasync def say_hello(name: str):\n    return &#123;&quot;message&quot;: f&quot;Hello &#123;name&#125;&quot;&#125;\n\n\n 启动命令\nuvicorn main:app --reload\n\nuvicorn？uvicorn 是一个高性能的 Python ASGI Web 服务器，专门用来运行 FastAPI / Starlette / Django ASGI 这类现代 Web 框架。\n–reload？支持热部署。\n 结果\n\n\n\n Python 装饰器\n在不改原函数代码的情况下，给函数外挂一层功能。\n Java 注解本身只是个标签\n@GetMapping(&quot;/hello&quot;)\npublic String hello() &#123;\n    return &quot;hi&quot;;\n&#125;\n\n@GetMapping 本身不拦截，不执行，不改方法，真正干活的是 Spring 框架 + 反射 + AOP\n Python 装饰器是可执行的\ndef log(func):\n    def wrapper(*args, **kwargs):\n        print(&quot;before&quot;)\n        return func(*args, **kwargs)\n    return wrapper\n\n@log\ndef hello():\n    print(&quot;hi&quot;)\n\n等价于：\nhello = log(hello)\n\n这里已经执行代码了\n 路由\n@app.get(&quot;/hello/&quot;)\nasync def say_hello():\n    return &#123;&quot;message&quot;: &quot;Hello World 这是默输出&quot;&#125;\n\n这是一个get方法，通过@app.get装饰。\n 接口的动态交互\n路径参数写在 URL 路径里，用来标识具体资源\n查询参数写在 URL ? 后面，用来筛选 / 附加条件\n请求体参数放在请求体中，用来提交复杂数据\n 路径参数\n@app.get(&quot;/book/&#123;id&#125;&quot;)\nasync def getBook(id: int):\n    return &#123;\n        &quot;id&quot;: id,\n        &quot;title&quot;: f&quot;FastAPI book-&#123;id&#125;&quot;\n    &#125;\n\n 校验——大小校验与长度校验\n@app.get(&quot;/book/&#123;id&#125;&quot;)\nasync def getBook(id: int = Path(description=&quot;The ID of the book to get&quot;, gt=0, lt=1000)):\n    return &#123;\n        &quot;id&quot;: id,\n        &quot;title&quot;: f&quot;FastAPI book-&#123;id&#125;&quot;\n    &#125;\n\n@app.get(&quot;/classify/&#123;name&#125;&quot;)\nasync def classify(name: str=Path(title=&quot;classify&quot;, min_length=3, max_length=10)):\n    return &#123;\n        &quot;name&quot;: name,\n        &quot;class&quot;: &quot;FastAPI&quot;\n    &#125;\n\n 查询参数\n@app.get(&quot;/query_book&quot;)\nasync def query_book(\n        classify: str = Query(&quot;默认分类&quot;, min_length=2, max_length=255),\n        price: float = Query(10.0, gt=50.0, le=100.0)):\n    return &#123;\n        &quot;classify&quot;: classify,\n        &quot;price&quot;: price\n    &#125;\n\n 请求体参数\n使用pydantic。\nfrom pydantic import BaseModel\nclass User(BaseModel):\n    username: str\n    password: str\n\n@app.post(&quot;/register&quot;)\nasync def register(user: User):\n    return user\n\nclass Book(BaseModel):\n    title: str\n    author: str\n    publisher: str\n    price: float\n\n@app.post(&quot;/book/add&quot;)\nasync def add_book(book: Book):\n    return book\n\n请求体参数的约束：\nclass User(BaseModel):\n    username: str = Field(default=&quot;默认用户名&quot;, min_length=2, max_length=10, description=&quot;用户名&quot;)\n    password: str = Field(default=&quot;默认密码&quot;, min_length=6, max_length=20, description=&quot;密码&quot;)\n\n\n@app.post(&quot;/register&quot;)\nasync def register(user: User):\n    return user\n\n 响应类型\n通过response_class指定\n\n JSON\n这个你返回一个字典对象的时候，FastAPI就自动帮你做好转换了。\n HTML / 纯文本\n@app.get(&quot;/html&quot;,response_class=HTMLResponse)\nasync def html():\n    return &quot;&quot;&quot;\n    &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;FastAPI&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Hello World&lt;/h1&gt;\n        &lt;/body&gt;\n    &lt;/html&gt;\n    &quot;&quot;&quot;\n\n File文件\n@app.get(&quot;/file&quot;)\nasync def file():\n    path = &quot;./files/dd.txt&quot;\n    return FileResponse(path)\n\n 自定义响应类型\nclass R(BaseModel):\n    code: int\n    msg: str\n    data: dict = Field(default=&#123;&#125;, description=&quot;返回数据&quot;)\n\n\n@app.get(&quot;/api&quot;, response_model=R)\nasync def api():\n    return &#123;\n        &quot;code&quot;: 200,\n        &quot;msg&quot;: &quot;success&quot;,\n        &quot;data&quot;: &#123;\n            &quot;name&quot;: &quot;KarryLiu&quot;,\n            &quot;age&quot;: 18\n        &#125;,\n    &#125;\n\n当你缺失数据时会报错，当你数据超出约定时会自动过滤掉。\n 异常处理\n@app.get(&quot;/api/news&quot;, response_model=R)\nasync def api_news(id: int = Query(1, gt=0, lt=1000)):\n    news_list = [1, 2, 3]\n    if id not in news_list:\n        raise HTTPException(status_code=404, detail=&quot;新闻不存在&quot;)\n    return &#123;\n        &quot;code&quot;: 200,\n        &quot;msg&quot;: &quot;success&quot;,\n        &quot;data&quot;: &#123;\n            &quot;id&quot;: id,\n            &quot;title&quot;: f&quot;新闻标题-&#123;id&#125;&quot;,\n            &quot;content&quot;: f&quot;新闻内容-&#123;id&#125;&quot;\n        &#125;,\n    &#125;\n\n 中间件\n中间件给每一个请求提供统一的处理逻辑。使用@app.middleware(&quot;http&quot;)来定义。多个中间件是自底向上执行的\n@app.middleware(&quot;http&quot;)\nasync def middleware1(request, call_next):\n    print(&quot;中间件1开始执行&quot;)\n    response = await call_next(request)\n    print(&quot;中间件1结束执行&quot;)\n    return response\n\n\n@app.middleware(&quot;http&quot;)\nasync def middleware2(request, call_next):\n    print(&quot;中间件2开始执行&quot;)\n    response = await call_next(request)\n    print(&quot;中间件2结束执行&quot;)\n    return response\n\n\n@app.get(&quot;/api/mid&quot;)\nasync def api_mid():\n    return &#123;\n        &quot;code&quot;: 200,\n        &quot;msg&quot;: &quot;success&quot;,\n        &quot;data&quot;: &#123;\n            &quot;name&quot;: &quot;KarryLiu&quot;,\n            &quot;age&quot;: 18\n        &#125;,\n    &#125;\n\n\n居然是自底向上执行的\n 依赖注入\n\n\nasync def common_parameters(skip: int = 0, limit: int = 10):\n    return &#123;&quot;skip&quot;: skip, &quot;limit&quot;: limit&#125;\n\n\n@app.get(&quot;/api/news_list&quot;)\nasync def get_news_list(commons=Depends(common_parameters)):\n    return &#123;&quot;msg&quot;: &quot;news_list&quot;&#125;\n\n\n@app.get(&quot;api/user_list&quot;)\nasync def get_user_list(commons=Depends(common_parameters)):\n    return &#123;&quot;msg&quot;: &quot;user_list&quot;&#125;\n\ncommon_parameters 里 return &#123;&quot;skip&quot;: skip, &quot;limit&quot;: limit&#125; 返回值不会直接返回给客户端， 而是 作为参数值，传给 get_news_list 里的 commons 变量。我们可以打印一下：\n@app.get(&quot;/api/news_list&quot;)\nasync def get_news_list(commons=Depends(common_parameters)):\n    print(commons)\n    print(commons[&quot;skip&quot;])\n    print(commons[&quot;limit&quot;])\n    return &#123;&quot;msg&quot;: &quot;news_list\n\n\n ORM工具——SQLAlchemy\n 安装sqlalchemy[asyncio]和aiomysql\npip install sqlalchemy[asyncio] aiomysql\n\n 自动建表\nASYNC_DATABASE_URL = &quot;mysql+aiomysql://root:123456@localhost:3306/fastapi_first?charsetutf-8&quot;\n\nengine = create_async_engine(ASYNC_DATABASE_URL, echo=True, pool_size=10, max_overflow=20, pool_recycle=3600,\n                             pool_pre_ping=True, pool_use_lifo=True, pool_timeout=30, future=True, )\n\n\nclass Base(DeclarativeBase):\n    create_time: Mapped[datetime] = mapped_column(DateTime, insert_default=func.now(), comment=&quot;创建时间&quot;)\n    update_time: Mapped[datetime] = mapped_column(DateTime, insert_default=func.now(), comment=&quot;更新时间&quot;)\n\n\nclass Book(Base):\n    __tablename__ = &quot;book&quot;\n\n    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True, comment=&quot;ID&quot;)\n    title: Mapped[str] = mapped_column(String(100), comment=&quot;标题&quot;)\n    author: Mapped[str] = mapped_column(String(50), comment=&quot;作者&quot;)\n    publisher: Mapped[str] = mapped_column(String(100), comment=&quot;出版社&quot;)\n    price: Mapped[float] = mapped_column(Float, comment=&quot;价格&quot;)\n\n\nclass User(Base):\n    __tablename__ = &quot;user&quot;\n\n    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True, comment=&quot;ID&quot;)\n    username: Mapped[str] = mapped_column(String(50), comment=&quot;用户名&quot;)\n    password: Mapped[str] = mapped_column(String(50), comment=&quot;密码&quot;)\n\n\nasync def create_table():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n\n@asynccontextmanager\nasync def lifespan(apps: FastAPI):\n    await create_table()\n    yield\n    await engine.dispose()\n\n\napp = FastAPI(lifespan=lifespan)\n\n\n ORM依赖注入与数据查询\nAsyncSessionLocal = async_sessionmaker(\n    engine,\n    class_=AsyncSession,\n    expire_on_commit=False,\n)\n\n\nasync def get_database():\n    async with AsyncSessionLocal() as session:\n        try:\n            yield session\n            await session.commit()\n        except Exception as e:\n            await session.rollback()\n            raise\n        finally:\n            await session.close()\n\n\n@app.get(&quot;/book/books&quot;)\nasync def get_books(session: AsyncSession = Depends(get_database)):\n    query = select(Book)\n    result = await session.execute(query)\n    books = result.scalars().all()\n    print(books)\n    return books\n\n\n 查询\n 比较判断\n==；&gt;；&lt;；&gt;=；&lt;=\n@app.get(&quot;/book/book/&#123;id&#125;&quot;)\nasync def get_book(id: int, session: AsyncSession = Depends(get_database)):\n    query = select(Book).where(Book.id == id)\n    result = await session.execute(query)\n    book = result.scalars().first()\n    return book\n\n@app.get(&quot;/book/book/&#123;id&#125;&quot;)\nasync def get_book(id: int, session: AsyncSession = Depends(get_database)):\n    query = select(Book).where(Book.id == id)\n    result = await session.execute(query)\n    book = result.scalars().one_or_none()\n    return book\n\n 模糊查询\n@app.get(&quot;/book/like&quot;)\nasync def get_like_book(title: str, session: AsyncSession = Depends(get_database)):\n    query = select(Book).where(Book.title.like(f&quot;%&#123;title&#125;%&quot;))\n    result = await session.execute(query)\n    book = result.scalars().all()\n    return book\n\n\n 取首条数据\n@app.get(&quot;/book/first&quot;)\nasync def get_first_book(session: AsyncSession = Depends(get_database)):\n    query = select(Book)\n    result = await session.execute(query)\n    book = result.scalars().first()\n    return book\n\n 多条件\n@app.get(&quot;/book/like_start&quot;)\nasync def get_like_start_book(title: str, session: AsyncSession = Depends(get_database)):\n    query = select(Book).where(Book.title.like(f&quot;&#123;title&#125;%&quot;)).where(Book.id == 2)\n    result = await session.execute(query)\n    book = result.scalars().all()\n    return book\n\n 聚合查询\nfunc——count / avg / max / min / sum\n@app.get(&quot;/book/get_avg_price&quot;)\nasync def get_avg_price(session: AsyncSession = Depends(get_database)):\n    query = select(func.avg(Book.price))\n    result = await session.execute(query)\n    avg_price = result.scalars().one()\n    return avg_price\n\n\n\n","categories":["开发经验"],"tags":["python","Fast API"]},{"title":"NLP自然语言处理","url":"/python/NLP/","content":" NLP自然语言处理\n\n学习笔记\n\n 常见任务\n\n文本分类：情感分析（积极/消极）、垃圾邮件识别、新闻主题分类【句子级别】\n序列标注：命名实体识别（找人名、地名、手机号）、文本生成、信息抽取、文本转化【Token级别】\n\n 文本表示\n 分词\n\n词级分词：将文本按照词切分在英语中空格往往是天然的切词标志，但是容易出现OOV问题（未登录词问题）\n字符级分词：一个字母、数字、标点甚至空格，都会被视作一个独立的 token，不会有OOV问题，但模型必须依赖更长的上下文来推断词义和结构，这显著增加了建模难度和训练成本。\n子词级分词：将词语切分为更小的单元——子词（subword），例如词根、前缀、后缀或常见词片段。\n\n以上是英文适用的分词方法，下面是中文适用的分词方法。\n\n字符级分词：一个字就进行一次切分，比英文中的字符分词，中文的字符分词更加“语义友好”。\n词级分词：由于中文没有空格等天然词边界，词级分词通常依赖词典、规则或模型来识别词语边界。\n子词级分词：它们以汉字为基本单位，通过学习语料中高频的字组合（如“自然”、“语言”、“处理”），自动构建子词词表。在当前主流的中文大模型（如通义千问、DeepSeek）中，子词分词已成为广泛采用的文本切分策略。\n\n JiebBa分词组件\nword_gen = jieba.cut(&quot;我的名字叫Karry，来自计算机科学技术学院&quot;)\nfor word in word_gen:\n    print(word)\n\n\n我\n的\n名字\n叫\nKarry\n，\n来自\n计算机\n科学技术\n学院\n\n 词表示\n\nOne-hot编码（独热编码）：它将词汇表中的每个词映射为一个稀疏向量，向量的长度等于整个词表的大小。该词在对应的位置为 1，其他位置为 0。在实际自然语言处理任务中，one-hot 表示已经很少被直接使用。\n语义化词向量：它通过对大规模语料的学习，为每个词生成一个具有语义意义的稠密向量表示。比如“女人”和“女孩”，这两个词向量就很接近。Word2Vec。\n\n Word2Vec\n\n左侧是CBOW，中间词是教师，以此来学习上下文。\n右侧是Skip-gram，上下文是教师，以此来学习中间词。\n GENSIM词向量组件\n 加载与使用公开词向量\nfrom gensim.models import KeyedVectors\n\nmodel_path = &#x27;sgns.weibo.word.bz2&#x27;\nmodel = KeyedVectors.load_word2vec_format(model_path)\n\nsimilarity = model.similarity(&#x27;公交&#x27;, &#x27;地铁&#x27;)\nprint(&#x27;公交和地铁的相似度&#x27;, similarity)\n\n\n公交和地铁的相似度 0.65458214\n\nmodel.most_similar(positive=[&#x27;男人&#x27;, &#x27;女孩&#x27;], negative=[&#x27;男孩&#x27;], topn=10)\n\n男人+女孩-男孩 = 女人\n\n[(‘女人’, 0.6578881740570068),\n(‘女孩子’, 0.515068531036377),\n(‘女生’, 0.4519447982311249),\n(‘女人真’, 0.44206273555755615),\n(‘女人们’, 0.4369858503341675),\n(‘女人爱’, 0.435453325510025),\n(‘寡言少语’, 0.42479249835014343),\n(‘男孩子’, 0.42177465558052063),\n(‘看女人’, 0.41949161887168884),\n(‘笨女人’, 0.4182003140449524)]\n\n 训练自己的词向量\nimport pandas as pd\nimport jieba\nfrom gensim.models import Word2Vec\n\ncomments = pd.read_csv(&#x27;./data/online_shopping_10_cats.csv&#x27;, encoding=&#x27;utf-8&#x27;)\nreviews = comments[&#x27;review&#x27;]\nreviews = reviews.dropna()\nsentences = [[token for token in jieba.__lcut(review) if token.strip() != &#x27;&#x27;] for review in reviews]\n\n\nmodel = Word2Vec(\n    sentences,            # 已分词的句子序列\n    vector_size=100,      # 词向量维度\n    window=5,             # 上下文窗口大小\n    min_count=2,          # 最小词频（低于将被忽略）\n    sg=1,                 # 1:Skip-Gram，0:CBOW\n    workers=4             # 并行训练线程数\n)\nmodel.wv.save_word2vec_format(&#x27;./data/word2vec.txt&#x27;)\n\n 词向量的应用\nimport torch\nfrom torch import nn\nfrom gensim.models import KeyedVectors\n\n# 加载词向量\nwv = KeyedVectors.load_word2vec_format(&#x27;./data/word2vec.txt&#x27;)\n\n# 构建词向量矩阵\nnum_embedding = len(wv.key_to_index)\nembedding_dim = wv.vector_size\nembedding_matrix = torch.randn(num_embedding, embedding_dim)\n\nfor word, index in wv.key_to_index.items():\n    embedding_matrix[index] = torch.from_numpy(wv[word])\n\nprint(embedding_matrix.shape)\n\n# 创建Embedding\nembedding = nn.Embedding.from_pretrained(embedding_matrix)\n\ntext = &#x27;我喜欢乘坐地铁&#x27;\ntokens= jieba.lcut(text)\ninput_ids = [wv.key_to_index[token] for token in tokens if token in wv.key_to_index]\n\ninput_tensor = torch.LongTensor(input_ids)\nembedding(input_tensor)\n\n ELMo模型——一词多义问题\nNNLM模型是在预测下一个词，而词向量是副产品。\nWord2Vec模型是在专门做词向量，有CBOW和Skip-gram。\nELMo模型解决的是一词多义的问题。\n\nELMo不仅仅是训练了一个Q矩阵，还把这个词的上下文信息融入到这个Q矩阵中，左边的LSTM获取E2的上文信息，右侧的LSTM获取下文信息。\nT1包含了第一个词的特征，与此同时也包含了语法特征和语义特征。\n然而LSTM无法并行计算，因此我们引入了注意力机制。\n RNN基本结构\nRNN以时间步为基本单位，逐个处理每一个Token，新的隐藏状态是由，上一个隐藏状态与当前步共同决定的。\n\nx1，x2，x3…xt这是一组特征，但是这是与时间有关的特征，hₜ是每时每刻的预测。\n比如一句话：“我出生在中国，我会说中文”\n我们可以按时间步展开：\n\n\n\n时间步\n输入 xₜ（词向量）\n隐藏状态 hₜ（编码了前面的上下文）\n可能的预测 yₜ（下一个词）\n\n\n\n\nt=1\n“我”\nh₁（编码了“我”）\n“出生”\n\n\nt=2\n“出生”\nh₂（编码了“我 出生”）\n“在”\n\n\nt=3\n“在”\nh₃（编码了“我 出生 在”）\n“中国”\n\n\nt=4\n“中国”\nh₄（编码了“我 出生 在 中国”）\n“，”\n\n\nt=5\n“，”\nh₅（编码了“我 出生 在中国 ，”）\n“我”\n\n\nt=6\n“我”\nh₆（编码了前半句+“我”）\n“会”\n\n\nt=7\n“会”\nh₇（编码了前半句+“我 会”）\n“说”\n\n\nt=8\n“说”\nh₈（编码了前半句+“我 会 说”）\n“中文”\n\n\n\nRNN的隐藏状态 hₜ 起到了**“记忆”**的作用：\n\n当模型看到“中国”时，h₄ 已经编码了“我出生在中国”这个信息。\n当模型看到后半句“我会说中文”时，h₈ 已经编码了整句话的上下文，因此它可以推断出“中文”是合理的下一个词，因为“出生在中国”和“会说中文”之间有语义关联。\n\n在RNN中，每个词 xₜ 是随时间输入的特征，隐藏状态 hₜ 是模型对“到目前为止所有词”的理解，而预测 yₜ 是基于这种理解做出的下一步判断。\n RNN与FCNN（全连接神经网络）的区别\n\n左侧是RNN，右侧是全连接神经网络。全连接神经网络是一个直筒的结构，RNN是一个与时间有关的循环结构。\n FCNN的展开\n\n RNN的展开\n\n 为什么？\nFCNN把每个输入当作独立静态向量\nRNN把输入看成随时间展开的序列，用共享参数+循环隐状态来显式建模“过去”对“现在”的影响。\n 数学表达\nht=tanh⁡(xtWx+ht−1Wh+b),\\begin {array}{c}\nh_t = \\tanh(x_t W_x + h_{t-1} W_h + b),\n\\end{array}\nht​=tanh(xt​Wx​+ht−1​Wh​+b),​\n[ht1ht2ht3ht4]=tanh⁡([ht−11ht−12ht−13ht−14][wh11wh12wh13wh14wh21wh22wh23wh24wh31wh32wh33wh34wh41wh42wh43wh44]+[xt1xt2xt3xt4][wx11wx12wx13wx14wx21wx22wx23wx24wx31wx32wx33wx34]+[b1b2b3b4])\\begin {array}{c}\n\\begin{bmatrix}\nh_t^1 &amp; h_t^2 &amp; h_t^3 &amp; h_t^4\n\\end{bmatrix}\n=\n\\tanh(\n\\begin{bmatrix}\nh_{t-1}^1 &amp; h_{t-1}^2 &amp; h_{t-1}^3 &amp; h_{t-1}^4\n\\end{bmatrix}\n\\begin{bmatrix}\nw_h^{11} &amp; w_h^{12} &amp; w_h^{13} &amp; w_h^{14} \\\\\nw_h^{21} &amp; w_h^{22} &amp; w_h^{23} &amp; w_h^{24} \\\\\nw_h^{31} &amp; w_h^{32} &amp; w_h^{33} &amp; w_h^{34} \\\\\nw_h^{41} &amp; w_h^{42} &amp; w_h^{43} &amp; w_h^{44}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\nx_t^1 &amp; x_t^2 &amp; x_t^3 &amp; x_t^4\n\\end{bmatrix}\n\\begin{bmatrix}\nw_x^{11} &amp; w_x^{12} &amp; w_x^{13} &amp; w_x^{14} \\\\\nw_x^{21} &amp; w_x^{22} &amp; w_x^{23} &amp; w_x^{24} \\\\\nw_x^{31} &amp; w_x^{32} &amp; w_x^{33} &amp; w_x^{34}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\nb_1 &amp; b_2 &amp; b_3 &amp; b_4\n\\end{bmatrix}\n)\n\\end{array}\n[ht1​​ht2​​ht3​​ht4​​]=tanh([ht−11​​ht−12​​ht−13​​ht−14​​]⎣⎢⎢⎢⎡​wh11​wh21​wh31​wh41​​wh12​wh22​wh32​wh42​​wh13​wh23​wh33​wh43​​wh14​wh24​wh34​wh44​​⎦⎥⎥⎥⎤​+[xt1​​xt2​​xt3​​xt4​​]⎣⎢⎡​wx11​wx21​wx31​​wx12​wx22​wx32​​wx13​wx23​wx33​​wx14​wx24​wx34​​⎦⎥⎤​+[b1​​b2​​b3​​b4​​])​\n 权重共享\n权重共享”是 RNN 与 CNN 里最常被提到的一个核心设计，但它不是“所有神经元共用同一个数”，而是同一组参数（矩阵/向量）在多个位置、多个时间步或空间位置反复使用。\n 双向RNN\n双向递归神经网络（Bidirectional Recurrent Neural Network，简称Bi-RNN）是一种特殊的递归神经网络（RNN），它在处理序列数据时，不仅考虑了过去的信息，还考虑了未来的信息。这使得Bi-RNN在处理诸如自然语言处理（NLP）任务时特别有用，因为它能够同时利用上下文信息。\n 在PyTorch使用RNN\nimport torch.nn as nn\n\n# 双层双向RNN\nrnn = nn.RNN(input_size=3, hidden_size=4, num_layers=2, batch_first=True, bidirectional=True)\n\n# shape: (batch, seq_len, input_size)\ninput = torch.randn(2, 4, 3)\n\noutput, hn = rnn(input)\nprint(output.shape)\nprint(hn.shape)\n\n\ntorch.Size([2, 4, 8])\ntorch.Size([4, 2, 4])\n\n 词嵌入层API应用\n 作用\n把词或词对应的索引转为词向量\n 使用框架实现\nimport torch\nimport jieba\nimport torch.nn as nn\n\n\ndef dm01():\n    text = &quot;当您登录平台即表示您接受本隐私政策的全部内容，并同意平台按本政策收集、使用和保护您的相关个人信息。&quot;\n    words = jieba.lcut(text)\n    print(words)\n    &#x27;&#x27;&#x27;\n    len(words) : 词表大小\n    embedding_dim : 词向量维度\n    &#x27;&#x27;&#x27;\n    embed = nn.Embedding(len(words), embedding_dim=4)\n    for i, word in enumerate(words):\n        # 词索引（张量）转变为词向量\n        word2Vec = embed(torch.tensor([i]))\n        print(word, word2Vec)\n\nif __name__ == &#x27;__main__&#x27;:\n    dm01()\n\n RNN的API使用\nRNN就像是你的大脑，在看电影的过程中记住剧情。\nimport torch\nimport torch.nn as nn\n\n&#x27;&#x27;&#x27;\ninput_size：词向量的维度\nhidden_size：隐藏状态的维度\nnum_layers：隐藏层数\nbatch_first：批次优先\n&#x27;&#x27;&#x27;\nrnn = nn.RNN(input_size=10, hidden_size=20, num_layers=1)\n&#x27;&#x27;&#x27;\n5：有5个词\n3：3个批次\n10：每个词的细节是10维度\n&#x27;&#x27;&#x27;\nx = torch.rand(5, 3, 10)\n&#x27;&#x27;&#x27;\n1：隐藏层层数\n3：句子数量\n20：有20个循环维度，也就是隐藏状态的维度\n&#x27;&#x27;&#x27;\nh0 = torch.zeros(1, 3, 20)\n\n&#x27;&#x27;&#x27;\nRNN处理\nx：本次的输入\nh0：上一次的隐藏状态\n输出\noutput：每个时间步的输出，包含了所有时间步骤的隐藏状态\nh1：最后一次隐藏状态，大脑里最新的剧情\n&#x27;&#x27;&#x27;\noutput, h1 = rnn(x, h0)\nprint(f&#x27;output_shape:&#123;output.shape&#125;&#x27;)\nprint(f&#x27;h1_shape:&#123;h1.shape&#125;&#x27;)\n\n RNN的文本生成\n# _*_ coding : utf-8 _*_\n# @Time : 2025/10/31 10:02\n# @Author : KarryLiu\n# File : 歌词生成\n# @Project : pytorchSTU\nimport math\n\nimport torch\nimport jieba\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.optim as optim\nimport time\n\nfrom tqdm import tqdm\n\n&quot;&quot;&quot;\n实现步骤：\n    1. 获取数据，进行分词\n    2. 获取词表，构建数据集\n    3. 搭建RNN神经网络模型\n    4. 训练模型\n    5. 模型预测\n    6. 测试\n&quot;&quot;&quot;\n\n\ndef build_vocab():\n    unique_words, all_words = [], []\n    with open(&#x27;data/jaychou_lyrics.txt&#x27;, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as f:\n        for line in tqdm(f, desc=&#x27;正在处理数据&#x27;, total=5819):\n            words = jieba.lcut(line)\n            all_words.append(words)\n            for word in words:\n                if word not in unique_words:\n                    unique_words.append(word)\n\n    word2index = &#123;word: index for index, word in enumerate(unique_words)&#125;\n    index2word = &#123;index: word for index, word in enumerate(unique_words)&#125;\n\n    # 将词表转为索引，【0，1，2，3】，这个是一句歌词\n    corpus_idx = []\n    for all_word in all_words:\n        tmp = []\n        for word in all_word:\n            tmp.append(word2index[word])\n        tmp.append(word2index[&#x27; &#x27;])\n        corpus_idx.extend(tmp)\n    word_count = len(unique_words)\n    return unique_words, word2index, word_count, corpus_idx\n\n\nclass LyricsDataset(Dataset):\n\n    def __init__(self, corpus_idx, num_chars):\n        super().__init__()\n        self.corpus_idx = corpus_idx\n        self.num_chars = num_chars\n        self.word_count = len(corpus_idx)\n        self.sentence_len = self.word_count // num_chars\n\n    def __len__(self):\n        return self.sentence_len\n\n    def __getitem__(self, index):\n        start = min(max(index, 0), self.word_count - self.num_chars - 1)\n        end = start + self.num_chars\n        sentence_x = self.corpus_idx[start:end]\n        sentence_y = self.corpus_idx[start + 1:end + 1]\n        sentence_x = torch.tensor(sentence_x, dtype=torch.long)\n        sentence_y = torch.tensor(sentence_y, dtype=torch.long)\n        return sentence_x, sentence_y\n\n\nclass TextGenerator(nn.Module):\n    def __init__(self, unique_words_count, ):\n        super().__init__()\n        self.embedding = nn.Embedding(unique_words_count, 128)\n        self.rnn = nn.RNN(128, 256, 1)\n        self.linear = nn.Linear(256, unique_words_count)\n\n    def forward(self, input, hidden):\n        embed = self.embedding(input)\n        output, hidden = self.rnn(embed.transpose(0, 1), hidden)\n        output = self.linear(output.reshape(shape=(-1, output.shape[-1])))\n        return output, hidden\n\n    def init_hidden(self, batch_size):\n        return torch.zeros(1, batch_size, 256)\n\n\ndef train():\n    unique_words, word2index, unique_word_count, corpus_idx = build_vocab()\n    dataset = LyricsDataset(corpus_idx, 32)\n    dataloader = DataLoader(dataset, batch_size=5, shuffle=True)\n    model = TextGenerator(unique_word_count)\n    optimizer = optim.Adam(model.parameters(), lr=0.01)\n    loss_fn = nn.CrossEntropyLoss()\n    min_loss = math.inf\n    loss = 0\n    total_loss = 0\n    iter_times = 0\n    for epoch in range(50):\n        for i, (x, y) in enumerate(tqdm(dataloader, desc=f&#x27;正在训练第&#123;epoch + 1&#125;轮&#x27;, total=len(dataset) // 5)):\n            hidden = model.init_hidden(5)\n            output, hidden = model(x, hidden)\n            y = torch.transpose(y, 0, 1).reshape(shape=(-1,))\n            loss = loss_fn(output, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            iter_times += 1\n            total_loss += loss.item()\n\n        print(f&#x27;第&#123;epoch + 1&#125;轮，总损失：&#123;total_loss / iter_times&#125;&#x27;)\n        if loss &lt; min_loss:\n            min_loss = loss\n            torch.save(model.state_dict(), &#x27;model.pth&#x27;)\n\n\ndef eval(start_word, seq_len):\n    unique_words, word2index, unique_word_count, corpus_idx = build_vocab()\n    model = TextGenerator(unique_word_count)\n    model.load_state_dict(torch.load(&#x27;model.pth&#x27;))\n    hidden = model.init_hidden(1)\n    word_index = word2index[start_word]\n    generate_sentence = [word_index]\n    for i in range(seq_len):\n        output, hidden = model(torch.tensor([[word_index]]), hidden)\n        output = output.reshape(shape=(-1,))\n        word_index = torch.argmax(output).item()\n        generate_sentence.append(word_index)\n    for e in [unique_words[index] for index in generate_sentence]:\n        print(e, end=&#x27;&#x27;)\n\n\nif __name__ == &#x27;__main__&#x27;:\n    train()\n    eval(&#x27;分手&#x27;, 100)\n\n\n分手的话像语言暴力你而香 牧草有没有 我马儿有些瘦\n天涯尽头 满脸风霜落寞 近乡情怯的我\n我说店小二 三两银够不够\n景色入秋 漫天黄沙凉过\n塞北的客栈人多 牧草有没有 我马儿有些瘦\n世事看透 江湖上潮起潮落 什么恩怨过错\n在多年以后 还是让人难过 心伤透\n娘子她人在江南等我 泪不休 语沉默\n\n 智能提示输入法的实现\n 最佳实践\nprint(__file__)\n\n__ file __ 是当前文件的绝对路径，py在处理相对路径时可能会遇到问题，最好使用绝对路径。\nfrom pathlib import Path\n\nprint(Path(__file__).parent.parent / &quot;data/raw/synthesized_.jsonl&quot;)\n\n所以我们通常会这样写：\nRAW_DATA_DIR = Path(__file__).parent.parent / &quot;data/raw&quot;\n\n\ndef process():\n    print(&quot;开始处理数据&quot;)\n    # 读取原始文件\n    df = pd.read_json(RAW_DATA_DIR / &quot;synthesized_.jsonl&quot;, lines=True, orient=&#x27;records&#x27;,\n                      encoding=&#x27;utf-8&#x27;)\n    print(df.head())\n\n    print(&quot;数据处理完成&quot;)\n\n 数据预处理\nimport jieba\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport config\n\n\ndef build_dataset(sentences, word2index, desc=&quot;构建数据集&quot;):\n    indexed_sentences = [[word2index.get(token, 0) for token in jieba.lcut(sentences)] for sentences in\n                         sentences]\n    dataset = []\n    for index, sentence in tqdm(enumerate(indexed_sentences), desc=desc):\n        for i in range(len(sentence) - config.SWQ_LEN):\n            input = sentence[i:i + config.SWQ_LEN]\n            target = sentence[i + config.SWQ_LEN]\n            dataset.append(&#123;\n                &#x27;input&#x27;: input,\n                &#x27;target&#x27;: target,\n            &#125;)\n    return dataset\n\n\ndef process():\n    print(&quot;开始处理数据&quot;)\n    # 读取原始文件\n    df = pd.read_json(config.RAW_DATA_DIR / &quot;synthesized_.jsonl&quot;, lines=True, orient=&#x27;records&#x27;,\n                      encoding=&#x27;utf-8&#x27;).sample(frac=0.1)\n    # 提取句子\n    sentences = []\n    for sentencesX in df[&#x27;dialog&#x27;]:\n        for sentencesXs in sentencesX:\n            sentences.append(sentencesXs.split(&#x27;：&#x27;)[1])\n    print(f&quot;句子的数量为：&#123;len(sentences)&#125;&quot;)\n    # 划分数据集\n    train_sentences, valid_sentences = train_test_split(sentences, test_size=0.2)\n    # 构建词表\n    vocab_set = set()\n    for sentence in tqdm(train_sentences, desc=&#x27;构建词表&#x27;):\n        vocab_set.update(jieba.lcut(sentence))\n    vocab_list = [&#x27;&lt;unk&gt;&#x27;] + list(vocab_set)\n    print(f&quot;词表的数量为：&#123;len(vocab_list)&#125;&quot;)\n    # 保存此表\n    with open(config.MODELS_DIR / &quot;vocab.txt&quot;, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as f:\n        f.write(&#x27;\\n&#x27;.join(vocab_list))\n\n    # 构建训练集\n    word2index = &#123;word: index for index, word in enumerate(vocab_list)&#125;\n    train_dataset = build_dataset(train_sentences, word2index)\n    # 保存训练集\n    pd.DataFrame(train_dataset).to_json(config.PROCESSED_DATA_DIR / &quot;train.jsonl&quot;, orient=&#x27;records&#x27;, lines=True)\n    # 构建验证集\n    valid_dataset = build_dataset(valid_sentences, word2index)\n    # 保存验证集\n    pd.DataFrame(valid_dataset).to_json(config.PROCESSED_DATA_DIR / &quot;valid.jsonl&quot;, orient=&#x27;records&#x27;, lines=True)\n    print(&quot;数据处理完成&quot;)\n\n\nif __name__ == &#x27;__main__&#x27;:\n    process()\n\n 数据集\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.dataset import _T_co\n\nfrom src.NLP.SmartPrompt.src import config\n\n\n# 定义Dataset\nclass InputMethodDataset(Dataset):\n\n    def __init__(self, path):\n        super().__init__()\n        self.data = pd.read_json(path, lines=True, orient=&#x27;records&#x27;).to_dict(&#x27;records&#x27;)\n\n    def __getitem__(self, index) -&gt; _T_co:\n        input_tensor = torch.tensor(self.data[index][&#x27;input&#x27;], dtype=torch.long)\n        target_tensor = torch.tensor(self.data[index][&#x27;target&#x27;], dtype=torch.long)\n        return input_tensor, target_tensor\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n\n# 提供一个获取DATALoader的方法\ndef get_dataloader(train=True):\n    path = config.PROCESSED_DATA_DIR / (&quot;train.jsonl&quot; if train else &quot;valid.jsonl&quot;)\n    dataset = InputMethodDataset(path)\n    return DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n\n\nif __name__ == &#x27;__main__&#x27;:\n    train_loader = get_dataloader(train=True)\n    valid_loader = get_dataloader(train=False)\n    print(len(train_loader))\n    print(len(valid_loader))\n\n    for input_tensor, target_tensor in train_loader:\n        print(input_tensor.shape)\n        print(target_tensor.shape)\n        break\n\n 训练\nimport time\n\nimport torch\nfrom torch import nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\n\nfrom src.NLP.SmartPrompt.src import config\nfrom src.NLP.SmartPrompt.src.dataset import get_dataloader\nfrom src.NLP.SmartPrompt.src.model import InputMethodModel\n\n\ndef train_onr_epoch(model, dataloader, loss_fn, optimizer, device):\n    &quot;&quot;&quot;\n    :param model: 模型\n    :param dataloader: 数据集\n    :param loss_fn: 损失函数\n    :param optimizer: 优化器\n    :param device: 设备\n    :return: 损失值\n    &quot;&quot;&quot;\n    model.train()\n    total_loss = 0\n    for batch in tqdm(dataloader, desc=&quot;训练中&quot;):\n        features, targets = batch\n        features = features.to(device)\n        targets = targets.to(device)\n        outputs = model(features)\n        loss = loss_fn(outputs, targets)\n        total_loss += loss.item()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    return total_loss / len(dataloader)\n\n\ndef train():\n    device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)\n    dataloader = get_dataloader(train=True)\n    with open(config.MODELS_DIR / &#x27;vocab.txt&#x27;, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as f:\n        vocab_list = f.readlines()\n        vocab_list = [vocab.strip() for vocab in vocab_list]\n    model = InputMethodModel(len(vocab_list)).to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n    writer = SummaryWriter(log_dir=config.LOGS_DIR/time.strftime(&#x27;%Y-%m-%d_%H-%M-%S&#x27;))\n    bestLoss = float(&#x27;inf&#x27;)\n    for epoch in range(1, 1 + config.EPOCHS):\n        print(&quot;-&quot; * 5, f&quot;Epoch &#123;epoch&#125;/&#123;config.EPOCHS&#125;&quot;, &quot;-&quot; * 5)\n        loss = train_onr_epoch(model, dataloader, loss_fn, optimizer, device)\n        print(f&quot;loss:&#123;loss&#125;&quot;)\n        writer.add_scalar(&#x27;loss&#x27;, loss, epoch)\n        if loss &lt; bestLoss:\n            bestLoss = loss\n            torch.save(model.state_dict(), config.MODELS_DIR / &#x27;model.pt&#x27;)\n    writer.close()\nif __name__ == &#x27;__main__&#x27;:\n    train()\n\n 模型\nfrom torch import nn\nimport config\n\n\nclass InputMethodModel(nn.Module):\n    def __init__(self, vocab_size, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=config.EMBEDDING_DIM)\n        self.rnn = nn.RNN(\n            input_size=config.EMBEDDING_DIM,\n            hidden_size=config.HIDDEN_SIZE,\n            batch_first=True,\n        )\n        self.linear = nn.Linear(in_features=config.HIDDEN_SIZE, out_features=vocab_size)\n\n    def forward(self, x, *args, **kwargs):\n        embedding = self.embedding(x)\n        output, hidden = self.rnn(embedding)\n        last_hidden_state = output[:,-1,:]\n        output = self.linear(last_hidden_state)\n        return output\n\n 预测\nimport jieba\nimport torch\n\nfrom src.NLP.SmartPrompt.src import config\nfrom src.NLP.SmartPrompt.src.model import InputMethodModel\n\ndevice = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)\n\nwith open(config.MODELS_DIR / &#x27;vocab.txt&#x27;, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as f:\n    vocab_list = f.readlines()\n    vocab_list = [vocab.strip() for vocab in vocab_list]\n\nword2index = &#123;word: index for index, word in enumerate(vocab_list)&#125;\nindex2word = &#123;index: word for index, word in enumerate(vocab_list)&#125;\nmodel = InputMethodModel(len(vocab_list)).to(device)\nmodel.load_state_dict(torch.load(config.MODELS_DIR / &#x27;model.pt&#x27;))\n\n\ndef predict(text):\n    tokens = jieba.lcut(text)\n    input = [word2index.get(token, 0) for token in tokens]\n    input_tenosr = torch.tensor([input], dtype=torch.long).to(device)\n    model.eval()\n    with torch.no_grad():\n        output = model(input_tenosr)\n    topk = torch.topk(output, k=5).indices.tolist()\n    return [index2word[index] for index in topk[0]]\n\n\nif __name__ == &#x27;__main__&#x27;:\n    user_str = &#x27;&#x27;\n    while user_str != &#x27;q&#x27;:\n        now_str = input(&quot;请输入：&quot;)\n        user_str += now_str\n        if now_str.strip() == &#x27;q&#x27;:\n            break\n        top5_tokens = predict(user_str)\n        print(top5_tokens)\n        print(&quot;当前输入：&quot;, user_str)\n\n LSTM\nLSTM灵感来原与计算机逻辑门。\n\n图片来源于：LSTM - 长短期记忆递归神经网络 - 知乎\n遗忘门：主要负责遗忘过去的记忆信息，最重要的是要去结合当下的状态信息去处理。\n输入门：输入门主要负责现在要记下什么，可以看到在输入门的右侧还有一个tanh的激活函数，\n输出门：\n\n 双向结构\n\n 注意力机制\n注意力机制的起源，是因为我们往往会聚焦于重要的信息。\n怎么做？\n我（查询对象Q），这张图（被查询对象V）\n我看一眼这张图，我就会去判断哪些东西对我而言是重要的，那些东西对我来说是是不重要的（去计算Q和V里的事物重要程度）。\n图片来自：https://www.cnblogs.com/nickchen121/p/16470710.html\n\n解码器在生成目标序列的每一步时，不再依赖于一个静态的上下文向量，而是根据当前的解码状态，动态地从编码器各个时间步的隐藏状态中选取最相关的信息，以辅助当前步生成。\n这种机制语法赋予模型对齐的能力，使其能够自动判断源句子中那些位置对当前目标词更为重要，从而有效缓解信息瓶颈问题，提升生成质量与表达能力。\n\n注意力机制实际上是在动态的提取当先最需要关心的数据。\n这一机制通常以下四个步骤来完成。\n\n相关性计算\n注意力权重计算\n上下文向量计算\n解码信息融合\n\n 相关性计算\n相关性的计算依赖于特定的函数，通常被称为注意力评分函数。\n\n 注意力权重计算\n得到所有源位置的注意力评分后，使用 Softmax 函数将其归一化为概率分布，作为注意力权重。得分越高的位置，其对应的权重越大，代表模型在当前生成中更关注该位置的信息。\n\n 上下文向量计算\n将所有编码器输出按照注意力权重进行加权求和，得到一个上下文向量。这个向量就表示当前时间步，模型从源句中提取出的关键信息。\n\n 解码信息融合\n在得到上下文向量后，解码器将其与当前时间步的隐藏状态进行拼接，以融合两者信息，最终通过线性变换和 Softmax，生成当前时间步目标词的概率分布。\n\n 注意力评分函数\n 点积评分\n它通过计算解码器当前时间步的隐藏状态与编码器每个时间步的隐藏状态的点积，来衡量二者之间的相关性。\n 通用点积评分\n通用点积评分在点积的基础上引入了一个可学习的权重矩阵W,用于先对编码器隐藏状态进行线性变换，再与解码器隐藏状态进行点积。\n 拼接评分\n拼接评分是一种表达能力更强的相关性评分方法。它的核心思想是：将解码器当前隐藏状态与编码器每个时间步的隐藏状态拼接为一个长向量，经过线性变换和非线性激活，最后用一个向量进行投影，得到最终打分值。\n\n Transformer\n\nTransformer其实就是Attention的一个堆叠。\n Transformer主要是在做什么\n把一个输入序列（如一段文字、一段语音、或一张图片的特征序列）——编码成表示语义的向量序列，然后（可选）再解码出另一个序列。Transformer 不是一个单纯的“算法”，而是一个序列建模框架。 它最擅长的事是：理解上下文中的关系，并基于此生成输出。\n\n Transformer的Encoder\nTransformer 的 Encoder 由 N 层堆叠的结构（通常是6层） 组成，每一层的结构几乎一样，每层包含两个主要子层：\n输入 → [多头自注意力机制] → [前馈神经网络] → 输出\n\n并且每个子层后都有：\n\n残差连接（Residual Connection）\n层归一化（Layer Normalization）\n\nRNN：逐个时间步处理序列，有记忆但慢。\nTransformer Encoder：一次性处理全序列，通过注意力机制“并行感知”所有词之间的关系，速度快、效果好。\n说白了还是在弄一个词向量，只不过这个词向量更加优秀！！\n每个 Encoder Layer都包含两个子层（sublayer），分别是自注意力子层（Self-Attention Sublayer）和前馈神经网络子层（Feed-Forward Sublayer）。\n\n 自注意力层\n自注意力机制的第一步，是将输入序列中的每个位置表示映射为三个不同的向量，分别是 查询（Query）、键（Key） 和 值（Value）。\n\nQuery：表示当前词的用于发起注意力匹配的向量；\nKey：表示序列中每个位置的内容标识，用于与 Query 进行匹配；\nValue：表示该位置携带的信息，用于加权汇总得到新的表示。\n自注意力的核心思想是：每个位置用自身的 Query 向量，与整个序列中所有位置的 Key 向量进行相关性计算，从而得到注意力权重，并据此对对应的 Value 向量加权汇总，形成新的表示。\n完成 Query、Key、Value 向量的生成后，模型会使用每个位置的 Query 向量与所有位置的 Key 向量进行相关性评分。\n在得到每个位置与所有位置之间的相关性评分后，模型会使用 softmax 函数进行归一化，确保每个位置对所有位置的关注程度之和为 1，从而形成一个有效的加权分布。对于整个序列，模型要做的是对之前得到的注意力评分矩阵的每一行进行softmax归一化。\n\n\n综上：\n\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n$$\n 前馈神经网络层\n\n Transformer的Decoder\n解码器接收编码器生成的词向量，然后通过这个词向量生成翻译的结果。\n 注意力与多头注意力\nimport torch\nimport torch.nn as nn\nimport math\n\n\nclass SelfAttention(nn.Module):\n\n    def __init__(self, dropout=0.1) -&gt; None:\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)  # 对10%的数据进行dropout，防止模型过拟合\n        self.softmax = nn.Softmax(dim=-1)  # 对最后一维进行softmax，为什么是最后一个维度？\n        # 因为是多头注意力机制，所以是多个头，每个头对输入进行softmax\n\n    def forward(self, Q, K, V, mask=None):\n        # X: [batch_size, seq_len, d_model]\n        # batch_size: 批次大小，一次送几个句子\n        # seq_len: 序列长度，一个句子中的Token数量\n        # d_model: 模型维度，这是个Embedding向量的维度\n        # Q：query向量     [batch_size, heads, seq_len_q, d_k]\n        # K：key向量       [batch_size, heads, seq_len_k, d_k]\n        # V：values向量    [batch_size, heads, seq_len)v, d_v]\n        # mask：掩码向量    [batch_size, seq_len, seq_len]   mask的意义是那些需要忽略，不要看见未来的信息\n        d_k = Q.size(-1)\n        # Q：                [batch_size, heads, seq_len_q, d_k]       seq_len_q, d_k\n        # K.T:               [batch_size, heads, d_k, seq_len_k]      d_k, seq_len_k\n        # attention_scores： [batch_size, heads, seq_len_q, seq_len_k]\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)  # (Q·K.T)/sqrt(d_k)\n        # 如果提供了mask，masked_fill则将mask中的值为0的元素替换为负无穷大\n        if mask is not None:\n            attention_scores = attention_scores.masked_fill(mask == 0, float(&#x27;-inf&#x27;))\n        # 获取注意力权重，对最后一位进行softmax\n        attn = self.softmax(attention_scores)\n        attn = self.dropout(attn)\n        # 获取注意力结果，对V进行矩阵乘法，[batch_size, heads, seq_len_q, d_v]\n        output = torch.matmul(attn, V)\n        return output, attn\n\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, d_model, n_heads, dropout=0.1) -&gt; None:\n        super().__init__()\n        # d_model: 模型维度，这是个Embedding向量的维度，一般是512\n        # n_heads: 多头注意力机制的个数，一般是8\n        assert d_model % n_heads == 0\n        self.d_k = d_model // n_heads\n        self.n_heads = n_heads\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.fc = nn.Linear(d_model, d_model)\n        self.attention = SelfAttention(dropout)\n        self.layer_norm = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, q, k, v, mask=None):\n        batch_size = q.size(0)\n        # Q: [batch_size, seq_len, d_model] -&gt; [batch_size, n_heads, seq_len, d_k]\n        # 为了获取多头注意力，需要将输入进行分割，分割成多个头，每个头对输入进行计算，然后进行拼接\n        Q = self.W_q(q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(k).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(v).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        output, attn = self.attention(Q, K, V, mask)\n        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)\n        output = self.fc(output)\n        output = self.layer_norm(q + self.dropout(output))\n        return output, attn\n\n Torch中使用TransformerAPI\ntorch.nn.Transformer(d_model=512, \n                     nhead=8, \n                     num_encoder_layers=6, \n                     num_decoder_layers=6, \n                     dim_feedforward=2048, \n                     dropout=0.1, \n                     activation=&#x27;relu&#x27;, \n                     custom_encoder=None, \n                     custom_decoder=None, \n                     layer_norm_eps=1e-05, \n                     batch_first=False, \n                     norm_first=False, \n                     bias=True, \n                     device=None, \n                     dtype=None)\n\n transformer\nfrom torch import nn\n\n# 初始化 Transformer\ntransformer = nn.Transformer(\n  d_model=512, \n  nhead=8, \n  num_encoder_layers=6, \n  num_decoder_layers=6, \n  batch_first=True\n)\n\n forward\noutput = transformer(\n    src=src_emb,\n    tgt=tgt_emb,\n    src_key_padding_mask=src_pad_mask,\n    tgt_key_padding_mask=tgt_pad_mask,\n    tgt_mask=tgt_mask,\n    memory_key_padding_mask=src_pad_mask\n)\n\n encoder\nfrom torch import nn\n\n# 初始化 Transformer\ntransformer = nn.Transformer(\n    d_model=512, nhead=8,\n    num_encoder_layers=6, num_decoder_layers=6,\n    batch_first=True\n)\n\n# 调用编码器\nmemory = transformer.encoder(\n    src=src_emb, \n    src_key_padding_mask=src_pad_mask\n)\n\n decoder\nfrom torch import nn\n\n# 初始化 Transformer\ntransformer = nn.Transformer(\n    d_model=512, nhead=8,\n    num_encoder_layers=6, num_decoder_layers=6,\n    batch_first=True\n)\n\n# 调用编码器\nmemory = transformer.encoder(\n    src=src_emb, \n    src_key_padding_mask=src_pad_mask\n)\n\n# 调用解码器（逐步生成）\noutput = transformer.decoder(\n    tgt=tgt_emb,\n    memory=memory,\n    tgt_mask=tgt_mask,\n    tgt_key_padding_mask=tgt_pad_mask,\n    memory_key_padding_mask=src_pad_mask\n)\n\n 参考文献\n\n【[手把手教学]基于RNN、LSTM神经网络单特征用电负荷预测】https://www.bilibili.com/video/BV1HN4y1Y7Lt\n【尚硅谷NLP教程，nlp自然语言处理，Transformer、LSTM、BERT等大模型技术全覆盖】https://www.bilibili.com/video/BV1k44LzPEhU\n【Word2Vec模型、Attention、Transformer 】https://space.bilibili.com/383551518\n【Transformer模型详解（图解最完整版）】https://zhuanlan.zhihu.com/p/338817680\n【黑马程序员AI大模型《神经网络与深度学习》】https://www.bilibili.com/video/BV1c5yrBcEEX\n\n","categories":["深度学习","深度学习基础"],"tags":["python","深度学习","RNN","LSTM","Transformer","NLP","GRU"]},{"title":"NumPy核心处理方法","url":"/python/numpy/","content":" NumPy核心处理方法\nNumPy（Numerical Python）是一个用于科学计算的 Python 第三方库，它提供了：\n 核心功能：\n\n多维数组对象\n\n高效地存储和操作大规模数字数据。\n比原生 Python 的列表（list）更快、更节省内存。\n\n\n广播功能\n\n允许不同形状的数组进行数学运算，简洁高效。\n\n\n数学函数库\n\n包括线性代数、傅里叶变换、随机数生成、统计分析等。\n\n\n数组索引与切片\n\n支持布尔索引、花式索引，比 Python 原生更灵活强大。\n\n\n\n 引用\nimport numpy as np\n\n 多维性\narr = np.array(5)  # 创建了一个0维度的数组\nprint(arr)\nprint(&#x27;arr的维度：&#x27;, arr.ndim)  # 打印维度ndim\n\narr = np.array([1, 2, 3])  # 创建了一个1维度的数组\nprint(arr)\nprint(&#x27;arr的维度：&#x27;, arr.ndim)  # 打印维度ndim\n\n 同质性\narr = np.array([1, &#x27;hello&#x27;, 3])  #测试不同质\nprint(arr)\n\n结果全部转化为字符串\n ndarray的属性\narr = np.array(1)\nprint(arr)\nprint(&#x27;arr的维度：&#x27;, arr.ndim)\nprint(&#x27;arr的形状：&#x27;, arr.shape)\nprint(&#x27;arr的元素个数&#x27;, arr.size)\nprint(&#x27;arr的数据类型&#x27;, arr.dtype)\n\n\n1\narr的维度： 0\narr的形状： ()\narr的元素个数 1\narr的数据类型 int64\n\narr = np.array([1, 2.7, 3])\nprint(arr)\nprint(&#x27;arr的维度：&#x27;, arr.ndim)\nprint(&#x27;arr的形状：&#x27;, arr.shape)\nprint(&#x27;arr的元素个数&#x27;, arr.size)\nprint(&#x27;arr的数据类型&#x27;, arr.dtype)\nprint(&#x27;arr的转置&#x27;, arr.T)\n\n\n[1.  2.7 3. ]\narr的维度： 1\narr的形状： (3,)\narr的元素个数 3\narr的数据类型 float64\narr的转置 [1.  2.7 3. ]\n\narr = np.array([[1, 2, 3], [4, 5, 6]])\nprint(arr)\nprint(&#x27;arr的维度：&#x27;, arr.ndim)\nprint(&#x27;arr的形状：&#x27;, arr.shape)\nprint(&#x27;arr的元素个数&#x27;, arr.size)\nprint(&#x27;arr的数据类型&#x27;, arr.dtype)\nprint(&#x27;arr的转置&#x27;, arr.T)\n\n\n[[1 2 3]\n[4 5 6]]\narr的维度： 2\narr的形状： (2, 3)\narr的元素个数 6\narr的数据类型 int64\narr的转置 [[1 4]\n[2 5]\n[3 6]]\n\n 指定元素数据类型\nlist1 = [4, 5, 6]\narr = np.array(list1, dtype=np.float64)\n\n 预定义数组\n 全零阵\narr = np.zeros((4, 4), dtype=np.int64)\narr = np.zeros((60,), dtype=np.int64)\n\n 全一阵\narr = np.ones((4, 4), dtype=np.int64)\n\n 空阵\narr = np.empty((3, 4))\n\n 满阵\narr = np.full((3, 4), 2)\n\n\n[[2 2 2 2]\n[2 2 2 2]\n[2 2 2 2]]\n\n 形状模仿\narr1 = np.zeros_like(arr)\narr2 = np.ones_like(arr)\narr3 = np.empty_like(arr)\n\n 定增量（等差数列）\narr = np.arange(1, 106, 1)\n\n 定间隔（等分数列）\napp = np.linspace(1, 100, 10)\n\n 对角阵\narr = np.diag([1, 2, 3, 4, 5])\n\n 随机数组\n 默认零到一\narr = np.random.rand(2, 4)# 均匀分布， 0~1\n\n 指定区间\narr = np.random.uniform(1, 4, (4, 4))\n\n 随机整数\narr = np.random.randint(1, 10, (4, 4))\n\n 随机正态分布\narr = np.random.randn(4, 4)\n\n 随机种子\n# 设置随机种子, 每次生成的随机数相同\nnp.random.seed(20)\narr = np.random.randint(1, 4, (4, 4))\n\n 单位矩阵\narr = np.eye(5, dtype=int)\n\n 数据类型\n ndarray的数据类型\n\n布尔类型\n整形\n浮点型\n复数型\n\n主要通过dtype指定\narr = np.array([1, 2, 3, 0, -1], dtype=np.int8)\narr = np.array([1, 2, 3, 0, -1], dtype=np.float32)\n\n 索引与切片\n 索引\n# 一维数组\narr = np.random.randint(10, 100, 30)\n# 一维索引\nprint(arr[3])\nprint(arr[7])\n# 一维布尔索引\nprint(arr[(arr &gt; 30) &amp; (arr &lt; 50)])\n# 二维数组\narr = np.random.randint(10, 100, (5, 8))\n# 二维索引\nprint(arr[2, 3])\nprint(arr[2][3])\n\n 切片\n# 一维切片\nprint(arr[3:7])\nprint(arr[slice(3, 7)])\nprint(arr[slice(0, 10, 2)])\n# 二维切片\nprint(arr[0, 1:3])\nprint(arr[arr &gt; 70])\nprint(arr[0][arr[0] &gt; 90])\nprint(arr[3])\nprint(arr[:, 3])\n\n 运算\n 常规运算\narr1 = np.array([1, 2, 3])\narr2 = np.array([4, 5, 6])\nprint(arr1 + arr2)\nprint(arr1 * arr2)\nprint(arr1 - arr2)\nprint(arr1 / arr2)\nprint(arr1 ** arr2)\n# python原生\nlistA = [1, 2, 3]\nlistB = [4, 5, 6]\nfor i in range(len(listA)):\n    print(listA[i] + listB[i])\n\npython原生只是纯粹的拼接，ndarray是在帮你做计算\narr1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\narr2 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(arr1 + arr2)\nprint(arr1 * arr2)\nprint(arr1 - arr2)\nprint(arr1 / arr2)\nprint(arr1 ** arr2)\n\n 广播机制\n# 广播机制，但是需满足维度一致\narr1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\narr2 = np.array([1, 2, 3])  # [1, 2, 3],[1, 2, 3],[1, 2, 3]\nprint(arr1 + arr2)\nprint(arr1 * arr2)\n\narr1 = np.array([[1, 2, 3]])\narr2 = np.array([[1], [2], [3]])\nprint(arr1 + arr2)\n\n 矩阵的运算\n# 矩阵运算\narr1 = np.array([[1, 2, 3], [4, 5, 6]])\narr2 = np.array([[4, 5], [1, 2], [7, 8]])\nprint(arr1.dot(arr2))  # 矩阵的乘法，矩阵的行数必须等于矩阵的列数，不是简单的对应相乘\nprint(arr1 @ arr2)\n\n 常用函数\n 数学函数\n 平方根\nprint(np.sqrt(9))\nprint(np.sqrt([1, 4, 9]))\n\n 指数\n# 计算指数\nprint(np.exp(1))  #  e^1\nprint(np.exp([0, 2, 3]))\n\n 自然对数\n# 计算自然对数\nprint(np.log(np.e))\nprint(np.log([1, np.e, np.e ** 2]))\nprint(np.log10(100))\n\n 三角函数\n# 计算三角函数\nprint(np.sin(np.pi / 2))  # sin(π/2)\nprint(np.cos(np.pi))  # cos(π)\n\n 绝对值\n# 计算绝对值\nprint(np.abs(-1))\nprint(np.abs(np.array([1, -1, 0, -2, 2, -3, 3, -4, 4, -5, 5])))\n\n a的b次幂\n# 计算a的b次幂\nprint(np.power(2, 3))\n\n 四舍五入\n# 四舍五入\nprint(np.round(3.14))\nprint(np.round([3.14, 2.7, 1.5, 1.2]))\n\n 上取整与下取整\n# 向上取整，向下取整\narr = np.array([1.2, 1.5, 1.6, 1.7, 2.2, 2.5, 2.6, 2.7])\nprint(np.ceil(arr))\nprint(np.floor(arr))\n\n 检测缺失值\narr = np.array([1, 2, np.nan, 4, np.nan])\nprint(np.isnan(arr))\n\n 统计函数\n求和、计算平均值、计算中位数、标准差、方差、最大值、最小值、计算分位数、累积和、累积差\narr = np.random.randint(1, 10, 10)\nprint(arr)  # 创建一个10个元素的数组\nprint(&#x27;排序&#x27;, np.sort(arr))\nprint(&#x27;求和&#x27;, np.sum(arr))  # 求和\nprint(&#x27;计算平均值&#x27;, np.mean(arr))  # 计算平均值\nprint(&#x27;计算中位数&#x27;, np.median(arr))  # 计算中位数\nprint(&#x27;标准差&#x27;, np.std(arr))  # 标准差 \nprint(&#x27;方差&#x27;, np.var(arr))  # 方差 ((1-2)^2 + (2-2)^2 + ... + (9-2)^2) / 9\nprint(&#x27;最大值&#x27;, np.max(arr))  # 最大值\nprint(&#x27;最小值&#x27;, np.min(arr))  # 最小值\nprint(&#x27;计算分位数&#x27;, np.quantile(arr, 0.5))  # 计算分位数\nprint(&#x27;累积和&#x27;, np.cumsum(arr))  # 累积和\nprint(&#x27;累积差&#x27;, np.cumprod(arr))  # 累积差\n\n 比较函数\n# 是否大于\nprint(np.greater([1, 3, 5, 1, 8, 0, -23, 3], 2))\n# 是否小于\nprint(np.less([1, 3, 5, 1, 8, 0, -23, 3], 2))\n# 是否等于\nprint(np.equal([1, 3, 5, 1, 8, 0, -23, 3], 3))\n\n 逻辑运算\n# 逻辑运算\nprint(np.logical_and([1, 3, 5, 1, 8, 0, -23, 3], [1, 3, 5, 1, 8, 0, -23, 3]))\nprint(np.logical_or([1, 3, 5, 1, 8, 0, -23, 3], [1, 3, 5, 1, 8, 0, -23, 3]))\n\n 自定义条件\narr = np.array([1, 3, 5, 1, 8, 0, -23, 3])\nprint(np.where(arr &gt; 3, arr, 0))\nprint(np.select([arr &gt; 3, arr &lt; 0], [arr, 0], default=arr))\n\n 排序函数\narr = np.random.randint(1, 10, 20)\nprint(arr)\nprint(np.sort(arr))\nprint(np.argsort(arr))  # 返回排序后的索引\n\n 去重\narr = np.array([1, 3, 5, 1, 8, 0, -23, 3])\nprint(np.unique(arr))\n\n 数组的拼接\narr1 = np.array([1, 2, 3])\narr2 = np.array([4, 5, 6])\nprint(np.concatenate((arr1, arr2)))\n\n 数组分割\narr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nprint(np.split(arr, [3, 5]))  # 分成3份\n\n 调整形状\narr = np.random.randint(1, 10, 20)\nprint(arr)\nprint(arr.reshape(4, 5))\n\n 案例练习\n 温度数据分析\n某城市一周的最高气温（℃）为 [28, 30, 29, 31, 32, 30, 29]。\n\n计算平均气温、最高气温和最低气温。\n找出气温超过 30℃ 的天数。\n\narr = np.array([28, 30, 29, 31, 32, 30, 29])\nprint(&#x27;平均天气：&#x27;, np.mean(arr))\nprint(&#x27;最高气温：&#x27;, np.max(arr))\nprint(&#x27;最低气温：&#x27;, np.min(arr))\nprint(&#x27;气温超过30℃的天数：&#x27;, np.sum(arr &gt; 30))\n\n 学生成绩统计\n某班级 5 名学生的数学成绩为 [85, 90, 78, 92, 88]。\n\n计算成绩的平均分、中位数和标准差。\n将成绩转换为百分制（假设满分为 100）。\n\narr = np.array([85, 90, 78, 92, 88])\nprint(&#x27;平均成绩：&#x27;, np.mean(arr))\nprint(&#x27;中位数：&#x27;, np.median(arr))\nprint(&#x27;标准差：&#x27;, np.std(arr))\nprint(&#x27;成绩转换为百分制：&#x27;, arr / 100 * 100)\n\n 数组变形\n题目：创建一个 1 到 12 的一维数组，并转换为 (3, 4) 的二维数组。\n\n计算每行的和与每列的平均值。\n将数组展平为一维数组。\n\narr = np.linspace(1, 12, 12, dtype=int)\narr = arr.reshape(3, 4)\nprint(arr)\nprint(arr.sum(axis=1))\nprint(arr.mean(axis=0))\nprint(arr.flatten())\n\n","categories":["深度学习","深度学习基础"],"tags":["python","python_NumPy"]},{"title":"Pandas核心处理方法","url":"/python/pandas/","content":" Pandas核心处理方法\n 什么是Pandas\nPandas 是一个基于 Python 的开源数据分析和数据处理库\n它提供了两种核心数据结构：\n\n\n\n数据结构\n描述\n类似于\n索引\n数据存储\n类比\n\n\n\n\nSeries\n一维带标签的数组\n一列\n单索引\n同质化\nExcel单列\n\n\nDataFrame\n二维带标签的表格数据结构\n一张表格（像 Excel 表）\n双索引\n各个列之间可以是不同的\n整个Excel表格\n\n\n\n应用场景：\n\n数据预处理（机器学习前）\n数据统计分析\n自动化报表\n数据可视化前处理\n\n Series\nSeries就像这样：\n\n\n\nSeries Index\nSeries Name\n\n\n\n\n1\nSeries Values\n\n\n2\nSeries Values\n\n\n3\nSeries Values\n\n\n\n 创建方法\n# series的创建\nimport pandas as pd\ns = pd.Series([1,2,3,4,5])\nprint(s)\n\n# 自定义索引\ns = pd.Series([1, 2, 3, 4, 5], index=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;])\nprint(s)\n\n# 定义Name\ns = pd.Series([1, 2, 3, 4, 5], index=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;], name=&#x27;我是xxx&#x27;)\nprint(s)\n\n左侧是索引，右侧是值\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\na    1\nb    2\nc    3\nd    4\ne    5\ndtype: int64\na    1\nb    2\nc    3\nd    4\ne    5\nName: 我是xxx, dtype: int64\n\n# 通过字典方式来创建\ns = pd.Series(&#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3, &#x27;d&#x27;: 4, &#x27;e&#x27;: 5&#125;)\nprint(s)\n# 通过Series对象来创建（截取）\ns1=pd.Series(s,index=[&#x27;a&#x27;,&#x27;c&#x27;])\nprint(s1)\n\n 基本属性\n\n\n\n属性\n说明\n示例\n\n\n\n\nindex\n行索引（标签）对象\ns.index\n\n\nvalues\n数据值组成的 numpy 数组\ns.values\n\n\ndtype\n数据类型\ns.dtype\n\n\nsize\n元素总个数\ns.size\n\n\nshape\n数据结构的形状（长度，）\ns.shape\n\n\nndim\n维度，Series 永远是 1\ns.ndim\n\n\nname\nSeries 的名称（可自定义）\ns.name\n\n\nis_unique\n是否全是唯一值\ns.is_unique\n\n\nhasnans\n是否含有缺失值（NaN）\ns.hasnans\n\n\n\n# Series对象属性\ns = pd.Series([1, 2, 3, 4, 5], index=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;])\nprint(s.values)\nprint(s.index)\nprint(s.name)\nprint(s.dtype)\nprint(s.size)\nprint(s.empty)\nprint(s.ndim)\nprint(s.shape)\nprint(s.is_unique)\nprint(s.loc[&#x27;e&#x27;])  # 通过索引获取，显示索引\nprint(s.iloc[4])  # 通过位置获取，隐式索引\n# 支持切片\nprint(s.loc[&#x27;a&#x27;:&#x27;d&#x27;])\nprint(s.iloc[0:3])\n\n 访问数据\nprint(s.iloc[2])\nprint(s[&#x27;b&#x27;])\nprint(s[s &lt; 3]) # 布尔索引\nprint(s.isin([1, 2, 3]))\nprint(s[s.isin([1, 2, 3])])\n\ns = pd.Series([1, np.nan, 3, None, 5, 6, 7, 8, 9, 10], index=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;g&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;j&#x27;], name=&#x27;testData&#x27;)\ns.head()\ns.tail(2)\n\n 计算与常用统计方法\n# 所有描述信息的语句\ns.describe()\n\n# 是否包含在其中\ns.isin([1, 9, 3])\n\n# 排序\ns.sort_values()\n\n# 分位数\ns.quantile(0.5)\n\n# 统计个数\ns.value_counts()\n\n# 众数\ns.mode()\n\n Series案例\n\n创建一个包含十名学生数学成绩的Series，成绩范围在50到100间，计算平均分、最高分、最低分、并找出高于平均分的学生人数。\n\nprint(scores.mean())\nprint(scores.max())\nprint(scores.min())\nprint(scores[scores.mean() &lt; scores].count())\n\n\n给定某城市一周每天的最高温度Series，完成以下任务：\n找出温度超过30度的天数\n计算平均温度将温度从高到低排序\n找出温度变化最大的两天\n\nprint(temperatures[temperatures &gt; 30].count())\nprint(temperatures.sort_values(ascending=False))\ntemperatures.diff().abs().sort_values(ascending=False).head(2)\n\n\n股票价格分析给定某股票连续10个交易日的收盘价Series:\n计算每日收益率(当日收盘价/前日收盘价-1)\n找出收益率最高和最低的日期\n计算波动率(收益率的标准差)\n\n# 计算收益率 pct-&gt;percent \nprice.pct_change()\na = price.pct_change()\na.idxmax()\nprice.pct_change().idxmin()\nprice.pct_change().std()\n\n\n某产品过去12个月的销售量Series:\n计算季度平均销量(每3个月为一个季度)\n找出销量最高的月份\n计算月环比增长率\n找出连续增长超过2个月的月份\n\n# resample-&gt;qs按照季度开始重新采样\nsales.resample(&#x27;QS&#x27;).mean()\nsales.idxmax()\n# 月百分比变化，即为环比\nsales.pct_change()\n# 环比大于零，使用滑动窗口，和为3\npct_c = sales.pct_change()\nb = pct_c &gt; 0\nb[b.rolling(window=3).sum() == 3].keys().tolist()\n\n\n某商店每小时销售额Series:\n按天重采样计算每日总销售额\n计算每天营业时间(8:00-22:00)和非营业时间的销售额比例\n找出销售额最高的3个小时\n\nhourly_sales.resample(&#x27;D&#x27;).sum()\n# 两种方法\nhourly_sales[(hourly_sales.index.hour&gt;=8)&amp;(hourly_sales.index.hour&lt;=22)].sum()\nhourly_sales.between_time(&#x27;8:00&#x27;, &#x27;22:00&#x27;)\n# 最终计算\nhourly_sales[(hourly_sales.index.hour&gt;=8)&amp;(hourly_sales.index.hour&lt;=22)].sum()/(hourly_sales.sum() - hourly_sales[(hourly_sales.index.hour&gt;=8)&amp;(hourly_sales.index.hour&lt;=22)].sum())\n# 可在全部的索引中去除营业索引\nhourly_sales.drop(hourly_sales[(hourly_sales.index.hour&gt;=8)&amp;(hourly_sales.index.hour&lt;=22)].index)\n# 销售额最高的3个小时，两种方法\nhourly_sales.sort_values(ascending=False).head(3)\nhourly_sales.nlargest(3).keys()\n\n DataFrame\n 读取JSONL\npd.read_json(&quot;../data/raw/synthesized_.jsonl&quot;, lines=True)\n\n 保存为JSON / JSONL\nimport pandas as pd\n\nframe = pd.DataFrame(dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)), columns=[&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;], index=[&#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;])\nprint(frame)\nprint(frame.to_json(orient=&#x27;index&#x27;))\nprint(frame.to_json(orient=&#x27;columns&#x27;))vprint(frame.to_json(orient=&#x27;split&#x27;))\nprint(frame.to_json(orient=&#x27;records&#x27;, lines=True))\n\n\nA  B  C\nx  1  4  7\ny  2  5  8\nz  3  6  9\n{“x”:{“A”:1,“B”:4,“C”:7},“y”:{“A”:2,“B”:5,“C”:8},“z”:{“A”:3,“B”:6,“C”:9}}\n{“A”:{“x”:1,“y”:2,“z”:3},“B”:{“x”:4,“y”:5,“z”:6},“C”:{“x”:7,“y”:8,“z”:9}}\n{“columns”:[“A”,“B”,“C”],“index”:[“x”,“y”,“z”],“data”:[[1,4,7],[2,5,8],[3,6,9]]}\n{“A”:1,“B”:4,“C”:7}\n{“A”:2,“B”:5,“C”:8}\n{“A”:3,“B”:6,“C”:9}\n\n","categories":["深度学习","深度学习基础"],"tags":["python","python_Pandas"]},{"title":"PyTorch深度学习","url":"/python/pytorch/","content":" PyTorch深度学习\n基于小土堆：https://www.bilibili.com/video/BV1hE411t7RN\n 安装\n基于conda环境来安装\nconda create --name pytorch python=3.11\n\n查询已有环境\nconda info --envs\n\n\nconda environments:\nbase                   * D:\\anaconda\npytorch              D:\\anaconda\\envs\\pytorch\n\n激活pytorch环境\nconda activate pytorch\n\n其他命令\nconda remove -n xxxxx(名字) --all\t# 环境删除命令\ndeactivate\t# 退出虚拟环境\npip list\t# 查看虚拟环境的库\n\n访问pytorch网站：https://pytorch.org/get-started/locally/\n\n截至2025年8月Start Locally条目给予python版本提示：\nNOTE: Latest PyTorch requires Python 3.9 or later.\n\n基于：Stable(2.7.1) - Windows - Pip - Python - CUDA 11.8\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n(pytorch) C:\\Users\\Karry&gt;pip list\nPackage           Version\n----------------- ------------\nfilelock          3.13.1\nfsspec            2024.6.1\nJinja2            3.1.4\nMarkupSafe        2.1.5\nmpmath            1.3.0\nnetworkx          3.3\nnumpy             2.1.2\npillow            11.0.0\npip               25.1\nsetuptools        78.1.1\nsympy             1.13.3\ntorch             2.7.1+cu118\ntorchaudio        2.7.1+cu118\ntorchvision       0.22.1+cu118\ntyping_extensions 4.12.2\nwheel             0.45.1\n\n验证：\nimport torch\ntorch.cuda.is_available()\t# True\n\n 两大法宝函数-dir与help\ndir主要是来查看一个工具包下还有什么子工具包或者工具\nhelp主要是查看一些工具有什么作用\n比如help(torch.cuda.is_available)\nhelp(torch.cuda.is_available)\nHelp on function is_available in module torch.cuda:\nis_available() -&gt; bool\n    Return a bool indicating if CUDA is currently available.\n\n\n PyTorch数据读取\n数据 —— Dataset（提供一种方式获取label） —— Dataloader（为后面的网络提供不同的数据形式）\n组织结构：\n+—— hymenoptera_data\n| +—— train\n| | —— ants\n| | —— bees\n\n对应代码：\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport os\n\n\n# MyData是自定义的数据集类，他继承了Dataset类，MyData(Dataset)这是继承动作\nclass MyData(Dataset):\n    # 初始化函数，初始化一些数据，比如图片路径，标签路径等等，__init__这是重写父类的方法\n    def __init__(self, root_dir, label_dir):\n        # 根集路径\n        self.root_dir = root_dir\n        # 标签路径，这其实表达了图片是什么分类\n        self.label_dir = label_dir\n        # 获取图片的路径，这个路径下存放着图片\n        self.path = os.path.join(self.root_dir, self.label_dir)\n        # 获取图片路径列表listdir\n        self.img_path = os.listdir(self.path)\n\n    # 获取数据，这个函数是必须写的，并且是必须返回两个值，一个是图片，一个是标签\n    def __getitem__(self, idx):\n        img_name = self.img_path[idx]\n        img_item_path = os.path.join(self.root_dir, self.label_dir, img_name)\n        img = Image.open(img_item_path)\n        label = self.label_dir\n        return img, label\n\n    def __len__(self):\n        return len(self.img_path)\n\n\nroot_dir = &quot;hymenoptera_data\\\\train&quot;\nant_label_dir = &quot;ants&quot;\nbees_label_dir = &quot;bees&quot;\nants_dataset = MyData(root_dir, ant_label_dir)\nbees_dataset = MyData(root_dir, bees_label_dir)\n\ntrain_dataset = ants_dataset + bees_dataset\n\n组织结构：\n+—— hymenoptera_data_ex\n| +—— train\n| | —— ants_image\n| | —— ants_label\n| | —— bees_image\n| | —— bees_label\n\n对应代码：\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport os\n\n\nclass MyData(Dataset):\n    def __init__(self, root_dir, img_dir, label_dir):\n        self.root_dir = root_dir\n        self.img_dir = img_dir\n        self.label_dir = label_dir\n        self.path_img = os.path.join(self.root_dir, self.img_dir)\n        self.path_label = os.path.join(self.root_dir, self.label_dir)\n        self.img_path = os.listdir(self.path_img)\n        self.label_path = os.listdir(self.path_label)\n\n    def __getitem__(self, item):\n        img_name = self.img_path[item]\n        img_item_path = os.path.join(self.path_img, img_name)\n        img = Image.open(img_item_path)\n        label_name = self.label_path[item]\n        label_item_path = os.path.join(self.path_label, label_name)\n        with open(label_item_path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:\n            label = f.read()\n            return img, label\nroot_dir = &quot;hymenoptera_data_ex\\\\train&quot;\nimg_dir = &quot;ants_image&quot;\nlabel_dir = &quot;ants_label&quot;\nants_dataset = MyData(root_dir, img_dir, label_dir)\n\n Tensorboard的使用\n add_scalar\nfrom torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter(&quot;logs&quot;)\n# writer.add_image()\n# y = x\nfor i in range(100):\n    writer.add_scalar(&quot;y=i^2&quot;, i * i, i)\n\nwriter.close()\n\n tensorboard --logdir=logs --port=6007\n\n\nwriter.add_scalar(“y=i^2”, i * i, i)，这个的参数分别是：标签、y、x\n add_image\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.tensorboard import SummaryWriter\n\nimg_path = &quot;hymenoptera_data/train/ants/0013035.jpg&quot;\nimg_PIL = Image.open(img_path)\nimg_array = np.array(img_PIL)\n# hymenoptera_data/train/ants/0013035.jpg\nwriter = SummaryWriter(&quot;logs&quot;)\nwriter.add_image(&quot;img&quot;, img_array, 1,dataformats=&quot;HWC&quot;)\n# y = x\nfor i in range(100):\n    writer.add_scalar(&quot;y=i^2&quot;, i * i, i)\n\nwriter.close()\n\n\nadd_image(“img”, img_array, 1,dataformats=“HWC”)，这个参数分别是：标签、ndarray类型的图片、步骤、dataformats-HWC\n Transforms的使用\nTransforms主要是对图片的各种变换，是预处理？\nfrom PIL import Image\nfrom torchvision import transforms\n\nimg = Image.open(&#x27;hymenoptera_data/train/ants/0013035.jpg&#x27;)\ntensor_trans = transforms.ToTensor()\ntensor_img = tensor_trans(img)\nprint(tensor_img)\n\nImage.open返回了PIL类型的图片，通过transforms.ToTensor()创建了工具对象，最后使用tensor_trans(img)转化为tensor类型\n\ntensor数据类型：包装了神经网络所需要的理论基础参数\n\n使用SummaryWriter将tensor_img写入\nfrom PIL import Image\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import transforms\n\nimg = Image.open(&#x27;hymenoptera_data/train/ants/0013035.jpg&#x27;)\ntensor_trans = transforms.ToTensor()\ntensor_img = tensor_trans(img)\n# print(tensor_img)\n\nwriter = SummaryWriter(&#x27;logs&#x27;)\nwriter.add_image(&#x27;tensor_img&#x27;, tensor_img)\nwriter.close()\n\n\n\n题外话，理解一下py的面向对象：\nclass Person:\n    def __init__(self, name):\n        self.name = name\n\n    def __call__(self):\n        print(&quot;hello&quot;, self.name)\n\n    def hello(self):\n        print(&quot;hello_ex&quot;, self.name)\n\n\nperson = Person(&quot;Karry&quot;)\nperson()\nperson.hello()\n\ninit方法实际上是一个构造方法，person = Person(“Karry”)执行后调用构造方法\ncall方法像初次见面问好一样person()执行后，调用call方法，是让实例像函数一样用的钩子方法\nperson.hello()只是一个类的普通方法\n\n ToTensor的使用\nToTensor图片张量化工具\nfrom PIL import Image\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import transforms\n\nwriter = SummaryWriter(&#x27;logs&#x27;)\nimg = Image.open(&#x27;images/pytorch.png&#x27;)\ntoTensorTools = transforms.ToTensor()\nimg_tensor = toTensorTools(img)\nwriter.add_image(&#x27;ToTensor&#x27;, img_tensor)\nwriter.close()\n\n Normalize的使用\nNormalize归一化、标准化，均值为0，方差为1，数值位于-1到1之间\n如果图片不是RGB模式需要做img.convert(‘RGB’)\nwriter = SummaryWriter(&#x27;logs&#x27;)\nimg = Image.open(&#x27;images/pytorch.png&#x27;)\n# img转化为RGB\nimg = img.convert(&#x27;RGB&#x27;)\ntoTensorTools = transforms.ToTensor()\nimg_tensor = toTensorTools(img)\nwriter.add_image(&#x27;ToTensor&#x27;, img_tensor)\nprint(img_tensor[0][0][0])\n# 归一化\ntransforms_normalize = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\nimg_normalize = transforms_normalize(img_tensor)\nprint(img_normalize[0][0][0])\n\nwriter.close()\n\ntensor(0.1333)\ntensor(-0.7333)\n0.1333*2 - 1 = -0.7333\n Resize的使用\nResize重调整\n# Resize的使用\nprint(img.size)\ntransforms_resize = transforms.Resize([128, 128])\nimg_resize = transforms_resize(img_tensor)\nwriter.add_image(&#x27;Resize&#x27;, img_resize, 0)\nprint(img_resize.size())\n\nresize_2 = transforms.Resize(512)\ntransforms_compose = transforms.Compose([resize_2, toTensorTools])\nimg_resize_2 = transforms_compose(img)\nwriter.add_image(&#x27;Compose&#x27;, img_resize_2, 1)\n\n起初我们使用img = Image.open(‘images/testFG.jpg’)，此时这是一个PIL图片，如何我们使用toTensorTools = transforms.ToTensor()创建张量转换工具，使用img_tensor = toTensorTools(img)将PIL图片转化为img_tensor张量图片，紧接着我们使用transforms_resize = transforms.Resize([128, 128])创建尺寸调整工具，使用img_resize = transforms_resize(img_tensor)对张量图片重调整。\nCompose的意义在于它可以将多个图像变换操作（如缩放、裁剪、归一化等）按顺序组合成一个流水线，输入图像会依次通过这些变换。\n我们使用transforms_compose = transforms.Compose([resize_2, toTensorTools])创建了一个工具链，resize_2用于调整图像尺寸，toTensorTools用于其转化为张量图片。\n从img_resize_2 = transforms_compose(img)我们可以看到，参数img是一个PIL图片他通过Compose先后进行了重调整和张量化，最终返回img_resize_2张量图片。\n\n提示：resize_2 = transforms.Resize(512)是一个等比例调整。\ntensor(0.4039)\ntensor(0.6797)\n(3600, 2700)\ntorch.Size([3, 128, 128])\ntorch.Size([3, 512, 682])\n\n RandomCrop的使用\nRandomCrop随机裁剪\n# RandomCrop的使用\n# random_crop = transforms.RandomCrop(500, 1000)\nrandom_crop = transforms.RandomCrop(512)\ncompose_random_crop = transforms.Compose([random_crop, toTensorTools])\nfor i in range(10):\n    img_random_crop = compose_random_crop(img)\n    writer.add_image(&#x27;RandomCrop&#x27;, img_random_crop, i)\n\n 使用TorchVision的数据集（DataSet）\nCIFAR —— Canadian Institute For Advanced Research（加拿大高级研究所）\nroot数据集所在目录、train是训练集还是测试集、transform应用的变换操作或操作集合、download是否启用下载\ntrain_set = torchvision.datasets.CIFAR10(root=&quot;./data&quot;, train=True, transform=transforms_compose_dataset, download=True)\ntest_set = torchvision.datasets.CIFAR10(root=&quot;./data&quot;, train=False, transform=transforms_compose_dataset, download=True)\n\n\nCIFAR-10 and CIFAR-100 datasets\n\nThe CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\nCIFAR-10 数据集由 10 类的 60000 张 32x32 彩色图像组成，每类 6000 张图像。有 50000 张训练图像和 10000 张测试图像。\nThe dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n数据集分为五个训练批次和一个测试批次，每个训练批次有 10000 张图像。测试批次恰好包含每个类中随机选择的 1000 张图像。训练批次以随机顺序包含剩余的图像，但某些训练批次可能包含来自一个类的图像多于另一个类的图像。在它们之间，训练批次恰好包含每个类的 5000 张图像。\n\n 数据集联动Tensorboard\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\n\ntensorboard = SummaryWriter(&quot;p10&quot;)\ntransforms_compose_dataset = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\ntrain_set = torchvision.datasets.CIFAR10(root=&quot;./data&quot;, train=True, transform=transforms_compose_dataset, download=True)\ntest_set = torchvision.datasets.CIFAR10(root=&quot;./data&quot;, train=False, transform=transforms_compose_dataset, download=True)\nprint(test_set[0])\nfor i in range(20):\n    img, target = test_set[i]\n    tensorboard.add_image(&quot;test_set&quot;, img, i)\ntensorboard.close()\n\n其中img, target = test_set[i]返回了一个元组\n\n0号位是转化过后的张量图，1号位是其标签索引，数据的标签列表可以在test_set.classes中看到。\n add_image源码提示\ndef add_image(\n    self, tag, img_tensor, global_step=None, walltime=None, dataformats=&quot;CHW&quot;\n):\n\ntag, img_tensor, global_step=None分别对应tensorboard标签，张量图，以及步骤i\ntensorboard.add_image(&quot;test_set&quot;, img, i)\n\n注意：tensorboard使用过后须关闭tensorboard.close()\n DataLoader的使用\ntorch.utils.data — PyTorch 2.8 documentation\n参数初见：\n\n\ndataset (Dataset) – dataset from which to load the data.\n\n\nbatch_size (int, optional) – how many samples per batch to load (default: 1).\n\n\nshuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).\n\n\nbatch_sampler (Sampler or Iterable*,* optional) – like sampler, but returns a batch of indices at a time. Mutually exclusive with batch_size, shuffle, sampler, and drop_last.\n\n\nnum_workers (int, optional) – how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)\n\n\ndrop_last (bool, optional) – set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False)\n\n\n# 准备的测试数据\ntest_data = torchvision.datasets.CIFAR10(root=&quot;./data&quot;, train=False, transform=torchvision.transforms.ToTensor(),\n                                         download=True)\n# 创建测试数据集\ntest_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=True, num_workers=0, drop_last=True)\n\ndataset数据集、batch_size一次打包多少个、shuffle是否打乱、num_workers加载数据子进程数、drop_last多余部分是否删除\ntorch.Size([3, 32, 32])\n3\ncat\n\n 联动Tensorboard\nimport torchvision.datasets\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\n# 准备的测试数据\ntest_data = torchvision.datasets.CIFAR10(root=&quot;./data&quot;, train=False, transform=torchvision.transforms.ToTensor(),\n                                         download=True)\n# 创建测试数据集\ntest_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=True, num_workers=0, drop_last=True)\n\nimage, target = test_data[0]\n# 测试数据第一张图像的shape和标签\nprint(image.shape)\nprint(target)\nprint(test_data.classes[target])\nprint(&quot;____________________&quot;)\n\nwriter = SummaryWriter(&quot;DataLoader&quot;)\n\nstep = 0\nfor loaderX in test_loader:\n    images, targets = loaderX\n    # print(images.shape)\n    # print(targets)\n    writer.add_images(&quot;test_data_drop_last&quot;, images, step)\n    step += 1\nwriter.close()\n\n 利用Epoch变量控制训练或测试轮次\nfor epoch in range(2):\n    step = 0\n    for loaderX in test_loader:\n        images, targets = loaderX\n        # print(images.shape)\n        # print(targets)\n        writer.add_images(&quot;Epoch:&#123;&#125;&quot;.format(epoch), images, step)\n        step += 1\n\n&quot;Epoch:&#123;&#125;&quot;.format(epoch) 是 Python 的 字符串格式化 方法，它会将 epoch 的值动态插入到字符串的 &#123;&#125; 占位符中。\n\n此时当shuffle=True时，Epoch0和Epoch1并不一样\ntest_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=True, num_workers=0, drop_last=True)\n\n\n 神经网络(Neural Network)基本骨架\ntorch.nn — PyTorch 2.8 documentation\nModule — PyTorch 2.8 documentation\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n\nforward前向传播：\ndef forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n\nx先经过一次conv1卷积，再经过一次relu非线性处理x = F.relu(self.conv1(x))\n然后x在经过一次conv2卷积，再经过一次relu非线性处理，最后返回return F.relu(self.conv2(x))\n 简单的骨架\n简单的骨架就是这样，有一个输入经过forward后每次加一\nimport torch\nfrom torch import nn\n\n\nclass Module(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n\n    def forward(self, input):\n        output = input + 1\n        return output\n\n\nkarry = Module()\nx = torch.tensor(1.0)\nprint(karry(x))\n\n convolution卷积操作\nhttps://www.bilibili.com/video/BV1hE411t7RN?p=17\n Stride跨步=1\nStride是每次跨步数，1就是每次跨一步\n\n注意到红色部分就是对应位置相乘再相加\nimport torch\nimport torch.nn.functional as F\n\n# 创建输入和核\ninput_matrix = torch.tensor([[1, 2, 0, 3, 1], [0, 1, 2, 3, 1], [1, 2, 1, 0, 0], [5, 2, 3, 1, 1], [2, 1, 0, 1, 1]])\nkernel = torch.tensor([[1, 2, 1], [0, 1, 0], [2, 1, 0]])\n# 改变维度\ninput_matrix = torch.reshape(input_matrix, (1, 1, 5, 5))\nkernel = torch.reshape(kernel, (1, 1, 3, 3))\n# 卷积\noutput_ans = F.conv2d(input_matrix, kernel, stride=1)\n\nprint(input_matrix.shape)\nprint(kernel.shape)\nprint(output_ans)\n\n\ntorch.Size([1, 1, 5, 5])\ntorch.Size([1, 1, 3, 3])\ntensor([[[[10, 12, 12],\n[18, 16, 16],\n[13,  9,  3]]]])\n\n Stride跨步=2\n\n# 卷积\noutput_ans = F.conv2d(input_matrix, kernel, stride=2)\nprint(output_ans)\n\n\ntensor([[[[10, 12],\n[13,  3]]]])\n\n Padding填充=1\nPadding填充就是在原始数据的最外侧填充一些数据（像素），一般情况下是设置为零\n\n一样地，红色部分对应位置相乘再相加，最外圈绿色为padding=1所产生的额外填充\n# 卷积\noutput_ans = F.conv2d(input_matrix, kernel, stride=1, padding=1)\nprint(output_ans)\n\n\ntensor([[[[ 1,  3,  4, 10,  8],\n[ 5, 10, 12, 12,  6],\n[ 7, 18, 16, 16,  8],\n[11, 13,  9,  3,  4],\n[14, 13,  9,  7,  4]]]])\n\n Convolution Layers卷积层\ntorch.nn — PyTorch 2.8 documentation\nclass torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode=‘zeros’, device=None, dtype=None)\n其中最主要的参数设置是这五个：in_channels, out_channels, kernel_size, stride=1, padding=0\n\nweight实际上就是卷积核，input是通道数据，bias是偏置\n\n卷积层的 每个输出通道 都是 所有输入通道的加权卷积结果相加，权重就是 weight[j, k]。\n假设输入是 RGB 彩色图像（3 个通道：R、G、B），卷积层的一个输出通道是这样算的：\n\n为什么要“所有输入通道相加”？\n\n图像的特征可能跨通道（比如红绿蓝组合才能构成颜色信息）\n一个卷积核只看单个通道的信息是不完整的\n把多个输入通道的卷积结果加在一起，就相当于在融合这些通道的信息\n\n这也是为什么多通道卷积的 weight 是四维的：C_out、C_in、k_h、k_w\n\nConvolution animations：conv_arithmetic/README.md at master · vdumoulin/conv_arithmetic · GitHub\n in_channel=1, out_channel=1\n只有一个输入通道和一个输出通道，不存在跨通道求和。\n\n1×1+2×0+3×0+4×(−1)=1+0+0−4=−3\n[[−3]]\nin_channel=1, out_channel=1 时，就是用一个卷积核直接作用于输入通道，卷积后加上 bias 得到结果。\n没有跨通道加权，没有额外求和步骤。\nimport torch\nfrom torch import nn\n\nnn_conv_d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride=1, padding=0, bias=False)\nnn_conv_d.weight.data = torch.tensor([[[[1, 0],\n                                        [0, -1]]]], dtype=torch.float32)\ninput = torch.tensor([[[[1, 2], [3, 4]]]], dtype=torch.float32)\n\nprint(nn_conv_d(input))\n\n\ntensor([[[[-3.]]]], grad_fn=)\n\n起步解释：\n\nnn_conv_d.weight.data自定义卷积核\nnn.Conv2d 要求 [batch, channels, height, width]，所以二维矩阵要用 unsqueeze 扩成 4 维，或者在初始化时就是四维的\n卷积层的权重和 bias 是 **浮点型 (torch.float32)**因此要dtype=torch.float32\n为了得到精确值偏置量应当设为假（不偏置）：bias=False\n\n以上均为torch框架中自带的参数调试变量，这些操作主要是 调试和理解卷积的计算过程\n在实际神经网络训练中：\n\n权重 weight 会被优化器自动更新\n输入通常是四维 tensor\nbias 是否启用视网络设计而定\n\n掌握这些基本参数调试方法，是为了：\n\n理解卷积计算机制\n验证卷积操作是否如预期\n为后续做图像或特征识别的神经网络打基础\n\n in_channel=1, out_channel=2\n每个输出通道是独立的卷积结果\n\n1×1+2×0+3×0+4×(−1)=1+0+0−4=−3\n1×0+2×1+3×(−1)+4×0=0+2−3+0=−1\n输出张量 (batch=1, out_channel=2, H=1, W=1)：output=[[−3],[−1]]\nin_channel=1 时，每个输出通道都直接在这个输入通道上用不同卷积核独立运算。\n最终的两个通道结果是并列存储，不做求和。\nimport torch\nfrom torch import nn\n\nnn_conv_d = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=2, stride=1, padding=0, bias=False)\nnn_conv_d.weight.data = torch.tensor([\n    [[[1, 0], [0, -1]]],  # 输出通道 1 的卷积核，对应输入通道 1\n    [[[0, 1], [-1, 0]]]  # 输出通道 2 的卷积核，对应输入通道 1\n], dtype=torch.float32)\ninput = torch.tensor([[[[1, 2], [3, 4]]]], dtype=torch.float32)\n\nprint(nn_conv_d(input))\n\n\ntensor([[[[-3.]],\n​\t\t\t\t[[-1.]]]], grad_fn=)\n\n in_channel=2, out_channel=1\n两个输入通道各自用自己的卷积核卷积 → 得到两个结果。\n\n1×1+2×0+3×0+4×(−1)=1+0+0−4=−3\n5×0+6×1+7×(−1)+8×0=0+6−7+0=−1\n加权求和（合成一个输出通道）：output=(−3)+(−1)=−4\n输出张量 (batch=1, out_channel=1, H=1, W=1)：[[−4]]\nimport torch\nfrom torch import nn\n\nnn_conv_d = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=2, stride=1, padding=0, bias=False)\nnn_conv_d.weight.data = torch.tensor([\n    [\n        [[1, 0], [0, -1]],    # 输入通道 1 的卷积核\n        [[0, 1], [-1, 0]]     # 输入通道 2 的卷积核\n    ]\n], dtype=torch.float32)\ninput = torch.tensor([\n    [\n        [\n            [1, 2],\n            [3, 4]\n        ], [\n        [5, 6],\n        [7, 8]\n    ]\n    ]\n], dtype=torch.float32)\n\nprint(nn_conv_d(input))\n\n\ntensor([[[[-4.]]]], grad_fn=)\n\n in_channel=2, out_channel=2\n\nX_channel1 * W_out1_in1 = [[11+20+30+41]] = [[1+0+0+4]] = [[5]]\nX_channel2 * W_out1_in2 = [[00+11+11+00]] = [[0+1+1+0]] = [[2]]\nY_out1 = 5 + 2 = 7\nX_channel1 * W_out2_in1 = [[1+2+3+4]] = [[10]]\nX_channel2 * W_out2_in2 = [[0* -1 + 10 + 10 + 0*-1]] = [[0]]\nY_out2 = 10 + 0 = 10\nY_out1 = [[7]]\t\tY_out2 = [[10]]\n结果为：[[7],[10]]\nimport torch\nfrom torch import nn\n\nnn_conv_d = nn.Conv2d(in_channels=2, out_channels=2, kernel_size=2, stride=1, padding=0, bias=False)\nnn_conv_d.weight.data = torch.tensor([\n    [\n        [[1, 0], [0, 1]],\n        [[0, 1], [1, 0]]\n    ], [\n        [[1, 1], [1, 1]],\n        [[-1, 0], [0, -1]]\n    ]\n], dtype=torch.float32)\ninput = torch.tensor([[\n    [\n        [1, 2],\n        [3, 4]\n    ], [\n        [0, 1],\n        [1, 0]\n    ]\n]], dtype=torch.float32)\n\nprint(nn_conv_d(input))\n\n\ntensor([[[[ 7.]],\n​\t\t\t\t[[10.]]]], grad_fn=)\n\n 向前传播的卷积（正向传播）\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\ndataset = torchvision.datasets.CIFAR10(&quot;data&quot;, train=False, transform=torchvision.transforms.ToTensor(),\n                                       download=True)\ndataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=0)\n\n\nclass Net(nn.Module):\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        return x\n\n\nwriter = SummaryWriter(&quot;logs&quot;)\nnet = Net()\nstep = 0\nfor data in dataloader:\n    imgs, targets = data\n    outputs = net(imgs)\n    print(imgs.shape)\n    print(outputs.shape)\n    # torch.Size([64, 3, 32, 32])\n    writer.add_images(&quot;input&quot;, imgs, global_step=step)\n    # torch.Size([64, 6, 30, 30])\n    outputs = torch.reshape(outputs, (-1, 3, 30, 30))\n    writer.add_images(&quot;output&quot;, outputs, global_step=step)\n    step += 1\n\nself.conv1 是一个 卷积层（nn.Conv2d），在 forward 里写 x = self.conv1(x)，就是把输入 x 通过卷积层进行前向计算。\n Pooling Layers池化层\ntorch.nn — PyTorch 2.8 documentation\n池化是卷积神经网络（CNN）中一个很重要的操作。它的主要作用可以总结为以下几点：\n\n降维与减少计算量\n\n\n为什么：卷积层输出的特征图通常很大，如果不缩小，后面网络层的计算量会非常庞大。\n怎么做：池化通过取局部区域的最大值（Max Pooling）或平均值（Average Pooling）来缩小特征图尺寸。\n效果：减少参数量和计算量，加快训练和推理速度。\n\n\n\n特征的平移不变性\n\n\n什么意思：如果图片里一个物体稍微移动了，网络依然能识别。\n为什么能做到：池化会在一个小范围内提取统计特征（最大值或平均值），因此即使输入图像有微小的偏移，结果变化也不会太大。\n\n\n\n突出重要特征，抑制不重要信息\n\n\nMax Pooling：保留一个区域的最大值，倾向于保留最显著的边缘或纹理特征。\nAverage Pooling：保留区域的平均值，得到更加平滑的特征。\n作用：相当于“特征压缩”，让后续层更容易提取全局信息。\n\n\n\n防止过拟合\n\n\n通过减少参数和对细节的依赖，网络更关注大局特征而不是局部噪声，从而减轻过拟合风险。\n\n\n核心参数\n\nkernel_size (Union[int, tuple[int, int]]) – the size of the window to take a max over\nstride (Union[int, tuple[int, int]]) – the stride of the window. Default value is kernel_size\npadding (Union[int, tuple[int, int]]) – Implicit negative infinity padding to be added on both sides\ndilation (Union[int, tuple[int, int]]) – a parameter that controls the stride of elements in the window\nreturn_indices (bool) – if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later\nceil_mode (bool) – when True, will use ceil instead of floor to compute the output shape\n\n池化示意图：\n\n dilation扩张率（空洞卷积）\n这个参数在 卷积（特别是卷积神经网络中的卷积层）里起很重要的作用。它和 kernel_size、stride 一样，决定了卷积核是怎么在输入特征图上取值的。\n\ndilation 卷积，其实就是我们常说的 空洞卷积 (Dilated Convolution / Atrous Convolution)。\n ceil模式和floor模式\n\nfloor 模式（默认）\n\n\n取整时向下取整（floor）。\n多余的边缘（不足一个 kernel 的区域）会被丢弃。\n比如：\n\n输入长度 5\nkernel=2, stride=2\n计算：(5−2)/2+1=2.5(5-2)/2 + 1 = 2.5(5−2)/2+1=2.5 → floor → 2\n输出长度就是 2（最后一个位置没覆盖到）。\n\n\n\n\nceil 模式（开启 ceil_mode=True）\n\n\n取整时向上取整（ceil）。\n边缘不足 kernel 的部分，也会被保留（通常会用 padding 补齐）。\n上面例子：\n\n输入长度 5\nkernel=2, stride=2, ceil_mode=True\n计算：2.5 → ceil → 3\n输出长度就是 3（最后一个区域会只覆盖部分输入，或者补 0）。\n\n\n\nfloor 模式（默认）：计算稳定，常用于训练。\nceil 模式：当你希望输入和输出严格对齐，或者想保留更多边缘信息时使用（比如某些图像分割任务）。\n 提示\n卷积是提取特征，池化是压缩特征\n1080P -&gt; 720P\n 最大池化\n池化的默认步长等于池化核的大小，池化的默认步长等于池化核的大小，池化的默认步长等于池化核的大小\n\n 最大池化实例\nceil_mode=True：\nimport torch\nimport torch.nn as nn\n\ninput = torch.tensor([[1, 2, 0, 3, 1],\n                      [0, 1, 2, 3, 1],\n                      [1, 2, 1, 0, 0],\n                      [5, 2, 3, 1, 1],\n                      [2, 1, 0, 1, 1]])\ninput = torch.reshape(input, (-1, 1, 5, 5))\n\n\nclass Net(nn.Module):\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.maxPool1 = nn.MaxPool2d(kernel_size=3, ceil_mode=True)\n\n    def forward(self, input):\n        output = self.maxPool1(input)\n        return output\n\n\nnet = Net()\noutput = net(input)\nprint(output.shape)\nprint(output)\n\n\ntorch.Size([1, 1, 2, 2])\ntensor([[[[2, 3],\n[5, 1]]]])\n\nceil_mode=False：\nself.maxPool1 = nn.MaxPool2d(kernel_size=3, ceil_mode=False)\n\n\ntorch.Size([1, 1, 1, 1])\ntensor([[[[2]]]])\n\n 图片操作\nimport torch\nimport torch.nn as nn\nimport torchvision.datasets\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\ninput = torch.tensor([[1, 2, 0, 3, 1],\n                      [0, 1, 2, 3, 1],\n                      [1, 2, 1, 0, 0],\n                      [5, 2, 3, 1, 1],\n                      [2, 1, 0, 1, 1]])\ninput = torch.reshape(input, (-1, 1, 5, 5))\n\ndata_set = torchvision.datasets.CIFAR10(root=&quot;data&quot;, train=False, transform=torchvision.transforms.ToTensor(),\n                                        download=True)\n\ndataloader = DataLoader(data_set, batch_size=64, shuffle=True, num_workers=0)\n\n\nclass Net(nn.Module):\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.maxPool1 = nn.MaxPool2d(kernel_size=3, ceil_mode=False)\n\n    def forward(self, input):\n        output = self.maxPool1(input)\n        return output\n\n\nwriter = SummaryWriter(&quot;logs_maxpool&quot;)\n\nnet = Net()\nstep = 0\nfor data in dataloader:\n    img, label = data\n    writer.add_images(&quot;input&quot;, img, global_step=step)\n    output = net(img)\n    writer.add_images(&quot;output&quot;, output, global_step=step)\n    step += 1\nwriter.close()\n\n可以看到，变成马赛克了：\n\n Padding Layers填充层\ntorch.nn — PyTorch 2.8 documentation\n填充是应用于图片外围的，主要进行一些值的填充，基本不用到\n最多的会使用到：nn.ZeroPad2d Pads the input tensor boundaries with zero.\n Non-linear Activations非线性激活\n最常见的是：ReLU — PyTorch 2.8 documentation\n\n其次是：Sigmoid — PyTorch 2.8 documentation\n\n ReLU示例\nimport torch\n\ninput = torch.tensor([[1, -0.5], [-1, 3]])\noutput = torch.reshape(input, (-1, 1, 2, 2))\nprint(output)\n\nclass Net(torch.nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.relu = torch.nn.ReLU()\n    def forward(self, input) -&gt; torch.Tensor:\n        output = self.relu(input)\n        return output\nnet = Net()\noutput = net(output)\nprint(output)\n\n inplace源码提示\ndef __init__(self, inplace: bool = False):\n    super().__init__()\n    self.inplace = inplace\n\n当inplace为假时，不改变源数据\n当inplace为真时，执行时改变原始数据（就地算法）\n 图片操作\nimport torch\nimport torchvision.datasets\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\ndataset = torchvision.datasets.CIFAR10(&quot;data&quot;, train=False, transform=torchvision.transforms.ToTensor(), download=True)\ndataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=0)\n\nclass Net(torch.nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.relu = torch.nn.ReLU()\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, input) -&gt; torch.Tensor:\n        # output = self.relu(input)\n        output = self.sigmoid(input)\n        return output\nnet = Net()\nwriter = SummaryWriter(&quot;logs_relu&quot;)\nstep = 0\nfor data in dataloader:\n    imgs, targets = data\n    writer.add_images(&quot;input&quot;, imgs, global_step=step)\n    output = net(imgs)\n    writer.add_images(&quot;output&quot;, output, global_step=step)\n    step += 1\n\nwriter.close()\n\n结果如下：\n\n Normalization Layers正则化层\nhttps://docs.pytorch.org/docs/stable/nn.html#normalization-layers\n有一篇论文提到正则化层可以加速训练\n Recurrent Layers循环层\n提示：RNN\ntorch.nn — PyTorch 2.8 documentation\n序列的每一步计算都会依赖前一步的隐藏状态，从而能捕捉序列的时间依赖性。\n Transformer Layers\nhttps://docs.pytorch.org/docs/stable/nn.html#transformer-layers\nTransformer 的核心思想就是用 自注意力机制（Self-Attention） 来替代传统 RNN 或 CNN 处理序列时的缺陷。\n Linear Layers线性层（全连接）\ntorch.nn — PyTorch 2.8 documentation\n\n 源码提示\ntorch.nn.Linear(in_features, out_features, bias=True)\n\nin_features含义：输入特征的维度（每个样本的输入向量长度）。\nout_features含义：输出特征的维度（每个样本的输出向量长度）。\nbias是否使用偏置项，默认 True\n\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\ndataset = torchvision.datasets.CIFAR10(&quot;data&quot;, train=False, transform=torchvision.transforms.ToTensor(),\n                                       download=True)\ndataLoader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=0, drop_last=True)\n\n\n# writer = SummaryWriter(&quot;log_liner&quot;)\n\nclass Net(nn.Module):\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.linear1 = nn.Linear(196608, 10)\n\n    def forward(self, input):\n        output = self.linear1(input)\n        return output\n\n\nnet = Net()\nfor data in dataLoader:\n    img, label = data\n    print(img.shape)\n    output = torch.flatten(img)\n    print(output.shape)\n    output = net(output)\n    print(output.shape)\n\n展平动作：torch.flatten(img)\n输入196608，输出10：nn.Linear(196608, 10)\n Dropout Layers\ntorch.nn — PyTorch 2.8 documentation\n Sequential序列操作——以简单网络模型实战为例\n相当于transforms的compose，将一些操作组装到一起\n\nimport torch\nfrom torch import nn\nfrom torch.nn import Conv2d, MaxPool2d, Flatten, Linear\n\n\nclass Net(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.conv1 = Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2)\n        self.maxPool1 = MaxPool2d(kernel_size=2, ceil_mode=False)\n        self.conv2 = Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2)\n        self.maxPool2 = MaxPool2d(kernel_size=2, ceil_mode=False)\n        self.conv3 = Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)\n        self.maxPool3 = MaxPool2d(kernel_size=2, ceil_mode=False)\n        self.flatten = Flatten()\n        self.linear0 = Linear(1024, 64)\n        self.linear1 = Linear(64, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.maxPool1(x)\n        x = self.conv2(x)\n        x = self.maxPool2(x)\n        x = self.conv3(x)\n        x = self.maxPool3(x)\n        x = self.flatten(x)\n        x = self.linear0(x)\n        x = self.linear1(x)\n        return x\n\n\nnet = Net()\nprint(net)\n# 测试网络\ninput = torch.ones(64, 3, 32, 32)\noutputs = net(input)\nprint(outputs.shape)\n\n输入是一张彩色图片（3个通道）\n\n首先用一个卷积层（conv1）去提取一些低级特征，比如边缘、颜色块，然后通过一次池化（maxPool1）把图片“缩小一半”，同时保留主要特征\n接着再来一次卷积（conv2），这时候的输入已经有32个通道了，网络会继续在前面提取到的特征基础上，找到更复杂的形状、纹理，然后再做一次池化（maxPool2），图像再缩小一半\n然后再卷积一次（conv3），这次输出通道变成64个，能提取更丰富、更抽象的特征，比如局部的结构、物体的一部分，再池化一次（maxPool3），图像又缩小\n接下来把这些“缩小后的特征图”拉直成一维向量（flatten），然后经过一个全连接层（linear0），把大规模的特征压缩成一个64维的向量\n最后再经过一个全连接层（linear1），输出10个数，这通常对应10个分类的可能性\n\n总结一下：它就是一个典型的卷积神经网络，前面几层卷积+池化负责逐步提取和浓缩图像特征，后面的全连接层负责把这些特征转成分类结果。\n Sequential\nclass Net(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.conv1 = Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2)\n        self.maxPool1 = MaxPool2d(kernel_size=2, ceil_mode=False)\n        self.conv2 = Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2)\n        self.maxPool2 = MaxPool2d(kernel_size=2, ceil_mode=False)\n        self.conv3 = Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)\n        self.maxPool3 = MaxPool2d(kernel_size=2, ceil_mode=False)\n        self.flatten = Flatten()\n        self.linear0 = Linear(1024, 64)\n        self.linear1 = Linear(64, 10)\n        self.model1 = Sequential(self.conv1, self.maxPool1, self.conv2, self.maxPool2, self.conv3, self.maxPool3,self.flatten, self.linear0, self.linear1)\n\n\n    def forward(self, x):\n        x = self.model1(x)\n        return x\n\n其中Sequential将一些操作组装到一起：\nself.model1 = Sequential(self.conv1, self.maxPool1, self.conv2, self.maxPool2, self.conv3, self.maxPool3,self.flatten, self.linear0, self.linear1)\n\n进一步地可以这样写：\nclass Net(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.model2 = Sequential(\n            Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Flatten(),\n            Linear(1024, 64),\n            Linear(64, 10))\n\n    def forward(self, x):\n        x = self.model2(x)\n        return x\n\n 流程可视化\nwriter = SummaryWriter(&quot;logs_seq&quot;)\nwriter.add_graph(net, input)\nwriter.close()\n\n\n可以观察到具体流程就是：卷积1、池化1、卷积2、池化2、卷积3、池化3、展平、全连接1（线性1）、全连接2（线性2）。\n 损失函数与反向传播\n计算实际输出和目标之间的差距，为我们更新输出提供一定的依据（反向传播）, grad\ntorch.nn — PyTorch 2.8 documentation\n\nnn.L1Loss\nnn.L1Loss：\nX:1, 2, 3\nY:1, 2, 5\nL1loss = (0+0+2) /3=0.6，这里自然是越小越好\nimport torch\nfrom torch.nn import L1Loss\n\ninput = torch.tensor([1, 2, 3], dtype=torch.float32)\ntarget = torch.tensor([1, 2, 5], dtype=torch.float32)\n\ninput = torch.reshape(input, (1, 1, 1, 3))\ntarget = torch.reshape(target, (1, 1, 1, 3))\n\nloss = L1Loss()\nresult = loss(input, target)\nprint(result)\n\n输出：tensor(0.6667)\n\nnn.MSELoss\nMSE = (0+0+2^2)/3=4/3=1.333\n注意每个位置要平方\nimport torch\nfrom torch.nn import L1Loss\n\ninput = torch.tensor([1, 2, 3], dtype=torch.float32)\ntarget = torch.tensor([1, 2, 5], dtype=torch.float32)\n\ninput = torch.reshape(input, (1, 1, 1, 3))\ntarget = torch.reshape(target, (1, 1, 1, 3))\n\nloss_Mse = torch.nn.MSELoss()\nresult_Mse = loss_Mse(input, target)\n\nprint(result_Mse)\n\n\n交叉熵：nn.CrossEntropyLoss分类问题\n\n此处log时以e为底数的（ln），常见target就是目标代号，1就是第二个标签\n\n使用result_loss.backward()开启反向传播\nbackward根据损失值 result_loss 自动计算出每个参数的梯度，并把梯度存到参数的 .grad 属性中。\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.nn import Conv2d, Sequential, MaxPool2d, Linear, Flatten\n\ndataset = torchvision.datasets.CIFAR10(&quot;data&quot;, train=False, transform=torchvision.transforms.ToTensor(),\n                                       download=True)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=1)\n\n\nclass Net(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.model1 = Sequential(\n            Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Flatten(),\n            Linear(1024, 64),\n            Linear(64, 10))\n\n    def forward(self, x):\n        x = self.model1(x)\n        return x\n\n\nnet = Net()\nloss = nn.CrossEntropyLoss()\nfor data in dataloader:\n    imgs, targets = data\n    outputs = net(imgs)\n    result_loss = loss(outputs, targets)\n    result_loss.backward()\n    print(&quot;ok&quot;)\n    print(result_loss)\n\n optim优化器\n它负责根据参数的梯度 .grad 来更新模型的参数，从而让模型越来越接近目标。\n训练一个神经网络时流程是这样的：\n\n前向传播 (forward)\n输入数据 → 模型输出 → 计算损失 loss。\n反向传播 (backward)\n调用 loss.backward()，PyTorch 会自动算出每个参数的梯度，并存到 param.grad 里。\n参数更新 (update step)\n这一步就是 优化器的作用。\n优化器会读取参数的 .grad，然后根据优化算法（如 SGD、Adam）来调整参数值。\n\noptim（update step）会根据backward计算出来的梯度来更新参数\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.nn import Conv2d, Sequential, MaxPool2d, Linear, Flatten\n\ndataset = torchvision.datasets.CIFAR10(&quot;data&quot;, train=False, transform=torchvision.transforms.ToTensor(),\n                                       download=True)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=1)\n\n\nclass Net(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.model1 = Sequential(\n            Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Flatten(),\n            Linear(1024, 64),\n            Linear(64, 10))\n\n    def forward(self, x):\n        x = self.model1(x)\n        return x\n\n\nnet = Net()\nloss = nn.CrossEntropyLoss()\noptim = torch.optim.SGD(net.parameters(), lr=0.01)\nfor epoch in range(20):\n    print(&quot;Epoch:&#123;&#125;&quot;.format(epoch))\n    running_loss = 0.0\n    for data in dataloader:\n        imgs, targets = data\n        outputs = net(imgs)\n        result_loss = loss(outputs, targets)\n        optim.zero_grad()\n        result_loss.backward()\n        optim.step()\n        running_loss += result_loss\n    print(&quot;Loss:&#123;&#125;&quot;.format(running_loss))\n\n关键在于：\noptim.zero_grad()\nresult_loss.backward()\noptim.step()\n首先optim梯度数据清零，然后backward计算这一次的梯度。最后optim做参数的优化\n对于这里的循环：\nfor data in dataloader:\n        imgs, targets = data\n        outputs = net(imgs)\n        result_loss = loss(outputs, targets)\n        optim.zero_grad()\n        result_loss.backward()\n        optim.step()\n        running_loss += result_loss\n\n只是对数据进行了一轮的学习，我们需要多轮学习才能最优化结果：\nfor epoch in range(20):\n    print(&quot;Epoch:&#123;&#125;&quot;.format(epoch))\n    running_loss = 0.0\n    for data in dataloader:\n        imgs, targets = data\n        outputs = net(imgs)\n        result_loss = loss(outputs, targets)\n        optim.zero_grad()\n        result_loss.backward()\n        optim.step()\n        running_loss += result_loss\n    print(&quot;Loss:&#123;&#125;&quot;.format(running_loss))\n\n因此我们这样操作\n SGD（随机梯度下降）\n训练神经网络的目标是让 损失函数 Loss 最小化\n想象损失函数是一个“山谷地形”，我们要沿着山坡往下走，直到找到最低点\n梯度（Gradient） 就是告诉我们“往哪个方向下坡最快”\n\noptim = torch.optim.SGD(net.parameters(), lr=0.01)\n\n模型参数即net.parameters()，学习速率即lr=0.01\n学习率 (learning rate, lr)：\nlr 太小：走得慢，收敛速度慢，可能训练很久 loss 才下降。\nlr 太大：走得快，但可能“跨过山谷底部”，造成震荡甚至发散（loss 变大）。\n 现有模型的使用与修改（迁移学习）\n借用别人训练好的模型知识，在新任务上减少训练成本，提高效果。\n在现有vgg16中加一些新的层\nimport os\n\nimport torchvision.datasets\nfrom torch import nn\n\nos.environ[&quot;TORCH_HOME&quot;] = &quot;E:/PyCharmCode/pytorchSTU/data&quot;\nvgg16_false = torchvision.models.vgg16(pretrained=False)\nvgg16_true = torchvision.models.vgg16(pretrained=True)\nprint(vgg16_true)\n\ntrain_data = torchvision.datasets.CIFAR10(root=&quot;data&quot;, train=True, transform=torchvision.transforms.ToTensor(),\n                                          download=True)\nvgg16_true.classifier.add_module(&quot;add_linear&quot;, nn.Linear(1000, 10))\nprint(vgg16_true)\n\n源：\n\n(classifier): Sequential(\n(0): Linear(in_features=25088, out_features=4096, bias=True)\n(1): ReLU(inplace=True)\n(2): Dropout(p=0.5, inplace=False)\n(3): Linear(in_features=4096, out_features=4096, bias=True)\n(4): ReLU(inplace=True)\n(5): Dropout(p=0.5, inplace=False)\n(6): Linear(in_features=4096, out_features=1000, bias=True)\n)\n\n现：\n\n(classifier): Sequential(\n(0): Linear(in_features=25088, out_features=4096, bias=True)\n(1): ReLU(inplace=True)\n(2): Dropout(p=0.5, inplace=False)\n(3): Linear(in_features=4096, out_features=4096, bias=True)\n(4): ReLU(inplace=True)\n(5): Dropout(p=0.5, inplace=False)\n(6): Linear(in_features=4096, out_features=1000, bias=True)\n(add_linear): Linear(in_features=1000, out_features=10, bias=True)\n)\n\n在现有vgg16中修改层\nimport os\n\nimport torchvision.datasets\nfrom torch import nn\n\n# train_data = torchvision.datasets.ImageNet(root=&quot;data&quot;, split=&quot;train&quot;, download=True,\n#                                            transform=torchvision.transforms.ToTensor())\nos.environ[&quot;TORCH_HOME&quot;] = &quot;E:/PyCharmCode/pytorchSTU/data&quot;\nvgg16_false = torchvision.models.vgg16(pretrained=False)\nvgg16_true = torchvision.models.vgg16(pretrained=True)\nprint(vgg16_false)\nvgg16_false.classifier[6] = nn.Linear(in_features=4096, out_features=10)\nprint(vgg16_false)\n\n源：\n\n(classifier): Sequential(\n(0): Linear(in_features=25088, out_features=4096, bias=True)\n(1): ReLU(inplace=True)\n(2): Dropout(p=0.5, inplace=False)\n(3): Linear(in_features=4096, out_features=4096, bias=True)\n(4): ReLU(inplace=True)\n(5): Dropout(p=0.5, inplace=False)\n(6): Linear(in_features=4096, out_features=1000, bias=True)\n)\n\n现：\n\n(classifier): Sequential(\n(0): Linear(in_features=25088, out_features=4096, bias=True)\n(1): ReLU(inplace=True)\n(2): Dropout(p=0.5, inplace=False)\n(3): Linear(in_features=4096, out_features=4096, bias=True)\n(4): ReLU(inplace=True)\n(5): Dropout(p=0.5, inplace=False)\n(6): Linear(in_features=4096, out_features=10, bias=True)\n)\n\n 模型的保存与加载\n保存：\nimport os\n\nimport torch\nimport torchvision.models\nfrom torch import nn\n\nos.environ[&quot;TORCH_HOME&quot;] = &quot;E:/PyCharmCode/pytorchSTU/data&quot;\nvgg16 = torchvision.models.vgg16(pretrained=False)\n# 方式1：保存模型结构+参数\ntorch.save(vgg16, &quot;vgg16_method1.pth&quot;)\n# 方式2：保存模型参数(官方推荐)\ntorch.save(vgg16.state_dict(), &quot;vgg16_method2.pth&quot;)\n\n# 陷阱1\nclass Net(nn.Module):\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.conv1 = nn.Conv2d(3, 32, 5, 1, 2)\n    def forward(self, x):\n        x = self.conv1(x)\n        return x\nnet = Net()\ntorch.save(net, &quot;net.pth&quot;)\n\n加载：\nimport torch\nimport torchvision.models\nimport model_save\nfrom torch import nn\n\n# 方式1：模型结构+参数加载1\ntorch_load = torch.load(&quot;vgg16_method1.pth&quot;, weights_only=False)\n# print(torch_load)\n# 方式2：模型参数加载2\nvgg16 = torchvision.models.vgg16(pretrained=False)\ntorch_load = torch.load(&quot;vgg16_method2.pth&quot;, weights_only=True)\nvgg16.load_state_dict(torch_load)\n\n\n# print(vgg16)\n\n# 陷阱1: 需要把模型引入才能进行加载import model_save\nmodel = torch.load(&quot;net.pth&quot;, weights_only=False)\nprint(model)\n\n 模型的完整训练套路\n\n准备数据集、获取数据集大小（可选）\ndataLoader加载数据集\n搭建神经网络Net\n创建损失函数loss_fn、创建优化器optimizer、可选使用tenser board\n设置训练网络一些参数：训练的轮数total_train_step、测试的轮数total_test_step、总训练的轮数epoch\n开始训练，导出imgs, targets并进入网络\n计算损失函数\n清零梯度、计算梯度、利用反向传播，致使优化器更新参数\n暂时关闭梯度计算、开始测试\n计算损失函数并累计、计算当前轮次准确率\n重复6~10，直到完成所有轮次\n\nimport torch\nimport torchvision.datasets\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom model import *\nfrom torch.utils.data import DataLoader\n\n# 准备数据集\ntrain_data = torchvision.datasets.CIFAR10(&quot;data&quot;, train=True, transform=torchvision.transforms.ToTensor(),\n                                          download=True)\ntest_data = torchvision.datasets.CIFAR10(&quot;data&quot;, train=False, transform=torchvision.transforms.ToTensor(),\n                                         download=True)\n# 获取数据集大小\ntrain_data_size = len(train_data)\ntest_data_size = len(test_data)\nprint(f&quot;训练数据集的长度为&#123;train_data_size&#125;&quot;)\nprint(f&quot;测试数据集的长度为&#123;test_data_size&#125;&quot;)\n# dataLoader加载数据集\ndataLoader_train = DataLoader(train_data, batch_size=64)\ndataLoader_test = DataLoader(test_data, batch_size=64)\n\n# 搭建神经网络\nnet = Net()\n\n# 创建损失函数\nloss_fn = nn.CrossEntropyLoss()\n\n# 创建优化器\nlr = 0.01\noptimizer = torch.optim.SGD(net.parameters(), lr=lr)\n\n# tenser board\nwriter = SummaryWriter(&quot;logs&quot;)\n\n# 设置训练网络一些参数\n# 训练的轮数\ntotal_train_step = 0\n# 测试的轮数\ntotal_test_step = 0\n# 训练的轮数\nepoch = 10\n\nfor i in range(epoch):\n    print(&quot;--------第 &#123;&#125; 轮训练开始--------&quot;.format(i + 1))\n    # 训练开始\n    net.train()\n    for data in dataLoader_train:\n        imgs, targets = data\n        outputs = net(imgs)\n        loss = loss_fn(outputs, targets)  # 损失函数\n        optimizer.zero_grad()  # 清零梯度\n        loss.backward()  # 反向传播\n        optimizer.step()  # 优化器更新参数\n        total_train_step += 1\n        if total_train_step % 100 == 0:\n            print(&quot;训练次数：&#123;&#125;, Loss: &#123;&#125;&quot;.format(total_train_step, loss.item()))\n            writer.add_scalar(&quot;train_loss&quot;, loss.item(), total_train_step)\n    # 测试步骤开始\n    net.eval()\n    total_test_loss = 0\n    total_acc = 0\n\n    with torch.no_grad():  # 不进行梯度计算 with的作用是：临时关闭梯度计算，退出后自动恢复。在上下文里关闭，出了上下文就恢复。\n        for data in dataLoader_test:\n            imgs, targets = data\n            outputs = net(imgs)  # 测试步骤开始\n            loss = loss_fn(outputs, targets)  # 损失函数\n            total_test_loss += loss.item()  # 求和\n            acc = (outputs.argmax(1) == targets).sum()\n            total_acc += acc\n        print(&quot;整体测试集上的Loss：&#123;&#125;&quot;.format(total_test_loss))\n        print(&quot;整体测试集上的正确率：&#123;&#125;&quot;.format(total_acc / test_data_size))\n        writer.add_scalar(&quot;test_loss&quot;, total_test_loss, total_test_step)\n        writer.add_scalar(&quot;test_acc&quot;, total_acc / test_data_size, total_test_step)\n        total_test_step += 1\n    torch.save(net, &quot;net_&#123;&#125;.pth&quot;.format(i))\n    print(&quot;模型已保存&quot;)\nwriter.close()\n\n10轮学习后，整体测试集上的正确率：0.5428000092506409\n test_acc\n\n test_loss\n\n train_loss\n\n 使用GPU训练1（.cuda）\n\n\n网络可以使用GPU加速\nnet = Net()\n# 交给GPU\nif torch.cuda.is_available():\n    net = net.cuda()\n\n\n\n损失函数可以使用GPU加速\nloss_fn = nn.CrossEntropyLoss()\n# 交给GPU\nif torch.cuda.is_available():\n    loss_fn = loss_fn.cuda()\n\n\n\n数据和标签和使用GPU加速\n# 交给GPU\nif torch.cuda.is_available():\n    imgs = imgs.cuda()\n    targets = targets.cuda()\n\n\n\n优化后：\n# _*_ coding : utf-8 _*_\n# @Time : 2025/8/26 14:38\n# @Author : KarryLiu\n# File : train_gpu_1\n# @Project : pytorchSTU\nimport torch\nimport torchvision.datasets\nfrom torch import nn\nfrom torch.nn import Sequential, Conv2d, MaxPool2d, Linear, Flatten\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom torch.utils.data import DataLoader\n\n# 准备数据集\ntrain_data = torchvision.datasets.CIFAR10(&quot;data&quot;, train=True, transform=torchvision.transforms.ToTensor(),\n                                          download=True)\ntest_data = torchvision.datasets.CIFAR10(&quot;data&quot;, train=False, transform=torchvision.transforms.ToTensor(),\n                                         download=True)\n# 获取数据集大小\ntrain_data_size = len(train_data)\ntest_data_size = len(test_data)\nprint(f&quot;训练数据集的长度为&#123;train_data_size&#125;&quot;)\nprint(f&quot;测试数据集的长度为&#123;test_data_size&#125;&quot;)\n# dataLoader加载数据集\ndataLoader_train = DataLoader(train_data, batch_size=64)\ndataLoader_test = DataLoader(test_data, batch_size=64)\n\n\n# 搭建神经网络\nclass Net(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.model = Sequential(\n            Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Flatten(),\n            Linear(1024, 64),\n            Linear(64, 10)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\nnet = Net()\n# 交给GPU\nif torch.cuda.is_available():\n    net = net.cuda()\n\n# 创建损失函数\nloss_fn = nn.CrossEntropyLoss()\n# 交给GPU\nif torch.cuda.is_available():\n    loss_fn = loss_fn.cuda()\n\n# 创建优化器\nlr = 0.01\noptimizer = torch.optim.SGD(net.parameters(), lr=lr)\n\n# tenser board\nwriter = SummaryWriter(&quot;logs&quot;)\n\n# 设置训练网络一些参数\n# 训练的轮数\ntotal_train_step = 0\n# 测试的轮数\ntotal_test_step = 0\n# 训练的轮数\nepoch = 10\n\nfor i in range(epoch):\n    print(&quot;--------第 &#123;&#125; 轮训练开始--------&quot;.format(i + 1))\n    # 训练开始\n    net.train()\n    for data in dataLoader_train:\n        imgs, targets = data\n        # 交给GPU\n        if torch.cuda.is_available():\n            imgs = imgs.cuda()\n            targets = targets.cuda()\n        outputs = net(imgs)\n        loss = loss_fn(outputs, targets)  # 损失函数\n        optimizer.zero_grad()  # 清零梯度\n        loss.backward()  # 反向传播\n        optimizer.step()  # 优化器更新参数\n        total_train_step += 1\n        if total_train_step % 100 == 0:\n            print(&quot;训练次数：&#123;&#125;, Loss: &#123;&#125;&quot;.format(total_train_step, loss.item()))\n            writer.add_scalar(&quot;train_loss&quot;, loss.item(), total_train_step)\n    # 测试步骤开始\n    net.eval()\n    total_test_loss = 0\n    total_acc = 0\n\n    with torch.no_grad():  # 不进行梯度计算 with的作用是：临时关闭梯度计算，退出后自动恢复。在上下文里关闭，出了上下文就恢复。\n        for data in dataLoader_test:\n            imgs, targets = data\n            # 交给GPU\n            if torch.cuda.is_available():\n                imgs = imgs.cuda()\n                targets = targets.cuda()\n            outputs = net(imgs)  # 测试步骤开始\n            loss = loss_fn(outputs, targets)  # 损失函数\n            total_test_loss += loss.item()  # 求和\n            acc = (outputs.argmax(1) == targets).sum()\n            total_acc += acc\n        print(&quot;整体测试集上的Loss：&#123;&#125;&quot;.format(total_test_loss))\n        print(&quot;整体测试集上的正确率：&#123;&#125;&quot;.format(total_acc / test_data_size))\n        writer.add_scalar(&quot;test_loss&quot;, total_test_loss, total_test_step)\n        writer.add_scalar(&quot;test_acc&quot;, total_acc / test_data_size, total_test_step)\n        total_test_step += 1\n    torch.save(net, &quot;net_&#123;&#125;.pth&quot;.format(i))\n    print(&quot;模型已保存&quot;)\nwriter.close()\n\n使用GPU后，学习100次只需要大约1.23s\n 使用GPU训练2（.to）\n使用：\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n\n\n\n网络可以使用GPU加速\nnet = Net()\n# 交给GPU\nnet = net.to(device)\n\n\n\n损失函数可以使用GPU加速\nloss_fn = nn.CrossEntropyLoss()\n# 交给GPU\nloss_fn = loss_fn.to(device)\n\n\n\n数据和标签和使用GPU加速\n# 交给GPU\nimgs = imgs.to(device)\ntargets = targets.to(device)\n\n\n\n优化后：\nimport torch\nimport torchvision.datasets\nfrom torch import nn\nfrom torch.nn import Sequential, Conv2d, MaxPool2d, Linear, Flatten\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom torch.utils.data import DataLoader\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n\n# 准备数据集\ntrain_data = torchvision.datasets.CIFAR10(&quot;data&quot;, train=True, transform=torchvision.transforms.ToTensor(),\n                                          download=True)\ntest_data = torchvision.datasets.CIFAR10(&quot;data&quot;, train=False, transform=torchvision.transforms.ToTensor(),\n                                         download=True)\n# 获取数据集大小\ntrain_data_size = len(train_data)\ntest_data_size = len(test_data)\nprint(f&quot;训练数据集的长度为&#123;train_data_size&#125;&quot;)\nprint(f&quot;测试数据集的长度为&#123;test_data_size&#125;&quot;)\n# dataLoader加载数据集\ndataLoader_train = DataLoader(train_data, batch_size=64)\ndataLoader_test = DataLoader(test_data, batch_size=64)\n\n\n# 搭建神经网络\nclass Net(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.model = Sequential(\n            Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Flatten(),\n            Linear(1024, 64),\n            Linear(64, 10)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\nnet = Net()\n# 交给GPU\nnet = net.to(device)\n\n# 创建损失函数\nloss_fn = nn.CrossEntropyLoss()\n# 交给GPU\nloss_fn = loss_fn.to(device)\n# 创建优化器\nlr = 0.01\noptimizer = torch.optim.SGD(net.parameters(), lr=lr)\n\n# tenser board\nwriter = SummaryWriter(&quot;logs&quot;)\n\n# 设置训练网络一些参数\n# 训练的轮数\ntotal_train_step = 0\n# 测试的轮数\ntotal_test_step = 0\n# 训练的轮数\nepoch = 10\nfor i in range(epoch):\n    print(&quot;--------第 &#123;&#125; 轮训练开始--------&quot;.format(i + 1))\n    # 训练开始\n    net.train()\n    for data in dataLoader_train:\n        imgs, targets = data\n        # 交给GPU\n        imgs = imgs.to(device)\n        targets = targets.to(device)\n        outputs = net(imgs)\n        loss = loss_fn(outputs, targets)  # 损失函数\n        optimizer.zero_grad()  # 清零梯度\n        loss.backward()  # 反向传播\n        optimizer.step()  # 优化器更新参数\n        total_train_step += 1\n        if total_train_step % 100 == 0:\n            print(&quot;训练次数：&#123;&#125;, Loss: &#123;&#125;&quot;.format(total_train_step, loss.item()))\n            writer.add_scalar(&quot;train_loss&quot;, loss.item(), total_train_step)\n    # 测试步骤开始\n    net.eval()\n    total_test_loss = 0\n    total_acc = 0\n\n    with torch.no_grad():  # 不进行梯度计算 with的作用是：临时关闭梯度计算，退出后自动恢复。在上下文里关闭，出了上下文就恢复。\n        for data in dataLoader_test:\n            imgs, targets = data\n            # 交给GPU\n            imgs = imgs.to(device)\n            targets = targets.to(device)\n            outputs = net(imgs)  # 测试步骤开始\n            loss = loss_fn(outputs, targets)  # 损失函数\n            total_test_loss += loss.item()  # 求和\n            acc = (outputs.argmax(1) == targets).sum()\n            total_acc += acc\n        print(&quot;整体测试集上的Loss：&#123;&#125;&quot;.format(total_test_loss))\n        print(&quot;整体测试集上的正确率：&#123;&#125;&quot;.format(total_acc / test_data_size))\n        writer.add_scalar(&quot;test_loss&quot;, total_test_loss, total_test_step)\n        writer.add_scalar(&quot;test_acc&quot;, total_acc / test_data_size, total_test_step)\n        total_test_step += 1\n    torch.save(net, &quot;net_&#123;&#125;.pth&quot;.format(i))\n    print(&quot;模型已保存&quot;)\nwriter.close()\n\n 验证套路（利用训练好的模型，给他提供输入）\nimport torch\nimport torchvision.transforms\nfrom PIL import Image\nfrom torch import nn\nfrom torch.nn import Sequential, Conv2d, MaxPool2d, Flatten, Linear\n\n# 如果是由GPU训练得到模型，则需要将模型移动到GPU上\n# 如果仅想使用CPU测试可以在model中使用：map_location=&quot;cpu&quot;\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\nimage_path = &quot;images/fff.png&quot;\n# 此处png是RGBA模式，我们转换成RGB模式\nimage = Image.open(image_path).convert(&quot;RGB&quot;)\ntrans = torchvision.transforms.Compose([\n    torchvision.transforms.Resize(size=(32, 32)),\n    torchvision.transforms.ToTensor(),\n])\nimage = trans(image)\n# 添加一个维度代表1张图片，并交给GPU\nimage = torch.reshape(image, (1, 3, 32, 32)).to(device)\nprint(image.shape)\n\n\n# 搭建神经网络\nclass Net(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.model = Sequential(\n            Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n            MaxPool2d(kernel_size=2, ceil_mode=False),\n            Flatten(),\n            Linear(1024, 64),\n            Linear(64, 10)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\n# 仅仅在CPU测试\n# model = torch.load(&quot;net_9.pth&quot;, weights_only=False, map_location=&quot;cpu&quot;)\nmodel = torch.load(&quot;net_9.pth&quot;, weights_only=False)\nmodel.eval()\nwith torch.no_grad():\n    output = model(image)\n    print(output)\n    print(torch.argmax(output))\n\n\ntorch.Size([1, 3, 32, 32])\ntensor([[ 4.1824, -0.9885,  2.4846, -0.8212,  0.5465, -1.7889, -1.6802, -0.3970,\n0.7540, -0.7934]])\ntensor(0)\n\n\n\n GPU50轮学习后\n\n后面出现了过拟合\n 附录\n简单入门了一下，后面还有很长的路要走，继续保持持续学习的动力。\n相关代码已公开在GitHub中：https://github.com/735690757/pytorch_stu_up\nSwim in the ocean of art and programming, weave the future with code art.\n","categories":["深度学习","深度学习基础"],"tags":["python","python_PyTorch"]},{"title":"基于PyTorch的手写数字识别","url":"/python/pytorch_nn_digital_identification/","content":" 基于PyTorch的手写数字识别\n MNIST数据集\nMNIST 是一个经典的手写数字识别数据集，包含 60,000 张用于训练的图片和 10,000 张用于测试的图片。每张图片分辨率为 28×28 像素，内容是数字 0–9 的手写体。我们将基于该数据集对模型进行训练和验证。\n\n 神经网络的定义\n 输入\n\n一张图片大小：1 × 28 × 28 （灰度图，通道数 = 1）\n\n\n 第一层卷积\nnn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n\n\n输入：1 × 28 × 28\n卷积核：5×5，padding=2 → 输出大小仍然是 28×28\n输出：16 × 28 × 28（提取 16 个不同特征图）\n\n\n ReLU 激活\nnn.ReLU()\n\n\n不改变形状，只是把负数变成 0。\n输出：16 × 28 × 28\n\n\n 第一次池化\nnn.MaxPool2d(kernel_size=2)\n\n\n池化窗口：2×2，步长=2 → 尺寸减半\n输出：16 × 14 × 14\n\n\n 第二层卷积\nnn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n\n\n输入：16 × 14 × 14\n输出：32 × 14 × 14\n\n\n ReLU 激活\nnn.ReLU()\n\n\n输出：32 × 14 × 14\n\n\n 第二次池化\nnn.MaxPool2d(kernel_size=2)\n\n\n尺寸再减半\n输出：32 × 7 × 7\n\n\n 展平\nnn.Flatten()\n\n\n把 32 × 7 × 7 展平成一维向量\n输出：32*7*7 = 1568\n\n\n 全连接层 1\nnn.Linear(32*7*7, 500)\n\n\n输入：1568\n输出：500\n\n\n ReLU 激活\nnn.ReLU()\n\n\n输出：500\n\n\n 全连接层 2\nnn.Linear(500, 100)\n\n\n输入：500\n输出：100\n\n\n 全连接层 3（输出层）\nnn.Linear(100, 10)\n\n\n输入：100\n输出：10（对应数字 0–9 的类别）\n\n 总结\n网络的流程就是：\n输入 1×28×28 图像 → 两次卷积 + ReLU + 池化提取特征 → 展平 → 三层全连接分类 → 输出 10 个类别的分数（预测数字 0–9）。\n\n 存在的问题\n全连接层数量和大小\n\n两个全连接层（500 → 100 → 10）对 MNIST 来说稍微大了一点\n可以简化为 500 → 10 或 256 → 10，训练更快，参数更少\n\n卷积层通道数\n\n第一层 16、第二层 32 可以再小一点（如 8 → 16），MNIST 太简单了，用太多特征图可能没必要\n\n 代码\nimport torch\nfrom torch import nn\n\n\nclass Net(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.model = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, ceil_mode=False),\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, ceil_mode=False),\n            nn.Flatten(),\n            nn.Linear(32 * 7 * 7, 500),\n            nn.ReLU(),\n            nn.Linear(500, 100),\n            nn.Linear(100, 10)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\nif __name__ == &#x27;__main__&#x27;:\n    net = Net()\n    print(net)\n    output = net(torch.ones((64, 1, 28, 28)))\n    print(output.shape)\n\n 模型训练\n 导入库\nimport torch\nimport torch.nn as nn\nimport torchvision.datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom nn_DI_NNFramework import Net\n\n\ntorch：PyTorch 主库\ntorch.nn：神经网络模块，包括各种层、损失函数\ntorchvision.datasets：常用数据集（这里用 MNIST）\nDataLoader：批量加载数据\ntransforms：对图像进行转换（如标准化、张量化）\nSummaryWriter：TensorBoard 日志写入，用于可视化训练过程\nNet：自己定义的神经网络\n\n\n 数据预处理\ntransform = transforms.Compose([\n    transforms.ToTensor(),  # 转为 PyTorch 张量，形状为 [C,H,W]\n    transforms.Normalize((0.1307,), (0.3081,))  # 标准化：均值0.1307，标准差0.3081\n])\n\n\n将图片从 [0,255] 映射到 [0,1]\n再做标准化，便于模型收敛更快\n\n\n TensorBoard 日志 &amp; 设备设置\nwriter = SummaryWriter(&quot;log_digital&quot;)\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n\n\nSummaryWriter 用于记录训练和测试的 loss、accuracy\ndevice 判断是否有 GPU 可用，有就用 GPU 否则 CPU\n\n\n 加载数据集\ndatasets_mnist_train = torchvision.datasets.MNIST(&quot;data&quot;, train=True, transform=transform, download=True)\ndatasets_mnist_test = torchvision.datasets.MNIST(&quot;data&quot;, train=False, transform=transform, download=True)\n\n\ntrain=True：训练集 60000 张图片\ntrain=False：测试集 10000 张图片\n\n\n DataLoader\ndataLoader_train = DataLoader(datasets_mnist_train, batch_size=64, shuffle=True, num_workers=0)\ndataLoader_test = DataLoader(datasets_mnist_test, batch_size=64, shuffle=False, num_workers=0)\n\n\nbatch_size=64：每次送入网络 64 张图片\nshuffle=True：打乱训练顺序\nnum_workers=0：加载数据的线程数\n\n\n 打印数据集大小\ntrain_data_size = len(datasets_mnist_train)\ntest_data_size = len(datasets_mnist_test)\nprint(f&quot;训练数据集的长度为&#123;train_data_size&#125;&quot;)\nprint(f&quot;测试数据集的长度为&#123;test_data_size&#125;&quot;)\n\n\n方便确认数据集是否加载成功\n\n\n 创建网络和损失函数\nnet = Net().to(device)\nloss_fn = nn.CrossEntropyLoss().to(device)\n\n\nNet()：实例化你的卷积神经网络\nCrossEntropyLoss：常用的多分类损失函数，内部包含 softmax\n\n\n 优化器\noptimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n\n\n使用随机梯度下降（SGD）优化\nlr=0.01：学习率\nmomentum=0.5：动量，帮助加速收敛\n\n\n 训练设置\ntotal_train_step = 0\ntotal_test_step = 0\nepoch = 15\n\n\ntotal_train_step：训练总步数\ntotal_test_step：测试总步数\nepoch=15：训练轮数\n\n\n 训练循环\nfor i in range(epoch):\n    print(&quot;--------第 &#123;&#125; 轮训练开始--------&quot;.format(i + 1))\n    net.train()  # 设置网络为训练模式\n\n\nnet.train()：启用训练模式（例如 Dropout、BatchNorm 生效）\n\n 训练步\nfor data in dataLoader_train:\n    imgs, targets = data\n    imgs = imgs.to(device)\n    targets = targets.to(device)\n    outputs = net(imgs)\n    loss = loss_fn(outputs, targets)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n\nimgs：图片 batch\ntargets：标签 batch\nloss.backward()：计算梯度\noptimizer.step()：更新参数\noptimizer.zero_grad()：清空上一步梯度\n\n 记录日志\nif total_train_step % 100 == 0:\n    print(f&quot;训练次数：&#123;total_train_step&#125;，loss：&#123;loss&#125;&quot;)\n    writer.add_scalar(&quot;train_loss&quot;, loss.item(), total_train_step)\n\n\n每 100 步打印 loss\n写入 TensorBoard\n\n\n 测试循环\nnet.eval()  # 设置网络为评估模式\ntotal_test_loss = 0\ntotal_test_accuracy = 0\nwith torch.no_grad():  # 不计算梯度\n    for data in dataLoader_test:\n        imgs, targets = data\n        imgs = imgs.to(device)\n        targets = targets.to(device)\n        outputs = net(imgs)\n        loss = loss_fn(outputs, targets)\n        total_test_loss += loss.item()\n        accuracy = (outputs.argmax(1) == targets).sum()\n        total_test_accuracy += accuracy.item()\n\n\nnet.eval()：关闭 Dropout、BatchNorm 等训练特性\ntorch.no_grad()：节省显存，不计算梯度\noutputs.argmax(1)：得到预测的数字\n累计 loss 和正确数\n\n 打印和记录\nprint(&quot;整体测试集上的Loss：&#123;&#125;&quot;.format(total_test_loss))\nprint(&quot;整体测试集上的正确率：&#123;&#125;&quot;.format(total_test_accuracy / test_data_size))\nwriter.add_scalar(&quot;test_loss&quot;, total_test_loss, total_test_step)\nwriter.add_scalar(&quot;test_accuracy&quot;, total_test_accuracy / test_data_size, total_test_step)\ntotal_test_step += 1\n\n\n 保存模型\ntorch.save(net, &quot;net_DI.pth&quot;)\nprint(&quot;模型已保存&quot;)\nwriter.close()\n\n\n将训练好的网络保存为 net_DI.pth\n关闭 TensorBoard 写入器\n\n 全部代码\nimport torch\nimport torch.nn as nn\nimport torchvision.datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom nn_DI_NNFramework import Net\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),  # 将图像转换为张量\n    transforms.Normalize((0.1307,), (0.3081,))  # 标准化图像\n])\nwriter = SummaryWriter(&quot;log_digital&quot;)\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\ndatasets_mnist_train = torchvision.datasets.MNIST(&quot;data&quot;, train=True, transform=transform,\n                                                  download=True)\ndatasets_mnist_test = torchvision.datasets.MNIST(&quot;data&quot;, train=False, transform=transform,\n                                                 download=True)\ndataLoader_train = DataLoader(datasets_mnist_train, batch_size=64, shuffle=True, num_workers=0)\ndataLoader_test = DataLoader(datasets_mnist_test, batch_size=64, shuffle=False, num_workers=0)\n\n# 获取数据集大小\ntrain_data_size = len(datasets_mnist_train)\ntest_data_size = len(datasets_mnist_test)\nprint(f&quot;训练数据集的长度为&#123;train_data_size&#125;&quot;)\nprint(f&quot;测试数据集的长度为&#123;test_data_size&#125;&quot;)\n\nnet = Net().to(device)\n# 创建损失函数\nloss_fn = nn.CrossEntropyLoss()\n# 交给GPU\nloss_fn = loss_fn.to(device)\n# 创建优化器\noptimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n# 设置训练网络一些参数\n# 训练的轮数\ntotal_train_step = 0\n# 测试的轮数\ntotal_test_step = 0\n# 训练的轮数\nepoch = 15\n\nfor i in range(epoch):\n    print(&quot;--------第 &#123;&#125; 轮训练开始--------&quot;.format(i + 1))\n    net.train()\n    for data in dataLoader_train:\n        imgs, targets = data\n        imgs = imgs.to(device)\n        targets = targets.to(device)\n        outputs = net(imgs)\n        loss = loss_fn(outputs, targets)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_train_step += 1\n        if total_train_step % 100 == 0:\n            print(f&quot;训练次数：&#123;total_train_step&#125;，loss：&#123;loss&#125;&quot;)\n            writer.add_scalar(&quot;train_loss&quot;, loss.item(), total_train_step)\n    net.eval()\n    total_test_loss = 0\n    total_test_accuracy = 0\n    with torch.no_grad():\n        for data in dataLoader_test:\n            imgs, targets = data\n            imgs = imgs.to(device)\n            targets = targets.to(device)\n            outputs = net(imgs)\n            loss = loss_fn(outputs, targets)\n            total_test_loss += loss.item()\n            accuracy = (outputs.argmax(1) == targets).sum()\n            total_test_accuracy += accuracy.item()\n\n        print(&quot;整体测试集上的Loss：&#123;&#125;&quot;.format(total_test_loss))\n        print(&quot;整体测试集上的正确率：&#123;&#125;&quot;.format(total_test_accuracy / test_data_size))\n        writer.add_scalar(&quot;test_loss&quot;, total_test_loss, total_test_step)\n        writer.add_scalar(&quot;test_accuracy&quot;, total_test_accuracy / test_data_size, total_test_step)\n        total_test_step += 1\n\ntorch.save(net, &quot;net_DI.pth&quot;)\nprint(&quot;模型已保存&quot;)\nwriter.close()\n\n 训练过程\n\n模型在15轮训练后，测试集上准确率超过99%\n 简单验证\n 导入库\nimport torch\nimport torchvision\nfrom PIL import Image\nimport nn_DI_NNFramework\n\n\ntorch：PyTorch 主库\ntorchvision：用于图像处理和变换\nPIL.Image：处理图片文件\nnn_DI_NNFramework：你自己定义的网络（Net）\n\n\n 设备设置\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n\n\n判断是否有 GPU 可用，如果有就用 GPU，否则用 CPU\n\n\n 加载模型\nmodel = torch.load(&quot;net_DI.pth&quot;, weights_only=False)\n\n\n加载之前训练好的模型 net_DI.pth\nweights_only=False 表示加载整个模型对象，而不仅仅是权重\n\n\n一般推荐保存 state_dict 加载权重，这样更灵活：\n\nnet = nn_DI_NNFramework.Net().to(device)\nnet.load_state_dict(torch.load(&quot;net_DI.pth&quot;))\n\n\n 打开图片\nimage_path = &quot;images/333.png&quot;\nimage = Image.open(image_path).convert(&quot;L&quot;)\n\n\n打开图片\n.convert(&quot;L&quot;)：将图片转换为灰度（1 通道）\n\n\n 图像预处理\ntrans = torchvision.transforms.Compose([\n    torchvision.transforms.Resize(size=(28, 28)),  # 调整图片大小\n    torchvision.transforms.ToTensor(),             # 转为张量\n])\nimage = trans(image)\n\n\n将图片缩放到 28×28（MNIST 输入尺寸）\n转成 PyTorch 张量，形状 [C,H,W]，范围 [0,1]\n\n\n 调整 batch 维度并发送到 GPU\nimage = torch.reshape(image, (1, 1, 28, 28)).to(device)\nprint(image.shape)\n\n\n模型期望输入 [batch, channel, height, width]\n这里 batch=1，channel=1，28×28\n.to(device)：把图片发送到 GPU 或 CPU\n\n\n 模型推理\nmodel.eval()\nwith torch.no_grad():\n    output = model(image)\n    print(output)\n\n\nmodel.eval()：评估模式（关闭 Dropout/BatchNorm）\ntorch.no_grad()：不计算梯度，节省显存\noutput：模型输出 logits（长度为 10 的向量，每个元素对应数字 0–9 的得分）\n\n\n 获取预测结果\nitem = torch.argmax(output).item()\nprint(item)\n\n\ntorch.argmax(output)：找到得分最大的索引（预测数字）\n.item()：把张量转成 Python 整数\n\n 全部代码\nimport torch\nimport torchvision\nfrom PIL import Image\n\nimport nn_DI_NNFramework\n\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n\nmodel = torch.load(&quot;net_DI.pth&quot;, weights_only=False)\nimage_path = &quot;images/333.png&quot;\nimage = Image.open(image_path).convert(&quot;L&quot;)\ntrans = torchvision.transforms.Compose([\n    torchvision.transforms.Resize(size=(28, 28)),\n    torchvision.transforms.ToTensor(),\n])\nimage = trans(image)\n# 添加一个维度代表1张图片，并交给GPU\nimage = torch.reshape(image, (1, 1, 28, 28)).to(device)\nprint(image.shape)\nmodel.eval()\nwith torch.no_grad():\n    output = model(image)\n    print(output)\n    item = torch.argmax(output).item()\n    print(item)\n\n 自定义测试\n\n\n\n0\n2\n3\n7\n9\n\n\n\n\n\n\n\n\n\n\n\n\n以上手写数字均准确识别\n","categories":["深度学习","项目与实战"],"tags":["python","python_PyTorch","pytorch_nn_digital_identification"]},{"title":"Methods and datasets on semantic segmentation for Unmanned Aerial Vehicle remote sensing images：A review","url":"/paper/review_remote_sensing_m_d/","content":" Methods and datasets on semantic segmentation for Unmanned Aerial Vehicle remote sensing images: A review\n\n\n【Original Link】 Methods and datasets on semantic segmentation for Unmanned Aerial Vehicle remote sensing images：A review\nDepartment of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, Sichuan, PR China\nVolume 211, May 2024, Pages 1-34\nAuthors\n\nJian Cheng\nChangjian Deng\nYanzhou Su\nZeyu An\nQi Wang\n\n\n\n\n Abstract\nUnmanned Aerial Vehicle (UAV) has seen a dramatic rise in popularity for remote-sensing image acquisition and analysis in recent years.\n近年来，无人机（UAV）在遥感图像采集和分析领域的流行度大幅上升。\nIt has brought promising results in low-altitude monitoring tasks that require detailed visual inspections.\n它在需要详细目视检查的低空监测任务中取得了令人鼓舞的成果。\nSemantic segmentation is one of the hot topics in UAV remote sensing image analysis, as its capability to mine contextual semantic information from UAV images is crucial for achieving a fine-grained understanding of scenes.\n语义分割是无人机遥感图像分析中的热门话题之一，其从无人机图像中挖掘上下文语义信息的能力对于实现对场景的细致理解至关重要。\nHowever, in the remote sensing field, recent reviews have not focused on combining ‘‘UAV remote sensing’’ and ‘‘semantic segmentation’’ to summarize the advanced works and future trends.\n然而，在遥感领域，近期综述并未将“无人机遥感”与“语义分割”结合起来，以总结先进的工作成果和未来趋势。\nIn this study, we focus primarily on describing various recent semantic segmentation methods applied in UAV remote sensing images and summarizing their advantages and limitations. According to the distinction in modeling contextual semantic information, we have categorized and outlined the methods based on graph-based contextual models and deep-learning-based models.\n本研究主要介绍了无人机遥感图像中应用的各种近期语义分割方法，并总结其优缺点。根据对上下文语义信息建模的区分，我们基于基于图的上下文模型和基于深度学习的模型对方法进行了分类和概述。\nPublicly available UAV-based image datasets are also gathered to encourage systematic research on advanced semantic segmentation methods. We provide quantitative results of representative methods on two high-resolution UAV-based image datasets for fair comparisons and discussions in terms of semantic segmentation accuracy and model inference efficiency.\n还收集了公开可用的无人机图像数据集，以鼓励系统性研究高级语义分割方法。我们提供了两种高分辨率无人机图像数据集中代表性方法的定量结果，以便公平比较和讨论语义分割准确性和模型推断效率。\nBesides, this paper concludes some remaining challenges and future directions in semantic segmentation for UAV remote sensing images and points out that methods based on deep learning will become the future research trend.\n此外，本文总结了无人机遥感图像语义分割中剩余的一些挑战和未来方向，并指出基于深度学习的方法将成为未来研究趋势。\n Introduction\nSemantic segmentation is the process of dividing an image into disjoint regions of the same class or semantics and labeling these regions with pixel-level annotations, where each pixel of the given image requires a label of the desired object class (Yu et al., 2018b; Mo et al., 2022).\n语义分割是将图像划分为同一类或语义的不相交区域，并用像素级注释标记这些区域的过程，其中给定图像的每个像素都需要对目标对象类别进行标签（Yu 等，2018b;Mo 等，2022）。\nUnlike image classification, which identifies the primary content of an entire image and categorizes it into a specific class, semantic segmentation provides detailed delineations of multiple classes and their respective regions in the image (Yuan et al., 2021)\n与图像分类不同，后者是识别整张图像的主要内容并将其归类到特定类别，语义分割则提供了图像中多个类别及其相应区域的详细划分（Yuan 等，2021）\nIn fact, image semantic segmentation is derived from image segmentation (Ohta et al., 1978), but puts more emphasis on capturing the semantic information of spatial context, so that the segmented areas have semantic meaning.\n事实上，图像语义分割源自图像分割（Ohta 等，1978），但更强调捕捉空间上下文的语义信息，使分割区域具有语义意义。\nIt is the perceptual basis of many computer vision tasks and has been widely employed in diverse image analysis applications, such as the natural and medical image parsing (Yu et al., 2018b; Guo et al., 2018; Asgari Taghanaki et al., 2021), automatic driving systems (Siam et al., 2017; Feng et al., 2020), road extraction (Abdollahi et al., 2020), perception of urban features (Neupane et al., 2021), landuse mapping (Zang et al., 2021), and many others.\n它是许多计算机视觉任务的感知基础，并被广泛应用于多种图像分析领域，如自然和医学图像解析（Yu 等，2018b;Guo 等，2018;Asgari Taghanaki 等，2021），自动驾驶系统（Siam 等，2017;Feng 等，2020）、道路提取（Abdollahi 等，2020）、城市特征感知（Neupane 等，2021）、土地利用地图绘制（Zang 等，2021）等。\nRemote sensing images collected by various platforms, such as satellites, aircraft, and UAVs (Toth and Jóźków, 2016; Bhardwaj et al., 2016), provide the data basis for semantic segmentation.\n遥感图像由卫星、飞机和无人机等多种平台收集的图像（Toth 和 Jóźków，2016;Bhardwaj 等，2016），为语义分割提供了数据基础。\nThe continuously evolving semantic segmentation technology not only demonstrates superior performance in diverse and data-rich remote sensing problems (Ball et al., 2017; Yuan et al., 2021), but its integration with UAVs also offers a feasible solution for the fine-grained scene analysis of low-altitude remote sensing (Toth and Jóźków, 2016; Bhardwaj et al., 2016; Yao et al., 2019; Nex et al., 2022; Zhang and Zhu, 2023).\n持续演进的语义分割技术不仅在多样且数据丰富的遥感问题中表现出优异性能（Ball 等，2017;Yuan 等，2021），其与无人机的集成也为低空遥感的细粒度场景分析提供了可行的解决方案（Toth 和 Jóźków，2016;Bhardwaj 等，2016;Yao 等，2019;Nex 等，2022;Zhang 和 Zhu，2023）。\nThe UAV platforms, also known as aerial robots or drones, have demonstrated many unique advantages compared with satellite remote sensing platforms, such as lower flight altitude close to the ground surface, well-controlled path planning without the restriction of revisit intervals, flexible flight schedules to access specific areas, flexible optical sensor integration, and especially, lower developing and operational costs for commercial and academic purposes (Yao et al., 2019; Osco et al., 2021; Zhang and Zhu, 2023).\n无人机平台，也称为空中机器人或无人机，相比卫星遥感平台展现出许多独特优势，如靠近地面的较低飞行高度、无重访间隔限制的良好路径规划、灵活的飞行计划以进入特定区域、灵活的光学传感器集成，尤其是在商业和学术用途上更低的开发和运营成本（Yao 等，2019;Osco 等，2021;Zhang 和 Zhu，2023）。\n\nThanks to the flexibility of flying altitudes and sensor integration, UAVs are suitable for resource management and visual inspection tasks that require high labor costs or may affect personal safety, such as bridge condition assessment (Liang et al., 2023), insulator inspections (Ma et al., 2021), early wildfire detection (Bouguettaya et al., 2022c), and bridge damage detection (Feroz and Abu Dabous, 2021).\n得益于飞行高度的灵活性和传感器集成，无人机适合资源管理和目视检查任，这些需要高人工成本或可能影响个人安全的任务，如桥梁状况评估（Liang 等，2023）、绝缘体检查（马等，2021）、早期野火检测（布盖塔亚等，2022c）以及桥梁损坏检测（费罗兹和阿布达布斯，2021）。\nThe hopeful prospect of UAV applications has been well demonstrated and tested in the current literature.\n无人机应用的前景在现有文献中已被充分验证和验证。\nFor instance, in forestry applications (Torresan et al., 2017; Guimarães et al., 2020; Diez et al., 2021), some efforts apply UAVs to pest infestation monitoring (Lehmann et al., 2015), forest fire detection (Yuan et al., 2015), individual tree detection (Chadwick et al., 2020; Gibril et al., 2021), tree species classification (Fujimoto et al., 2019; Kentsch et al., 2020; Egli and Höpke, 2020), among others.\n例如，在林业应用中（Torresan 等，2017;Guimarães 等，2020;Diez 等，2021），部分研究将无人机应用于害虫侵扰监测（Lehmann 等，2015）、森林火灾检测（Yuan 等，2015）、单棵树木检测（Chadwick 等，2020;Gibril 等，2021），树种分类（Fujimoto 等，2019;Kentsch 等，2020;Egli 和 Höpke，2020），等。\nIn damage assessment tasks, relevant literature has demonstrated the efficiency and adaptability of analyzing structure cracks (Zhong et al., 2018; Han et al., 2022), bridge inspection and management (Ayele et al., 2020; Feroz and Abu Dabous, 2021), and disaster damage mapping (Kerle et al., 2019).\n在损伤评估任务中，相关文献已证明结构裂纹分析的效率和适应性（Zhong 等，2018;Han 等，2022），桥梁检查与管理（Ayele 等，2020;Feroz 和 Abu Dabous，2021），以及灾害损害测绘（Kerle 等，2019）。\nIn precision agriculture (Adão et al., 2017; Tsouros et al., 2019), many advanced works have focused on weed mapping (Huang et al., 2018a; Sa et al., 2018; Deng et al., 2020), crop lodging detection (Song et al., 2020; Yang et al., 2020; Su et al., 2022), vegetation growth and health monitoring (Jung et al., 2018; Han et al., 2018; Kerkech et al., 2018, 2020), and many others.\n在精准农业中（Adão等，2017;Tsouros 等，2019），许多先进研究聚焦于杂草测绘（Huang 等，2018a;Sa 等，2018;邓等，2020），作物倒伏检测（宋等，2020;Yang 等，2020;Su 等，2022），植被生长与健康监测（Jung 等，2018;Han 等，2018;Kerkech等，2018,2020），以及许多其他研究。\nMoreover, other literature in which the use of UAV delivered promising results in monitoring and counting wildlife (Chamoso et al., 2014; Sundaram and Loganathan, 2020), land use and land cover (LULC) (Zhang et al., 2017; Al-Najjar et al., 2019), urban mapping (Chen et al., 2018c; Lyu et al., 2020), and 3D mapping applications (Nex and Remondino, 2014; Kölle et al., 2021).\n此外，其他文献中无人机在监测和计数野生动物方面取得了有希望的成果（Chamoso 等，2014;Sundaram 和 Loganathan，2020），土地利用与土地覆盖（LULC）（Zhang 等，2017;Al-Najjar 等，2019），城市制图（Chen 等，2018c;Lyu 等，2020），以及三维映射应用（Nex 和 Remondino，2014;Kölle 等，2021）。\nAmong these applications, the combination of UAV remote sensing and image semantic segmentation exhibits several distinctive advantages compared with satellite remote sensing.\n在这些应用中，无人机遥感与图像语义分割的结合相比卫星遥感展现出多项独特优势。\nFirstly, UAV-based semantic segmentation is more suitable for practical application scenarios requiring higher temporal and spatial resolution, such as environmental and agricultural monitoring (Adão et al., 2017; Tsouros et al., 2019), emergency response (Shamsoshoara et al., 2021), and disaster assessment (Rahnemoonfar et al., 2021; Chowdhury et al., 2022, 2020).\n首先，基于无人机的语义分割更适合需要更高时间和空间分辨率的实际应用场景，如环境和农业监测（Adão 等，2017;Tsouros 等，2019）、应急响应（Shamsoshoara 等，2021）和灾难评估（Rahnemoonfar 等，2021;Chowdhury 等，2022,2020）。\nBesides, semantic segmentation models can extract diverse and finegrained semantic feature representations from high-resolution remote sensing images captured at different altitudes and angles by UAVs, contributing to the comprehensive understanding of complex scenes from multi-view directions.\n此外，语义切割模型还能从无人机在不同高度和角度拍摄的高分辨率遥感图像中提取多样且细粒度的语义特征表示，有助于从多视角方向全面理解复杂场景。\nHence, UAV-based image semantic segmentation plays a crucial role in image analysis tasks with fine semantic objects and complex backgrounds, such as urban scene perception (Chen et al., 2018c; Nigam et al., 2018; Lyu et al., 2020) and crack detection (Zhong et al., 2018; Feroz and Abu Dabous, 2021; Han et al., 2022; Liang et al., 2023).\n因此，基于UAV的图像语义分割在涉及精细语义对象和复杂背景的图像分析任务中起着关键作用，如城市场景感知（Chen 等，2018c;Nigam 等，2018;Lyu 等，2020）和裂纹检测（Zhong 等，2018;Feroz 和 Abu Dabous，2021;Han 等，2022;Liang 等，2023）。\nAdditionally, autonomous UAVs without human intervention have been the research focus in recent years, which requires UAVs to possess independent capabilities for scene perception, data processing, and decision-making (Nex et al., 2022).\n此外，近年来研究重点关注无人机自主无人机，要求无人机具备独立的场景感知、数据处理和决策能力（Nex 等，2022）。\nThe development and application of UAV platforms have contributed to the emergence of comprehensive reviews.\n无人机平台的发展和应用促进了综合评审的出现。\nGreat efforts had been made to summarize the research status and specific applications of UAVs in various fields, such as 3D mapping for geomatics applications (Nex and Remondino, 2014), urban land cover classification (Yan et al., 2015), glaciological research and applications (Bhardwaj et al., 2016), automatic cadastral mapping (Crommelinck et al., 2016), agriculture and forestry (Adão et al., 2017; Guimarães et al., 2020), structural damage mapping (Kerle et al., 2019), monitoring in mining areas (Ren et al., 2019), and cultural heritage and archaeology (Themistocleous, 2020).\n在总结无人机在多个领域的研究现状和具体应用方面，已付出了大量努力，如地理信息技术应用的三维制图（Nex 和 Remondino，2014）、城市土地覆盖分类（Yan等，2015）、冰川学研究与应用（Bhardwaj 等，2016）、自动地籍测绘（Crommelinck 等，2016）、农业和林业（Adão 等， 2017;Guimarães等，2020年）、结构损伤测绘（Kerle等，2019年）、矿区监测（任等，2019年）以及文化遗产与考古学（Themistocleous，2020年）。\nThese reviews provided an overview of data acquisition sensors, data processing and data analysis techniques, and practical applications in their respective research fields.\n这些综述概述了数据采集传感器、数据处理和分析技术及其在各自研究领域的实际应用。\nMoreover, some other reviews were not limited to specific research fields  and comprehensively summarized the advanced technologies, broad applications, and future trends from the UAV-based remote sensing perspective.\n此外，一些综述不仅限于特定研究领域，全面总结了基于无人机的遥感技术、广泛应用和未来趋势。\nColomina and Molina (2014) presented a discussion about the evolution and advanced technology of Unmanned Aerial Systems (UAS) in the field of photogrammetry and remote sensing.\nColomina和Molina（2014）就无人机系统（UAS）在摄影测量和遥感领域的演变与先进技术进行了讨论。\nYao et al. (2019) provided a comprehensive review of recent advanced studies involving UAV sensors, data analysis, and their remote sensing applications.\nYao等人（2019）对近期涉及无人机传感器、数据分析及其遥感应用的先进研究进行了全面综述。\nNex et al. (2022) investigated the best practices of remote sensing and mapping applications by UAVs and discussed the future trend and impact. Unfortunately, only a few of these reviews briefly mentioned high-level scene understanding and semantic analysis methods that related to pixel-accurate semantic segmentation for UAV-based images.\nNex 等人（2022）研究了无人机遥感和制图应用的最佳实践，并讨论了未来趋势和影响。遗憾的是，只有少数综述简要提及与无人机图像像素精确语义分割相关的高级场景理解和语义分析方法。\nFor instance, Crommelinck et al. (2016) and Kerle et al. (2019) had mentioned the applications of random fields and convolutional neural networks for UAV image analysis, but the summary of the characteristics of different data analysis methods is still lacking.\n例如，Crommelinck等（2016）和Kerle等（2019）提到了随机场和卷积神经网络在无人机图像分析中的应用，但不同数据分析方法的特性总结仍然不足。\nNex et al. (2022) briefly summarized commonly used semantic segmentation methods and best practices, but did not analyze the advantages and disadvantages of semantic segmentation methods in detail.\nNex 等人（2022）简要总结了常用的语义分割方法及其最佳实践，但未详细分析语义分割方法的优缺点。\nMore recently, Deep Learning (DL) has a profound impact on remote sensing image semantic segmentation (Kotaridis and Lazaridou, 2021; Neupane et al., 2021; Yuan et al., 2021).\n近年来，深度学习（DL）对遥感图像语义分割产生了深远影响（Kotaridis 和 Lazaridou，2021;Neupane 等，2021;Yuan 等，2021）。\nAs of recently, some literature reviews combining DL and UAV image parsing tasks focus on various approaches within different task-oriented applications.\n最近，一些结合深度学习和无人机图像解析任务的文献综述聚焦于不同面向任务的应用中的不同方法。\nFor instance, Bouguettaya et al. (2022c) briefly introduced DL-based algorithms applied to early wildfire detection, such as image classification, object detection, and semantic segmentation.\n例如，Bouguettaya 等人（2022c）简要介绍了应用于早期野火检测的基于DL的算法，如图像分类、物体检测和语义分割。\nBouguettaya et al. (2022a) reported object-based and pixel-based crop classification methods combining DL-based algorithms.\nBouguettaya等人（2022a）报告了结合DL算法的基于对象和像素的作物分类方法。\nBouguettaya et al. (2022b) summarized various types of DL-based architectures for vehicle detection in UAV images.\nBouguettaya 等人（2022b）总结了无人机图像中基于DL的各种车辆检测架构。\nShahi et al. (2023) reviewed the recent advances in crop disease detection of UAV remote sensing based on machine learning and deep learning technologies.\nShahi等人（2023）回顾了基于机器学习和深度学习技术的无人机遥感作物病害检测的最新进展。\nOsco et al. (2021) and Nex et al. (2022) are more concerned with DL-based methods as a future trend for the field of UAV remote sensing.\nOsco等（2021）和Nex等（2022）更关注基于DL的方法，视其为无人机遥感领域的未来趋势。\nAlthough, from these recent reviews, various DL-based methods have been validated in many UAV image analysis tasks, such as image classification, object detection, and semantic segmentation, a comprehensive summary and comparison of the characteristics, advantages, and limitations of semantic segmentation methods combined with UAV remote sensing images is currently absent.\n尽管这些最新综述已验证多种基于DL的方法在许多无人机图像分析任务中，如图像分类、物体检测和语义分割，但目前尚缺乏对语义分割方法与无人机遥感图像结合的特性、优势和局限性的全面总结与比较。\nTherefore, it is necessary to summarize the characteristics and development of specific semantic segmentation algorithms, provide scholars and practitioners with the quantitative comparison of representative models, as well as identify remaining challenges and future research directions in the image semantic segmentation of UAV remote sensing.\n因此，有必要总结特定语义分割算法的特性和发展，向学者和实践者提供代表性模型的定量比较，并识别无人机遥感图像语义分割的剩余挑战和未来研究方向。\nDifferent from orbital and other aerial sensing methods of acquisition, the remote sensing images captured from UAVs have the characteristics of higher resolution, complex scenes, and appearance variations caused by multi-direction angles of view (Nigam et al., 2018; Chen et al., 2018c; Lyu et al., 2020).\n与轨道及其他空中感测采集方法不同，从无人机捕获的遥感图像具有更高分辨率、复杂场景以及多方向视角引起的外观变化（Nigam 等，2018;Chen 等，2018c;Lyu 等，2020）。\nTypically, semantic segmentation methods designed for natural images or remote sensing images require single-channel or multi-channel images of a fixed size as the data input (Yu et al., 2018b; Dias et al., 2020; Asgari Taghanaki et al., 2021; Kotaridis and Lazaridou, 2021; Yuan et al., 2021).\n通常，为自然图像或遥感图像设计的语义分割方法需要以固定大小的单通道或多通道图像作为数据输入（Yu 等，2018b;Dias 等，2020;Asgari Taghanaki 等，2021;Kotaridis 和 Lazaridou，2021年;Yuan 等，2021）。\nBy slightly changing the input image scale and the number of input image channels of the model structure, these methods can also be applied to process UAV-acquired remote sensing images (Huang et al., 2018b, 2021a; Osco et al., 2021).\n通过稍微改变输入图像比例和模型结构的输入图像通道数，这些方法也可以应用于处理无人机采集的遥感图像（Huang 等，2018b，2021a;Osco等，2021）。\nBesides, we found that back-end users also prefer to make minor modifications to the internal architecture of models, which have been thoroughly validated for semantic segmentation tasks in natural or remote sensing images, to accomplish semantic segmentation for UAV-based images (Sui et al., 2020; Barmpoutis et al., 2020; Huang et al., 2021a).\n此外，我们发现后端用户也更倾向于对模型内部架构做小幅修改，这些模型已经过充分验证，适用于自然或遥感图像的语义分割任务，以实现基于无人机的图像语义分割（Sui 等，2020;Barmpoutis 等，2020;Huang 等，2021a）。\nAlthough performance-proven image semantic segmentation models have been shown to be applicable to UAV-based images, the off-the-shelf models may not always offer the optimal solution for UAV-based image semantic segmentation (Nex et al., 2022).\n尽管经过性能验证的图像语义分割模型已被证明可应用于无人机图像，但现成的模型可能并不总是为无人机图像语义分割提供最佳解决方案（Nex 等，2022）。\nThere remains a need to explore unique spatial, spectral, geometric, and multi-modal patterns (Beleznai et al., 2018; Maimaitijiang et al., 2020; Song et al., 2020) in UAV remote sensing images to design effective feature representations of semantic objects for accurate image semantic segmentation.\n仍然需要探索独特的空间、光谱、几何和多模态模式（Beleznai 等，2018;Maimaitijiang 等，2020;Song 等，2020）在无人机遥感图像中用于设计有效的语义对象特征表示，实现准确的图像语义分割。\nBased on the differences in modeling semantic contextual information, semantic segmentation methods for UAV remote sensing images can be divided into two categories: graphbased contextual models and DL-based models.\n基于对语义上下文信息建模的差异，无人机遥感图像的语义分割方法可分为两类：基于图的上下文模型和基于DL的模型。\nThe former explicitly establishes stochastic dependencies between pixels or regions by probabilistic undirected graphs (Jordan, 2004; Drton and Maathuis, 2017) to integrate local or global spatial context information, while the latter adaptively extracts high-level semantic features of semantic objects in images (LeCun et al., 2015).\n前者通过概率无向图明确建立了像素或区域之间的随机依赖关系（Jordan，2004;Drton 和 Maathuis，2017）以整合局部或全局的空间上下文信息，而后者则自适应地提取图像中语义对象的高层次语义特征（LeCun 等，2015）。\nBesides, most advanced semantic segmentation works applied to UAV-based images did not open-source their datasets due to flight rules and potential confidentiality.\n此外，大多数用于无人机图像的高级语义分割研究因飞行规则和潜在保密性问题，未将数据集开源。\nTo reduce the labor cost for investigating relevant open-source UAV remote sensing image datasets, another motivation for this review is to summarize publicly available datasets and list the corresponding parameters, such as applications, spatial resolution, classes, and modalities.\n为了降低研究相关开源无人机遥感图像数据集的人工成本，本综述的另一个动机是总结公开数据集并列出相应参数，如应用、空间分辨率、类别和模态。\nWe concentrate our review on advanced methods and available datasets for semantic segmentation applied to UAV optical imagery, rather than onboard instrumentation, payload, flight autonomy, data acquisition techniques, or specific applications.\n我们重点回顾应用于无人机光学影像的语义分割的先进方法和可用数据集，而非机载仪器、有效载荷、飞行自主性、数据采集技术或具体应用。\nThe remainder of the paper is organized as follows.\n论文其余部分的组织如下。\nSection 2 presents the graph-based contextual models for semantic segmentation and analyzes the advantages and limitations of the individual models according to the model characteristics.\n第二部分介绍了基于图的语义分割上下文模型，并根据模型特性分析各模型的优缺点。\nSection 3 reviews the popular semantic segmentation methods built on the DL framework, including encoder–decoder architectures, multi-scale and feature fusion strategies, relationship modeling methods, vision transformer architectures, and light-weight methods.\n第三部分回顾基于DL框架的流行语义分割方法，包括编码器-解码器架构、多尺度和特征融合策略、关系建模方法、视觉变换器架构以及轻量级方法。\nSection 4 summarizes the available datasets for UAV-based image semantic segmentation.\n第四节总结了基于无人机的图像语义分割可用的数据集。\nSection 5 provides an experimental evaluation of representative semantic segmentation models on two highresolution UAV-based datasets.\n第五节对两个高分辨率无人机数据集上的代表性语义分割模型进行了实验评估。\nSemantic segmentation accuracy and model inference efficiency are the main aspects to be analyzed and discussed in our experimental assessments.\n语义分割的准确性和模型推断效率是我们实验评估中主要需要分析和讨论的方面。\nSection 6 provides a general summary of semantic segmentation models for UAV-based images as well as describes the remaining challenges and future research directions.\n第六节概述了基于无人机图像的语义分割模型，并描述了剩余挑战和未来研究方向。\nSection 7 concludes the paper with a brief discussion of future trends and prospects.\n第7节以对未来趋势和前景的简要讨论结束了本文。\n Methods based on graph-based contextual models\n基于基于图的上下文模型方法\nIndividual pixels in a UAV image are insufficient to convey specific semantic meanings, but the image containing all pixels exhibits spatial contextual semantic information, such as scene details, semantic objects, and spatial relationships, among others.\n无人机图像中的单个像素不足以传达特定的语义意义，但包含所有像素的图像展现了空间上下文语义信息，如场景细节、语义对象和空间关系等。\nHence, one feasible approach for semantic feature extraction is to establish dependencies between pixels or homogeneous regions within the image.\n因此，一种可行的语义特征提取方法是建立图像中像素或同质区域之间的依赖关系。\nGraphbased contextual models (Jordan, 2004; Drton and Maathuis, 2017) are practical methods that explicitly establish stochastic dependencies between pixels or regions using probabilistic theories to capture spatial contextual semantic information (Yu et al., 2018b).\n基于图的上下文模型（Jordan，2004;Drton 和 Maathuis， 2017）是一种实用方法，通过概率理论明确建立像素或区域之间的随机依赖关系，以捕捉空间上下文语义信息（Yu 等，2018b）。\nFurthermore, the considerable amount of complex semantic information within the UAV images can vary with changes in the scene, camera view, and flight altitude.\n此外，无人机图像中大量的复杂语义信息会随着场景、摄像机视角和飞行高度的变化而变化。\nGraph-based contextual models can reduce the mis-classification caused by this complexity by integrating semantic information from the neighborhoods when assigning class labels to each pixel or region in a given image (Solberg et al., 1996; Tso and Mather, 1999; Zheng and Wang, 2015; Yao et al., 2015; Gu et al., 2017; Yang et al., 2018).\n基于图的上下文模型可以通过在给图像中的每个像素或区域分配类别标签时整合邻域的语义信息，减少因复杂性导致的错误分类（Solberg 等，1996;Tso 和 Mather，1999;郑和王，2015;Yao等，2015;Gu 等，2017;Yang 等，2018）。\nGraph-based probabilistic models attribute the feature learning task to computing the probability distribution of variables.\n基于图的概率模型将特征学习任务归因于计算变量的概率分布。\nThe Markov Random Field (MRF) and the Conditional Random Field (CRF) are the most popular graph-based contextual semantic algorithms.\n马尔可夫随机场（MRF）和条件随机场（CRF）是最流行的基于图的上下文语义算法。\nBoth approaches map images onto an undirected graph model, where each vertex in the graph represents a pixel or feature vector of the image, and the edges between the vertices represent contextual dependencies.\n这两种方法都将图像映射到无向图模型上，图中的每个顶点代表图像的一个像素或特征向量，顶点之间的边代表上下文依赖关系。\n MRF\nMRF is essentially a probabilistic generative model that predicts each class of images based on the joint probability distribution (Solberg et al., 1996; Tso and Mather, 1999; Kasetkasem and Varshney, 2002; Zheng et al., 2017; Yu et al., 2018b).\nMRF本质上是一个概率生成模型，基于联合概率分布预测每类图像（Solberg等，1996;Tso 和 Mather，1999;Kasetkasem和Varshney，2002年;Zheng 等，2017;Yu 等，2018b）。\nThe pixel distribution in an image can be viewed as the MRF since each pixel is related to its corresponding neighborhood.\n图像中的像素分布可以看作MRF，因为每个像素都与其对应的邻域相关。\nIt has been regarded as the practical model for describing spatial patterns and image characteristics.\n它被视为描述空间模式和图像特征的实用模型。\nLet Y denote the label field with L semantic classes, i.e. Y=y1,y2,...,yLY = {y_1, y_2, ... , y_L  }Y=y1​,y2​,...,yL​, and the x is the observed image.\n设Y表示具有L语义类的标签域，即Y=y1,y2,...,yLY = {y_1, y_2, ... , y_L  }Y=y1​,y2​,...,yL​，X是观测到的图像。\nAccording to the Bayesian formula, the posterior distribution should be\n根据贝叶斯公式，后验分布应为\nP(y∣x)=P(x∣y)P(y)P(x)∝P(x∣y)P(y)P(y|x)=\\frac{P(x|y)P(y)}{P(x)}∝P(x|y)P(y)\nP(y∣x)=P(x)P(x∣y)P(y)​∝P(x∣y)P(y)\nwhere P(x∣y)P(x|y)P(x∣y) is the likelihood function and P(y)P(y)P(y)is the part that describes the joint probability distribution of the label field.\n其中P(x∣y)P(x|y)P(x∣y)  是似然函数，P（y）P（y）P（y）是描述标签场联合概率分布的部分。\nBased on the assumption that the label field is independent and identically distributed, then the segmentation problem is equivalent to maximum a posterior (MAP)\n基于标签场独立且分布均匀的假设，则分割问题等价于最大后验（MAP）\ny^=arg⁡max⁡y∈YP(y∣x)\\hat{y} = \\arg\\max_{y \\in Y} P(y \\mid x)\ny^​=argy∈Ymax​P(y∣x)\nMRF has been adopted to model interactions of image features or contextual information of UAV photogrammetry and remote sensing tasks, such as target detection and tracking (Yu et al., 2006; Wan et al., 2019), image despeckling (Alparone et al., 2010), individual tree crown delineation (Harikumar et al., 2020), image classification (Fang et al., 2018b), and image segmentation (Zhang et al., 2013).\nMRF已被用于建模无人机摄影测量和遥感任务（如目标检测和跟踪）中图像特征或上下文信息的相互作用（Yu 等，2006;Wan等，2019）、图像去斑点（Alparone等，2010）、单个树冠的划分（Harikumar等，2020）、图像分类（Fang等，2018b）以及图像分割（Zhang等，2013）。\nHowever, the original pixel-based MRF is difficult to model the spatial pattern of the entire high-resolution image, it is more effective in tasks that only need to model a small number of pixels (Wan et al., 2019).\n然而，原始基于像素的MRF难以建模整个高分辨率图像的空间模式，在只需建模少量像素的任务中效果更佳（Wan等，2019）。\n\n像素级 MRF 不适合用于建模整个高分辨率图像的全局空间结构，但在只涉及少量像素或局部区域的任务中仍然具有较好的效果。\n\nBesides, the local Markovian property of the pixel-based MRF may not be preserved due to image transformations such as subsampling, block averaging, and subtraction of two images (Kasetkasem and Varshney, 2002; Gu et al., 2017).\n此外，基于像素的MRF的局部马尔可夫性质可能无法保持，因为图像变换如子采样、块平均和两张图像的相减（Kasetkasem和Varshney，2002;Gu 等，2017）。\nA multi-layer MRF can be established to achieve stable image point classification by constructing various features describing image points, such as color and texture, to model the feature interaction between layers (Kato et al., 2002; Benedek et al., 2007).\n可以通过构建描述图像点的各种特征（如颜色和纹理）来建立多层MRF，以实现稳定的图像点分类，以模拟层间特征的相互作用（Kato 等，2002;Benedek 等，2007）。\nHowever, this approach can introduce more computational overhead proportional to the number of layers.\n然而，这种方法可能会带来与层数成正比的计算开销。\nTo improve the image segmentation efficiency, Alparone et al. (2010) adopted the constrained MRF based on a binary tree structure (D’Elia et al., 2003) to recursively divide the image into smaller regions.\n为了提高图像分割效率，Alparone 等人（2010）采用了基于二叉树结构的约束 MRF（D’Elia 等，2003），递归地将图像划分为更小的区域。\nBut this approach is still pixel-oriented and unsuitable for high-resolution UAV images with extensive texture details.\n但这种方法仍然是像素导向的，不适合具有大量纹理细节的高分辨率无人机图像。\nA set of similar and connected pixels in an image can be grouped as an object with meaningful representation according to specific properties such as shape, color, and spectral information (Levinshtein et al., 2009; Achanta et al., 2012; Crommelinck et al., 2017; He et al., 2019). The object-based image analysis (OBIA) (Castilla and Hay, 2008; Lang, 2008; Blaschke, 2010; Chen et al., 2018a; Hossain and Chen, 2019) can serve as an alternative to the per-pixel analysis method, since it cannot only reduce the computational overhead by decreasing the number of vertex of MRF but also reduce the impact of complex texture details within the pixel-set object on the semantic representation of the target to be segmented.\n图像中一组相似且相连的像素可以根据形状、颜色和光谱信息等特定属性被归为具有有意义表示的对象（Levinshtein 等，2009;Achanta等，2012;Crommelinck 等，2017;他等，2019）。基于对象的图像分析（OBIA）（Castilla 和 Hay，2008;Lang，2008;Blaschke，2010年;Chen 等，2018a;Hossain 和 Chen，2019）可以作为每像素分析方法的替代方案，因为它不仅通过减少 MRF 顶点数来降低计算开销，还能减少像素集对象中复杂纹理细节对待分割目标语义表示的影响。\nThe MRF model can also be extended to object-based MRF (OMRF) for semantic segmentation, in which each vertex of the adjacency graph represents a region over-segmented by object-based methods instead of pixels (Zheng and Wang, 2014; Zheng et al., 2017).\nMRF模型也可以扩展到基于对象的MRF（OMRF）进行语义分割，其中邻接图的每个顶点代表一个被基于对象的方法而非像素过度分割的区域（Zheng 和 Wang，2014;Zheng 等，2017）。\nFor instance, Zhao et al. (2022) utilized the multi-scale line segment detector to obtain power line region proposals, then combined the Gaussian mixture model (GMM) and the weighted region adjacency graph (WRAG) to construct an OMRF-based power line detection model.\n例如，赵等人（2022）利用多尺度线段探测器获得了电力线区域的建议，随后结合高斯混合模型（GMM）和加权区域邻接图（WRAG）构建基于OMRF的电力线检测模型。\nGMM constructed the likelihood function to describe the conditional probability, and the WRGA utilized the spatial contextual information and interactions between objects.\nGMM构建了似然函数来描述条件概率，WRGA利用了空间上下文信息和物体间的交互。\nadopted an improved watershed algorithm to avoid over-segmentation and combined with edge-based coupled MRF to reduce false positives caused by speckles in PolSAR images.\n采用改进的流域算法以避免过度分割，并结合基于边缘的耦合MRF，减少PolSAR图像中斑点引起的误报。\nIt enhanced the accuracy of change detection between two UAVSAR images while also improving the weak missing discontinuous boundaries between the image objects.\n它提高了两幅UAVSAR图像之间变化检测的准确性，同时也改善了图像对象之间缺失的弱不连续边界。\nBoth utilized the MRF to model the interaction between pixel groups generated from UAV-based images and assign the pixels belonging to a certain pixel group to have the same label.\n两者都利用MRF模拟由无人机图像生成的像素组之间的相互作用，并将属于某个像素组的像素赋予相同标签。\nHowever, improper modeling of spatial relationships of OMRF might lead to misclassification or over-smoothing for high-resolution remote sensing images.\n然而，OMRF空间关系建模不当可能导致高分辨率遥感图像的误分类或过度平滑。\nBesides, most MRF-based semantic segmentation methods did not fully consider hierarchical semantic information or interlayer relationships between pyramid multi-resolution images.\n此外，大多数基于MRF的语义分割方法并未充分考虑层级语义信息或金字塔多分辨率图像之间的层间关系。\nYao et al. (2021) proposed pyramid OMRF with dual-track information transmission (POMRF-DIT) to mine and transfer pyramid multilayer information.\nYao等人（2021）提出了带有双轨信息传输（POMRF-DIT）的金字塔OMRF，用于挖掘和传输金字塔多层信息。\nClose-loop dual-track paths were also adopted to optimize the segmentation of each layer and achieve more physically meaningful structure modeling and interlayer information transfer mechanism.\n还采用了闭环双轨路径，以优化每层的分段，实现更具物理意义的结构建模和层间信息传递机制。\nMRF provides a solution for the semantic segmentation of UAV-based images, which constructs the interactions between pixels or groups of pixels with joint probability distribution to capture the semantic context information so that the segmentation results can be given semantic meanings.\nMRF为无人机图像的语义分割提供了解决方案，该方法通过构建像素或像素组之间的交互关系，并以联合概率分布捕捉语义上下文信息，从而赋予分割结果语义意义。\nNevertheless, there are still many difficulties in applying MRF to high-resolution UAV remote sensing images. For instance, the updating and inference process of the joint probability distribution in MRF is complicated and time-costing (Zheng et al., 2017).\n然而，将MRF应用于高分辨率无人机遥感图像仍存在许多困难。例如，MRF中联合概率分布的更新和推断过程复杂且耗时（Zheng 等，2017）。\nIn addition, the object scale variation and complex texture of UAV images make it difficult to design reasonable semantic rules.\n此外，无人机图像的物体尺度变化和复杂的纹理使得设计合理的语义规则变得困难。\nOnly a few works are found in the literature where MRF is utilized for the semantic segmentation of UAV images.\n文献中只有少数文献将MRF用于无人机图像的语义分割。\nIn many scenarios, CRF is a more commonly used graph-based contextual model, which will be described in the next section.\n在许多场景中，CRF是一种更常用的基于图的上下文模型，将在下一节中详细说明。\n CRF\nConditional Random Field\n条件随机场\nUnlike MRF, CRF for semantic segmentation is a probabilistic discriminant model that specifies the conditional probability distribution of the semantic labels given the observation data (Lafferty et al., 2001).\n与MRF不同，CRF用于语义分割是一种概率判别模型，指定了在给定观察数据后语义标签的条件概率分布（Lafferty等，2001）。\nIt had cast a bright light on many aspects of the graph-based context tasks and profoundly impacted the semantic segmentation of UAV remote sensing images.\n它为基于图表的上下文任务的许多方面带来了亮点，并深刻影响了无人机遥感图像的语义分割。\nThe inference process in CRF usually consists of association potential and interaction potential.\nCRF中的推断过程通常包括关联势和相互作用势。\nLet S denotes all nodes that reflect the pixels or regions in an image, the index i ∈ S indicates the node site while the j represents the node site in a neighborhood Ni ∈ S.\n设S表示所有反映图像像素或区域的节点，索引i∈S表示节点站点，j表示邻域Ni∈S中的节点站点。\nThe posterior probability distribution P (y|x) directly modeled by the CRF can be described as (Kumar and Hebert, 2006; Hoberg et al., 2014)\nCRF直接建模的后验概率分布P（y|x）可以描述为（Kumar和Hebert，2006;Hoberg 等，2014）\nP(y∣x)=1Zexp⁡[∑i∈SAi(yi,x)+∑i∈S∑j∈NiIij(yi,yj,x)]P(y \\mid x) = \\frac{1}{Z} \\exp\\left[\n\\sum_{i \\in S} A_i(y_i, x)\n+ \\sum_{i \\in S} \\sum_{j \\in N_i} I_{ij}(y_i, y_j, x)\n\\right]\nP(y∣x)=Z1​exp⎣⎢⎡​i∈S∑​Ai​(yi​,x)+i∈S∑​j∈Ni​∑​Iij​(yi​,yj​,x)⎦⎥⎤​\n\nZZZ：配分函数（partition function / normalization constant）\nAi(yi,x)A_i(y_i, x)Ai​(yi​,x)：一元势函数（unary potential）\nIij(yi,yj,x)I_{ij}(y_i, y_j, x)Iij​(yi​,yj​,x)：二元势函数（pairwise potential）\nSSS：节点（像素）集合\nNiN_iNi​：节点 iii 的邻域\n\nwhere the term Ai(yi,x)A_i (y_i, x)Ai​(yi​,x) is the association (or unary) potential and the term Iij(yi,yj,x)I_{ij} (y_i, y_j , x)Iij​(yi​,yj​,x) is the interaction (or pair-wise) potential. The association potential reflects the applicability to a pixel or region xi given a label yiy_iyi​, while the interaction potential models the relationship between label yiy_iyi​ and yjy_jyj​ , which can be regarded as a constraint to ensure the consistency of semantic label predictions. ZZZ is the partition function that normalized the sum of potentials. ZZZ is defined as\n其中项Ai(yi,x)A_i (y_i, x)Ai​(yi​,x)是关联势（或一元势），项 Iij(yi,yj,x)I_{ij} (y_i, y_j , x)Iij​(yi​,yj​,x) 是相互作用（或两对势）。关联势反映了给定标签 yiy_iyi​时，像素或区域习适用性，而交互势能则模拟标签  yiy_iyi​ 与  yiy_iyi​ 之间的关系，这可被视为确保语义标签预测一致性的约束。 ZZZ  是归一化势和的配分函数。 ZZZ  定义为\nZ=∑yi∈Yexp⁡[∑i∈SAi(yi,x)+∑i∈S∑j∈NiIij(yi,yj,x)]Z = \\sum_{y_i \\in Y}\n\\exp\\left[\n\\sum_{i \\in S} A_i(y_i, x)\n+ \\sum_{i \\in S} \\sum_{j \\in N_i} I_{ij}(y_i, y_j, x)\n\\right]\nZ=yi​∈Y∑​exp⎣⎢⎡​i∈S∑​Ai​(yi​,x)+i∈S∑​j∈Ni​∑​Iij​(yi​,yj​,x)⎦⎥⎤​\nThe unary potential of CRF for semantic segmentation is generally defined by the label assignment probability at pixels.\nCRF在语义分割中的一元势能通常由像素处的标签分配概率定义。\nIn the early UAV semantic segmentation tasks, Random Forest (Quang et al., 2015), Support Vector Machine (Zhang et al., 2018), or boosted classifiers (Girisha et al., 2019; Shotton et al., 2009) have been adopted to obtain the unary potential.\n在早期的无人机语义分割任务中，随机森林（Quang 等，2015）、支持向量机（Zhang 等，2018）或增强分类器（Girisha 等，2019;Shotton等，2009）已被采用来获得一元势。\nHowever, constructing unary potential through traditional  classifiers lacks the integration of spatial context information, which may lead to misclassifications of pixel labels in high-resolution UAV images (Zhang et al., 2018).\n然而，通过传统分类器构建一元势缺乏空间上下文信息的整合，可能导致高分辨率无人机图像中像素标签的错误分类（Zhang 等，2018）。\nZeggada et al. (2018), in another way, subdivided UAV images into a grid of tiles of equal size, then combined the Bag of Words, Autoencoder, and Multi-Layer Perceptron to construct the unary potential of CRF.\nZeggada等人（2018）则以另一种方式将无人机图像细分为大小相等的网格，然后结合词袋、自编码器和多层感知器，构建了CRF的一元势能。\nIt independently integrated the local spatial context of each grid in UAV imagery to extract compact signatures that effectively describe the corresponding tiles.\n它独立整合了无人机图像中每个网格的局部空间上下文，提取出有效描述对应瓦片的紧凑签名。\nIn the subsequent practices (Zhang et al., 2019b; Chiu et al., 2020; Huang et al., 2020; Lobo Torres et al., 2020; Girisha et al., 2020; Song et al., 2020; Zhong et al., 2020; Girisha et al., 2021a; Zhang et al., 2022a) of semantic segmentation for UAV images, Deep Convolutional Neural Networks (DCNNs) have gradually replaced the unary potential construction methods that require hand-crafted feature extraction.\n在后续实践中（Zhang 等，2019b;Chiu 等，2020;Huang 等，2020;Lobo Torres 等，2020;Girisha等，2020;Song 等，2020;Zhong 等，2020;Girisha等，2021a;Zhang 等人，2022a）关于无人机图像语义分割的应用，深度卷积神经网络（DCNNs）逐渐取代了需要手工特征提取的一元势构建方法。\nThe stacked convolutional layers of DCNNs expand the receptive field (Krizhevsky et al., 2012) layer by layer, making it possible to automatically capture rich spatial contextual feature information of UAV images.\n堆叠卷积的DCNN层扩展感受场（Krizhevsky等，2012），使得能够自动捕捉无人机图像丰富的空间上下文特征信息。\nThe last feature layer with softmax activation function (Krizhevsky et al., 2012; Long et al., 2015; Badrinarayanan et al., 2017) can be utilized as the label assignment probability required by the unary potential. Thus, researchers can focus on designing pair-wise potential functions applied to different UAV images and scenarios to capture the dependencies between pixels or objects.\n==最后一个带有softmax激活函数的特征层（Krizhevsky等，2012;Long等，2015;Badrinarayanan 等，2017）可以作为一元势所需的标签赋值概率。==因此，研究人员可以专注于设计应用于不同无人机图像和场景的成对势函数，以捕捉像素或物体之间的依赖关系。\nThe pair-wise potential of the most commonly used plain CRF is usually defined by the Potts model or its variants (Yu et al., 2018b; Girisha et al., 2019).\n最常用的纯CRF的两对势能通常由Potts模型或其变体定义（Yu等，2018b;Girisha 等，2019）。\nHowever, the plain CRF is limited to capturing long-range contextual semantic information due to the improper penalizing function.\n然而，由于惩罚函数不当，纯CRF仅限于捕捉长距离语义信息。\nKong et al. (2020) argued that the part of UAV images where complex textures exist might result in label misclassification, and it was not convincible to penalize all inconsistent labels equally using the Potts model.\nKong 等人（2020）认为，存在复杂纹理的无人机图像部分可能导致标签错误分类，且用 Potts 模型对所有不一致标签均等惩罚是不可信的。\nThe pair-wise potentials applied to the semantic segmentation of UAV images can vary according to the specific tasks. For instance, Kong et al. (2020) considered that pixels with similar positions, similar colors, and small height differences should have a higher probability of assigning the same label for urban semantic segmentation.\n应用于无人机图像语义分割的两对势能会根据具体任务而异。例如，Kong 等人（2020）认为位置相似、颜色相似且高度差异较小的像素，在城市语义分割中应有更高的概率被赋予相同标签。\nThe pair-wise potential they designed had considered features such as color, position, height, and scale. It achieved a more reasonable penalty for inconsistent label predictions of the semantic segmentation for UAV images.\n他们设计的配对潜力考虑了颜色、位置、高度和比例等特征。它对无人机图像语义分割标签预测不一致的惩罚更为合理。\nFurthermore, the long-range and shortrange dependencies between pixels are also essential for capturing the global contextual information needed for the semantic segmentation of UAV images.\n此外，像素之间的长距离和短距离依赖关系对于捕捉无人机图像语义分割所需的全局上下文信息也至关重要。\nFully connected CRF (FC-CRF) (Krähenbühl and Koltun, 2011) contains pair-wise potentials of all individual pixels in an image.\n全连通 CRF（FC-CRF）（Krähenbühl 和 Koltun，2011）包含图像中所有单个像素的两两势。\nThe spatial dependency between pixels established by FC-CRF is more effective in reducing the cluttered pixel-level misclassification in the semantic segmentation map and improving the smoothness of the segmentation boundaries of semantic objects (Quang et al., 2015; Zhang et al., 2018; Zhuo et al., 2018a; Girisha et al., 2019; Zhang et al., 2019b; Zhuo et al., 2019; Chiu et al., 2020; Huang et al., 2020; Lyu et al., 2020; Zhang et al., 2022a).\nFC-CRF确立的像素间空间依赖性在减少语义分割图中像素层级的杂乱错误分类和改善语义对象分割边界的平滑性方面更为有效（Quang等，2015;Zhang 等人，2018;Zhuo等，2018a;Girisha 等，2019;Zhang 等人，2019b;Zhuo 等，2019;Chiu 等，2020;Huang 等，2020;Lyu 等，2020;Zhang 等，2022a）。\nIn addition, FC-CRF can also be extended from the spatial dimension to the spatio-temporal dimension for comprehensive context integration and accurate scene understanding of UAV-based videos (Cao et al., 2019; Lyu et al., 2020).\n此外，FC-CRF还可以从空间维度扩展到时空维度，实现无人机视频的全面上下文整合和准确的场景理解（Cao 等，2019;Lyu 等，2020）。\nHowever, the FC-CRF was no longer popular in recent remote sensing image parsing tasks due to the expensive computational overhead. To solve the abovementioned shortcomings of dense CRFs, Teichmann and Cipolla (2018) proposed Convolutional CRF (ConvCRF) based on the conditional independence assumption between two pixels.\n然而，由于计算开销较高，FC-CRF在近期遥感图像解析任务中已不再流行。为了解决上述密集CRF的不足，Teichmann和Cipolla（2018）提出了基于两个像素之间条件独立性假设的卷积CRF（ConvCRF）。\nThe model utilized a convolutional neural network to construct unary potentials and set the interaction potentials to zero as the Manhattan distance between pixels exceeded the threshold. Based on the locality assumption, the processing speed of segmentation increased by two orders of magnitude while improving segmentation accuracy.\n该模型利用卷积神经网络构造一元势，并在像素间曼哈顿距离超过阈值时将交互势设为零。基于局部性假设，分割的处理速度提高了两个数量级，同时提高了切割准确性。\nSong et al. (2020) used ConvCRF as the post-processing of SegNet for sunflower lodging detection.\nSong 等人（2020）使用ConvCRF作为向日葵倒伏检测的SegNet后处理。\nThis algorithm combination corrected the wrong prediction of the lodging area in UAV images and further optimized the segmentation boundaries, achieving the balance between segmentation accuracy and computational overhead.\n该算法组合纠正了无人机图像中倒伏区域的错误预测，并进一步优化了分割边界，实现了分割精度与计算开销之间的平衡。\nHere we introduce several public graph-based models and highlight their advantages and shortcomings, shown in Table 1. Although only a few works can be found where the graph-based models are utilized in recent research on semantic segmentation for UAV remote sensing images, the ideas of integrating semantic information by interaction terms between pixels or regions have encouraged the relationship modeling methods in deep feature-learned methods. We refer readers to Section 3.3 for more details.\n这里我们介绍了几个基于公开的基于图的模型，并重点介绍它们的优缺点，见表1。尽管目前在无人机遥感图像语义分割研究中应用基于图的模型的作品很少，但通过像素或区域间交互项整合语义信息的理念，推动了深度特征学习方法中关系建模方法的发展。更多详情请阅读第3.3节。\nSummary of graph-based contextual models for UAV semantic segmentation.\n无人机语义分割的基于图的上下文模型总结。\n\n\n\n类别\n模型\n应用\n图像模态\n特点\n性能与优势\n局限性\n\n\n\n\nMRF\nOMRF\n输电线检测（Zhao et al., 2022）\nRGB\n使用 GMM 和 WRAG 来定义目标之间的关系\n• 具有更好的抗干扰能力• 提供了较高的检测精度并减少了误检线条\n• 对复杂图像需要额外的计算时间• 参数选择较为复杂\n\n\nMRF\nPOMRF-DIT\n航空场景（Yao et al., 2021）\nRGB\n基于金字塔的目标级 MRF，引入双通道信息传输以挖掘和传递金字塔多层信息\n• 能够从金字塔结构中挖掘并传递图像信息• 分割收敛速度更快\n• 对光谱特征相似的目标容易产生误分类\n\n\nCRF\nTextonBoost\n航空场景（Girisha et al., 2019）\nRGB\n利用纹理布局和低层图像特征，引入像素间相关性以实现语义分割\n• 学习纹理特征以捕获图像中的纹理布局上下文• 能够捕获更多图像细节\n• 基于像素的 CRF• 处理时间较长\n\n\nCRF\nAF-CRF\n城市场景（Kong et al., 2020）\nRGB、DSM\n将多尺度和注意力分析引入分割过程\n• 采用均值场近似的快速求解算法以加速计算• 提供更高的精度和分割置信度\n• 过度考虑长程依赖可能导致误分类\n\n\nCRF\nDCNN + DenseCRF\n城市 / 航空场景（Zhang et al., 2019b；Chiu et al., 2020；Huang et al., 2020；Lobo Torres et al., 2020；Girisha et al., 2020；Zhong et al., 2020；Girisha et al., 2021a；Zhang et al., 2022a）\nRGB、红外\n使用稠密 CRF 作为后处理，对卷积神经网络输出进行细化\n• 能生成更加精细的分割结果• 修正了 CNN 中的一些误分类\n• 稠密 CRF 中成对势函数的计算复杂度较高\n\n\nCRF\nDCNN + ConvCRF\n农作物倒伏检测（Song et al., 2020）\nRGB、红边、近红外（NIR）\n每幅图像的单势由 CNN 的 softmax 输出设定，并采用局部性假设设计成对势\n• 降低了成对势的计算复杂度• 更快且更具实用性\n• ConvCRF 在计算效率与像素间长程依赖之间存在折中\n\n\n\n Methods based on deep-learning models\n基于深度学习模型的方法\nUAV-based images with higher resolution typically contain semantic objects or regions that hold intricate and detailed semantic representations.\n基于无人机的更高分辨率图像通常包含包含复杂且详细语义表示的语义对象或区域。\nHand-engineered graph-based contextual models struggle to cope with these intricate semantic representations, and these techniques of image semantic segmentation are gradually being overtaken by DL-based architectures in UAV remote sensing (Osco et al., 2021).\n手工设计的基于图的上下文模型难以应对这些复杂的语义表示，这些图像语义分割技术正逐渐被无人机遥感中的基于DL架构所取代（Osco等，2021）。\nDLbased semantic segmentation not only eliminates the tedious process of hand-crafted feature extraction with the well-designed combinations of various trainable operators, such as convolution layers, linear layers, and activation layers (Yu et al., 2018b; Yuan et al., 2021), but also presents better segmentation performance to the complex semantic representations of high-resolution images in UAV remote sensing.\n基于DL的语义分割不仅消除了手工制作特征提取的繁琐过程，采用了多种可训练算符的良好组合，如卷积层、线性层和激活层（Yu 等，2018b;Yuan 等，2021），同时也在无人机遥感中对高分辨率图像的复杂语义表示提供了更好的分割性能。\nOur literature review in this section focuses on five aspects of DL-based advances for image semantic segmentation in UAV remote sensing: (1) Encoder–decoder architectures; (2) Multi-scale and feature fusion strategies; (3) Relationship modeling methods; (4) Vision transformer architectures; (5) Light-weight methods.\n本节文献综述聚焦于无人机遥感图像语义分割基于DL的五个方面：\n\n编码-解码器架构\n多尺度和特征融合策略\n关系建模方法\n视觉变换器架构\n轻量化方法。\n\n Encoder–decoder architectures\nIn previous studies, the encoder–decoder architecture had been widely adopted to achieve pixel-wise labeling when designing DCNN for semantic segmentation tasks.\n在以往的研究中，编码器-解码器架构已被广泛采用，以实现针对语义分割任务的 DCNN 实现像素标记。\nThe encoder–decoder architectures usually consist of two primary components.\n编码器-解码器架构通常由两个主要组成部分组成。\nThe first component is the encoder structure, which is generally composed of an efficient backbone of a pre-trained classification network.\n第一个组成部分是编码器结构，通常由预训练分类网络的高效骨干组成。\nIt aims to reduce the spatial scale of the input image and extract feature maps with context semantic information.\n其目标是降低输入图像的空间尺度，并提取带有上下文语义信息的特征图。\nBesides, pooling-based and convolution-based down-sampling strategies have been wildly adopted as the standard techniques in the encoder process for increasing the local prospective field and reducing computation costs.\n此外，基于池和卷积的下采样策略已被广泛采用为编码器过程中的标准技术，用于增加局部前景场并降低计算成本。\nAnother component is the decoder structure, which typically utilizes bilinear interpolation or deconvolution (Dumoulin and Visin, 2016) up-sample strategies.\n另一个组成部分是解码器结构，通常采用双线性插值或反卷积（Dumoulin 和 Visin，2016）上采样策略。\nIt maps the low-resolution feature blocks of the encoder structure back to feature blocks or prediction outputs with higher spatial resolution.\n它将编码器结构中的低分辨率特征块映射回具有更高空间分辨率的特征块或预测输出。\nIt maps the low-resolution feature blocks of the encoder structure back to feature blocks or prediction outputs with higher spatial resolution.\n它将编码器结构中的低分辨率特征块映射回具有更高空间分辨率的特征块或预测输出。\n FCN\nThe pioneering end-to-end encoder–decoder structure for the pixelwise semantic segmentation tasks is fully convolutional neural network (FCN) proposed by Long et al. (2015).\n像素语义分割任务的开创性端到端编码-解码结构是Long等人（2015）提出的全卷积神经网络（FCN）。\nFCN adopted the performanceproven convolutional classification network, such as VGG16 (Simonyan and Zisserman, 2015), as the feature encoder to integrate contextual and location information.\nFCN采用了性能验证的卷积分类网络，如VGG16（Simonyan和Zisserman，2015），作为整合上下文和位置信息的特征编码器。\nAs shown in Fig. 1, FCN replaced the fully connected layers of CNN with only convolutional layers, making it possible to predict the class labels of all pixels with different spatial resolutions.\n如图1所示，FCN用卷积层替代了CNN的全连通层，使得预测所有具有不同空间分辨率像素的类标签成为可能。\nThe structures of three types of FCNs followed the principle of combining coarse to fine layers to make the local predictions consistent with the global context information. For instance, the prediction result of FCN32s was computed by up-sampling the last max-pooling layer of VGG16 by a factor of 32, while that of FCN8s could be obtained by up-sampling the combination of the last three max-pooling layers by a factor of 8.\n三种类型的全卷积网络（FCN）的结构遵循从粗到细层结合的原则，以使局部预测与全局上下文信息保持一致。例如，FCN32s 的预测结果是通过对 VGG16 的最后一个最大池化层进行 32 倍上采样计算得到的，而 FCN8s 的预测结果则可以通过对最后三个最大池化层的组合进行 8 倍上采样得到。\nFig. 1. Upsampling and fusion step of the FCN. Three upsampling and fusion strategies correspond to the network structure of FCN32s, FCN16s, and FCN8s. The fully connected layers of the feature backbone are replaced by the convolutional layers.\n图1。FCN的上采样和融合步骤。三种上采样和融合策略对应FCN32、FCN16和FCN8的网络结构。特征骨干的全连通层被卷积层取代。\n\nFig. 2. Graphical visualization of the original SegNet architecture. SegNet adopts a symmetric encoder–decoder structure without fully connected layers.\n图2。原始SegNet架构的图形可视化。SegNet采用对称编码-解码结构，没有完全连接的层。\nAs far as we know, FCN had been widely used for weed mapping (Huang et al., 2018a; Deng et al., 2020), crop lodging detection (Yang et al., 2020), road extraction (Kestur et al., 2018; Varia et al., 2018; Senthilnath et al., 2020), cadastral boundary extraction (Xia et al., 2019a,b), building footprint detection (Zhuo et al., 2018b), and many others. Comparative experiments (Huang et al., 2018c; Xia et al., 2019b; Yang et al., 2020) also demonstrated that the FCN had more excellent semantic aggregation ability and outstanding image processing efficiency than traditional image segmentation methods, such as CRF, Maximum Likelihood Classification (MLC), and Multi-Resolution Segmentation (MRS).\n据我们所知，FCN已被广泛用于杂草测绘（Huang等，2018a;邓等，2020）、作物沉伏检测（Yang等，2020）、道路开采（Kestur等，2018;Varia 等，2018;Senthilnath 等，2020）、地籍边界提取（Xia 等，2019a，b）、建筑基准检测（Zhuo 等，2018b）以及其他许多方法。比较实验（Huang等，2018c;Xia 等，2019b;Yang 等，2020）还证明，FCN 比传统图像分割方法（如 CRF、最大似然分类（MLC）和多分辨率分割（MRS））具有更优异的语义聚合能力和卓越的图像处理效率。\nHowever, there were still some drawbacks of FCN in handling high-resolution UAV-based images.\n然而，FCN在处理高分辨率无人机图像时仍存在一些缺点。\nHuang et al. (2018b) had proven the efficiency of FCN on semantic segmentation of weed maps by transfer learning strategy, but their subsequent work (Huang et al., 2018a) found that the last pooling operation of FCN8s might result in losing spatial information and reduced accuracy.\nHuang等人（2018b）已证明FCN通过迁移学习策略在杂草图语义分割中的高效性，但他们后续的研究（Huang等，2018a）发现FCN8的最后一次池化作可能导致空间信息丢失和准确性下降。\nXia et al. (2019a) also utilized FCN to obtain the fragmented cadastral boundaries from UAV remote sensing images and then applied the OWT-UCM algorithm to produce connected boundaries.\nXia等人（2019a）还利用FCN从无人机遥感图像中获取碎片化地层边界，随后应用OWT-UCM算法生成连通边界。\nUnfortunately, the smoothing effect caused by max-pooling layers of FCN resulted in discontinuous boundaries. Therefore, their subsequent work (Xia et al., 2019b) on FCN discarded the max-pooling layers to avoid fuzzy boundary predictions.\n不幸的是，最大池化层造成的平滑效果导致边界不连续。因此，他们后续关于FCN的研究（Xia等，2019b）为避免模糊边界预测，舍弃了最大池层。\nThe literature abovementioned exemplifies the suitability of FCN for UAV remote sensing image parsing tasks. But it is worth noting that the max pooling layers make the FCN insensitive to the details in high-resolution images, which leads to fuzzy pixel-level semantic predictions.\n上述文献展示了FCN在无人机遥感图像解析任务中的适用性。但值得注意的是，最大池化层使得FCN对高分辨率图像的细节不敏感，导致像素级语义预测模糊。\nSome attention has focused on custom improvements to FCN, such as skip connections (Kestur et al., 2018), to exploit rich spatial patterns and semantic representations of UAV images.\n一些关注点集中在对FCN的自定义改进上，如跳跃连接（Kestur等，2018），以利用无人机图像丰富的空间模式和语义表示。\nComparative experiments (Song et al., 2020; Lobo Torres et al., 2020) also illustrated that skip connection and discarding the max-pooling layers allowed FCN to preserve fine-grained spatial information and object overall consistency.\n比较实验（Song 等，2020;Lobo Torres 等，2020）还指出，跳跃连接和丢弃最大池层使 FCN 能够保持细粒度的空间信息和整体对象的一致性。\n SegNet\nThe down-sampling strategy of FCN did not take the pixel-to-pixel contextual relationship and object overall consistency into consideration, resulting in the segmentation predictions with insufficient refinement.\nFCN的下采样策略未考虑像素对像素的上下文关系和对象整体一致性，导致分割预测细化不足。\nTo address these drawbacks of lacking a reasonable mapping mechanism from feature map to input image spatial resolution, SegNet (Badrinarayanan et al., 2017) adopted a structurally symmetrical encoder–decoder network with skip connections.\n为解决特征映射到输入图像空间分辨率缺乏合理映射机制的不足，SegNet（Badrinarayanan 等，2017）采用了结构对称的编码-解码器网络，采用跳跃连接。\nSpecifically, SegNet decoded multi-scale encoded feature maps layer by layer to enhance the capacity of global contextual information.\n具体来说，SegNet 逐层解码多尺度编码特征图，以增强全局上下文信息的容量。\nMeanwhile, it utilized the max-pooling indices of encoder layers to maintain the overall consistency of semantic information and avoid ambiguous predictions. The illustration of the SegNet architecture is shown in Fig. 2.\n同时，它利用编码器层的最大池化索引来保持语义信息的整体一致性，并避免模糊的预测。SegNet 架构的示意图如图 2 所示。\n\nSegNet 的想法：既然下采样时我丢了位置，那我就顺手把位置记下来\n\n\n\n对比项\n池化索引\n反卷积\n\n\n\n\n是否学习\n❌\n✅\n\n\n保存内容\n位置\n卷积核\n\n\n上采样方式\n放回原位\n学习插值\n\n\n是否会产生伪影\n几乎不会\n可能有棋盘效应\n\n\n\n\nBased on the improvements, a significant amount of research had been devoted to segmenting crops (Fawakherji et al., 2019; Song et al., 2020), trees (Lobo Torres et al., 2020), wetlands (Fu et al., 2021a), buildings (Boonpook et al., 2018), and roads (Boonpook et al., 2021) from high-resolution images.\n基于这些改进，大量研究致力于作物分段（Fawakherji 等，2019;Song 等，2020）、树木（Lobo Torres 等，2020）、湿地（Fu 等，2021a）、建筑物（Boonpook 等，2018）和道路（Boonpook 等，2021），均来自高分辨率图像。\nThe modifications on SegNet for segmenting different objects can vary substantially due to the physical and spatial characteristics of UAV remote sensing images.\nSegNet在分割不同物体时的修改可能因无人机遥感图像的物理和空间特性而有很大差异。\nFor instance, Li et al. (2022a) reduced the number of convolutional blocks in the encoding and decoding block to improve segmentation efficiency.\n例如，Li 等人（2022a）减少了编码和解码块中的卷积块数量，以提高分割效率。\nThen they adopted the dilated convolution to expand the lost receptive field.\n然后他们采用扩张卷积法来扩展失去的感受野。\nZhong et al. (2022) proposed W-SegNet, the two encoder–decoder structure built on top of SegNet, to achieve hierarchical, multi-scale, and multi-level feature fusion. In addition, benefiting from excellent feature extraction and overall consistency predictions, SegNet has also received great attention for multi-modal UAV image segmentation tasks.\nZhong 等人（2022）提出了基于 SegNet 构建的双编码-解码器结构 W-SegNet，以实现层级、多尺度和多层次的特征融合。此外，凭借出色的特征提取和整体一致性预测，SegNet在多模态无人机图像分割任务中也备受关注。\nSong et al. (2020) combined SegNet with ConvCRF to refine sunflower lodging prediction on high-resolution multispectral images collected by UAVs.\nSong 等人（2020）结合 SegNet 与 ConvCRF，优化了无人机采集的高分辨率多光谱图像向日葵倒伏预测。\nYang et al. (2020) achieved the highest mapping accuracy of plastic mulched farmland and the smoothest mapping boundary based on 6-band image data.\nYang等人（2020）基于6波段图像数据，实现了塑料覆盖农田的最高测绘精度和最平滑的测绘边界。\nKerkech et al. (2020) proposed a fusion of multi-modal segmentation based on dual SegNets to obtain robust vine disease maps after image registration between the visible and infrared ranges.\nKerkech等人（2020）提出了基于双重SegNet的多模态分割融合，以获得可见光与红外范围图像配线后的稳健藤蔓病害图谱。\nLi et al. (2021b) concluded that combining NIR and RGB images could improve the accuracy of sunflower lodging, while the red-edge data acted as a side effect.\nLi等人（2021b）得出结论，结合近红外和RGB图像可以提高向日葵倒伏的准确性，而红边数据则是副作用。\n\n在实际数据中，“红边数据”通常是：\n\n无人机 / 卫星的 红边波段影像\n常见波段中心：705 nm 717 nm 740 nm\n\n红边更擅长“健康评估”，而不是“形态/倒伏检测”\n\nBoonpook et al. (2018) also demonstrated that different combinations of modalities (RGB, visible-band difference vegetation index (VDVI), and DSM) of UAV sensors allowed SegNet to distinguish building and ground features more accurately.\nBoonpook等人（2018）还证明，不同无人机传感器的模态组合（RGB、可见波段差植被指数（VDVI）和DSM）使SegNet能够更准确地区分建筑物和地面特征。\nThe wildly adopted SegNet selected VGG (Simonyan and Zisserman, 2015) as the feature extraction backbone and utilized max-pooling layers in the encoding process.\n广受欢迎的SegNet选择了VGG（Simonyan和Zisserman，2015）作为特征提取骨干，并在编码过程中使用了最大池化层。\nSuch network architecture offered the unique advantages of maintaining overall semantic consistency for large-scale scenes such as buildings and geographical areas.\n这种网络架构提供了保持大型场景（如建筑物和地理区域）整体语义一致性的独特优势。\nBut the significant limitation was that those scenes with apparent edge features and texture information, such as vegetation and crops, suffered poor segmentation accuracy as max-pooling layers lost detailed spatial contents.\n但主要局限是那些带有明显边缘特征和纹理信息的场景，如植被和作物，由于最大层叠会丢失详细的空间内容，分割精度较低。\nFu et al. (2021a) conducted comparative experiments with optimized objects-based Random Forest-Decision Tree (RF-DT) and SegNet on the UAV dataset of digital orthophoto map (DOM) and digital surface model (DSM) imagery for Karst wetland vegetation communities.\nFu 等人（2021a）在无人机数字正射照片地图（DOM）和数字地表模型（DSM）数据集上，针对喀斯特湿地植被群落进行了基于优化对象的随机森林决策树（RF-DT）和 SegNet 的比较实验。\nSurprisingly, the optimized RF-DT algorithm achieved better overall accuracy than SegNet. After further analysis of the contribution of the four multi-source features, they found that the geometric feature describing the boundaries and shapes of vegetations as well as the texture feature presenting the surface properties of the scene played an important role in vegetation classification.\n==令人惊讶的是，优化后的射频-DT算法整体精度优于SegNet。==经过对四大多源特征贡献的进一步分析，他们发现描述植被边界和形状的几何特征，以及呈现场景表面属性的纹理特征，在植被分类中起到了重要作用。\nSubsequent work, a fusion of multiple SegNet, conducted by Deng et al. (2022) further demonstrated that texture features could provide the visible intensity complement for karst wetland vegetation mapping.\n随后，邓等人（2022年）对多重SegNet的融合进一步证明，纹理特征可以为喀斯特湿地植被绘制提供可见强度的补充。\nBoth verified that fine-scale information could enhance the spectral separability of vegetation in wetlands (Szantoi et al., 2013; Hu et al., 2020).\n两者都证实了细微尺度信息能够增强湿地植被的光谱分离性（Szantoi 等，2013;胡等，2020）。\nUnfortunately, SegNet was still deficient in extracting and handling these detailed features.\n遗憾的是，SegNet在提取和管理这些详细功能方面仍然不足。\n DeconvNet\nImproper down-sampling operations may lead to loss of spatial features, and thus decoding or mapping low-resolution feature maps to pixel-level predictions is the primary concern in encoder–decoder architectures.\n不当的下采样作可能导致空间特征丢失，因此解码或映射低分辨率特征映射到像素级预测是编码器-解码器架构中的主要关注点。\nFCN adopted deconvolution to produce nonlinear pixel-level predictions, while SegNet utilized max-pooling indices to upsample feature maps from the corresponding encoder layers.\nFCN采用了反卷积技术来生成非线性像素级预测，而SegNet则利用最大池化指标从相应编码器层对特征图进行上采样。\nAnother effective encoder–decoder structure, DeconvNet proposed by Noh et al. (2015), aimed to accurately reconstruct the high-dimensional nonlinear structural information with deep deconvolution up-sample mechanism.\n另一种有效的编码器-解码器结构，由Noh等人（2015年）提出的DeconvNet，旨在通过深度反卷积上采样机制，准确重建高维非线性结构信息。\nHuang et al. (2016) constructed two isolated DeconvNet for optical images and NRG data, respectively.\nHuang 等人（2016）分别构建了两个独立的去整合网，分别用于光学图像和 NRG 数据。\nThe quantitative evaluation proved the efficiency of DeconvNet in extracting buildings with rich topological appearances.\n定量评估证明了DeconvNet在提取具有丰富拓扑外观建筑方面的高效性。\nTo handle the complex texture information and visual intensity change of remote sensing images, Cheng et al. (2016) designed a structured edge network (SeNet) with skip connections based on DeconvNet.\n为了处理遥感图像的复杂纹理信息和视觉强度变化，Cheng 等人（2016）设计了一个基于 DeconvNet 的结构化边缘网络（SeNet），带有跳跃连接。\nThe SeNet that combined local regularization with structured edge supervision achieved better spatial consistency and edge accuracy.\n将局部正则化与结构化边缘监督结合的SeNet实现了更好的空间一致性和边缘精度。\nHowever, due to the larger number of parameters of the trainable deconvolution, DeconvNet requires longer training and inference time than FCN and SegNet (Yi et al., 2019). It curbed the application of DeconvNet and there are few papers can be found where DeconvNet was adopted for the semantic segmentation of UAV remote sensing images.\n然而，由于可训练反卷积的参数数量更多，DeconvNet 需要比 FCN 和 SegNet 更长的训练和推理时间（Yi 等，2019）。它限制了DeconvNet的应用，目前很少有论文将DeconvNet用于无人机遥感图像的语义分割。\n\nIn this section, we introduced three types of encoder–decoder models for semantic segmentation of high-resolution remote sensing images, including FCN, SegNet, and DeconvNet.\n本节介绍了三种用于高分辨率遥感图像语义分割的编码器-解码器模型，包括FCN、SegNet和DeconvNet。\nThe network structure of FCN and SegNet is simple yet efficient, and it is feasible to perform rapid evaluation and deployment based on these models for various tasks.\nFCN和SegNet的网络结构简单高效，基于这些模型可以快速评估和部署各种任务。\nThey have been the most popular DL-based methods in the recent advances of semantic segmentation for UAV remote sensing images.\n它们是近年来无人机遥感图像语义分割进展中最受欢迎的基于深度学习的方法。\nHowever, the compressed latent representation of encoders makes the decoders difficult to recover spatial details and leads to ambiguity in the segmentation results, such as inconsistent intra-class predictions and fuzzy boundaries.\n然而，编码器的压缩潜在表示使得解码器难以恢复空间细节，并导致分割结果存在歧义，如类内预测不一致和边界模糊。\n Multi-scale and feature fusion strategies\n多尺度和特征融合策略\nThe rapid increment in the resolution of UAV remote sensing images leads to more complex intra-class variations of pixels within semantic objects or regions (Aplin, 2006; Torres-Sánchez et al., 2015).\n无人机遥感图像分辨率的快速提升导致语义对象或区域内像素的类别内变异更加复杂（Aplin，2006;Torres-Sánchez 等，2015）。\nThe rich spatial details provided by high-resolution images may result in variation within semantic features (Aplin, 2006), which will also weaken the models’ capacity to maintain intra-class consistency and inter-class variance for semantic segmentation (Torres-Sánchez et al., 2015; Tamouridou et al., 2017).\n高分辨率图像提供的丰富空间细节可能导致语义特征的变异（Aplin，2006），这也会削弱模型维持类内一致性和类间语义分割变异的能力（Torres-Sánchez 等，2015;Tamouridou 等，2017）。\nBesides, due to the unfixed perspective and varying flight altitudes of UAV platforms, semantic representations extracted at different spatial resolutions in high-resolution images may yield different semantic segmentation accuracies (Aplin, 2006; Woodcock and Strahler, 1987), which can be one of the primary challenges for image semantic segmentation in UAV remote sensing.\n此外，由于无人机平台的视角不固定且飞行高度不同，在高分辨率图像中以不同空间分辨率提取的语义表示可能产生不同的语义分割精度（Aplin，2006;Woodcock 和 Strahler，1987），这可能是无人机遥感图像语义分割的主要挑战之一。\nEarly studies (Baatz and Schape, 2000; Hay et al., 2003; Drˇaguţ et al., 2010) on high-resolution remote sensing image parsing demonstrated that multiresolution techniques were able to address these problems.\n早期研究（Baatz 和 Schape，2000;Hay 等，2003;Drˇaguţ 等，2010）关于高分辨率遥感图像解析的研究表明，多分辨率技术能够解决这些问题。\nSimilarly, in the field of DL, the strategy based on multi-scale feature extraction and feature fusion has been developed to extract effective feature representations at different spatial scales.\n同样，在深度学习领域，基于多尺度特征提取和特征融合的策略已被开发出来，以在不同空间尺度下提取有效的特征表示。\nIntuitively, this strategy extracts coarse to fine feature maps at multiple spatial scales from high-resolution UAV images simultaneously, then integrates these into a fused feature map at a particular spatial scale after feature  alignment (Chen et al., 2017a; Ronneberger et al., 2015; Chen et al., 2017a,b, 2018b; Wang et al., 2020c).\n直观上，该策略从高分辨率无人机图像中同时提取多个空间尺度的粗到细特征图，然后在特征对齐后将其整合到特定空间尺度的融合特征图中（Chen 等，2017a;Ronneberger 等，2015;Chen 等，2017a，b，2018b;Wang 等，2020c）。\nIt not only allows for adaptive selection of feature maps with optimal spatial scales based on the size of semantic objects in high-resolution UAV images, but also compensates for the loss of spatial details caused by the down-sampling operators in the feature encoder structure.\n它不仅允许基于高分辨率无人机图像中语义对象大小，自适应选择具有最佳空间尺度的特征图，还能补偿特征编码结构中下采样算子导致的空间细节损失。\n DeepLab series\nThe pioneering encoder–decoder architectures, such as SegNet and FCN, integrate multi-scale features from the last encoder layers to simultaneously handle large-scale and fine-scale objects.\n开创性的编码器-解码器架构，如SegNet和FCN，整合了上一代编码器层的多尺度特征，以同时处理大尺度和细尺度对象。\nBut it is still deficient in achieving accurate segmentation of fine-structure objects, such as reliable edge prediction and texture refinement.\n但在实现精细结构对象的准确分割方面，如可靠的边缘预测和纹理细化方面，仍然存在不足。\nTo avoid the shortcomings of losing detailed information in max-pooling layers, the atrous convolution (Yu and Koltun, 2016), also called dilated convolution, has been explored to expand the receptive field and integrate uncontiguous elements with discontinuous convolution kernels.\n为避免最大池化层中丢失详细信息的缺陷，采用了极其复杂的卷积（Yu 和 Koltun，2016），也称为膨胀卷积，用于扩展感受场并积分具有不连续卷积核的非连续元素。\nDeepLabV1 (Chen et al., 2017a) and DeepLabV2 (Chen et al., 2017a) were among the most instructive image segmentation models designed to solve three crucial challenges in DCNN: the loss of detailed information in the down-sampling process, multi-scale objects in images, and fuzzy boundary predictions.\nDeepLabV1（Chen 等，2017a）和 DeepLabV2（Chen 等，2017a）是最具启发性的图像分割模型之一，旨在解决 DCNN 中的三个关键挑战：下采样过程中细节信息的丢失、图像中多尺度物体的出现以及模糊的边界预测。\nThe latter introduced two essential improvements to the DCNN structure of semantic segmentation. One was to adopt the Atrous Spatial Pyramid Pooling (ASPP) that applied atrous convolution with the same kernel but different atrous sample rates to generate multi-scale feature maps in parallel.\n后者为DCNN语义分割结构引入了两项重要改进。其中一种是采用Atrous空间金字塔池化（ASPP），该方法在相同核但不同atrous采样率下应用atrous卷积，并行生成多尺度特征图。\nAnother was to refine of segmentation boundary with the probability graph model.\n另一个方法是用概率图模型细化分割边界。\nHowever, it failed to consider the balance between model complexity and prediction accuracy, resulting in inefficient processing efficiency for large-scale remote sensing images (Xia et al., 2021b). Subsequently, they revisited the structure of the previous semantic segmentation networks and proposed DeepLabV3 (Chen et al., 2017b) and DeepLabV3+ (Chen et al., 2018b).\n然而，它未能考虑模型复杂度与预测准确性之间的平衡，导致大规模遥感图像的处理效率低下（Xia 等，2021b）。随后，他们重新审视了之前语义分割网络的结构，提出了DeepLabV3（Chen等，2017b）和DeepLabV3+（Chen等，2018b）。\nDeepLabV3 applied the multigrid to create a deeper network and added a global pooling layer to ASPP to address the degradation of the atrous convolution when the atrous rate was excessive. DeepLabV3+ adopted the encoder structure of the DeepLabV3, combining shallow and deeper features by feature concatenation in the decoder network to refine the boundary details. Besides, DeeplabV3+ utilized pointwise-wise and depth-wise convolutions (Chollet, 2017) to design the encoder–decoder structure, which achieved a significant balance between semantic feature extraction and computation overhead.\nDeepLabV3 应用多网格创建更深层的网络，并在 ASPP 中增加了全局池层，以解决当衰败率过高时卷积衰减的问题。DeepLabV3+采用了DeepLabV3的编码结构，将浅层和深层特征通过特征链连接在解码器网络中细化边界细节。此外，DeeplabV3+ 利用了点向和深度向的卷积设计编码器-解码器结构，实现了语义特征提取和计算开销之间的显著平衡。\nThe multi-scale feature map generated by the ASPP module can expand the receptive field of the convolution kernel without adding additional parameters, thereby capturing more spatial context information. In offshore aquaculture area monitoring tasks, the background water in the high-resolution UAV images can confuse the extraction of floating raft aquaculture areas due to excessive suspended impurities in the seawater.\nASPP模块生成的多尺度特征映射可以在不增加额外参数的情况下扩展卷积核的感受场，从而捕捉更多空间上下文信息。在近海水产养殖区监测任务中，高分辨率无人机图像中的背景水可能因海水中过多悬浮杂质而干扰浮筏养殖区的提取。\nHowever, Sui et al. (2020) proposed OAE-V3 based on DeepLabV3, which achieved smoother boundaries with less noise prediction compared with FCN, even in some floating raft areas without significant spectral information. In addition, multi-scale feature maps also contribute to segmenting semantic objects at different spatial scales. For instance, in UAV-based images, DeepLabV3 accurately segmented eelgrass from meadow-scale to small-scale patches less than one meter in size (Tallam et al., 2023). Further analysis of the segmentation results found that DeepLabV3 was able to identify thousands of smaller eelgrass patches that human annotators ignored.\n然而，Sui 等人（2020）提出了基于 DeepLabV3 的 OAE-V3，该方法在某些浮筏区域中也实现了更平滑的边界和更少的噪声预测，相较于 FCN，且没有显著的频谱信息。此外，多尺度特征图还有助于在不同空间尺度下对语义对象进行切段。例如，在基于无人机的图像中，DeepLabV3能够准确地将鳗草从草地大小到小于一米的斑块进行细分（Tallam 等，2023）。对分割结果的进一步分析发现，DeepLabV3能够识别出数千个人工注释者忽视的较小鳗草斑块。\nThe application of DeepLabV3+ in semantic segmentation for UAV remote sensing is even more extensive.\nDeepLabV3+在无人机遥感语义分割中的应用更为广泛。\nFor example, with different image resolutions, white balance settings, lighting conditions, and other defects in UAV-based images, the DeepLabV3+, adopted by Morales et al. (2018), still yielded 98.03% segmentation accuracy for the automatic segmentation of Mauritania flexuosa.\n例如，在无人机图像中存在不同的图像分辨率、白平衡设置、光照条件及其他缺陷时，Morales等人（2018）采用的DeepLabV3+在毛里塔尼亚柔性鸟的自动分割中仍能实现98.03%的分割准确率。\nIn road extraction, Mahmud et al. (2021) considered that the heterogeneity of roads in terms of location, size, shape, and color complicates the implementation of semantic segmentation algorithms.\n在道路提取中，Mahmud 等人（2021）认为，道路在位置、大小、形状和颜色上的异质性使语义分割算法的实现变得复杂。\nBut DeepLabV3+ with ResNet50 (He et al., 2016) as the feature backbone effectively reduces the negative impact of the complex noise background in UAV images on semantic segmentation.\n但DeepLabV3+搭配ResNet50（He等，2016）作为特征骨干，有效减少了无人机图像中复杂噪声背景对语义分割的负面影响。\nThe crack scale on the surface of steel structures in UAV images is relatively small, which presents insufficient detailed information.\n无人机图像中钢结构表面的裂纹尺度相对较小，这导致信息的详细信息不足。\nHan et al. (2022) utilized the pixel-level crack segmentation results of DeepLabV3+ as a secondary determination for crack detection.\nHan等人（2022）利用DeepLabV3+的像素级裂纹分割结果作为裂纹检测的二次判定。\nThe authors claimed that UAVs about 5 m away from steel surfaces could identify cracks about 5 cm long and 0.5 cm width.\n作者声称，距离钢制表面约5米的无人机可以识别约5厘米长、0.5厘米宽的裂缝。\nA system established by Njane et al. (2023) for automatically evaluating the phenotypic properties of potatoes adopted DeepLabV3+ to initially segment crop parameters from the bare soil in UAV images. Compared to segmentation methods based on thresholding, DeepLabV3+ could adapt to sunny conditions or crops completely covering the image. Moreover, the deep features generated by DeepLabV3+ can be used as effective feature representations of salient objects in UAV images. Specifically, Megir et al. (2021) employed a pre-trained DeepLabV3+ model as a salient object detector and clustered the deep features with K-means.\nNjane等人（2023年）建立的自动评估土豆表型特性的系统采用了DeepLabV3+，最初在无人机图像中从裸土中切割作物参数。与基于阈值的分割方法相比，DeepLabV3+能够适应阳光条件或完全覆盖图像的作物。此外，DeepLabV3+生成的深度特征还可作为无人机图像中显著物体的有效特征表示。具体来说，Megir 等人（2021）采用预训练的 DeepLabV3+ 模型作为显著物体探测器，并将深层特征与 K 均值聚类。\nIt had been demonstrated effective even though the semantic class of the object of interest is unknown and no additional training is performed. However, it should be noted that DeepLabV3+, constructed based on a complex model structure with more parameters, is more difficult to train when there are fewer semantic labels and limited training data, which ultimately results in a significant deviation in test accuracy after training (Jeon et al., 2021).\n尽管目标对象的语义类别未知且未进行额外培训，但该方法已被证明有效。然而，需要注意的是，基于复杂模型结构和更多参数构建的 DeepLabV3+，当语义标签减少且训练数据有限时，训练难度更高，最终导致训练后测试准确性显著偏差（Jeon 等，2021）。\n UNet\nUNet (Ronneberger et al., 2015) is another feature-engineered encoder–decoder architecture with more multi-scale feature extraction and fusion operations. It was originally designed to solve the problem of biomedicine image segmentation with small image samples. As illustrated in Fig. 3(a), the feature backbone of UNet labeled by the dashed box was called the contracting path, which consisted of four feature blocks and the last max-pooling layer. The contracting path compressed the feature resolution and increased the feature channels. The part on the right side of the dashed box was called the expansive path, which also contained four feature blocks that up-sampled the feature map of the previous layer to twice its scale by deconvolution and fused it with the feature output of the contracting path on the left side.\nUNet（Ronneberger 等，2015）是另一种特征工程编码-解码器架构，具有更多多尺度的特征提取和融合作。它最初设计用于解决生物医学图像分割中小样本图像的问题。如图3（a）所示，UNet中虚线框标记的特征骨干称为收缩路径，由四个特征块和最后一个最大池层组成。收缩路径压缩了特征分辨率并增加了特征通道。虚线框右侧的部分称为扩展路径，包含四个特征块，通过反卷积将上一层的特征映射放大到其尺度的两倍，并与左侧收缩路径的特征输出融合。\nBy leveraging a contracting path to gather contextual information and a symmetric expansion path for precise positioning, UNet efficiently integrates multi-scale features across various layers from shallow to deep, facilitating accurate semantic segmentation of finestructure objects in UAV remote sensing images. Kattenborn et al. (2019) tested the fine-grained mapping of vegetation species and communities obtained from high-resolution RGB UAV image datasets based on UNet.\n通过利用收缩路径收集上下文信息和对称扩展路径实现精确定位，UNet高效整合了从浅层到深层的多尺度特征，促进无人机遥感图像中精细结构对象的准确语义分割。Kattenborn等人（2019）测试了基于UNet的高分辨率RGB无人机图像数据集中对植被物种和群落的细粒度映射。\nAs the resolution increased, they found that fine-structured spatial patterns, such as leaf shapes and branching patterns, played a  more important role than overall reflective features. Zhao et al. (2019) equipped the UAV with a high-resolution RGB and multispectral camera and established a dataset of lodging and un-lodging rice image samples. The dice coefficients of UNet with different modal inputs indicated that UNet provides the ability to extract spatial features and patterns of rice lodging directly from high-resolution UAV images. Both demonstrated the superiority of UNet in integrating fine-grained spatial features of optical images and aggregating semantic representations of multimodal data. To make the network more trainable, Zhang et al. (2019b) proposed Deep Res-UNet combined with the residual module of ResNet (He et al., 2016) for infrared image segmentation of photovoltaic panels. And (Zhang et al., 2021a) modified UNet by embedding an irregular encoder–decoder module and content-aware channel re-weight module to address the drawback of irregular and fuzzy boundaries in wheat yellow rust disease detection. However, the classic UNet consumes a large amount of computational and memory resources due to the complex encoder–decoder framework.\n随着分辨率的提升，他们发现细结构的空间图案，如叶片形状和分支图案，比整体反射特征更为重要。赵等人（2019）为无人机配备了高分辨率RGB和多光谱相机，建立了稻米沉积和脱落水稻图像样本数据集。UNet在不同模态输入下的骰子系数表明，UNet能够直接从高分辨率无人机图像中提取稻米着伏的空间特征和模式。两者都展示了UNet在整合光学图像细粒度空间特征和聚合多模态数据语义表示方面的优越性。为了使网络更易训练，Zhang 等人（2019b）提出了将 Deep Res-UNet 与 ResNet 残差模块结合（He 等，2016）用于光伏面板的红外图像分割。此外（Zhang 等，2021a）通过嵌入不规则编码-解码器模块和内容感知通道重权模块，对 UNet 进行了修改，以解决小麦黄锈病检测中不规则和模糊边界的不足。然而，经典的UNet由于复杂的编码器-解码器框架，会消耗大量的计算和内存资源。\nTo meet the demand for real-time image processing on embedding UAV platforms, Shi et al. (2022) used EfficientNet-B4 (Tan and Le, 2019), an efficient network with balanced model width and depth, as the feature backbone to speed up model convergence. Lan et al. (2021) explored MobileNetV2 (Sandler et al., 2018) as the backbone network to reduce parameters and computations of UNet and achieved a frame rate of 45.05 on the embedded platform with 16-bit floating-point weights. He et al. (2023) considered that it was necessary to combine the light-weight UNet with sub-modules, designed for local feature refinement and global receptive field enhancement, to avoid intermittent segmentation or mis-segmentation of transmission lines in UAV images with complex backgrounds. Gao et al. (2023) embedded Depth Separable Residue Block (DR-Block) and Atrous Spatial Pyramid Fusion Attention Module (ASAM) in UNet, which could adapt to the irregular topological shapes of tiny cracks, reduce the considerable interference in complex backgrounds, and ensure the integrity of fracture features in the feature extraction process. Such advanced practices illustrate that UNet provides a reliable baseline for semantic segmentation of UAV remote sensing images, as researchers can either add or modify the original network structure according to the diverse semantic representations of specific scenes to achieve higher semantic segmentation accuracy.\n为满足嵌入式无人机平台对实时图像处理的需求，Shi 等人（2022）采用 EfficientNet-B4（Tan 和 Le，2019），这是一种高效且模型宽度和深度平衡的网络，作为加速模型融合的特征骨干。Lan 等人（2021）探索了 MobileNetV2（Sandler 等，2018）作为减少 UNet 参数和计算的骨干网络，在嵌入式平台上以 16 位浮点权重实现了 45.05 帧率。他等人（2023）认为，为了避免复杂背景的无人机图像中传输线间歇性分割或误分，有必要将轻量级 UNet 与设计用于局部特征细化和全局感受场增强的子模块结合。Gao等（2023）在UNet中嵌入了深度可分离残留块（DR-Block）和Atrous空间金字塔融合注意模块（ASAM），能够适应微小裂纹的不规则拓扑形状，减少复杂背景中的显著干扰，并确保特征提取过程中断裂特征的完整性。这些先进实践表明UNet为无人机遥感图像的语义分割提供了可靠的基线，研究人员可以根据特定场景的多样语义表示添加或修改原始网络结构，以实现更高的语义分割精度。\nUNet++ (Zhou et al., 2018), a variant of UNet, aimed to bridge the semantic gap via nested dense skip connections of feature maps in encoder–decoder networks. As shown in Fig. 3(b), the number of features to be fused at each node of the expansive path in UNet++ depended on the corresponding pyramid level. Nested dense connections also allowed the expansive path to integrate semantic information close to the contracting path. The dense connections among feature maps in each layer of UNet++ provide a significant advantage for processing spatial details of high-resolution UAV images on a large spatial scale. Tran et al. (2020) combined UNet and UNet++ and proposed a two-stage semantic network for damaged area estimation after a forest fire. Specifically, UNet++ was the first stage to process spatial detail information in large-scale UAV image patches, while UNet was the second stage for refining the segmentation results with the output of the first stage in small-scale patches. To streamline the wetland mapping of UAV images, Hu et al. (2021a) proposed Auto-UNet++ incorporated with multi-views, unsupervised clustering, multi-scale CNN, and attention mechanism.\nUNet++（周等，2018）是UNet的一个变体，旨在通过编码器-解码器网络中特征图的嵌套稠密跳跃连接来弥合语义差距。如图3（b）所示，UNet中扩展路径每个节点需要融合的特征数量取决于对应的金字塔层级。嵌套稠密连接还允许扩展路径整合接近收缩路径的语义信息。UNet每层特征图之间的密集连接为处理高分辨率无人机图像的空间细节在大尺度上提供了显著优势。Tran等人（2020）结合了UNet和UNet++，提出了一个用于森林火灾后受损区域估计的两阶段语义网络。具体来说，UNet是处理大尺度无人机图像片段空间细节信息的第一阶段，而UNet则是第二阶段，用于细化小尺度片段中第一阶段输出的分割结果。为了简化无人机图像的湿地映射，胡等人（2021a）提出了整合多视图、无监督聚类、多尺度卷积神经网络和注意力机制的Auto-UNet。\n\nFig. 3. Graphical visualization of the network structure of UNet (a) and UNet++ (b). Both UNet and UNet++ start with an encoder of feature backbone followed by a decoder network. The main difference between these two networks is that UNet++ has nested dense skip connections at the same feature resolution to bridge the semantic gap between the encoder and decoder, while the UNet is composed of plain skip connections.\n图3。UNet （a） 和 UNet++ （b） 网络结构的图形可视化。UNet 和 UNet++ 都以特征骨干编码器开始，随后是解码器网络。这两个网络的主要区别在于，UNet++ 采用嵌套的密集跳跃连接，且具有相同特征分辨率，以弥合编码器和解码器之间的语义差距，而 UNet 则由纯跳跃连接组成。\nDespite computational limitations and longer deployment cycles, sufficient experiments prove its efficiency and practicality. Cao et al. (2023) attempted to address the suboptimal segmentation for irregular cracks caused by the lack of special treatment of deep feature maps in the UNet++ by integrating a deep parallel feature fusion module, which captured higher-level semantic information and enhanced the network sensitivity to the features of road cracks. Experiments showed that the proposed model effectively eliminated the interference of complex backgrounds in high-resolution images and improved the network’s ability to identify irregular cracks.\n尽管计算能力有限且部署周期较长，但足够的实验证明了其效率和实用性。Cao等人（2023）试图通过集成深度并行特征融合模块，解决UNet++中因缺乏对深度特征图特殊处理而导致不规则裂缝分割的不优问题，该模块捕捉了更高层次的语义信息，并增强了对道路裂缝特征的网络敏感性。实验表明，所提出的模型有效消除了高分辨率图像中复杂背景的干扰，并提升了网络识别不规则裂纹的能力。\nThe paradigm of improving and fine-tuning the models for specific scenarios in UAV remote sensing seems to become an effective solution for enhancing the ability of CNNs to extract contextual information. Nevertheless, it is worth noting that during the model inference process, UNet++ needs to consume more memory and computational resources to store the feature maps for dense connections, leading to a slower inference speed in semantic segmentation for large-scale high-resolution UAV images (Cao et al., 2023; Xiao et al., 2023b).\n在无人机遥感中，为特定场景改进和微调模型的范式似乎成为提高卷积神经网络（CNN）提取上下文信息能力的有效解决方案。然而，值得注意的是，在模型推理过程中，UNet 需要消耗更多的内存和计算资源来存储用于密集连接的特征图，从而导致在大规模高分辨率无人机图像的语义分割中推理速度较慢（Cao 等, 2023；Xiao 等, 2023b）。\n High-resolution network\n高分辨率网络\nHigh-resolution representation learning plays a crucial role in UAV semantic segmentation due to the ultra-high resolution and varying object scale of UAV remote sensing imagery. There are two approaches to computing high-resolution representation.\n由于无人机遥感图像具有超高分辨率和多变物体尺度，高分辨率表示学习在无人机语义分割中起着关键作用。计算高分辨率表示有两种方法。\nOne is to up-sample and recover the low-resolution feature maps of CNNs to high-resolution representations. Another one is to maintain the high-resolution information by convolution operations and strengthen semantic context by integrating parallel multi-scale low-resolution feature maps (Zhou et al., 2015; Saxena and Verbeek, 2016; Fourure et al., 2017; Sun et al., 2019). Since high-resolution representation relies on the latent semantic information of multi-scale feature maps, Wang et al. (2020c) designed a high-resolution network (HRNet) that repeatedly exchanges semantic information across adjacent multi-resolution subnetworks. As shown in Fig. 4, vertically, HRNet starts from the first high-resolution feature stream and adds high-to-low resolution subnetworks step by step to form a new multi-resolution stage. To enhance the high-resolution representation and position sensitivity, it connected high-resolution and low-resolution features in parallel and exchanged the semantic information across adjacent multi-resolution sub-networks.\n一种是对卷积网络的低分辨率特征映射进行上采样，并恢复为高分辨率表示。另一种是通过卷积作保持高分辨率信息，并通过集成并行多尺度低分辨率特征图来强化语义上下文（周等，2015;Saxena 和 Verbeek，2016;Fourure 等，2017;Sun 等，2019）。由于高分辨率表示依赖于多尺度特征图的潜在语义信息，Wang 等人（2020c）设计了一个高分辨率网络（HRNet），能够反复在相邻多分辨率子网络间交换语义信息。如图4所示，垂直方向，HRNet 从第一个高分辨率特征流开始，逐步添加高分辨率到低分辨率子网络，形成新的多分辨率阶段。为了增强高分辨率表示和位置灵敏度，它并行连接了高分辨率和低分辨率特征，并在相邻的多分辨率子网络之间交换语义信息。\n\nFig. 4. Illustration of the structure of HRNet that consists of four high-to-low resolution sub-networks. In the vertical direction, HRNet repeatedly exchanges semantic information across adjacent multi-resolution sub-networks with up-sampling, down-sampling, and convolutional units.\n图4。HRNet结构示意，由四个高分辨率到低分辨率子网络组成。在纵向方向上，HRNet通过上采样、下采样和卷积单元反复在相邻的多分辨率子网络间交换语义信息。\nExtracting and preserving semantic features of detailed textures and boundaries from high-resolution images presents one of the major challenges in designing semantic parsing methods applicable to highresolution remote sensing images, as the down-sampling operation during feature extraction process compresses spatially detailed features. However, high-resolution representation learning based on HRNet provides an intuitively feasible solution. It has been demonstrated in recent advances in semantic segmentation (Xu et al., 2020b; Zhang et al., 2020b; Huang et al., 2021a; Tian et al., 2022) for remote sensing images. For instance, since both semantic class imbalance and uncertain boundary information exist in remote sensing images, Xu et al. (2020b) further incorporated three vital factors, including spatial representation, contextual information, and boundary details, when applying HRNet to the semantic segmentation of high-resolution remote sensing images. In UAV remote sensing, Huang et al. (2021a) tested the performance of the object contextual representations network  (OCRNet) based on a backbone of HRNet for recognizing zucchinis intercropped with sunflowers in UAV images. Both studies demonstrated that the reasonable use of parallel features with different weights could improve the accuracy of semantic segmentation. Moreover, by extending the spatial resolution gradient and broadening the spectral band dimensions, Liu et al. (2021b) utilized multi-source data composed of satellite and UAV remote sensing images to analyze the classification capabilities of DeepLabV3+ and HRNet for marsh vegetation. Xie et al. (2021b) built a branch for land cover classification with HRNet and introduced a self-supervised generative adversarial network (GAN) to recover low-resolution images acquired by UAV sensors into corresponding high-resolution images.\n从高分辨率图像中提取和保留详细纹理和边界的语义特征，是设计适用于高分辨率遥感图像的语义解析方法的主要挑战之一，==因为特征提取过程中的下采样作会压缩空间细节特征。==然而，==基于HRNet的高分辨率表示学习提供了一个直观可行的解决方案。==这一技术已在语义分割的最新进展中得到验证（Xu 等，2020b;Zhang 等，2020b;Huang 等，2021a;Tian 等，2022）用于遥感图像。例如，由于遥感图像中既存在语义类别失衡，也存在边界信息不确定，Xu等人（2020b）在将HRNet应用于高分辨率遥感图像的语义分割时，进一步纳入了空间表现、上下文信息和边界细节三个关键因素。在无人机遥感领域，Huang等人（2021a）基于HRNet骨干测试了对象上下文表示网络（OCRNet）在识别无人机图像中与向日葵交错的西葫芦的性能。两项研究都表明，合理使用不同权重的平行特征可以提高语义分割的准确性。此外，通过扩展空间分辨率梯度和拓宽光谱带维度，刘等人（2021b）利用卫星和无人机遥感图像组成的多源数据，分析了DeepLabV3+和HRNet对沼泽植被的分类能力。谢等人（2021b）利用HRNet建立了土地覆盖分类分支，并引入了自监督生成对抗网络（GAN），将无人机传感器采集的低分辨率图像恢复为相应的高分辨率图像。\nThe accuracy gains in land cover classification suggested that the generated high-resolution images compensated for the loss of information due to the feature down-sampling process of HRNet, as well as contributed to the finegrained feature extraction of edges and texture details. Ye et al. (2022) considered that the background of the post-earthquake bridge damage images collected by UAVs was cluttered, and the damage features of several components were inconspicuous. Accordingly, they constructed a multi-task HRNet for recognizing the components and damage of post-earthquake bridges by combining the loss functions of multiple single-task HRNet models. Despite such advanced practices, the native HRNet contains a large number of parameters, making it laborious to deploy in UAV devices to perform online image analysis. Hence, Huang et al. (2021b) had modified HRNet into a sparse multi-scale structure to reduce the network parameters so that it could be flexibly deployed on UAVs for multiple objects tracking.\n土地覆盖分类的准确性提升表明，生成的高分辨率图像弥补了HRNet特征下采样过程导致的信息丢失，并有助于边缘和纹理细节的细粒度特征提取。Ye等人（2022）认为，无人机收集的地震后桥梁损伤图像背景杂乱，多个部件的损伤特征不明显。因此，他们构建了一个多任务HRNet，通过结合多个单任务HRNet模型的损耗函数，识别地震后桥梁的组成部分和损坏情况。尽管采用了如此先进的技术，HRNet本身包含大量参数，使得在无人机设备中部署进行在线图像分析变得繁琐。因此，Huang等人（2021b）将HRNet改进为稀疏的多尺度结构，以降低网络参数，使其能够灵活部署在无人机上进行多目标跟踪。\n Other relevant networks\nThe open-source frameworks (Abadi et al., 2016; Paszke et al., 2019) and detailed technical and coding support for DL developers allow researchers to build deeper and more complex topologies by skipping connections between latent feature layers. Hence, featurelearning models that employ multi-scale and feature fusion strategies to deal with context knowledge integration are not limited to the DeepLab series, HRNet, UNet, and their variants.\n开源框架（Abadi 等，2016;Paszke 等，2019）以及为 DL 开发者提供的详细技术和编码支持，使研究人员能够通过跳过潜在特征层之间的连接来构建更深层、更复杂的拓扑。因此，采用多尺度和特征融合策略处理上下文知识集成的特征学习模型，并不限于DeepLab系列、HRNet、UNet及其变体。\nIn CNNs, each output pixel corresponds to a fixed perceptual field. The high accuracy semantic segmentation requires the integration of multi-scale contexts to handle complicated spatial patterns. PSPNet (Zhao et al., 2017) is a representative method of multi-scale feature fusion, which proposes a pyramid pooling module based on the global prior representation that aggregates contextual information from different regions. However, the plain and simple feature aggregation in PSPNet is struggling to handle the fine-grained features of highresolution UAV images compared to networks with more efficient feature aggregation topology, such as DeepLabV3+ and UNet (Sudarshan Rao et al., 2020; Hota et al., 2020). A recent work (Zhong et al., 2022) proposed a W-shape multi-scale feature fusion network for distress segmentation via UAVs, which adopted skip connections between the encoder–decoder structure to integrate low-level features and high-level semantic features. Inspired by BiSeNet, Zhang et al. (2021b) proposed dual branch-based ICENETv2 for fine-grained river ice segmentation, which integrated low-resolution semantic features from the deep branch and high-resolution finer features from the shallow branch. Lyu et al. (2020) proposed the multi-scale-dilation network, which created three model streams with different spatial scales through space-to-batch operation and batch-to-space operation. After that, they utilized skip connections to concatenate different scale features within each stream to complement the multi-scale semantic information. However, feature space optimization (FSO), a post-processing technique, was required to refine the final results.\n在卷积神经网络中，每个输出像素对应一个固定的感知场。高精度语义分割需要整合多尺度上下文以处理复杂的空间模式。PSPNet（Zhao 等，2017）是一种多尺度特征融合的代表性方法，提出了基于全局先验表示的金字塔池模块，汇总来自不同区域的上下文信息。然而，PSPNet中简单的特征聚合在处理高分辨率无人机图像的细粒度特征时，与具有更高效特征聚合拓扑的网络（如DeepLabV3+和UNet）相比，仍然难以应对（Sudarshan Rao等，2020;Hota 等，2020）。一项最新工作（Zhong 等，2022）提出了一种W形多尺度特征融合网络，用于无人机的遇险分割，采用编码器-解码器结构之间的跳跃连接，整合低层特征和高层语义特征。受BiSeNet启发，Zhang等人（2021b）提出了基于双分支的ICENETv2，用于细粒度河流冰段分割，整合了深支的低分辨率语义特征和浅支的高分辨率细致特征。Lyu 等人（2020）提出了多尺度膨胀网络，通过空间到批次和批对空间作创建了三种不同空间尺度的模型流。之后，他们利用跳跃连接在每个流中串接不同的尺度特征，以补充多尺度语义信息。然而，需要特征空间优化（FSO）作为一种后处理技术来完善最终结果。\nMost published works on multi-scale and feature fusion strategies devoted to connecting features across layers or branches. However, the multi-scale feature fusion of UAV images inevitably lead to feature redundancy. To achieve efficient multi-scale feature representation and reduce feature redundancy, He et al. (2022a) proposed a multi-scale aware-relation network (MANet) that utilized inter-class and intra-class region refinement to reduce redundancy caused by feature fusion. Then they adopted multi-scale collaborative learning to explore the relationship between different scales and to enhance the diversity of multi-scale feature representations. In addition, some recent studies (Zhang et al., 2020b, 2021b; Anand et al., 2021) had focused on combining attention mechanism to measure the importance of feature maps at different scales for adaptively aggregating multi-scale feature representations.\n大多数关于多尺度特征融合策略的已发表作品都致力于连接各层或各分支的特征。然而，无人机图像的多尺度特征融合不可避免地会导致特征冗余。为了实现高效的多尺度特征表示并减少特征冗余，He等人（2022a）提出了一种多尺度感知关系网络（MANet），该网络利用类间和类内区域细化来减少特征融合引起的冗余。然后，他们采用多尺度协同学习来探索不同尺度之间的关系，并增强多尺度特征表示的多样性。此外，一些近期研究（Zhang等人，2020b，2021b；Anand等人，2021）专注于结合注意力机制来衡量不同尺度下特征图的重要性，以自适应地聚合多尺度特征表示。\nTable 2 compiled literature on multi-scale and feature fusion methods for UAV image semantic segmentation. Despite multi-scale features providing more fine-grained contextual semantic information of UAV images, inappropriate multi-scale model designs and feature fusion strategies increased the model complexity and result in feature redundancy. Several recent models incorporated relationship modeling techniques, such as attention models, to enhance the representation of multi-scale features. Nevertheless, the deployment and optimization of the model inference require massive memory allocation and intensive computation, which poses additional challenges in balancing the computational overhead and efficient multi-scale feature aggregation.\n表2汇总了关于无人机图像语义分割的多尺度与特征融合方法的文献。尽管多尺度特征为无人机图像提供了更为精细的上下文语义信息，但不恰当的多尺度模型设计和特征融合策略却增加了模型的复杂性，并导致了特征冗余。近期的一些模型采用了关系建模技术，如注意力模型，以增强多尺度特征的表示能力。然而，模型推理的部署和优化需要大量的内存分配和密集计算，这为平衡计算开销与高效的多尺度特征聚合带来了额外的挑战。\n\n\n\n模型\n特点\n性能与优势\n局限性\n文献\n图像模态与分辨率\n\n\n\n\nDeepLabV3\n采用多网格（multi-grid）和空洞卷积（atrous convolution）以生成更大的感受野；引入全局上下文特征以缓解空洞卷积带来的性能退化问题。\n• 每个像素对应更大的感受野• 能够在多尺度上进行语义分割\n• 在大规模和高分辨率图像上应用时耗时较长\nSui et al. (2020)Wang et al. (2022d)Tallam et al. (2023)\nRGB, 1.2 mRGB, ThermalRGB\n\n\nDeepLabV3+\n采用编码器–解码器结构与空洞可分离卷积；以 Xception 作为高效特征骨干网络。\n• 高效的编码–解码网络，能够捕获细粒度目标• 在特征提取能力与计算开销之间取得良好平衡\n• 在语义标注较少、训练数据有限的情况下，相比参数更少的简单结构更难训练\nMorales et al. (2018)Barmpoutis et al. (2020)Megir et al. (2021)Gibril et al. (2021)Fu et al. (2021b)Jeon et al. (2021)Han et al. (2022)Njane et al. (2023)\nRGB, 1.4–2.5 cmRGBRGBRGBRGB, DEM, 30 mRGB, 4.5–7 cmRGBRGB, Red-edge, NIR\n\n\nUNet\n对称的 U 形全卷积网络结构；通过收缩路径与扩展路径进行多尺度特征提取与融合。\n• 编码器与解码器之间的对称跳跃连接有助于细节特征表示• 为 UAV 遥感图像语义分割提供了可靠基线，且结构易于根据具体场景进行扩展和修改\n• 编码器与解码器子网络中，不同分辨率特征图之间存在语义鸿沟\nZhang et al. (2019b)Zhao et al. (2019)Kattenborn et al. (2019)Zhang et al. (2021a)Tan et al. (2021)Shi et al. (2022)Yi et al. (2022a)Gao et al. (2023)He et al. (2023)Wang et al. (2023b)Xiao et al. (2023b)\nIRRGB, NIR, 2.2 cmRGB, 3–5 cmRGB, Red-edge, NIR, 1.2 cmRGB, 0.6 cmRGB, 25 cmRGBRGBRGBRGBRGB\n\n\nUNet++\n在相同特征分辨率下，引入编码器与解码器之间的密集跳跃连接以进行特征拼接。\n• 多语义表征的灵活特征融合• 多空间尺度特征融合提升了分割精度\n• 在大规模和高分辨率图像上应用时耗时较长\nTran et al. (2020)Hu et al. (2021a)Cao et al. (2023)\nRGBRGB, 2.902 cm, Thermal: 15 cm, Red-edge &amp; NIR: 8.106 cmRGB\n\n\nHRNet\n通过跨分辨率特征集成与语义信息交换，保持高分辨率特征表示。\n• 更丰富、更精确的空间语义表征• 更强的由高到低分辨率表征能力\n• 复杂的模型结构需要更多内存分配和计算开销\nHuang et al. (2021a)Xie et al. (2021b)Ye et al. (2022)\nRGB, 7.5 cmRGB, 15 cmRGB\n\n\nPSPNet\n使用金字塔池化模块聚合来自不同区域的多尺度上下文信息。\n• 局部与全局语义融合保证了分割结果的整体一致性\n• 细粒度预测效果不理想• 目标边界模糊\nSudarshan Rao et al. (2020)Hota et al. (2020)\nRGB, Thermal, Red-edge, NIRRGB, Thermal, Red-edge, NIR\n\n\nMSDNet\n三个结构相同的模型分支，融合多尺度特征图以增强高分辨率预测能力。\n• 像素、图像–标签对及特征图严格对齐• 对尺度变化较大的目标提供一致预测\n• 需要后处理以平滑最终预测结果• 非端到端方法\nLyu et al. (2020)\nRGB\n\n\nICENetV2\n深层分支提取高层语义上下文，浅层分支保持精细空间细节。\n• 双分支的多尺度特征融合可处理更细粒度特征并获得更高精度\n—\nZhang et al. (2021b)\nRGB\n\n\nMANet\n通过类间与类内区域细化及多尺度协同学习，构建多尺度感知关系网络。\n• 判别性强且多样化的多尺度表征• 减少特征融合冗余信息，提升特征表示效率\n—\nHe et al. (2022a)\nRGB\n\n\n\n\nHRNet在无人机图像语义分割方面展现了最强的整体能力。通过在整个网络中保持高分辨率表示，并在多个尺度上反复交换信息，HRNet实现了更丰富、更精确的空间语义表示。尽管计算成本高昂，但其设计特别适合高分辨率无人机图像。性能上限高，但工程成本大。\n\n Relationship modeling methods\n关系建模方法\nWeight-sharing multi-channel convolution kernels, the primary feature extraction unit of DCNNs for semantic segmentation of UAV images, enjoy remarkable spatial-agnostic and channel-specific properties (Li et al., 2021a). Spatial-agnostic property means that the convolution kernel produces similar responses to similar patterns in local space, while channel-specific property implies that the extracted feature maps may have redundancy along the feature channel dimension. Besides, the increasing resolution of UAV remote sensing images generates data redundancy that may seriously interfere with the feature-extracting and decision-making process (Xu et al., 2020b). Hence, establishing the global relationship between pixels or object regions in UAV remote sensing imagery is critical for removing data redundancy and capturing more meaningful context information. As of recently, many advanced studies for mining effective semantic representation in images based on DCNNs have focused on establishing global relationships along spatial and channel dimensions of feature maps (Hu et al., 2018; Woo et al., 2018; Fu et al., 2019; Wang et al., 2020e). According to the motivation and module structure for establishing the global contextual information, we have categorized the relationship modeling methods into non-local mappings, self-attention mechanism and hierarchical attention mechanism.\n权重共享的多通道卷积核是深度卷积神经网络（DCNNs）用于无人机图像语义分割的主要特征提取单元，具有显著的空间无关性和通道特异性属性（Li等，2021a）。空间无关性意味着卷积核对局部空间中的相似模式产生相似的响应，而通道特异性则意味着提取的特征图在特征通道维度上可能存在冗余。此外，无人机遥感图像分辨率的提高产生了数据冗余，可能会严重干扰特征提取和决策过程（Xu等，2020b）。因此，在无人机遥感图像中建立像素或目标区域之间的全局关系对于去除数据冗余和捕捉更有意义的上下文信息至关重要。近期，许多基于DCNNs挖掘图像中有效语义表示的先进研究都集中在沿着特征图的空间和通道维度建立全局关系上（Hu等，2018；Woo等，2018；Fu等，2019；Wang等，2020e）。根据建立全局上下文信息的动机和模块结构，我们将关系建模方法分为非局部映射、自注意力机制和分层注意力机制。\n Non-local mappings\n非局部映射\nNon-local mappings can be viewed as a contextual relationship modeling approach that adaptively recalibrates the feature response to emphasize meaningful information by explicitly establishing the spatial and channel relationships of features captured within each local receptive field, which is defined as the region size in the input image that produces the feature pixel in the feature map (Araujo et al., 2019).  Although the receptive field of the feature maps of convolutional networks with sufficient layer depth is able to cover the entire image, the local receptive field of shallow layers can only establish short-range dependencies of pixels within a limited region in images (Ramachandran et al., 2019; Khan et al., 2022). Non-local mappings not only permit the model to exploit a broader range of spatial context information but also empower accurate prediction of texture details and boundaries of high-resolution images.\n非局部映射可视为一种情境关系建模方法，通过显式建立每个局部感受场内捕获特征的空间和通道关系，以自适应重新校准特征响应，强调有意义的信息，该区域定义为输入图像中产生特征像素的区域大小（Araujo 等， 2019年）。尽管卷积网络具有足够层深的特征图的感受野能够覆盖整个图像，但浅层的局部接收野只能在图像中有限区域内建立像素的短距离依赖关系（Ramachandran 等，2019;Khan 等，2022）。非本地映射不仅使模型能够利用更广泛的空间上下文信息，还能够准确预测高分辨率图像的纹理细节和边界。\nA representative method of non-local mappings is the Convolutional Block Attention Module (CBAM) proposed by Woo et al. (2018), which focuses on extracting essential features and suppressing unnecessary ones. It consists of the Channel Attention Module (CAM) and Spatial Attention Module (SAM). Specifically, CAM used max-pooling and average-pooling operations along the spatial dimension to obtain the most significant and global channel context information, respectively. SAM recalibrated the feature maps along the channel axis with max-pooling and average-pooling operations to highlight informative regions. For image semantic segmentation of UAV remote sensing, CBAM is typically applied to enhance the models’ ability to concentrate on spatial details and restore disconnected boundaries and cluttered false-positive predictions. For instance, Bo et al. (2022) integrated the CBAM into the burned area detection network to identify the most distinctive objects in UAV images and capture the spatial location and edge information for refining the final salient region map. Hong et al. (2021) believed that road cracks in UAV remote sensing images were very narrow since the cracks were composed of only a few pixels. Therefore, they added CBAM before the last convolutional layer of UNet to focus on the crack area, improving the segmentation accuracy of crack edge details and the connecting parts of the cracks. Bisio et al. (2023) also argued that CBAM help extract global information about road traffic density, flow patterns, and road capacity from UAV images for efficient traffic analysis. Furthermore, CBAM can be flexibly placed in convolutional networks at a negligible cost of model modification. Wang et al. (2022d) believed that the dam-surface vegetation in thermal images with low resolution and low signal-to-noise ratio was similar to dam-surface seepage, which caused the networks to generate fuzzy boundaries of the seepage area. Adding CBAM to the skip connections of the network enhanced the semantic and spatial information recognition ability for accurate seepage profiles with clear boundaries and less false-alarm rate caused by ‘‘seepage-like’’ interference on the thermograms. Chen et al. (2023) believed small forest flame regions in high-resolution UAV images led to class imbalance issues between foreground and background. They added CBAM to the decoder of the segmentation network to extract details from the low-level semantic features, allowing the model to focus more on the flame regions and achieve a 5.43% improvement in the Intersection over Union (IoU) metric. Yan et al. (2023) embedded CBAM into the feature extraction layer of the convolutional network to suppress unnecessary feature extraction. Experimental results confirmed that CBAM promoted the model to have detailed treatments at the boundaries of rice fields and the voids in large areas.\n非局部映射的代表性方法是Woo等人（2018）提出的卷积块注意力模块（CBAM），该模块侧重于提取关键特征并抑制不必要的特征。它由通道注意力模块（CAM）和空间注意力模块（SAM）组成。具体来说，CAM沿空间维度使用最大池和平均池作，分别获得最重要和全局的信道上下文信息。SAM通过最大池化和平均池作重新校准了通道轴线的特征图，以突出显示有用的区域。对于无人机遥感的图像语义分割，通常应用CBAM以增强模型对空间细节的聚焦能力，并恢复断开的边界和杂乱的误报预测。例如，Bo等人（2022）将CBAM集成到烧毁区域检测网络中，以识别无人机图像中最显著的物体，并捕捉空间位置和边缘信息，以优化最终突出区域地图。Hong等人（2021）认为，无人机遥感图像中的道路裂缝非常狭窄，因为裂缝仅由几个像素组成。因此，他们在UNet最后一个卷积层之前加入了CBAM，以聚焦裂纹区域，提高了裂纹边缘细节和连接部分的分割精度。Bisio 等人（2023）还认为，CBAM 有助于从无人机图像中提取全球道路交通密度、流量模式和道路容量的信息，从而高效分析交通。此外，CBAM可以灵活地嵌入卷积网络中，模型修改成本极低。Wang 等人（2022d）认为，低分辨率和低信噪比的热成像中大坝表面植被与大坝表面渗漏相似，导致网络产生了渗漏区模糊边界。在网络跳跃连接中加入CBAM提升了语义和空间信息识别能力，实现了准确的渗漏剖面，边界清晰，且热图上“渗漏状”干扰引起的误报率更低。Chen 等人（2023）认为，高分辨率无人机图像中小范围的森林火焰区域会导致前景与背景之间的类别不平衡问题。他们在分段网络的解码器中添加了CBAM，以提取低层语义特征的细节，使模型能够更专注于火焰区域，并在交叉与联合（IoU）指标上实现了5.43%的提升。Yan等（2023）将CBAM嵌入卷积网络的特征提取层，以抑制不必要的特征提取。实验结果证实，CBAM推动模型在稻田边界和大面积空隙处进行详细处理。\n\nFig. 5. Graphical visualization of the structure of the SE module and the ECA module. Both SE and ECA modules share the similar module pipeline and are channel attention modules. The nonlinear mapping function φ© in the SE module is formed by two fully connected layers, while the nonlinear mapping function φ© in the ECA module is two 1-dimensional convolutional layers.\n图5。SE模块和ECA模块结构的图形可视化。SE和ECA模块共享相似的模块流水线，都是通道注意力模块。SE模块中的非线性映射函数φ(C)φ(C)φ(C)由两个全连通层构成，而ECA模块中的非线性映射函数φ(C)φ(C)φ(C)由两个一维卷积层组成。\nSimilar to CAM, the Squeeze and Excitation (SE) module proposed by Hu et al. (2018) converts information along the channel dimension of a feature map into channel weight coefficients that are relevant to particular tasks. As illustrated in Fig. 5, the SE module utilizes spatial global average pooling (GAP) to obtain the average feature response of each channel. Then, it adopts two fully connected layers as the non-local mapping function φ© to integrate global information across channels. Lastly, the output of the softmax activation function serves as the weight for each feature channel. Su et al. (2022) introduced the SE module in the multi-scale feature fusion stage of UNet, and the experimental results suggested that the combination of UNet and SE provided faster training convergence and better accuracy on farmland data with small samples and fragmented information.\n类似于CAM的作，胡等人（2018）提出的挤压与激发（SE）模块将特征图的通道维度信息转换为与特定任务相关的通道权重系数。如图5所示，SE模块利用空间全局平均池（GAP）来获得每个通道的平均特征响应。然后，采用两层全连通图层作为非本地映射函数φ（C），以整合跨信道的全局信息。最后，softmax激活函数的输出作为每个特征通道的权重。Su 等人（2022）在UNet多尺度特征融合阶段引入了SE模块，实验结果表明，UNet和SE的结合在样本较小且信息碎片化的农田数据中提供了更快的训练收敛和更高精度。\nOn the basis of SENet, Sun et al. (2022) designed the concurrent spatial and channel squeeze and excitation (scSE) module by combining spatial and channel context information to further enhance the model capacity to extract  meaningful semantic representations for accurately segmenting trees  and roads in the orchard. However, the non-linear mapping function  of the SE module adopts two fully connected layers that are computationally intensive, and the channel dimension reduction destroys  the direct correspondence between channels and their weights.\n在SENet的基础上，Sun等人（2022）通过结合空间和通道上下文信息，设计了并行空间和通道挤压与激励（scSE）模块，以进一步提升模型提取有意义语义表征的能力，从而准确分割果园中的树木和道路。然而，SE模块的非线性映射函数采用了两个计算密集的全连接层，且通道维度降低破坏了通道与其权重之间的直接对应关系。\nWang et al. (2020e) revisited the non-linear mapping mechanism of the SE module and proposed an Efficient Channel Attention (ECA) module that adopted one-dimensional convolution as mapping function φ© to avoid dimensionality reduction and implement cross-channel interaction. As an extremely light-weight module, ECA has little impact on the number of parameters of the network regardless of where it is inserted. Han et al. (2021) embedded ECA module in the encoder stage of UNet to focus on extracting semantic features for insulators with diverse, damaged, and ambiguous appearances. Huan et al. (2022) added a parallel global maximum pooling on the top of ECA for exploring the most significant spatial information along channels to improve the accuracy of segmenting farmland edges and identifying narrow farmland ridges from UAV images. A comparative experiment conducted by Cai et al. (2023) tested three non-local mapping methods, CBAM, SE, and ECA, for identifying weeds in pineapple fields in high-resolution UAV images. The results demonstrated that ECA outperformed the other methods in terms of parameter count, computational consumption, and semantic segmentation accuracy.\nWang等人（2020e）重新审视了SE（Squeeze-and-Excitation）模块的非线性映射机制，并提出了一种高效通道注意力（Efficient Channel Attention，ECA）模块。该模块采用一维卷积作为映射函数φ(C)φ(C)φ(C)，以避免降维并实现跨通道交互。作为一种极其轻量级的模块，无论插入在网络的哪个位置，ECA对网络参数数量的影响都很小。Han等人（2021）将ECA模块嵌入到UNet的编码器阶段，专注于提取具有多样、受损和模糊外观的绝缘体的语义特征。Huan等人（2022）在ECA的顶部增加了一个并行的全局最大池化操作，以探索通道中最显著的空间信息，从而提高从无人机图像中分割农田边缘和识别狭窄农田垄埂的准确性。Cai等人（2023）进行了一项对比实验，测试了三种非局部映射方法：CBAM、SE和ECA，用于识别高分辨率无人机图像中菠萝田的杂草。结果表明，在参数数量、计算消耗和语义分割准确性方面，ECA均优于其他方法。\n Self-attention mechanism\n自注意力机制\nRecently, the self-attention mechanism, initially celebrated for its superior performance in sequence modeling (Vaswani et al., 2017), such as natural language processing and machine translation, has been widely adopted in the field of image processing (Koščević et al., 2019; Niu et al., 2021; Ghaffarian et al., 2021). In the context of image analysis, it aimed to compute the response matrix of one position attending to all positions and aggregate long-distance contextual dependency adaptively. Self-attention is a soft, deterministic, and differentiable attention mechanism that focuses more on regions or channels, which means that attention weights can be acquired through the learning process. It requires the input feature map from the previous layer to calculate the attention weights, reducing the dependence on external information (Wang et al., 2018). Well-known plug-and-play selfattention methods, such as Non-local Mapping Networks (Wang et al., 2018), Dual Attention Networks (DANet) (Fu et al., 2019), Criss-Cross Attention Networks (CCNet) (Huang et al., 2019b), have been developed to capture long-range contextual information in spatial and channel dimensions respectively, thus improving feature representation for accurate image segmentation. According to our investigation, DANet has received more attention for improving semantic segmentation accuracy in UAV remote sensing compared to other self-attention implementations.\n近年来，自注意机制最初因其在序列建模中的优异表现而备受赞誉（Vaswani 等，2017），如自然语言处理和机器翻译，已被广泛应用于图像处理领域（Koščević 等，2019;Niu 等，2021;Ghaffarian 等，2021）。在图像分析的背景下，它旨在计算一个位置对所有位置的响应矩阵，并自适应地汇总长距离上下文依赖性。自我关注是一种软性、确定性且可微分的注意力机制，更多关注区域或通道，这意味着注意力权重可以通过学习过程获得。它需要上一层的输入特征图来计算注意力权重，从而减少对外部信息的依赖（Wang 等，2018）。著名的即插即用自关注方法，如非本地映射网络（Wang 等，2018）、双注意力网络（DANet）（Fu 等，2019）、交叉注意力网络（CCNet）（Huang 等，2019b），已被开发出来，分别用于捕捉空间维度和通道维度的长距离上下文信息，从而提升特征表示以实现图像切割的准确性。根据我们的调查，DANet在提升无人机遥感语义分割准确性方面，相较于其他自注意实现，受到了更多关注。\nFig. 6 shows a diagram of the network topology of the Position Attention Module (PAM) and Channel Attention Module (CAM) of DANet (Fu et al., 2019) to facilitate an intuitive understanding of the self-attention mechanism. The Position Attention Module firstly convert the input feature map f ∈ RH×W ×C into three tensors θ ∈ R(H×W )×C , φ ∈ RC×(H×W ), and g ∈ R(H×W )×C . Then attention matrix Ms ∈ R(H×W )×(H×W ) that captures the dependencies between any two spatial positions can be obtained by the matrix product of θ and φ. Finally, it performs an element-wise sum operation on the original feature f and the matrix multiplication result of Ms and g to obtain the final attention-based representation. The network topology of the Channel Attention Module is similar to that of the Position Attention Module, which aims to establish channel dependencies between any two channels and adaptively update each channel map according to the weighted sum of all channel maps.\n图6展示了DANet（Fu等，2019）中位置注意力模块（PAM）和通道注意力模块（CAM）的网络拓扑图，便于直观地理解自注意机制。位置注意力模块首先将输入特征映射 f ∈ RH×W ×C 转换为三个张量 θ ∈ R（H×W ）×C 、φ ∈ RC×（H×W ） 和 g ∈ R（H×W ）×C 。那么，通过θ和 φ的矩阵积，可以用θ和  的矩阵积得到注意力矩阵 Ms ∈ R（H×W ）×（H×W ）捕捉任意两个空间位置之间的依赖关系。最后，它对原始特征f和矩阵乘法结果Ms和g进行逐元素求和，得到基于注意力的最终表示。信道注意力模块的网络拓扑类似于位置注意力模块，后者旨在建立任意两个信道之间的信道依赖关系，并根据所有信道图的加权和自适应地更新每个信道映射。\n\nFig. 6. Graphical visualization of the Position Attention Module (a) and the Channel Attention Module (b) of DANet.\n图6。DANet中位置注意力模块（a）和通道注意力模块（b）的图形可视化。\nChowdhury and Rahnemoonfar (2021a) reported the complexities in distinguishing the similar textures of debris, sand, and buildings with total destruction damage in high-resolution natural disaster images captured by UAVs (Chowdhury et al., 2020), as the shallow layers of the semantic segmentation backbone extracted fine-grained spatial texture information with low-level meanings but poor semantic consistency. However, the combination of PAM with the shallow layers encouraged the exploration of the global autocorrelation (Chen et al., 2012) of pixels or objects between each local receptive field in high-resolution images to enhance the distinguishability of similar textures of different classes (Chowdhury and Rahnemoonfar, 2021a,b). Regarding the semantic segmentation of tree species, Huang et al. (2023) contended that significant coincidence existed among the information of various tree species growing in the same geographic location. Therefore, the author proposed a dual-attention residual module to process the strong correlation between local features and global dependence of tree species information from spatial and channel dimensions. Jiang et al. (2022) argued that the ground fissure images in coal mine areas obtained by UAVs were heavily affected by strong noise, such as different illumination and shadow, making conventional segmentation techniques, such as Canny (1986) and Adaptive Threshold (Chang et al., 2000), suffered poor generalization performance. In contrast, the proposed MFPA-Net composed of dilated residual networks and the DANet generated clear and accurate shapes of the ground fissures by utilizing plenty of valuable global features and contextual information to overcome the interference of various noises. Zhang et al. (2021b) found that the spatial patterns of the same type of river ice could vary significantly, while different types of river ice sometimes appear similar due to the complex formation and evolution processes. The authors accordingly adopted the DANet to highlight distinguishable semantic representations of drift ice and shore ice.\nChowdhury和Rahnemoonfar（2021a）报告了在高分辨率自然灾害图像中区分碎片、沙土和完全毁坏建筑物的相似纹理的复杂性，这些图像由无人机（Chowdhury等人，2020）拍摄。由于语义分割主干网络的浅层提取了具有低级含义但语义一致性较差的细粒度空间纹理信息，因此，将点调整模块（PAM）与浅层相结合，鼓励在高分辨率图像中探索每个局部感受野之间像素或对象的全局自相关性（Chen等人，2012），以提高不同类别相似纹理的可区分性（Chowdhury和Rahnemoonfar，2021a，b）。关于树种的语义分割，Huang等人（2023）认为，生长在同一地理位置的各种树种的信息存在显著的重合。因此，作者提出了一个双注意力残差模块，从空间和通道维度处理树种信息局部特征与全局依赖之间的强相关性。Jiang等人（2022）认为，无人机拍摄的煤矿区地裂缝图像受到强烈噪声（如不同光照和阴影）的严重影响，使得传统分割技术（如Canny（1986）和自适应阈值（Chang等人，2000））的泛化性能较差。相比之下，所提出的MFPA-Net由扩张残差网络和DANet组成，通过利用大量有价值的全局特征和上下文信息，克服了各种噪声的干扰，生成了清晰准确的裂缝形状。Zhang等人（2021b）发现，同种河冰的空间模式可能存在显著差异，而不同类型的河冰有时由于复杂的形成和演化过程而显得相似。因此，作者采用DANet来突出漂冰和岸冰的可区分语义表示。\nIn summary, the self-attention mechanism represented by the DANet can be utilized to enhance the ability of semantic segmentation models to extract distinguishable features from ambiguous semantic objects in high-resolution UAV images by establishing global contextual relationships across the spatial and channel dimensions of the feature maps. However, it should be noted that the full matrix multiplication operations in both PAM and SAM are computationally intensive (Huang et al., 2019b; Khan et al., 2022). In fact, there are other light-weight yet efficient modules (Huang et al., 2019a; Zhu et al., 2019; Huang et al., 2019b) built on the top of the self-attention mechanism, but the best practices of them or similar pipelines have not been validated in semantic segmentation for UAV images.\n总之，DANet所代表的自注意机制可用于增强语义分割模型从高分辨率无人机图像中模糊语义对象中提取可区分特征的能力，通过建立跨空间和通道维度的全局上下文关系。然而，需要注意的是，PAM和SAM中的全矩阵乘法运算都具有计算密集型（Huang等，2019b;Khan 等，2022）。事实上，还有其他轻量级但高效的模块（Huang 等，2019a;Zhu 等，2019;Huang 等，2019b）建立在自注意机制之上，但它们或类似流程的最佳实践尚未在无人机图像的语义分割中得到验证。\n Hierarchical attention mechanism\n层级注意力机制\nThe abovementioned attention models can be regarded as singleinput representation models designed to calculate the attention weights of independent branches. Another co-attention mode is the hierarchical attention mechanism (Chen et al., 2016; Tao et al., 2020) that parallelly establishes the contextual dependency on the multi-input presentations and finally merge the attentions of these parallel branches. However, only a few studies were found in literature where the hierarchical attention was used for UAV segmentation tasks in high-resolution remote sensing images. Inspired by the hierarchical attention mechanism proposed by Tao et al. (2020) for predicting relative weights between adjacent scale pairs, Anand et al. (2021) proposed AgriSegNet for IoT-assisted precision agriculture monitoring. One of the distinctive characteristics of the hierarchical attention mechanism was that the model inference with multiple image scales, such as three image scales, only required two image scales in the training phase. Lyu et al. (2021) improved it and proposed bidirectional Multi-scale Attention Network (BiMSANet) that integrated two feature-level hierarchical multi-scale attention networks corresponding to two bypasses for hierarchical feature fusion. Specifically, benefiting from attention heads with the same pipeline, these two bypasses were able to obtain the optimal scale for feature fusion from both directions. Comparative experiments on the UAVid dataset had shown that BiMSANet delivered an excellent performance on processing small targets and adaptively adjusted multiscale attention patterns for different object scales. It achieved the best segmentation performance in the Human class compared to other methods and reached a superior result with the mIoU of 70.8% for the UAVid benchmark.\n上述注意力模型可视为单输入表示模型，旨在计算独立分支的注意力权重。==另一种共注意模式是层级注意力机制（Chen 等，2016;Tao 等，2020），它并行建立对多输入呈现的上下文依赖，并最终合并这些并行分支的注意力。然而，文献中只有少数研究将分层注意力用于高分辨率遥感图像中的无人机分割任务。==受 Tao 等人（2020）提出的分层注意力机制的启发，用于预测相邻尺度对之间的相对权重，Anand 等人（2021）提出了 AgriSegNet 用于物联网辅助精准农业监测。分层注意力机制的一个显著特点是，多图像尺度（如三图像尺度）的模型推断在训练阶段只需两个图像尺度。Lyu 等（2021）改进了该机制，提出了双向多尺度注意力网络（BiMSANet），集成了两个特征级层级多尺度注意力网络，对应于分层特征融合的两个旁通。具体来说，借助同一流水线的注意力头，这两种旁通能够从两个方向获得特征融合的最优尺度。UAVid 数据集的对比实验表明，BiMSANet 在处理小目标和对不同对象尺度的多尺度注意力模式进行自适应调整方面表现出色。它在人类类别中实现了最佳的分割性能，UAVid 基准测试的 mIoU 达到了 70.8%。\nTable 3 provides a summary of several relevant relationship modeling methods that have been adopted in the semantic segmentation tasks for UAV-based images. The relationship modeling methods applied in the literature can be summarized as spatial-wise relationship modeling,  channel-wise relationship modeling, scale-wise relationship modeling, and hybrid methods. Developing a more effective relationship modeling mechanism for obtaining diverse and discriminative feature representation and improving the segmentation performance of UAV images are the recent trend that requires full comparisons and evaluations. As shown in Table 3, the DA modules have received significant attention in the recent literature on contextual relationship modeling. In fact, the self-attention mechanism is an essential component of Transformers (Khan et al., 2022; Jamil et al., 2023; Xiao et al., 2023a), the novel network architecture that explicitly learns the intrinsic relationships of elements or sequences. In recent years, Transformer-based semantic segmentation models have also profoundly impacted the development of semantic segmentation methods for UAV images, and we will provide details in the next section.\n表3总结了几种被无人机图像语义分割任务采用的相关关系建模方法。文献中应用的关系建模方法可归纳为空间关系建模、通道关系建模、尺度关系建模和混合方法。开发更有效的关系建模机制以获得多样且判别性强的特征表示，并提升无人机图像的分割性能，是近期需要全面比较和评估的趋势。如表3所示，DA模块在近期关于情境关系建模的文献中获得了广泛关注。事实上，自我关注机制是Transformers（Khan等，2022;Jamil 等，2023;Xiao 等，2023a），这是一种新颖的网络架构，明确学习元素或序列的内在关系。近年来，基于Transformer的语义分割模型也深刻影响了无人机图像语义分割方法的发展，我们将在下一节提供详细介绍。\n\n\nCBAM —— 性价比高，但能力有限\nSE / ECA —— 轻量化最优，但表达能力偏弱\nDual Attention (DA) —— 综合能力最强，显式建模长程上下文依赖，同时处理 空间注意力 + 通道注意力带来更精确的语义分割结果，但矩阵运算，计算量大\nHierarchical Attention —— 折中方案\nDA &gt; Hierarchical Attention &gt; CBAM &gt; SE ≈ ECA\n\n Vision transformer architectures\nViT\nThe remarkable performance of Transformer models on natural language tasks (Vaswani et al., 2017; Ott et al., 2018) has piqued the interest of the computer vision community to explore potential applications in computer vision problems. Both Khan et al. (2022) and Jamil et al. (2023) provided a comprehensive review on transformers in computer vision. According to the review conducted by Khan et al. (2022), Transformer-based models applied to vision tasks can be broadly categorized into models with single-head attention and models with multi-head attention. The combination of DCNNs and selfattention mechanism in the previous section can be classified as the former, while this section focuses on the latter.\nTransformer模型在自然语言任务中的卓越表现（Vaswani 等，2017;Ott 等，2018）引起了计算机视觉界对探索计算机视觉问题潜在应用的兴趣。Khan等人（2022年）和Jamil等人（2023年）都提供了关于计算机视觉中变换器的完整综述。根据Khan等人（2022）的综述，基于Transformer的模型应用于视觉任务，大致可分为单头注意力模型和多头注意力模型。前一节中DCNNs与自我关注机制的结合可归为前者，而本节则聚焦后者。\nAs of recently, we observed that the transformer-based semantic segmentation method for UAV remote sensing images follows the encoder–decoder structure, which is the same as the underlying structure of CNN-based methods. Besides, transformer models with multi-head attention can be flexibly combined with convolutional networks, thus giving rise to more diverse encoder–decoder structures for UAV image semantic segmentation (Wang and Mahmoudian, 2023; Li and Hsu, 2022; Ghali and Akhloufi, 2023). In this section, we investigated and outlined the best practices of vision transformers (ViT) with multi-head attention in the semantic segmentation of UAV remote sensing images.\n近期，我们观察到，用于无人机遥感图像的基于Transformer的语义分割方法遵循编码器-解码器结构，这与基于卷积神经网络（CNN）的方法的基础结构相同。此外，具有多头注意力机制的Transformer模型可以与卷积网络灵活结合，从而为无人机图像语义分割提供了更多样化的编码器-解码器结构（Wang and Mahmoudian, 2023; Li and Hsu, 2022; Ghali and Akhloufi, 2023）。在本节中，我们研究和概述了具有多头注意力机制的视觉Transformer（ViT）在无人机遥感图像语义分割中的最佳实践。\nViT, designed by Dosovitskiy et al. (2021) for image recognition tasks, first divides an image into equal-sized patches and then flattens patches into 2D-patches sequences that are fed into linear projections to obtain patch embeddings. The transformer encoder, which contributes to capturing global relationships between image pixels at shallow layers, takes these patch embeddings along with position embeddings as input (Dosovitskiy et al., 2021). However, obtaining feature representation containing global contextual information in CNN-based encoders without attention modules necessitates deeper convolutional layers. The success of ViT in image analysis contributed to promising developments and innovations applicable to image segmentation in UAV remote sensing (Li and Hsu, 2022; Ghali and Akhloufi, 2023). For instance, due to significant variations of object scale, object appearance, and background clutters in remotely captured UAV images (Lyu et al., 2020), Kumar et al. (2022, 2023) designed a ViT-based encoder with a convolution-based Token Spatial Information Fusion (TSIF) module that captured the local context details about neighboring pixels. The proposed method exhibited competitive accuracy in the semantic segmentation of small-scale objects in the UAVid dataset compared with previous CNN-based methods. Zhou et al. (2022) argued that developing a practical solution that placed greater emphasis on global contextual information and the significant variation of broccoli in terms of texture, size, and shape is crucial when utilizing UAVs for broccoli detection and characterization. They adopted TransUNet (Chen et al., 2021), the first transformer-based model (Xiao et al., 2023a) built on the top of the UNet structure (Ronneberger et al., 2015), for broccoli canopy mapping. The feature encoder of TransUNet combined transformers with convolution layers to reduce the loss of low-level semantic information in the transformer and enhance the spatial details in local space. In the decode stage, a cascaded up-sampler (CUP) with multiple up-sampling steps was leveraged to generate the final mask. Similarly, Luo et al. (2023) applied TransUNet to poppy segmentation in UAV images. Experiment results demonstrated that TransUNet obtained higher segmentation accuracy with sharper boundaries and fewer false positives than the representative CNN-based UNet (Ronneberger et al., 2015) and DeepLabV3+ (Chen et al., 2018b). Inspired by TransUNet, Niu et al. (2022) designed the HSI-TransUNet for crop mapping from UAV hyperspectral imagery. Specifically, the encoder of HSI-TransUNet adopted spectral-feature attention modules for spectral feature aggregation as well as the residual Transformer layers for global contextual feature extraction. Competitive segmentation accuracies of HSI-TransUNet on the UAV-HSI-Crop dataset against TransUNet and its variants demonstrated that it was sufficient to process the characteristics of UAV HSI data, which had both very high spatial and spectral resolutions.\nViT由Dosovitskiy等人（2021年）设计用于图像识别任务，首先将图像分割成等大小的块状，然后将这些斑块平展成二维斑块序列，再输入线性投影以获得斑块嵌入。变压器编码器负责捕捉浅层图像像素之间的全局关系，其输入是将这些补丁嵌入和位置嵌入一起进行（Dosovitskiy 等，2021）。然而，在无注意力模块的CNN编码器中获得包含全局上下文信息的特征表示，需要更深层的卷积层。ViT在图像分析中的成功推动了无人机遥感图像分割领域的有前景的发展和创新（Li 和 Hsu，2022;Ghali和Akhloufi，2023年）。例如，由于远程捕捉无人机图像中物体比例、物体外观和背景杂乱存在显著差异（Lyu 等，2020），Kumar 等人（2022,2023）设计了一个基于 ViT 的编码器，采用基于卷积的令牌空间信息融合（TSIF）模块，能够捕捉邻近像素的局部上下文细节。与以往基于CNN的方法相比，该方法在UAVid数据集中小尺度对象的语义分割表现出竞争力。周等人（2022）认为，开发一种实用解决方案，更加重视全球情境信息以及西兰花在质地、大小和形状上的显著变异，对于利用无人机进行西兰花检测和特征分析至关重要。他们采用了TransUNet（Chen 等，2021），这是首个基于变压器的模型（Xiao 等，2023a），构建在UNet结构顶部（Ronneberger等，2015），用于西兰花树冠映射。TransUNet 的特征编码器将变换器与卷积层结合，以减少变换器中低级别语义信息的丢失，并增强局部空间的空间细节。在解码阶段，采用了具有多重上采样步骤的级联上采样器（CUP）来生成最终的掩模。同样，Luo等人（2023）将TransUNet应用于无人机图像中的罂粟切片。实验结果表明，TransUNet在具有代表性的基于CNN的UNet（Ronneberger等，2015）和DeepLabV3+（Chen等，2018b）中，在分割准确性上、边界更锐利且假阳性更少。受TransUNet启发，Niu等人（2022年）设计了用于无人机高光谱图像裁判映射的HSI-TransUNet。具体来说，HSI-TransUNet的编码器采用了谱特征注意模块进行谱特征聚合，以及剩余的Transformer层用于全局上下文特征提取。HSI-TransUNet在UAV-HSI-Crop数据集上的划分准确度与TransUNet及其变体的竞争性，证明处理无人机HSI数据的特性足以处理，这些数据具有极高的空间和光谱分辨率。\nAlthough TransUNet captures global semantic information about complex objects from fine-resolution UAV images, the transformer significantly increases the computational complexity as it is quadratically related to the number of tokens (Khan et al., 2022; Xiao et al., 2023a). However, in real-time urban applications using UAV platforms, prioritizing high image processing speed with light-weight models outweighs the demand for extreme accuracy (Wang et al., 2022c). Hence, Wang et al. (2022c) adopted the light-weight ResNet18 (He et al., 2016) as the encoder of the semantic segmentation network, while the decoder utilized the Global-local Transformer Block (GLTB) and the Feature Refinement Head (FRH) to capture the global context and maintain spatial details, in which the GLTB adopted the computation-friendly cross-shaped window context interaction module to capture the crosswindow relationships. Other researchers have also demonstrated that  an excellent encoder design also benefited the acquisition of semantic feature representations composed of global context information and local details for the accurate semantic segmentation of UAV images. For instance, Ding et al. (2023) proposed the IBR-Former, which consisted of two parallel crack segmentation branches, for UAV-oriented concrete crack detection and quantification. The coarse segmentation branch leveraged the Swin-Transformer (Swin-T) (Liu et al., 2021d) as the feature encoder for effectively extracting feature representations crack with different sizes and insufficient pixels, while the other crack offset generating branch aimed to fix irregularly shaped thin crack boundaries of the coarse segmentation maps. Lu et al. (2023) and Xiang et al. (2023) coincidentally adopted a dual-encoder design composed of both Transformer-based and CNN-based encoders for semantic segmentation on UAV remote sensing imagery. Both demonstrated the heterogeneous dual-encoder structure was complementary, since the transformer-based encoder established global relationships between pixels or semantic objects of the UAV image but lost local details, while the CNN-based encoder extracted spatial local features of the UAV image, such as edge details, color, texture and shape of the land covers or crops, which compensated for the loss of local information in transformers. Inspired by CBNet (Liu et al., 2020) and CBNetv2 (Liang et al., 2022), Yi et al. (2023a,b) designed UAVformer and CCSeg for urban scene segmentation and UAV visual perception. Both networks adopted a composite structure backbone composed of a main encoder and single or multiple auxiliary encoders to satisfy the semantic segmentation requirement for UAV images, which was characterized by objects at various spatial scales, complex backgrounds, and fuzzy boundaries (Lyu et al., 2020). Besides, the decoder of both networks utilized multiple down-sampling paths and up-sampling paths to integrate multi-scaled features extracted by the encoders to retain sharper boundaries of segmented objects. In contrast, Wang et al. (2023a) straightforwardly leveraged Swin-T as an encoder for the UAV semantic segmentation model, while putting more emphasis on the decoder design, which consists of four NeW FC-CRFs (Yuan et al., 2022) in series. Compared to the state-of-the-art methods, the proposed method, Swin-T-NFC CRF, obtained 0.32% and 1.41% performance gains in OA and mIoU metrics, respectively.\n尽管TransUNet能够从精分辨率无人机图像中捕捉复杂物体的全局语义信息，但由于变换器与令牌数量成二次方相关，计算复杂度显著增加（Khan等，2022;Xiao 等，2023a）。然而，在使用无人机平台的实时城市应用中，优先考虑轻量化模型的高图像处理速度，超过了对极高精度的需求（Wang 等，2022c）。因此，Wang 等人（2022c）采用了轻量级的 ResNet18（He 等，2016）作为语义分割网络的编码器，而解码器则利用全局-局部变换器块（GLTB）和特征细化头（FRH）捕捉全局上下文并维护空间细节，而 GLTB 采用了计算友好的交叉形状窗口上下文交互模块来捕捉跨窗口关系。其他研究者还证明，优秀的编码器设计也有助于获取由全局上下文信息和局部细节组成的语义特征表示，从而实现无人机图像的准确语义分割。例如，丁等人（2023）提出了IBR-Former，由两条平行的裂纹分割分支组成，用于无人机导向混凝土裂纹的检测和量化。粗分割分支利用Swin-Transformer（Swin-T）（Liu等，2021d）作为特征编码器，有效提取不同大小和像素不足的特征裂纹，而另一分支则旨在固定粗分割图中形状不规则的薄裂纹边界。Lu 等人（2023）和 Xiang 等人（2023）巧合地采用了一种双编码器设计，结合基于 Transformer 和基于 CNN 的编码器，用于无人机遥感图像的语义分割。两者都证明了异构双编码器结构是互补的，因为基于变压器的编码器建立了无人机图像像素或语义对象之间的全局关系，但会丢失局部细节;而基于卷积神经网络的编码器则提取无人机图像的空间局部特征，如边缘细节、颜色、纹理和土地覆盖或作物的形状，从而补偿变压器中局部信息的丢失。受CBNet（Liu等，2020）和CBNetv2（Liang等，2022）启发，Yi等（2023a，b）设计了UAVformer和CCSeg，用于城市场景分割和无人机视觉感知。两个网络都采用了由主编码器和单个或多个辅助编码器组成的复合结构骨干，以满足无人机图像的语义分割需求，该图像的特征包括不同空间尺度、复杂背景和模糊边界的对象（Lyu 等，2020）。此外，两个网络的解码器都采用多条下采样和上采样路径，整合编码器提取的多尺度特征，以保持更清晰的分段对象边界。相比之下，Wang 等人（2023a）直接利用 Swin-T 作为无人机语义分割模型的编码器，同时更强调解码器设计，该设计由四个串联的 NeW FC-CRF（Yuan 等，2022）组成。与最先进方法相比，所提方法Swin-T-NFC CRF在OA和mIoU指标上分别获得了0.32%和1.41%的性能提升。\nSeveral comparative studies were conducted to verify the semantic segmentation models combined with transformers outperformed the CNN-based method in terms of accuracy and generalization on RGBbased UAV imagery. For instance, Ghali et al. (2021, 2022) explored the potential of transformers for wildfire segmentation using ground and aerial imagery. Compared with CNN-based segmentation methods, Transformer-based methods, such as TransUNet (Chen et al., 2021), TransFire (Ghali et al., 2021), and MedT (Ghali et al., 2022), presented more excellent performance for localizing and segmenting forest fire pixels as well as small fire regions due to their ability to determine long-range dependencies and extract fine-grained details within input features. Gibril et al. (2023) evaluated the generalizability and the transferability of deep vision transformers and CNN-based semantic segmentation models for accurate mapping of date palm trees from very-high spatial resolution UAV-based and aerial images. The segmentation performance of the Segformer (Xie et al., 2021a) and the UperNet-Swin outperformed that of previous date palm tree mapping from UAV images, which demonstrated their superiority in processing UAV images with limited spectral information, high intra-class variance, variations in the spatial resolutions, and differences in image contexts and backgrounds. Wang and Mahmoudian (2023) permuted five encoder backbones and three decoder structures to construct multiple semantic segmentation methods for aerial fluvial images. Comparative results concluded that the semantic segmentation models utilizing the transformer as the encoder presented better generalization performance than CNNs as the encoder. Similar conclusions can also be found in the study conducted by Asad et al. (2023) on whether Transformers can be applied to natural disaster assessment in high-resolution UAV imagery.\n进行了多项比较研究，以验证语义分割模型结合变换器在基于RGB的无人机图像中，准确性和泛化性优于基于CNN的方法。例如，Ghali等人（2021,2022）探索了利用地面和航拍影像在野火分割中的变压器潜力。与基于CNN的分割方法相比，基于Transformer的方法，如TransUNet（Chen等，2021）、TransFire（Ghali等，2021）和MedT（Ghali等，2022），在定位和分割森林火灾像素以及小火灾区域方面表现更佳，因为它们能够确定远距离依赖关系并提取输入特征中的细粒度细节。Gibrinl 等人（2023）评估了深视变换器和基于卷积神经网络的语义分割模型在高空间分辨率无人机和航拍图像中枣椰树准确定位的普遍性和可迁移性。Segformer（Xie 等，2021a）和 UperNet-Swin 的分割性能优于以往无人机图像的枣椰树映射，后者展示了它们在处理光谱信息有限、类内方差大、空间分辨率差异以及图像上下文和背景差异的无人机图像方面的优势。Wang和Mahmoudian（2023）对五个编码主链和三个解码结构进行了置换，构建了多重航拍河流图像的语义分割方法。比较结果得出结论，使用变换器作为编码器的语义分割模型，其泛化性能优于作为编码器的CNN。类似结论也出现在Asad等人（2023年）关于变压器是否可应用于高分辨率无人机影像自然灾害评估的研究中。\nHowever, Zefri et al. (2023) discovered that the semantic segmentation training process of TransUNet exhibited more instability and slower convergence than that of CNN-based models on thermal image data collected by UAVs for photovoltaic array inspections. They argued that recent advanced models had been evaluated using multiclass RGB datasets, whose strength could not be directly highlighted or demonstrated in restricted and domain-specific tasks. It is justifiable as most ViTs trained on a new medium-range dataset would not give competitive results (Khan et al., 2022; Xiao et al., 2023a). Many Transformer-based models require very large-scale data to learn prior knowledge about the images, also called inductive biases, such as translation equivariance and spatial invariance (Khan et al., 2022; Xiao et al., 2023a).\n然而，Zefri等人（2023）发现，在无人机收集的用于光伏阵列检查的热像数据上，TransUNet的语义分割训练过程比基于卷积神经网络（CNN）的模型表现出更不稳定且收敛更慢。他们认为，近期的高级模型都是使用多类RGB数据集进行评估的，在受限且特定领域的任务中，这些模型的优点无法直接凸显或展示出来。这是有道理的，因为大多数在新的中距离数据集上训练的视觉变换器（ViTs）都无法给出具有竞争力的结果（Khan等人，2022年；Xiao等人，2023a）。许多基于Transformer的模型需要非常大规模的数据来学习图像的先验知识，也称为归纳偏置，如平移等变性和空间不变性（Khan等人，2022年；Xiao等人，2023a）。\n Light-weight methods\n轻量化方法\nAlthough most semantic segmentation tasks for UAV remote sensing images, such as ecological monitoring, urban scene, and agricultural management, can benefit from efficient feature extraction based on DL methods, these methods are computationally intensive and require professional GPU servers for back-end image processing. A large amount of computation, memory overhead, and computational power consumption also curb the application of the recently feature-learning-based algorithms on embedding UAV platforms. Therefore, it is essential to carry out edge computing and light-weight artificial intelligence technologies so that the UAVs themselves can capture images and extract contextual information, thereby providing UAVs with autonomous perceptual capabilities of the surrounding environment. There are two concurrent research directions in this area: one is to develop more lightweight algorithms suitable for embedding platforms, while the other is to design or utilize parallel vector processors, such as Tensor Processing Unit (TPU) and Neural Processing Unit (NPU). In this section, we will focus on exploring light-weight algorithms for semantic segmentation on UAVs.\n虽然大多数无人机遥感图像的语义分割任务，如生态监测、城市景观和农业管理，都可以通过基于深度学习方法的高效特征提取受益，但这些方法计算量大，需要专业GPU服务器进行后端图像处理。大量的计算量、内存开销和计算功耗也限制了近期基于特征学习的算法在嵌入无人机平台上的应用。因此，必须开展边缘计算和轻量化人工智能技术，使无人机能够捕捉图像并提取上下文信息，从而赋予无人机对周围环境的自主感知能力。该领域有两个并行的研究方向：一是开发更轻量化的嵌入平台算法，二是设计或利用并行向量处理器，如张量处理单元（TPU）和神经处理单元（NPU）。本节将重点探讨无人机语义分割的轻量级算法。\nThe light-weight methods of DL-based models can be divided into 6 groups: tensor decomposition (Jaderberg et al., 2014; Szegedy et al., 2016), low-precision quantization (Gholami et al., 2021), model pruning (Cheng et al., 2017), knowledge distillation (KD) (Hinton et al., 2015), neural architecture search (NAS) (Elsken et al., 2019), and efficient structures of light-weight networks (Howard et al., 2017; Tan and Le, 2019). Tensor decomposition aims to represent a high-dimensional tensor as the product of low-dimensional tensors, which can reduce the memory overhead of models and enable faster convolution with fewer and cheaper operations (Lebedev and Lempitsky, 2018). The lowprecision quantization method converts 32-bit floats into 16-bit floats or 8-bit integers. For instance, most processors allow faster processing for addition and multiplication operations of 8-bit integers, which means the light-weight CNNs after 8-bit quantization can be 4 times more efficient than 32-bit CNNs (Krishnamoorthi, 2018). NAS aims to automatically design and search efficient network structures beyond human experiences based on observation data. However, NAS on the large-scale datasets of UAV remote sensing imagery requires unaffordable computational resources for most researchers. Unfortunately, few recent literature found that the aforementioned light-weight methods are utilized in the research in the context of semantic segmentation of UAV remote sensing images due to limited expertise, repetitive fine-tuning, and restricted hardware resources.\n基于DL模型的轻量级方法可分为6类：张量分解（Jaderberg等，2014;Szegedy 等，2016）、低精度量化（Gholami 等，2021）、模型剪枝（Cheng 等，2017）、知识蒸馏（KD）（Hinton 等，2015）、神经结构搜索（NAS）（Elsken 等，2019）以及轻量级网络的高效结构（Howard 等，2017;谭和黎，2019）。张量分解旨在将高维张量表示为低维张量的乘积，这可以降低模型的内存开销，并以更少且更便宜的作实现更快的卷积（Lebedev 和 Lempitsky，2018）。低精度量化方法将32位浮点数转换为16位浮点或8位整数。例如，大多数处理器支持更快的8位整数加法和乘法处理，这意味着8位量化后的轻量级卷积神经网络效率可是32位卷积神经网络的4倍（Krishnamoorthi，2018）。NAS旨在基于观察数据自动设计和搜索超越人类经验的高效网络结构。然而，NAS用于大规模无人机遥感图像数据集，对大多数研究人员来说需要负担得起的计算资源。遗憾的是，由于专业知识有限、重复微调和硬件资源有限，近期文献很少发现上述轻量化方法在无人机遥感图像语义分割的研究中被广泛应用。\nModel pruning is one of the model compression methods that remove network branches or connection nodes that have low impacts on model performance, facilitating the deployment of deep networks to resource-constrained devices. To address the challenge of real-time scene parsing on UAV platforms, Zhang et al. (2019c) proposed slimYOLOv3, a narrow yet faster deep object detector by pruning less important feature channels of YOLOv3 (Redmon and Farhadi, 2018). The number of parameters and the amount of floating-point operations (FLOPS) of slimYOLOv3 were less than one-tenth of the original YOLOv3. However, it achieved about 2 times faster inference efficiency  and comparable detection accuracy to the YOLOv3. In addition, knowledge distillation (KD), as proposed by Hinton et al. (2015), is also one of the efficient methods for model compression. The knowledge distillation follows the fundamental rules that small models (students) can be trained by the output probability distribution or feature space distribution of large models (teachers). Recent studies had demonstrated the efficiency of knowledge distillation for object detection and action recognition in UAV remote sensing images (Liu et al., 2021a) or videos (Ding et al., 2020). To accommodate the need of embedded intelligence on UAVs, Wang et al. (2021b) proposed a real-time forest fire monitoring model on the top of YOLOv4 (Bochkovskiy et al., 2020) by combining model pruning and knowledge distillation. Specifically, they utilized channel-level sparsity-induced regularization to eliminate redundant feature channels and applied the knowledge distillation algorithm to enhance the detection accuracy of the pruned model. The light-weight model with pruning and KD fell to only 2.64M parameters, a reduction of 95.87% compared to the original model. The inference time was 4.11 times faster, while the mean average precision (mAP) is 3.9% lower than that of the original YOLOv4.\n模型剪枝是一种模型压缩方法，用于去除对模型性能影响较小的网络分支或连接节点，便于将深度网络部署到资源受限的设备上。为解决无人机平台上实时场景解析的挑战，Zhang 等人（2019c）提出了slimYOLOv3，这是一种通过修剪YOLOv3中较不重要的特征通道，实现狭窄但更快的深层物体探测器（Redmon和Farhadi，2018）。slimYOLOv3 的参数数量和浮点运算（FLOPS）数量不到原始 YOLOv3 的十分之一。然而，它的推断效率约是YOLOv3的两倍，检测精度相当。此外，Hinton 等人（2015）提出的知识蒸馏（KD）也是模型压缩的高效方法之一。知识提炼遵循基本规则：==小模型（学生）可以通过大型模型（教师）的输出概率分布或特征空间分布来训练。==近期研究已证明，在无人机遥感图像（Liu 等，2021a）或视频（Ding 等，2020）中，知识蒸馏在物体检测和动作识别中的高效性。为满足无人机嵌入式智能的需求，==Wang等（2021b）提出了基于YOLOv4的实时森林火灾监测模型（Bochkovskiy等，2020），结合模型修剪与知识蒸馏。==具体来说，他们利用通道级稀疏诱导的正则化消除了冗余特征通道，并应用知识蒸馏算法提升了修剪模型的检测准确率。轻量模型加修剪和KD后参数降至仅264万，比原始模型减少了95.87%。推断时间快了4.11倍，平均精度（mAP）比原始YOLOv4低了3.9%。\nThe design of efficient structures for light-weight networks is another commonly used technique for real-time semantic segmentation (Yu et al., 2018a; Orsic et al., 2019; Wang et al., 2020d; Gao et al., 2021a). The light-weight models designed following this paradigm are highly flexible and editable, as researchers can reduce the width, depth, and interlayer connections of the light-weight models according to the practical demands. Hence, the design of efficient structures can dynamically balance the computational complexity, number of parameters, computational power consumption, and model accuracy. Nguyen et al. (2019) designed MAVNet, a small but efficient light-weight network, for semantic segmentation on micro aerial vehicles (MAV) based on dilated convolution and depth-wise feature aggregation block (DWFab). Zuo et al. (2021) introduced BiSeNet (Yu et al., 2018a), a bilateral light-weight semantic segmentation network, to carry out the realtime aerial video semantic segmentation of UAV streetscape sequences. Both of their experiments were sufficient to demonstrate that efficient structures of light-weight networks achieve not only a good tradeoff between inference time, model size, and model accuracy, but also the rapid deployment without post-processing of the trained models. In addition, the combination of light-weight methods and efficient attention mechanisms enabled networks with limited parameters to capture more contextual information. Gao et al. (2021a) explored an asymmetric encoder–decoder network composed of factorization depthwise and dilation convolution using the multi-scale context fusion scheme and multiple efficient attention branches. Pang et al. (2022) also proposed the Semantics Guided Bottleneck Network (SGBNet) based on BiSeNet and the Channel Pooled Attention (CPA) mechanism to balance segmentation accuracy, model size, and inference speed on the Land Cover Dataset. These models utilized semantic enhancement modules with small parameters increment to preserve the spatial presentation with more details and enhanced the contextual relationships and generalization performance of light-weight models.\n为轻量级网络设计高效结构是另一种常用的实时语义分割技术（Yu 等，2018a;Orsic 等，2019;Wang 等，2020d;Gao 等，2021a）。按照这一范式设计的轻量级模型高度灵活且可编辑，研究人员可以根据实际需求缩小轻量级模型的宽度、深度和层间连接。因此，高效结构的设计能够动态平衡计算复杂度、参数数量、计算功耗和模型准确性。Nguyen等人（2019）设计了MAVNet，这是一个小巧但高效的轻量级网络，用于微型航空载具（MAV）语义分割，基于膨胀卷积和深度特征聚合块（DWFab）。Zuo等（2021）引入了BiSeNet（Yu等，2018a），这是一种双边轻量级语义分割网络，用于实现无人机街景序列的实时航拍视频语义分割。==他们的两个实验都足以证明，高效的轻量级网络结构不仅在推理时间、模型规模和模型准确性之间取得了良好权衡，还能在无需后处理的情况下快速部署训练模型。==此外，轻量化方法和高效的注意力机制相结合，使参数有限的网络能够捕捉更多上下文信息。Gao等（2021a）利用多尺度上下文融合方案和多高效注意力分支，探索了由深度分解和膨胀卷积组成的非对称编码器-解码器网络。Pang 等人（2022）还提出了基于 BiSeNet 和通道集中注意力（CPA）机制的语义引导瓶颈网络（SGBNet），以平衡土地覆盖数据集上的切分准确性、模型规模和推断速度。这些模型使用了参数增量较小的语义增强模块，以更详细的空间呈现方式保持空间表现，并提升了轻量级模型的上下文关系和泛化性能。\nHere we have described light-weight semantic segmentation methods in the context of UAVs, which is still a young research area awaiting in-depth explorations, with minimal published papers in this field. The design of efficient network structures seems to be the most practical approach in current literature compared to tensor decomposition, low-precision quantization, and NAS, for it does not require complex fine-tunings and post processes for model deployment on UAVs. Additionally, there are probably considerable benefits in combining different light-weight methods for model acceleration: more significant reduction of the model size and power consumption while maintaining model accuracy and accelerating the inference process (Wang et al., 2021b; Aghli and Ribeiro, 2021). Though light-weight semantic segmentation methods show encouraging achievements, the pretraining process of light-weight models is restricted by the limited image datasets with fine-gained annotations.\n本文中我们描述了无人机背景下的轻量级语义分割方法，该领域仍是一个年轻且等待深入探索的研究领域，且该领域的发表论文极少。与张量分解、低精度量化和无人机导航（NAS）相比，高效网络结构的设计似乎是当前文献中最实用的方法，因为它无需复杂的微调和后期处理即可在无人机上部署模型。此外，结合不同的轻量化模型加速方法可能带来显著好处：在保持模型准确性和加速推理过程的同时，更显著地减少模型规模和功耗（Wang 等，2021b;Aghli 和 Ribeiro，2021年）。尽管轻量级语义分割方法取得了令人鼓舞的成就，但轻量级模型的预训练过程受限于有限的图像数据集和细致增益注释。\nResearchers prefer to transfer and fine-turn the pre-trained backbones for real-time semantic segmentation on UAVs due to the computational burden of pre-training the model from scratch, which also limits the diversification of efficient network structures.\n由于从零开始预训练模型的计算负担较大，且会限制高效网络结构的多样性，研究人员更倾向于迁移并微调预训练的主干网络，以便在无人机上进行实时语义分割。\n Datasets for UAV semantic segmentation\nUAV语义分割数据集\nThe rapid development of semantic segmentation owes much to large-scale datasets and relevant DL-based image analysis methods. Undoubtedly, large-scale datasets and massive semantic annotations allow models to improve generalization performance and learn more diverse semantic information. The data sources of remote sensing semantic segmentation datasets are mainly collected by satellite-based platforms, aerial platforms, and embedding UAV platforms (Toth and Jóźków, 2016). Advanced surveys that summarized the remote sensing image datasets for different application scenarios mainly focused on datasets collected by satellite platforms (Song et al., 2019; Yuan et al., 2021; Osco et al., 2021). Satellites are constrained to catch the top vertical view from the sky, while UAVs are free to capture the perspective view in three dimensions, including vertical view, horizontal view, and oblique view, allowing researchers and practitioners to obtain unexplored viewpoints at various spatial scales than prior benchmarks (Nigam et al., 2018; Lyu et al., 2020). Due to many restrictions such as flight authorization and potential confidentiality requirements, we found that many restricted and domain-specific datasets collected by UAVs are not publicly available. Nevertheless, we believe that summarizing the publicly available UAV-based image datasets for semantic segmentation up to now may help scholars and practitioners understand the application trend of UAV platforms, reduce the labor cost for investigating relevant open-source datasets, as well as promote the emergence of more advanced semantic segmentation methods for UAVbased images. In this review, we have listed publicly available UAV datasets for semantic segmentation applied to different applications, as shown in Table 4.\n语义分割的快速发展很大程度上得益于大规模数据集和相关的基于深度学习的图像分析方法。毫无疑问，大规模数据集和海量语义注释使模型能够提升泛化性能，学习更多样的语义信息。遥感语义分割数据集的数据主要由卫星平台、空中平台和嵌入无人机平台收集（Toth 和 Jóźków，2016）。高级调查总结了不同应用场景下的遥感图像数据集，主要聚焦于卫星平台收集的数据集（Song 等，2019;Yuan 等，2021;Osco等，2021）。卫星受限于捕捉天空顶部的垂直视角，而无人机则可自由捕捉三维透视视角，包括垂直视角、水平视角和斜视，使研究人员和从业者能够获得不同空间尺度上未探索的视角（Nigam等，2018;Lyu 等，2020）。由于飞行授权和潜在保密要求等诸多限制，我们发现许多由无人机收集的受限和特定领域数据集并未公开。尽管如此，我们认为，汇总迄今为止公开的无人机图像数据集用于语义分割，有助于学者和实践者理解无人机平台的应用趋势，降低研究相关开源数据集的劳动成本，并推动更先进的无人机图像语义分割方法的出现。在本综述中，我们列出了公开可用的UAV数据集，用于不同应用的语义分割，如表4所示。\nInspired by Cityscapes (Cordts et al., 2016; Nigam et al., 2018) released AeroScapes, which is designed for the image semantic segmentation tasks of the low-altitude scenes. It provides 3269 images  acquired from 141 video sequences of UAV fleets. Researchers prelabeled 11 ground truth classes with dense annotations, but the view variations caused by flexible flight altitude and movable perspective make it challenging for semantic segmentation tasks. Semantic Drone Dataset is another publicly available urban scene semantic understanding dataset that aims to increase the safety of UAV flight and landing procedures. The dataset consists of 400 training images with densely labeled semantic labels and 200 private testing images, all acquired by autonomous drones at an altitude of 5 to 30 m above the ground. Urban Drone Dataset (Chen et al., 2018c) (UDD5) is an urban scene collection of drone images acquired from 4 different cities in China at flight altitudes between 60 and 100 m. The original dataset comprises 5 categories, splitting 160 frames for the training set and 45 images for the verification set. In addition, the authors released a new public dataset with a new Roof class in 2020, which is also called UDD6. ManipalUAVid (Girisha et al., 2019) is a new UAV aerial video dataset collected in a closed university campus. It provides 33 videos and 667 key frames collected at six locations during the day. Only the key frames were finely annotated into 4 semantic classes, including constructions, greeneries, roads, and waterbodies. More recently, Lyu et al. (2020) released a high-resolution urban dataset called UAVid. The most characteristic of UAVid is the severe class imbalance of pixel distribution, making it challenging to preserve temporal consistency and obtain accurate segmentation results. It should be noted that this dataset only provides semantic label masks for specific image frames at selected time points within the video rather than for the entire video sequence. Their subsequent work added extra 12 sequences to strengthen the original 30 video sequences and released 42 sequences as UAVid2020, which can be available on their benchmark website.\n受城市景观启发（Cordts 等，2016;Nigam等人，2018年）发布了AeroScapes，该软件专为低空场景的图像语义分割任务设计。它提供了从141段无人机机队视频序列中获取的3269张图像。研究人员预先标记了11个地面真实类并带有密集注释，但由于灵活飞行高度和可移动视角引起的视角变化，使得语义分割任务具有挑战性。语义无人机数据集是另一个公开可用的城市场景语义理解数据集，旨在提高无人机飞行和着陆程序的安全性。该数据集包含400张带有密集语义标签的训练图像和200张私有测试图像，均由自主无人机在地面5至30米高空拍摄。城市无人机数据集（Chen 等，2018c）（UDD5）是一组从中国4个不同城市在60至100米飞行高度拍摄的城市场景图像。原始数据集包含5个类别，训练集分为160帧，验证集分为45张图像。此外，作者于2020年发布了一个新的公开数据集，包含一个新的Roof类，也称为UDD6。ManipalUAVid（Girisha 等，2019）是一个在关闭大学校园内收集的新无人机航拍视频数据集。它提供33个视频和667个关键帧，这些视频在一天内收集到六个地点。只有关键帧被精细注释为4个语义类别，包括建筑、绿地、道路和水体。最近，Lyu 等人（2020）发布了一个名为 UAVid 的高分辨率城市数据集。UAVid最显著的特点是像素分布的严重类别失衡，这使得保持时间一致性和获得准确分割结果变得具有挑战性。需要注意的是，该数据集仅为视频中特定时间点的特定图像帧提供语义标签遮罩，而非整个视频序列。他们后续的作品增加了12个序列以加强原有30个视频序列，并发布了42个序列\n\nDroneDeploy is a large-scale segmentation benchmark challenge that encourages state-of-the-art machine learning on aerial drone data. It consists of aerial orthomosaics and elevation images, and there are 55 tiles have been finely annotated into 6 categories. Inria Aerial Image Labeling Dataset (IAILD) (Maggiori et al., 2017) is another aerial dataset covering a wide range of urban landscapes with a spatial resolution of 30 cm. The 360 images, including 180 trading sets and 180 testing sets, have been annotated into two semantic classes. The scenes of different cities do not overlap in the training and testing sets, making it possible to powerfully validate the generalization of models for semantic segmentation.\nDroneDeploy是一项大规模分割基准挑战赛，旨在鼓励对无人机航拍数据进行最先进的机器学习研究。该数据集包含航拍正射影像和高程图像，共有55个图块被精细标注为6个类别。Inria航拍图像标注数据集（IAILD）（Maggiori等人，2017）是另一个航拍数据集，覆盖了多种城市景观，空间分辨率为30厘米。该数据集包含360幅图像，包括180个训练集和180个测试集，均已标注为两个语义类别。训练集和测试集中的不同城市场景互不重叠，从而能够有力地验证语义分割模型的泛化能力。\nUAVs have received much attention in crack detection on highrise infrastructures due to their low budget and high efficiency. Yang et al. (2017) proposed the Concrete Structure Spalling and Crack (CSSC) database for automatic concrete inspection by UAVs, consisting of 278 spalling images and 954 crack images collected through web keyword search. Hong et al. (2021) relabeled the AerialCrackDetection dataset (Wang, 2017) that only provided bounding rectangles for crack detection. The proposed Highway Crack Dataset contains 1157 images with corresponding pixel-wise segmentation annotations. CrackUAS, a high-resolution crack dataset built by Arda et al. (2021), provided crack images with and without shell obstructions. In addition, the dataset containing synthetic images of surface cracks with shell obstructions enables researchers to verify the robustness of models and further validating their transfer learning performance. UAVs have also shown great potential for crop management, especially for tasks requiring human visual inspection, such as weed detection, lodging detection, and many others. Sa et al. (2017) released WeedNet, a multispectral (NR, Red channel, and NDVI) dataset with three semantic annotations, containing 132 crop images, 243 weeds images, and 90 crop-weeds mixtures. Another large-scale multispectral crop-weed dataset with 8 image modalities, called WeedMap Dataset (Sa et al., 2018) in this review, was proposed in their subsequent work. The spatial resolution of this dataset reached about 1 cm per pixel given the camera specification and flight altitude. To achieve precise crop mapping with hyperspectral feature space, Niu et al. (2022) acquired high-resolution images of 27 kinds of crops with a complex plantation structure based on UAV hyperspectral systems and then released the UAV-HSI-Crop dataset. The dataset is densely labeled with 29 classes, and the long-tail distribution of the pixel number histogram poses a great challenge for accurate crop mapping. In addition, the ease with which UAV sensors can provide high resolution is a unique advantage in infrastructure monitoring. The Massachusetts Dataset, published by Mnih (2013), contains 1711 road images and 151 building images at a spatial resolution of 100 cm. Since most datasets for road extractions from UAV images are not publicly available, the Massachusetts Roads Dataset (MRD) can be employed to validate the performance of UAV road extraction methods (Wang et al., 2021c). The Topo-boundary Dataset (Xu et al., 2021) is a largescale high-resolution road boundary extraction dataset applied to 8 sub-tasks. The dataset provides 25 295 optical images and eight pixellevel annotation labels. The most significant feature of this dataset is the imbalance between positive and negative sample pixels, which poses a significant challenge for the semantic segmentation of road boundaries. Besides, the heterogeneity in color intensity and significant variations in road width also curbed accurate segmentation of roads in high-resolution UAV images. Hence, Behera et al. (2023) built the NITRDrone dataset, which contains the occluded_road and vegetation classes in addition to the two commonly used binary semantic annotations of road and background. It presents more complex and diverse road context information due to its characteristics, such as occlusions, discontinuities, different viewpoints, and various road types.\n由于低成本和高效，无人机在高层基础设施的裂纹检测中备受关注。Yang等人（2017）提出了混凝土结构剥落与裂缝（CSSC）数据库，用于无人机自动进行混凝土检查，包含278张剥落图像和954张裂纹图像，均通过网络关键词搜索收集。Hong 等人（2021）重新标记了仅提供边界矩形用于裂纹检测的 AerialCrackDetection 数据集（Wang， 2017）。拟议的公路裂缝数据集包含1157张图像及相应的像素分割注释。CrackUAS是Arda等人（2021年）构建的高分辨率裂纹数据集，提供了有壳体障碍和无壳体障碍的裂纹图像。此外，包含带有壳体障碍的表面裂纹合成图像的数据集，使研究人员能够验证模型的稳健性，并进一步验证其迁移学习性能。无人机在作物管理方面也展现出巨大潜力，尤其是在需要人工目视检查的任务中，如杂草检测、停泊检测等。Sa 等人（2017）发布了 WeedNet，这是一个多光谱（NR、红色通道和 NDVI）数据集，包含三个语义注释，包含 132 张裁剪图像、243 张杂草图像和 90 张裁剪-杂草混合图。他们在后续研究中提出了另一个具有8种图像模态的大规模多光谱作物-杂草数据集，称为WeedMap数据集（Sa等，2018）。根据相机规格和飞行高度，该数据集的空间分辨率约为每像素1厘米。为了实现高光谱特征空间的精确作物映射，Niu等人（2022）基于无人机高光谱系统，拍摄了27种具有复植结构作物的高分辨率图像，随后发布了无人机-HSI-作物数据集。该数据集密集地标注了29个类别，像素数直方图的长尾分布对准确的裁判映射构成了巨大挑战。此外，无人机传感器能够轻松提供高分辨率，这也是基础设施监测的独特优势。马萨诸塞州数据集由Mnih（2013年）发布，包含1711张道路图像和151张建筑图像，空间分辨率为100厘米。由于大多数无人机图像中道路提取的数据集尚未公开，马萨诸塞州道路数据集（MRD）可用于验证无人机道路提取方法的性能（Wang 等，2021c）。地形边界数据集（Xu 等，2021）是一个大规模高分辨率道路边界提取数据集，应用于8个子任务。该数据集提供25,295张光学图像和8个像素级注释标签。该数据集最重要的特点是正负样本像素的不平衡，这对道路边界的语义分割构成了重大挑战。此外，色彩强度的异质性和道路宽度的显著变化也限制了高分辨率无人机图像中道路的精确分割。因此，Behera等人（2023）构建了NITRDrone数据集，该数据集除了常用的道路和背景这两个常用二进制语义注释外，还包含了occluded_road类和植被类。由于其特性，如阻塞、不连续、不同视角和多种道路类型，它呈现了更复杂和多样化的道路背景信息。\nIn recent years, the rapid dispatch of UAVs has been an efficient approach for emergency response and damage assessment of disaster scenarios. Chowdhury et al. (2020) collected a natural disaster dataset (HRUD) based on a high-resolution UAV platform. The dataset provides 1973 images with 4 kinds of damage polygons based on damage levels. The dataset also provided semantic labels of 8 objects and background to annotate all damaged objects in an image. Their subsequent work released RescueNet (Chowdhury et al., 2022), a highresolution post-disaster dataset, for damage assessment after the natural disaster. The dataset is annotated in a similar format to HRUD, which consists of 4494 high-resolution UAV images with 11 categories of  pixel-level annotations of damaged objects. Rahnemoonfar et al. (2021) published a flood disaster assessment dataset (FloodNet) based on the UAV platform applied to damage classification, semantic segmentation, and visual question answering (VQA). It is the first dataset designed to simultaneously address three critical computer vision tasks, presenting new opportunities and challenges for disaster assessment. Furthermore, we found a dataset called FLAME (Shamsoshoara et al., 2021), which exhibits prescribed burnings of piled slash in the Arizona pine forest, to motivate advanced solutions for early fire detection and management. It provides 2003 image frames of video recordings and corresponding frame-level semantic annotations for the fully-supervised binary semantic segmentation.\n近年来，快速派遣无人机成为应急响应和灾害情景损害评估的高效方法。Chowdhury等人（2020）基于高分辨率无人机平台收集了自然灾害数据集（HRUD）。该数据集提供了1973张图像，包含基于损伤等级的4种损坏多边形。数据集还提供了8个物体的语义标签，并用背景注释图像中所有受损物体。他们的后续工作发布了RescueNet（Chowdhury等，2022），这是一个高分辨率的灾后数据集，用于自然灾害后的损害评估。该数据集的注释格式与HRUD类似，后者包含4494张高分辨率无人机图像，包含11类像素级的受损物体注释。Rahnemoonfar等人（2021）发表了基于无人机平台的洪水灾害评估数据集（FloodNet），该数据集应用于损害分类、语义分割和视觉问答（VQA）。这是首个同时设计用于解决三个关键计算机视觉任务的数据集，为灾害评估带来了新的机遇和挑战。此外，我们还发现了一个名为FLAME的数据集（Shamsoshoara等，2021），该数据集展示了亚利桑那松林中桩柱的规定烧除，以激励早期火灾检测和管理的先进解决方案。它提供了2003帧视频录制的图像帧及相应的帧级语义注释，用于全监督二元语义分割。\nTo quantify whether DL-based classifiers can imitate human experts and achieve higher accuracy in RGB fluvial scene classification on hyperspatial resolution airborne images, Carbonneau et al. (2020) released the 11 Rivers dataset, which comprises 305 hyperspatial (&lt;10 cm) resolution color imagery as well as corresponding label masks with 5 semantic classes. Wang and Mahmoudian (2023) created the Aerial River Image Dataset (AFID), which offers various perspectives of river scenes and focuses specifically on labeling water and waterborne obstacles. This dataset also suffers from imbalanced distribution in class pixel percentages and contains a small part of blurred images due to residual moisture on the camera lens surface. To promote studies on river ice monitoring based on DL technologies, Zhang et al. (2020d) released a publicly available UAV-based image dataset, called NWPU_YRCC, for river ice segmentation. This dataset provides 814 images captured by two UAVs in the region of the Yellow River with various appearances at different times, as well as three categories, including ice, water, and shore. To satisfy the training requirements of the neural network, they then added 74 images to the original dataset and renamed it NWPU_YRCC_EX. In their subsequent work (Zhang et al., 2021b), a new NWPU_YRCC2 dataset, which contains 1525 typical river ice images with different characteristics, was built for fine-grained river ice semantic segmentation. The new dataset puts more emphasis on distinguishing between drift ice and shore ice, and provides two other semantic categories, including water and other. In this section, we have introduced semantic segmentation datasets of UAV remote sensing images applied to different scenarios. In addition to optical UAV images, multispectral and hyperspectral modalities are gradually adopted in current UAV image acquisition tasks. Turning to 3D point cloud labeling, UAVs have also attracted attention in the semantic understanding of fine-grained geographic scenes. Researchers have already published relevant 3D point cloud datasets collected by UAVs, including The Hessigheim 3D (H3D) benchmark (Kölle et al., 2021), SensatUrban (Hu et al., 2021b), Swiss3DCities (Can et al., 2021), and many others. Recent publicly available UAV datasets suggest that the semantic segmentation on UAV-based platforms is trending towards sophisticated, multi-tasking, and multimodal.\n为了量化基于DL的分类器是否能模仿人类专家，并在高空间分辨率的空中图像上实现RGB河流场景分类的更高准确性，Carbonneau等人（2020）发布了11 Rivers数据集，该数据集包含305张超空间（&lt;10厘米）分辨率彩色图像及对应的标签掩码，包含5个语义类别。Wang和Mahmoudian（2023）创建了航拍河流图像数据集（AFID），提供了河流场景的多样视角，并特别关注水体及水上障碍物的标注。该数据集还存在类别像素百分比分布不平衡的问题，且由于相机镜头表面残留的湿气，图像出现了少量模糊。为促进基于DL技术的河川冰监测研究，Zhang等人（2020d）发布了一项公开的基于无人机的图像数据集，称为NWPU_YRCC，用于河川冰层切割。该数据集包含两架无人机在黄河地区拍摄的814张不同时间出现的图像，以及冰、水、岸三类。为了满足神经网络的训练需求，他们向原始数据集添加了74张图像并将其更名为NWPU_YRCC_EX。在他们后续的工作中（Zhang 等，2021b），构建了一个新的NWPU_YRCC2数据集，包含1525张具有不同特征的典型河流冰图像，用于细粒度河川冰语义分割。新数据集更强调区分漂流冰和岸冰，并提供了另外两个语义类别，包括水和其他。本节介绍了应用于不同场景的无人机遥感图像语义分割数据集。除了光学无人机图像外，多光谱和高光谱模态也逐渐被应用于当前无人机图像采集任务中。转向三维点云标记，无人机在细粒度地理场景的语义理解中也受到关注。研究人员已经发布了由无人机收集的相关三维点云数据集，包括赫西格海姆3D（H3D）基准测试（Kölle等，2021）、SensatUrban（胡等，2021b）、Swiss3DCities（Can等，2021）等。近期公开的无人机数据集表明，基于无人机平台的语义分割正趋向于复杂化、多任务处理和多模态化。\n Evaluations and comparisons\n评估与比较\nEvaluation metrics are necessary to assess the effectiveness and reliability of semantic segmentation methods for UAV remote sensing images. Commonly used evaluation metrics evaluating the accuracy of semantic segmentation methods consist of overall accuracy (OA), mean intersection over union (mIoU) (Eigen and Fergus, 2015; Yu et al., 2018b), Precision, Recall, and F1-score. Due to the distinctions in features, datasets, algorithms, or tricks, it is tough to demonstrate which methods are better than others by comparing published papers directly. Hence, we conduct comparative experiments to evaluate the semantic segmentation accuracy of representative methods according to OA, mIoU, and F1-score on two recent UAV-based RGB image datasets from different scenarios, including the UAVid dataset and the FloodNet dataset. In addition, we adopt the number of parameters, Multiply-Accumulate Operations (MACs), and Frames Per Second (FPS) to evaluate the model inference efficiency, considering the necessity of loading the model onto the airborne device for real-time image processing.\n评估指标对于评估无人机遥感图像语义分割方法的有效性和可靠性是必要的。常用的评估指标包括整体准确率（OA）、平均交集与并集（mIoU）（Eigen 和 Fergus，2015;Yu 等，2018b）、精确度、回忆和F1评分。由于特征、数据集、算法或技巧的差异，直接比较已发表论文很难证明哪些方法优于其他方法。因此，我们进行了比较实验，评估代表性方法根据OA、mIoU和F1评分，针对两个来自不同场景的无人机RGB图像数据集（包括UAVid数据集和FloodNet数据集）的语义分割准确性。此外，我们采用参数数量、乘法累加作（MACs）和每秒帧数（FPS）来评估模型推理效率，同时考虑将模型加载到机载设备进行实时图像处理的必要性。\n","categories":["深度学习","论文阅读"],"tags":["python","深度学习","分割方法与数据集","语义分割","综述","遥感图像"]},{"title":"语义分割","url":"/python/ss/","content":" 语义分割\n 常见分割任务\n\n语义分割\n实例分割\n全景分割\n\n 常见语义分割评价指标\n Pixel Accuracy / Global Acc像素准确率\n这是最直观的指标，反映了模型对所有像素点分类正确的比例。\nPA=∑inii∑itiPA = \\frac{\\sum_{i} n_{ii}}{\\sum_{i} t_{i}}\nPA=∑i​ti​∑i​nii​​\nniin_{ii}nii​：表示第 iii 类被正确预测为第 iii 类的像素数量（对角线元素）。\ntit_{i}ti​：表示第 iii 类总共包含的像素点数量。\n所有分类正确的像素总数 ÷\\div÷ 图像像素总数。\n局限性：由于它计算的是全局准确度，如果图像中某个类别如背景占比极大，PA 就会被该类别主导，无法反映模型对小目标的识别能力。\n mean Accuracy平均准确率\n为了解决上述类别不平衡问题，该指标先计算每个类别的准确率，再取平均值.\nmAcc=1ncls⋅∑iniitimAcc = \\frac{1}{n_{cls}} \\cdot \\sum_{i} \\frac{n_{ii}}{t_{i}}\nmAcc=ncls​1​⋅i∑​ti​nii​​\nnclsn_{cls}ncls​：类别的总数。\nniiti\\frac{n_{ii}}{t_{i}}ti​nii​​：第 iii 个类别的分类准确率。\n意义：它给每个类别（无论大小）分配了相同的权重。如果你的任务中包含很多细长或微小的物体，这个指标比 PA 更有参考价值。\n mean IoU平均交并比\nmIoU=1ncls⋅∑iniiti+∑jnji−niimIoU = \\frac{1}{n_{cls}} \\cdot \\sum_{i} \\frac{n_{ii}}{t_{i} + \\sum_{j} n_{ji} - n_{ii}}\nmIoU=ncls​1​⋅i∑​ti​+∑j​nji​−nii​nii​​\n分母：ti+∑jnji−niit_{i} + \\sum_{j} n_{ji} - n_{ii}ti​+∑j​nji​−nii​ 实际上就是并集。它由“真实区域”加上“预测区域”，再减去重复计算的“交集部分”组成。\n分子：niin_{ii}nii​ 就是**交集 。\n核心逻辑：IoU=交集并集IoU = \\frac{交集}{并集}IoU=并集交集​。只有当预测的形状与真实的形状高度重合时，mIoU 才会接近 1。\n 一个例子\n\n\n\n\n\n 全局准确率\nPA=16+3+16+12+864≈0.859PA=\\frac{16+3+16+12+8}{64}\\approx0.859\nPA=6416+3+16+12+8​≈0.859\n 平均准确率\n分母是列之和，分子是对角那个元素。\ncls0acc=1620cls0_{acc}=\\frac{16}{20}cls0acc​=2016​，cls1acc=34cls1_{acc}=\\frac{3}{4}cls1acc​=43​，cls2acc=1616cls2_{acc}=\\frac{16}{16}cls2acc​=1616​，cls3acc=1216cls3_{acc}=\\frac{12}{16}cls3acc​=1612​，cls0acc=88cls0_{acc}=\\frac{8}{8}cls0acc​=88​\nmA=cls0acc+cls1acc+cls2acc+cls3acc+cls4acc5mA=\\frac{cls0_{acc}+cls1_{acc}+cls2_{acc}+cls3_{acc}+cls4_{acc}}{5}\nmA=5cls0acc​+cls1acc​+cls2acc​+cls3acc​+cls4acc​​\n 平均交并比\n分母是行与列之和减去重复部分那个，分子就是对角线那个元素。\ncls0IoU=1616+1+1+2+2cls0_{IoU}=\\frac{16}{16+1+1+2+2}cls0IoU​=16+1+1+2+216​，cls1IoU=33+1+1cls1_{IoU}=\\frac{3}{3+1+1}cls1IoU​=3+1+13​，cls2IoU=1616+1+1cls2_{IoU}=\\frac{16}{16+1+1}cls2IoU​=16+1+116​，cls3IoU=1212+2cls3_{IoU}=\\frac{12}{12+2}cls3IoU​=12+212​，cls0IoU=88+2+2cls0_{IoU}=\\frac{8}{8+2+2}cls0IoU​=8+2+28​\nmA=cls0IoU+cls1IoU+cls2IoU+cls3IoU+cls4IoU5mA=\\frac{cls0_{IoU}+cls1_{IoU}+cls2_{IoU}+cls3_{IoU}+cls4_{IoU}}{5}\nmA=5cls0IoU​+cls1IoU​+cls2IoU​+cls3IoU​+cls4IoU​​\n Transposed Convolution 转置卷积\nTransposed Convolution/ fractionally-strided / deconvolution\n转置卷积不是卷积的逆运算\nN.B.: Blue maps are inputs, and cyan maps are outputs.\n\n  \n    \n    \n    \n    \n  \n  \n    No padding, no strides, transposed\n    Arbitrary padding, no strides, transposed\n    Half padding, no strides, transposed\n    Full padding, no strides, transposed\n  \n  \n    \n    \n    \n    \n  \n  \n    No padding, strides, transposed\n    Padding, strides, transposed\n    Padding, strides, transposed (odd)\n    \n  \n\nHout=(Hin−1)×stride[0]−2×padding[0]+kernel_size[0]H_{out} = (H_{in} - 1) \\times stride[0] - 2 \\times padding[0] + kernel\\_size[0]\nHout​=(Hin​−1)×stride[0]−2×padding[0]+kernel_size[0]\nWout=(Win−1)×stride[1]−2×padding[1]+kernel_size[1]W_{out} = (W_{in} - 1) \\times stride[1] - 2 \\times padding[1] + kernel\\_size[1]\nWout​=(Win​−1)×stride[1]−2×padding[1]+kernel_size[1]\n 下采样 vs 上采样\n普通卷积：主要用于特征提取和空间降维（下采样）。通过滑动窗口将多个输入像素映射为一个输出像素，通常会减小特征图的尺寸。\n转置卷积：主要用于恢复空间分辨率或进行可学习的上采样。将一个输入像素映射到多个输出像素，从而产生比输入更大的输出特征图。常用于语义分割、生成对抗网络等需要还原图像尺寸的任务。\n FCN\n\n21深度就是21种类别，VOC是20类然后还有一个是背景一共21类，然后经过上采样得到每一个像素的类别就完成分割了，FCN使用全卷积操作，这避免了全连接层对图片大小的严格限制所导致的报错问题。\n\n其实这个主要是把VGG的全连接层修改为卷积，当然还有更多细节，下面是FCN-32S，即直接进行对pool5的32倍上采样，VGG16的骨干网络就是对原图进行32倍下采样\n FCN-32S\n\n FCN-16S\n\n FCN-8S\n\n FCN损失函数-像素级多类别交叉熵损失\n对于单张图片，总损失 LLL 定义为：\nL=−1H×W∑i=1H×W∑c=1Cyi,clog⁡(y^i,c)L = -\\frac{1}{H \\times W}\\sum_{i=1}^{H \\times W} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})\nL=−H×W1​i=1∑H×W​c=1∑C​yi,c​log(y^​i,c​)\nFCN的损失函数是对每一个像素进行softmax分类交叉熵后，在对全图进行平均。\n 空洞卷积 / 膨胀卷积\nPyTorch深度学习 - 基础 - 深度学习 | KarryLiu = 诗岸梦行舟 = 分享计算机知识以及各种心得总结\n增大感受野，一般我们会主动保持特征图大小。\n FCN模型训练\n现在基本上把骨干网络都换成ResNet-50了。\n\n环境为RTX 3080 x4 基于fcn_resnet50: https://download.pytorch.org/models/fcn_resnet50_coco-1167a1af.pth预训练权重训练20轮。\n\n结果：\n\n  \n  \n\n实际上这个预训练已经不错了，我这20轮基本上没啥贡献，也就让meanIoU提升了一点点。。\n U-Net\nU-Net 是由 Ronneberger 等人于 2015 年提出的一种端到端的全卷积网络，主要用于像素级分类。\n\n这个中间的连接并不是全像素的连接，他会从源特征图裁剪一部分连接到上采用后的特征图。\n\n 在DRIVE数据集上进行训练\nDRIVE 数据集一般指的是是视网膜血管分割领域最经典、使用最广泛的数据集之一。\n\n\n200轮训练后…\n[epoch: 199]\ntrain_loss: 0.3152\nlr: 0.000000\ndice coefficient: 0.814\nglobal correct: 95.3\naverage row correct: [&#x27;97.2&#x27;, &#x27;82.0&#x27;]\nIoU: [&#x27;94.7&#x27;, &#x27;68.8&#x27;]\nmean IoU: 81.7\n\n\n  \n  \n\n### 使用VGG-U-Net在CamVid-32数据集进行训练\nCamVid是语义分割领域里一个非常经典、但规模较小的自动驾驶场景数据集，经常被用来做教学、算法验证和轻量实验。真实城市驾驶视频 + 像素级语义标注。\n VOC镜像站\nhttps://data.brainchip.com/dataset-mirror/voc\n","categories":["深度学习","深度学习基础"],"tags":["python","深度学习","CNN","FCN"]},{"title":"v2yWdtc：无聊写了一个格式转换工具","url":"/python/v2yWdtc/","content":" v2yWdtc\nv2yWdtc是我无聊编写的一款将VOC格式转换为YOLO格式的工具，它的构成非常简单，尽管如此它很强大，它可以辅助你分析你的数据集是否有异常的数据，同时针对空标签还有过滤功能。\n\n快速转换：一键将 VOC 格式标签转换为 YOLO 格式。\n数据预检测：统计数据集，辅助你发现异常标签或标注问题。\n空标签过滤：自动过滤空标签，保证训练数据质量。\n简单易用：无需复杂配置，几步操作即可完成数据预处理。\n\n 开源地址 - ⭐！\nv2yWdtc：https://github.com/735690757/v2yWdtc\n 项目结构\n\n\n\n目录/文件\n核心职能说明\n\n\n\n\nAnnotations/\n原始标签存放地。通常存放标准的 XML 格式文件，包含目标类别和边界框坐标。\n\n\nImagesets/\n数据集索引中心。存放 train.txt, val.txt, test.txt 等。之前的报错正是因为程序无法在此找到这些索引文件。\n\n\nJPEGImages/\n原始图像库。存放所有训练和测试用的图片（ .jpg 格式）。\n\n\nTools/\n数据诊断工具箱。包含用于分析数据集质量的脚本（如 check.py 检查完整性，cbox.py 分析边界框分布）。\n\n\nimages_tag.py\n标签处理模块。可能涉及图像标注的自动化处理或类别名称映射。\n\n\nimgsWlabels.py\n数据分发引擎。负责根据索引文件将图片和转换后的标签文件分发到指定的训练/验证文件夹中。\n\n\nmain.py\n项目总入口。一键化操作的控制台，通过调用其他模块完成全流程转换。\n\n\nvoc_to_yolo.py\n格式转换核心。执行最关键的 XML 到 TXT 归一化转换逻辑。\n\n\n\n\n train与val模式、trainval模式\n切换模式只需要修改main.py的此处：\n\n train与val模式（0模式）（默认模式）\n在这种模式下，数据集被分为三个部分：训练集、验证集、测试集，这种模式的特点是：训练集和验证集是分开的，避免数据泄露，确保模型的泛化能力。\n trainval模式（1模式）\n在 trainval 模式下，训练集和验证集被合并为一个整体的数据集，模型在整个 trainval 数据集上进行训练和验证。这种模式通常用于特定的任务，例如数据量不大时，可以通过合并训练集和验证集来提升训练集的多样性和大小。\n 使用方法\n\n将图片数据放在JPEGImages下\n将xml放在Annotation下\n执行主函数\nok了就\n\n 执行过程\n\n 结果文件\n\n紫色输出结果是图片（训练、验证、测试）和标签（训练、验证、测试）\n白色输出结果是这样的索引记录格式：\n\n 参考开源脚本\nhttps://github.com/JieZzzoo/Data_Trans\n","categories":["深度学习","项目与实战"],"tags":["python"]},{"title":"YOLO-V1","url":"/python/YOLO/YOLO_V1/","content":" YOLO系列\nYOLO（You Only Look Once）\n YOLOv1\n\n输入是448x448x3，最终输出是7x7x30。对应原始图片对应区域的结果。就是所谓的将原始图片划分为7x7个小方格大小的图片。\n其实这是一种一一映射7x7这样的小方格，7×7 个 cell，每个 cell 负责“这个 cell 中心落在这里的目标”的检测任务。\n\n x, y, w, h\nx,y是中心点坐标，w,h是预测框的宽和高。\n每个 bounding box 的 5 个参数：x,y,w,h,confidence,\nx,y：框中心相对于当前 cell 的坐标（一般在 0～1）\nw,h：框的宽高（通常相对于整张图像归一化）\nconfidence：这个框中有物体的置信度 × 该框与真实框的 IoU\n每个cell的输出维度=B×5+C\nB×5+C=2×5+20=10+20=30\n 置信度\n公式就是：\nIoU=Intersection AreaUnion Area\\text{IoU} = \\frac{\\text{Intersection Area}}{\\text{Union Area}}\nIoU=Union AreaIntersection Area​\n\nIntersection Area：两个框相交的那一块区域的面积（交集）\nUnion Area：两个框合起来覆盖住的总面积（并集）\n\n\n这里所说的物体真实的box实际是不存在的，这只是模型表达自己框出了物体的自信程度。因此此时置信度的公式为:\nC=Pr(Object)∗IoUpredtruthC=Pr(Object)*IoU_{pred}^{truth}\nC=Pr(Object)∗IoUpredtruth​\n NMS非极大值抑制\n想象一群候选框在参加选秀，规则如下：\n排序：把所有候选框按置信度从高到低排好队。\n选冠军：把得分最高的框拿出来，确定它就是一个检测结果。\n剔除跟风者：\n\n计算剩余所有框与这个“冠军框”的重合度（IOU）。\n如果某个框与冠军框的 IOU 超过了预设的阈值，说明它们在预测同一个物体。\n由于它的得分没冠军高，这个“跟风框”就会被直接踢出局（删除）。\n\n循环：在剩下的框里继续找得分最高的，重复上述过程，直到所有框都被处理完。\n YOLOV1损失函数\n损失函数为：\n\n YOLOV1总结\nYOLO非常快，因为将物体检测定义为回归问题，所以检测也不需要复杂组件。\nYOLO基于全图进行检测，所以不像晃动窗口和预选区技术，YOLO中隐含着隐式编码的上下文信息。\nYOLOv1的准确率不够高，YOLO在定位小物体上表现偏差，可以检测到的目标物体较少。\n以上缺点将会在下面持续改进。\n 参考\n\n【YOLOv1、YOLOv2、YOLOv3目标检测算法原理与实战】https://www.bilibili.com/video/BV1WT421r72w\n\n","categories":["深度学习","YOLO"],"tags":["python","深度学习","CNN","YOLO"]},{"title":"深度学习反演网络与重力异常之初窥门径","url":"/python/Geophysics/hello/","content":" 深度学习反演网络与重力异常之初窥门径\n 正演？\n已知地下体的物理属性，比如密度、磁化率、电阻率和空间分布，预测或计算在地表或者测量点上观测到的地球物理异常。\n已知地下体的ρ(x,y,z)ρ(x, y, z)ρ(x,y,z)密度矩阵、已知体素在空间中的位置，计算g(x,y)g(x, y)g(x,y)表面重力异常，这就是正演。\n正演公式可以理解为：\ng(x,y)=∑i,j,kf(ρi,j,k,ri,j,k)g(x, y) = \\sum_{i, j, k} f(\\rho_{i, j, k}, r_{i, j, k})\ng(x,y)=i,j,k∑​f(ρi,j,k​,ri,j,k​)\ng(x,y)g(x, y)g(x,y)： 这是你在地表（坐标 x,yx, yx,y）观测到的物理场信号，比如重力异常、磁力异常或地震波响应。\n∑i,j,k\\sum_{i, j, k}∑i,j,k​（过程）： 它意味着我们要把地下所有“小方块”（网格单元）产生的贡献全部加起来。\nf(… )f(\\dots)f(…)： 这是单个单元对地表的贡献计算方式。\n\nρi,j,k\\rho_{i, j, k}ρi,j,k​： 该单元的物性参数（如密度、磁化率、速度等）。\nri,j,kr_{i, j, k}ri,j,k​： 该单元距离地表观测点的几何位置关系。\n\n针对重力方面，重力场的基础公式来源于牛顿万有引力定律：对于地表上的一点P(x,y,z=0)P(x,y,z=0)P(x,y,z=0)，受到地下体中一个体素ΔV\\Delta VΔV的密度ρ\\rhoρ 产生的垂直向重力加速度gzg_zgz​为：\nΔgz=G⋅ρ⋅ΔV⋅z′(x′2+y′2+z′2)3/2\\Delta g_z = G \\cdot \\rho \\cdot \\Delta V \\cdot \\frac{z&#x27;}{(x&#x27;^2 + y&#x27;^2 + z&#x27;^2)^{3/2}}\nΔgz​=G⋅ρ⋅ΔV⋅(x′2+y′2+z′2)3/2z′​\nG≈6.674×10−11m3kg−1s−2G\\approx 6.674 \\times 10^{-11} \\text{m}^3\\text{kg}^{-1}\\text{s}^{-2}\nG≈6.674×10−11m3kg−1s−2\n这里只考虑垂直方向的重力加速度，因为重力测量仪器主要测垂直分量。\ngz(x,y)=G∭Vρ(x′,y′,z′)⋅(z′−zobs)[(x−x′)2+(y−y′)2+(z′−zobs)2]3/2dx′dy′dz′g_z(x,y) = G \\iiint_V \\frac{\\rho(x&#x27;,y&#x27;,z&#x27;) \\cdot (z&#x27; - z_{obs})}{[(x - x&#x27;)^2 + (y - y&#x27;)^2 + (z&#x27; - z_{obs})^2]^{3/2}} dx&#x27; dy&#x27; dz&#x27;\ngz​(x,y)=G∭V​[(x−x′)2+(y−y′)2+(z′−zobs​)2]3/2ρ(x′,y′,z′)⋅(z′−zobs​)​dx′dy′dz′\n分子上的 (z′−zobs)(z&#x27; - z_{obs})(z′−zobs​) 实际上代表了方向的投影，它决定了我们测量的到底是“哪个方向”的力。\n当你将地下空间划分为一个个微小的三维网格时，公式演变为在代码中尝试实现的叠加形式：\nΔg(x,y)=G∑i∑j∑kρijk⋅ΔV⋅(zk−zobs)[(xi−x)2+(yj−y)2+(zk−zobs)2]3/2\\Delta g(x, y) = G \\sum_{i} \\sum_{j} \\sum_{k} \\frac{\\rho_{ijk} \\cdot \\Delta V \\cdot (z_k - z_{obs})}{[ (x_i - x)^2 + (y_j - y)^2 + (z_k - z_{obs})^2 ]^{3/2}}\nΔg(x,y)=Gi∑​j∑​k∑​[(xi​−x)2+(yj​−y)2+(zk​−zobs​)2]3/2ρijk​⋅ΔV⋅(zk​−zobs​)​\n 反演\n简单来说，正演是“从因到果”，而反演是“由果溯因”。\n地心是去不了的，地壳是看不透的。 我们手里只有地表测到的那一张重力值地图 gz(x,y)g_z(x, y)gz​(x,y)，而我们真正的目的是想知道地下埋了什么 ρ(ξ,η,ζ)\\rho(\\xi, \\eta, \\zeta)ρ(ξ,η,ζ)。反演的目标就是利用地表观测到的重力值，通过数学计算，倒推出地下的密度、形状和深度。\nρ(x,y,z)=F−1[gobs(x,y)]ρ(x,y,z)=F^{-1}[gobs(x,y)]\nρ(x,y,z)=F−1[gobs(x,y)]\n虽然反演很重要，但它有一个巨大的挑战：多解性。物理上，深处的一个大矿块和浅处的一个小矿块，可能在地表产生一模一样的重力异常。这就是为什么要在反演时需要加入“约束条件”比如已知的一口井的数据，或重力梯度异常 (VzzV_{zz}Vzz​) 等高阶数据，因为梯度数据对浅部细节更敏感，能有效减少反演的模糊性。\n 深度学习反演\n反演法有很多，这里主要展开深度学习反演法。组会中师兄的U-Net反演令我印象深刻，接下来就实现一下最初始版本的吧。\n 模拟数据集\n数据集这方面我还没有了解很多，但是师兄汇报时候使用了模拟集，我也根据这个正演法生成一些模拟集吧。我的格子是Z x H x W是16 x 64 x 64，我还加入了一些噪声。\n下面是我模拟数据集的样子：\n\n\n 模型：U-Net_ResNet50\nimport math\nimport torch.nn as nn\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n    def forward(self, x):\n        identity = x\n        \n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=1000):\n        super(ResNet, self).__init__()\n\n        self.inplanes = 64\n\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        self.avgpool = nn.AvgPool2d(7)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        feat1 = self.relu(x)\n\n        x = self.maxpool(feat1)\n\n        feat2 = self.layer1(x)\n        feat3 = self.layer2(feat2)\n        feat4 = self.layer3(feat3)\n        feat5 = self.layer4(feat4)\n\n        return [feat1, feat2, feat3, feat4, feat5]\n\n\ndef resnet50(**kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    del model.avgpool\n    del model.fc\n    return model\n\nclass unetUp(nn.Module):\n    def __init__(self, in_size, out_size):\n        super(unetUp, self).__init__()\n        self.conv1 = nn.Conv2d(in_size, out_size, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size=3, padding=1)\n        self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, inputs1, inputs2):\n        outputs = torch.cat([inputs1, self.up(inputs2)], 1)\n        outputs = self.conv1(outputs)\n        outputs = self.relu(outputs)\n        outputs = self.conv2(outputs)\n        outputs = self.relu(outputs)\n        return outputs\n\nclass Unet(nn.Module):\n    def __init__(self, num_classes=21):\n        super(Unet, self).__init__()\n\n        self.resnet = resnet50()\n        in_filters = [192, 512, 1024, 3072]\n        out_filters = [64, 128, 256, 512]\n\n        self.up_concat4 = unetUp(in_filters[3], out_filters[3])\n        self.up_concat3 = unetUp(in_filters[2], out_filters[2])\n        self.up_concat2 = unetUp(in_filters[1], out_filters[1])\n        self.up_concat1 = unetUp(in_filters[0], out_filters[0])\n\n        self.up_conv = nn.Sequential(\n            nn.UpsamplingBilinear2d(scale_factor=2),\n            nn.Conv2d(out_filters[0], out_filters[0], kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(out_filters[0], out_filters[0], kernel_size=3, padding=1),\n            nn.ReLU(),\n        )\n        self.final = nn.Conv2d(out_filters[0], num_classes, 1)\n\n    def forward(self, inputs):\n        [feat1, feat2, feat3, feat4, feat5] = self.resnet.forward(inputs)\n\n        up4 = self.up_concat4(feat4, feat5)\n        up3 = self.up_concat3(feat3, up4)\n        up2 = self.up_concat2(feat2, up3)\n        up1 = self.up_concat1(feat1, up2)\n\n        if self.up_conv != None:\n            up1 = self.up_conv(up1)\n\n        final = self.final(up1)\n        return final\n\n\nTotal params: 43,927,504\nTrainable params: 43,927,504\nNon-trainable params: 0\nInput size (MB): 0.02\nForward/backward pass size (MB): 40.58\nParams size (MB): 167.57\nEstimated Total Size (MB): 208.16\n\n 随便练20轮\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\nmodel = Unet(num_classes=16).to(device)\n\nepoch = 20\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nmin_loss = np.inf\nfor e in tqdm(range(epoch)):\n    for X, Y in train_loder:\n        X = X.to(device)\n        Y = Y.to(device)\n        optimizer.zero_grad()\n        pred = model(X)\n        loss = criterion(pred, Y)\n        loss.backward()\n        optimizer.step()\n    print(f&quot;Epoch &#123;e + 1&#125;/&#123;epoch&#125;, Loss: &#123;loss.item():.4f&#125;&quot;)\n    if loss &lt; min_loss:\n        min_loss = loss.item()\n        torch.save(model.state_dict(), f&quot;model_epoch_&#123;e + 1&#125;.pth&quot;)\n\n\n 预测一下\n一个异常体的效果还不错。\n\n多个异常体的效果也还行，但是如果离得太近就会出现连体现象。\n\n\n当然这个确实是源数据就在一起了，但是有微小分开的数据也是连在一起的。\n 总结\n用深度学习做重力反演，从 地表重力异常 g(x, y)， 预测地下三维密度分布 ρ(x, y, z)，反演是病态的，而正演是确定的，深度学习适合“约束型反演”，深度学习在这里学一个稳定的近似逆算子，而真实的逆是不存在的，深度学习正把这种不可能变成有些可能。\n","categories":["深度学习","Geophysics"],"tags":["python","深度学习","CNN","UNet"]},{"title":"YOLO-V2","url":"/python/YOLO/YOLO_V2/","content":" YOLOv2\n 原文\nYOLO9000: Better, Faster, Stronger(YOLOv2): [https://arxiv.org/abs/1612.08242]\n\nYOLO-v2相较于v1，不仅准确率高，而且检测速度更快，mAP指标由63.4%提升到78.6%。\n V2 的改进\n\nBatch Normalization-批归一化 ：是深度学习中一种常用的技术，用于加速神经网络的训练并提高模型的稳定性。它通过对每一层的输入进行标准化处理来解决“内部协变量偏移”问题。\nHi-res classifier：Yolov2相较于Yolovl采用更高分辨率的网络进行分类主干网络的训练。\nConvolutional+anchor：Yolov2相较于Yolovl去除全连接层，采用卷积层进行模型的输出;同时采用与锚框(预选框)进行boundingbox的预测\nnewnetwork：采用新的网络架构Darknet-19\ndimension priors：采用k-means聚类方法对训练集中的标准框做了聚类分析，获取anchor boxes;\nlocation prediction：使用sigmoid函数处理位置预测值。\npassthrough：passthrough网络模型的连接方式(类似resnet)\nmulti-scale：多尺度输入数据训练模型\nhi-res detector：Yolov2相较于Yolovl采用更高分辨率的网络进行检测主干网络的训练\n anchor box\n在 YOLO v2 中，**Anchor 框（锚框/先验框）**是这个算法的一项重大改进。如果说“中心点”决定了物体在哪个位置，那么 Anchor 框就决定了物体的“体型”和“长相”。\n在一开始，这 5 个 Anchor 框就长得不一样。它们不是随意生成的，而是根据数据统计出来的最典型的 5 种形状。\n当图片输入模型时，每一个网格里的这 5 个 Anchor 都会去和真实物体比对：\n\n计算重合度 IoU： 模型会计算真实物体的形状和这 5 个模板哪个最接近。\n分配任务： 重合度IoU最高的那个 Anchor 被选为“天选之子”，只有它负责预测这个物体。其他的 4 个 Anchor 在这个格子里就被当作“背景”忽略掉。\n\n一旦选定了最像的那个框，它只针对选中的这一个进行修正。\n虽然我们说是“挑一个”，但在推理 / 预测阶段，其实 5 个框都会给出自己的分数（置信度）。只有那些得分超过你设定阈值的框才会被显示出来。如果一个格子里确实有两个重叠的物体，比如人挡住了车，那么负责人的 Anchor 和负责车的 Anchor 都会给出高分，这样两个物体就都能被挑出来！\n 流程描述\nYOLOv2 把一张图分成 S × S 个 grid，每个 grid 预测 B 个候选框（anchor）。\n每个候选框输出：\n(bx,by,bw,bh,confidence,p1,p2,…,pC)(b_x, b_y, b_w, b_h, \\text{confidence}, p_1, p_2, \\dots, p_C)\n(bx​,by​,bw​,bh​,confidence,p1​,p2​,…,pC​)\n其中：\n\nconfidence = Pr(object) × IoU(pred, gt)\np_c = Pr(class_c | object)\n\n真正用于排序和筛选的是：\nscorec=confidence×pc\\text{score}_{c} = \\text{confidence} \\times p_c\nscorec​=confidence×pc​\n对同一类别来说，score 最大的那个框\nscore=Pr(object)×IoU×Pr(class)score=Pr(object)×IoU×Pr(class)\nscore=Pr(object)×IoU×Pr(class)\n\n\n\n因素\n含义\n\n\n\n\nPr(object)\n有没有目标\n\n\nIoU\n框得准不准\n\n\nPr(class)\n是不是这个类别\n\n\n\n DarkNet-19 分类模型\n\n DarkNet-19 检测模型（非完整版）\n\n 凭什么13x13可以代表那些格子？\n不是 YOLO 把图片“切成了 13×13”，而是卷积网络在经过多次 stride=2 下采样后，只剩下 13×13 个空间位置可以说话。YOLO 的 grid 结构并非人为划分，而是全卷积网络在多次下采样后自然形成的空间离散化结果。\n 应当有5个锚框是如何得出的？\nYOLOv2使用K-means 聚类来统计，K-means 聚类实际上是通过计算训练数据中每个目标框的 宽度 和 高度 来进行的，并且这些框的 中心点 被视为一个关键的参照来进行聚类。\n每个训练样本的目标框通常由其 宽度 和 高度 组成。YOLOv2 并不会直接使用每个框的左上角坐标，而是将框的 宽度 和 高度 作为输入特征。\n聚类的目的是找到一组能够很好地覆盖这些框的 中心点 和 尺寸。\nK-means 聚类在选择锚框时使用的是 IoU，即交并比，来度量物体框与锚框之间的相似度。\nIoU=Area_of_overlapArea_of_unionAreaIoU=\\frac{Area\\_of\\_overlap}{Area\\_of\\_unionArea}\nIoU=Area_of_unionAreaArea_of_overlap​\n\n对于每个训练集中的物体框，YOLOv2 会计算它与每个候选锚框的 IoU。物体框和锚框之间的 IoU 值越大，表示它们越相似。\n聚类之后得到的 5 个框它们 不是拿来直接用的预测结果 而是用来 约束网络“怎么预测框”，让回归问题变得容易、稳定、可学\nK-means聚类后，得到的是：\n(w1,h1),(w2,h2),…,(w5,h5){(w_1,h_1),(w_2,h_2),…,(w_5,h_5)}\n(w1​,h1​),(w2​,h2​),…,(w5​,h5​)\n这 5 组 宽高比例只和 形状、尺度 有关，不含位置，这通常是相对于输入图像的比例\n在 YOLO 模型中，网络会为每个目标预测四个偏移量：tx,ty,tw,tht_x, t_y, t_w, t_htx​,ty​,tw​,th​。通过以下公式，我们可以得到最终的边界框坐标：\nbx=σ(tx)+cxb_x = \\sigma(t_x) + c_x\nbx​=σ(tx​)+cx​\nby=σ(ty)+cyb_y = \\sigma(t_y) + c_y\nby​=σ(ty​)+cy​\nbw=pw⋅etwb_w = p_w \\cdot e^{t_w}\nbw​=pw​⋅etw​\nbh=ph⋅ethb_h = p_h \\cdot e^{t_h}\nbh​=ph​⋅eth​\ncx,cyc_x, c_ycx​,cy​：当前网格左上角的坐标（通常以网格大小为单位）。\npw,php_w, p_hpw​,ph​：锚框的宽度和高度。这是预设好的参考基准，在公式中，pwp_wpw​ 和 php_hph​ 就是直接从锚框里拿出来的。后面乘个e的指数倍。\n YOLOv2 边界框预测 + 损失计算流程\n 跑完一遍输出\n网络最后一层输出的张量：\n[S,S,B×(5+C)][S, S, B \\times (5 + C)]\n[S,S,B×(5+C)]\n\nS×S = 网格数（比如 13×13）\nB = anchor 数，在YOLOv2里是5\n5 = [t_x, t_y, t_w, t_h, t_&#123;obj&#125;]\nC = 类别数\n\n\ntx,ty,tw,tht_x, t_y, t_w, t_htx​,ty​,tw​,th​都是网络预测的偏移量（实数），还不是框的真实坐标。\n\n 解码为真实框\n使用公式：\nbx=σ(tx)+cxb_x = \\sigma(t_x) + c_x\nbx​=σ(tx​)+cx​\nby=σ(ty)+cyb_y = \\sigma(t_y) + c_y\nby​=σ(ty​)+cy​\nbw=pw⋅etwb_w = p_w \\cdot e^{t_w}\nbw​=pw​⋅etw​\nbh=ph⋅ethb_h = p_h \\cdot e^{t_h}\nbh​=ph​⋅eth​\n输出bx,by,bw,bhb_x, b_y, b_w, b_hbx​,by​,bw​,bh​，这是相对整个输入图像真实框坐标，\n 匹配 ground-truth 框\n训练时：\n\n对每个 GT 框：\n\n找到 IoU 最大的 anchor → 这个 anchor 来预测它\n其它 anchor 忽略或当负样本\n\n\n计算预测框bx,by,bw,bhb_x, b_y, b_w, b_hbx​,by​,bw​,bh​与 GT 框的误差\n\n 损失计算（v2对比v1）\nYOLOv1损失 = 边界框回归 + 置信度 + 分类损失\nLoss=coord_loss+obj_loss+noobj_loss+class_lossLoss=coord\\_loss+obj\\_loss+noobj\\_loss+class\\_loss\nLoss=coord_loss+obj_loss+noobj_loss+class_loss\ncoord_losscoord\\_losscoord_loss坐标偏移损失，这里直接在偏移量空间计算误差，而不是在转换后的像素空间\nobj_lossobj\\_lossobj_loss有物体置信度损失，如果这里有物体，模型预测的置信度应该接近它与真实目标的重合度。\nnoobj_lossnoobj\\_lossnoobj_loss无物体置信度损失。\nclass_lossclass\\_lossclass_loss分类损失，YOLO v2 在分类上也使用了均方误差，而后续版本改成了交叉熵。\n有物体置信度损失和无物体置信度损失，像是模型里的“安检员”和“纠错员”，它们共同解决目标：如何从成千上万个候选框中，把真正包含物体的那个找出来，并过滤掉背景。\n在一张图中，背景框的数量远多于物体框。如果两者的权重一样，模型会发现，把所有框都预测成背景是最省力的做法，但其实这样并没有达到我们的预期。\n因此，我们通常设置 λnoobj\\lambda_{noobj}λnoobj​ 较小例如 0.5，而 λobj\\lambda_{obj}λobj​ 较大。这样即使背景框很多，它们产生的总梯度也不会完全淹没掉少数物体框带来的信号。\n我们可以看到，有好多λ\\lambdaλ ，总损失 LossLossLoss 是所有子损失的加权和，如果你发现模型框画得不准，就调大 λcoord\\lambda_{coord}λcoord​，模型通过误差反向传播，会告诉自己：下次把框往左挪点，胆子大一点，提高 objobjobj 分数，并且多看看猫的特征！\nYOLOv2损失 = 背景置信度损失 + 预热期 Anchor 匹配损失 + 有物体坐标损失+有物体置信度损失+分类损失\nLossv2=NoObjectLoss+PriorLoss+CoordinateLoss+ObjectLoss+ClassLossLoss_{v2}=NoObjectLoss+PriorLoss+CoordinateLoss+ObjectLoss+ClassLoss\nLossv2​=NoObjectLoss+PriorLoss+CoordinateLoss+ObjectLoss+ClassLoss\nLoss=∑i=0W∑j=0H∑k=0A1Max IoU&lt;Thresh⋅λnoobj⋅(0−bijko)2+1t&lt;12800⋅λprior⋅∑r∈{x,y,w,h}(priorkr−bijkr)2+1ktruth[λcoord⋅∑r∈{x,y,w,h}(truthr−bijkr)2+λobj⋅(IOUtruthk−bijko)2+λclass⋅∑c=1C(truthc−bijkc)2]\\begin{aligned}\nLoss = &amp; \\sum_{i=0}^{W} \\sum_{j=0}^{H} \\sum_{k=0}^{A} \\mathbb{1}_{\\text{Max IoU} &lt; \\text{Thresh}} \\cdot \\lambda_{noobj} \\cdot (0 - b_{ijk}^o)^2 \\\\\n&amp; + \\mathbb{1}_{t &lt; 12800} \\cdot \\lambda_{prior} \\cdot \\sum_{r \\in \\{x,y,w,h\\}} (prior_k^r - b_{ijk}^r)^2 \\\\\n&amp; + \\mathbb{1}_k^{truth} \\left[ \\lambda_{coord} \\cdot \\sum_{r \\in \\{x,y,w,h\\}} (truth^r - b_{ijk}^r)^2 \\right. \\\\\n&amp; + \\lambda_{obj} \\cdot (IOU_{truth}^k - b_{ijk}^o)^2 \\\\\n&amp; + \\left. \\lambda_{class} \\cdot \\sum_{c=1}^{C} (truth^c - b_{ijk}^c)^2 \\right]\n\\end{aligned}\nLoss=​i=0∑W​j=0∑H​k=0∑A​1Max IoU&lt;Thresh​⋅λnoobj​⋅(0−bijko​)2+1t&lt;12800​⋅λprior​⋅r∈{x,y,w,h}∑​(priorkr​−bijkr​)2+1ktruth​⎣⎢⎡​λcoord​⋅r∈{x,y,w,h}∑​(truthr−bijkr​)2+λobj​⋅(IOUtruthk​−bijko​)2+λclass​⋅c=1∑C​(truthc−bijkc​)2]​\n\n 背景置信度损失 No-Object Loss\n\n公式：λnoobj⋅(0−bijko)2\\lambda_{noobj} \\cdot (0 - b_{ijk}^o)^2λnoobj​⋅(0−bijko​)2，那么长一串核心就是这个，前面的都是一些条件。\n含义：如果预测框与所有真实框的 IoU 都小于阈值（通常为 0.6），它就被判定为背景。\n目标：强制该框的置信度 bob^obo 趋向 0，所以采用0减去它自己。\n权重：λnoobj=1\\lambda_{noobj} = 1λnoobj​=1。\n\n 预热期 Anchor 匹配损失 Prior Loss\n\n公式：1t&lt;12800⋅λprior…\\mathbb{1}_{t &lt; 12800} \\cdot \\lambda_{prior} \\dots1t&lt;12800​⋅λprior​…\n含义：这是 YOLOv2 的特殊设计。在训练的前 12800 步，让预测框去拟合 Anchor Box 的原始形状。\n作用：防止训练初期预测框乱跳，起到稳定模型的作用。过了这个阶段，这一项就消失了。\n\n 有物体坐标损失 Coordinate Loss\n\n公式：λcoord⋅∑(truth−b)2\\lambda_{coord} \\cdot \\sum (truth - b)^2λcoord​⋅∑(truth−b)2\n注意：这里的计算是在 ttt 空间（偏移量空间）进行的，即模型直接输出的数值与转化后的标签进行对比。\n权重：λcoord=1\\lambda_{coord} = 1λcoord​=1。\n\n 有物体置信度损失 Object Loss\n\n公式：λobj⋅(IOUtruthk−bijko)2\\lambda_{obj} \\cdot (IOU_{truth}^k - b_{ijk}^o)^2λobj​⋅(IOUtruthk​−bijko​)2\n核心细节：\n\n标签不是 1：这里使用的是预测框与真实框的 实际 IoU 作为学习目标。\n高权重：λobj=5\\lambda_{obj} = 5λobj​=5。这反映了 YOLOv2 非常重视对真实目标的提取。\n\n\n\n 分类损失 Class Loss\n\n公式：λclass⋅∑(truthc−bijkc)2\\lambda_{class} \\cdot \\sum (truth^c - b_{ijk}^c)^2λclass​⋅∑(truthc−bijkc​)2\n计算方式：YOLOv2 依然使用了 MSE 均方误差，而不是后来版本常用的交叉熵。\n\n 总结\n\n\n\n损失项\n权重变量\n推荐值\n作用\n\n\n\n\n背景置信度\nλnoobj\\lambda_{noobj}λnoobj​\n1\n抑制误报\n\n\n物体置信度\nλobj\\lambda_{obj}λobj​\n5\n提高召回率\n\n\n坐标回归\nλcoord\\lambda_{coord}λcoord​\n1\n精准定位\n\n\n分类概率\nλclass\\lambda_{class}λclass​\n1\n类别判定\n\n\nAnchor 预热\nλprior\\lambda_{prior}λprior​\n0.01\n初期稳定训练\n\n\n\n passthrough\n在 YOLOv2 架构中，Passthrough Layer 直通层 / 重组层是一个非常精妙的设计。它的核心目的是为了更好地检测微小物体。\n\n DarkNet-19 检测模型（完整版）\n\n 多尺度训练\n多尺度训练是 YOLOv2 提出的一种非常聪明的训练技巧。它的核心思想是让模型在训练过程中不断看到不同分辨率的图片，从而增强模型对不同尺寸物体的鲁棒性。\n模型不再依赖于物体的“绝对像素大小”，而是学习物体的“相对形状特征”。\n 参考\n\n【YOLOv1、YOLOv2、YOLOv3目标检测算法原理与实战】https://www.bilibili.com/video/BV1WT421r72w\n\n","categories":["深度学习","YOLO"],"tags":["python","深度学习","CNN","YOLO"]},{"title":"YOLO-V3","url":"/python/YOLO/YOLO_V3/","content":" YOLOv3\n 原文\nYOLOv3: An Incremental Improvement(YOLOv3): [https://arxiv.org/abs/1804.02767]\n 网络结构\n\n在检测头，他这个数值标错了。\n\n\n他这个用步幅为2的卷积带代替池化操作了，可以看到他又把全连接拿回来了。\n\n 图像金字塔FPN\n\n特征金字塔网络FPN，这是它相比 YOLOv2 最重大的改进之一。这一设计极大地提升了模型对多尺度目标尤其是小目标的检测能力。\nYOLOv3 在主干网络 Darknet-53 的基础上，提取了三个不同跨度的特征图进行预测：\n\n13×1313 \\times 1313×13 输出层： 对应深层特征，感受野最大，负责检测大尺寸物体。\n26×2626 \\times 2626×26 输出层： 对应中层特征，负责检测中等尺寸物体。\n52×5252 \\times 5252×52 输出层： 对应浅层特征，感受野较小，负责检测小尺寸物体。\n\n为了让浅层也拥有深层的语义信息，YOLOv3 执行了以下操作：\n\n上采样： 将 13×1313 \\times 1313×13 的特征图通过上采样放大 2 倍，变为 26×2626 \\times 2626×26。\n拼接： 将上采样后的特征图与 Darknet-53 中对应的 26×2626 \\times 2626×26 原始特征图进行通道拼接，原始 FPN 是逐元素相加，而 YOLOv3 是沿通道维度的 Concat，这点非常重要。\n循环操作： 同样的逻辑也应用在 26×2626 \\times 2626×26 到 52×5252 \\times 5252×52 的过程中。\n\n在深度学习的特征融合中，主要有两种方式：Add逐元素相加 和 Concat通道拼接。YOLOv3 的 FPN 结构明确使用了 Concat。\n上采样后，对于原始格子来说相当于变多了，这意味这感受野下降，但更聚焦于小目标。\n 为什么要用Concat而不是Add？\nConcat 不会改变原始特征图的数值，它只是把深层的“语义信息”和浅层的“位置信息”放在一起，交给后面的卷积层去自动学习如何加权利用。通过增加通道数，网络可以同时看到来自不同尺度的特征表达。\n 好像U-Net？\nYOLOv3 的 FPN 构建了一个类似的流程：\n\n左侧下采样 / 编码器：在 YOLOv3 中是 Darknet-53 主干网络，负责提取特征。\n右侧上采样 / 解码器：YOLOv3 通过上采样不断恢复分辨率。\n中间使用跳跃连接：两者都将左侧的特征图直接传递到右侧，与上采样后的特征进行 Concat。\n\n Concat\n在 YOLO 里，这个 Concat 本质上就是堆叠通道\n上采样后的高层特征: (B, 256, 26, 26)\n浅层特征:           (B, 256, 26, 26)\n----------------------------------\nConcat(dim=1)\n↓\n输出特征:           (B, 512, 26, 26)\n\n\n 检测头\n \n如上图所示，YOLOv3 一共有 9 个 Anchor，但在每一个预测层上，它只使用其中的 3 个。\n\n\n\n预测层尺度\n负责目标\n分配的 Anchor 数量\n说明\n\n\n\n\n13×1313 \\times 1313×13\n大物体\n3 个\n感受野大，适合抓大个子。\n\n\n26×2626 \\times 2626×26\n中物体\n3 个\n兼顾语义和细节。\n\n\n52×5252 \\times 5252×52\n小物体\n3 个\n分辨率高，专门抓细小目标。\n\n\n\n如果只说聚类出来的，为什么不聚类成 6 个或 12 个？我想这是经验科学与计算效率妥协的结果。\nYOLOv3 的特征金字塔有 3 个输出层。为了保证模型的对称性和代码实现的简洁性，作者给每个层分配了相同数量的 Anchor。如果每层只给 1 个 Anchor，覆盖的形状太少，如果每层给 5 个，总共 15 个，模型最后一层的通道数会变得非常臃肿，推理速度大幅下降。也可能是超过 9 个以后，准确率的提升已经微乎其微，但计算量却在持续线性增加，这是覆盖率的边际效益。\n 重提x,y,w,h,c\n\n\n\n字母\n全称\n含义\n\n\n\n\nx,yx, yx,y\nCoordinates\n预测框的中心点坐标相对于当前格子的偏移量。\n\n\nw,hw, hw,h\nWidth / Height\n预测框的宽度和高度相对于 Anchor 尺寸的缩放比例。\n\n\nccc\nConfidence\n置信度，代表这个框里“有物体”的概率以及框得“准不准”。\n\n\n\n 损失函数\nLoss=Losscoordinate+Lossconfidence+LossclassificationLoss = Loss_{coordinate} + Loss_{confidence} + Loss_{classification}\nLoss=Losscoordinate​+Lossconfidence​+Lossclassification​\nCoordinate Loss坐标损失 ：这部分负责让框画得更准。它只针对正样本计算。统一使用 BCE Loss。\nConfidence Loss置信度损失：这部分负责判断“有没有物体”。它是训练中最关键的部分，因为它要处理严重的正负样本不平衡。统一使用 BCE Loss。\nClassification Loss：这部分负责判断“是什么物体”。统一使用 BCE Loss。\nL=λcoord∑i=0G2∑j=0B1ijobj[BCE(x,x^)+BCE(y,y^)+MSE(w,w^)+MSE(h,h^)]+∑i=0G2∑j=0B1ijobj[BCE(c,1)]+λnoobj∑i=0G2∑j=0B1ijnoobj[BCE(c,0)]+∑i=0G2∑j=0B1ijobj∑k∈classesBCE(pk,p^k)\\begin{aligned}\nL = \\lambda_{coord} \\sum_{i=0}^{G^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} \\left[ BCE(x, \\hat{x}) + BCE(y, \\hat{y}) + MSE(w, \\hat{w}) + MSE(h, \\hat{h}) \\right] \\\\\n+ \\sum_{i=0}^{G^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} \\left[ BCE(c, 1) \\right] + \\lambda_{noobj} \\sum_{i=0}^{G^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{noobj} \\left[ BCE(c, 0) \\right] \\\\\n+ \\sum_{i=0}^{G^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} \\sum_{k \\in classes} BCE(p_k, \\hat{p}_k)\n\\end{aligned}\nL=λcoord​i=0∑G2​j=0∑B​1ijobj​[BCE(x,x^)+BCE(y,y^​)+MSE(w,w^)+MSE(h,h^)]+i=0∑G2​j=0∑B​1ijobj​[BCE(c,1)]+λnoobj​i=0∑G2​j=0∑B​1ijnoobj​[BCE(c,0)]+i=0∑G2​j=0∑B​1ijobj​k∈classes∑​BCE(pk​,p^​k​)​\n置信度损失分为两部分：有物体的网格（正样本） 和 没有物体的网格（负样本）。\n二值交叉熵BCE公式：\nBCE(y,y^)=−[ylog⁡(y^)+(1−y)log⁡(1−y^)]BCE(y, \\hat{y}) = - [y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]\nBCE(y,y^​)=−[ylog(y^​)+(1−y)log(1−y^​)]\n最终的展开：\nLossconf=∑i=0G2∑j=0B1ijobj[BCE(c,1)]+λnoobj∑i=0G2∑j=0B1ijnoobj[BCE(c,0)]=−∑i=0G2∑j=0B1ijobj[c^ijlog⁡(cij)+(1−c^ij)log⁡(1−cij)]−λnoobj∑i=0G2∑j=0B1ijnoobj[c^ijlog⁡(cij)+(1−c^ij)log⁡(1−cij)]\\begin{aligned} Loss_{conf} =\\sum_{i=0}^{G^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} \\left[ BCE(c, 1) \\right] + \\lambda_{noobj} \\sum_{i=0}^{G^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{noobj} \\left[ BCE(c, 0) \\right]\\\\\n=-\\sum_{i=0}^{G^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} \\left[ \\hat{c}_{ij} \\log(c_{ij}) + (1 - \\hat{c}_{ij}) \\log(1 - c_{ij}) \\right] \\\\ - \\lambda_{noobj} \\sum_{i=0}^{G^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{noobj} \\left[ \\hat{c}_{ij} \\log(c_{ij}) + (1 - \\hat{c}_{ij}) \\log(1 - c_{ij}) \\right] \\end{aligned}\nLossconf​=i=0∑G2​j=0∑B​1ijobj​[BCE(c,1)]+λnoobj​i=0∑G2​j=0∑B​1ijnoobj​[BCE(c,0)]=−i=0∑G2​j=0∑B​1ijobj​[c^ij​log(cij​)+(1−c^ij​)log(1−cij​)]−λnoobj​i=0∑G2​j=0∑B​1ijnoobj​[c^ij​log(cij​)+(1−c^ij​)log(1−cij​)]​\n1ijobj\\mathbb{1}_{ij}^{obj}1ijobj​：这是一个“开关”。只有当第 iii 个网格的第 jjj 个 Anchor 被分配为正样本时，第一行公式才生效。\n1ijnoobj\\mathbb{1}_{ij}^{noobj}1ijnoobj​：只有当 Anchor 是负样本时，第二行公式才生效。\ncijc_{ij}cij​：模型预测出的置信度值，经过 Sigmoid 激活，在 0 到 1 之间。\nc^ij\\hat{c}_{ij}c^ij​​：真实值（Label）对于正样本，c^ij=1\\hat{c}_{ij} = 1c^ij​=1，对于负样本，c^ij=0\\hat{c}_{ij} = 0c^ij​=0。\nλnoobj\\lambda_{noobj}λnoobj​：权重惩罚系数（通常设为 0.5）。\n 我要再解释一遍，这是求什么？\n∑i=0G2∑j=0B\\sum_{i=0}^{G^2} \\sum_{j=0}^{B}\ni=0∑G2​j=0∑B​\n第一个$ \\sum_{i=0}{G2}$：遍历所有网格，第二个 ∑j=0B\\sum_{j=0}^{B}∑j=0B​：遍历每个网格里的 Anchor\nG2G^2G2：指的是特征图上的网格总数，比如在 13×1313 \\times 1313×13 的尺度下，这个求和符号就会从第 1 个方格一直数到第 169 个方格。\n在 YOLOv3 中，B=3B = 3B=3，每个网格里有 3 个不同形状的 Anchor。这个求和符号就是要在每一个格子里，把这 3 个 Anchor 挨个拎出来算一遍 Loss。\n这个总损失告诉模型：“在这一张图中，你一共犯了多少错？”\n某个格子的 Anchor 1 没对准，产生定位损失。\n某个格子的 Anchor 2 把背景看成了车，产生置信度损失。\n所有的错误加在一起，模型再根据这个总分进行反向传播，去调整全图的权重。\n 为什么 YOLOv3 用 BCE 而不是多分类交叉熵？\n这是 YOLOv3 的一个重要进步：\n\n多标签分类：YOLOv3 认为一个物体可以同时属于多个类，比如它是“卡车”也是“汽车”。\nSoftmax 的缺陷：Softmax 会强迫模型只能选出一个最高分，这在处理重叠标签时效果不好。\nBCE 的优势：YOLOv3 对每个类别都独立运行一个 Sigmoid + BCE。这意味着每个类别都在做一个“是或不是该类”的二元判断，从而支持了多标签分类。注意它是对每一个标签独立做的，这一点非常重要。\n\n\n在实际写 YOLOv3 代码时，通常不会直接手写这个 log⁡\\loglog 公式，而是调用：torch.nn.BCEWithLogitsLoss()\n 这个函数名里为什么多了一个 “WithLogits”？\n\nLogits 是指卷积层输出的原始数值。\n这个函数会自动在内部帮你做一遍 Sigmoid，然后再算 BCE。\n好处：由于 log⁡\\loglog 函数在自变量接近 0 时极其不稳定，官方这种合并写法在数学上做了优化，比你自己先写 Sigmoid 再写 BCE 要更数值稳定。\n\n 强大的类别损失，但是谁在贡献损失？\n∑i=0G2∑j=0B1ijobj∑k∈classesBCE(pk,p^k)\\sum_{i=0}^{G^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} \\sum_{k \\in classes} BCE(p_k, \\hat{p}_k)\ni=0∑G2​j=0∑B​1ijobj​k∈classes∑​BCE(pk​,p^​k​)\n就像我们之前讨论的前两个求和符号负责遍历全图所有的网格和所有的 Anchor。\n指示函数 1ijobj\\mathbb{1}_{ij}^{obj}1ijobj​：这是最关键的过滤器，只有当这个框是正样本即负责预测某个真实物体的那个框时，后面的类别损失才会被计算，负样本或者说是背景不计算类别损失。\n核心公式：\n∑k∈classesBCE(pk,p^k)\\sum_{k \\in classes} BCE(p_k, \\hat{p}_k)\nk∈classes∑​BCE(pk​,p^​k​)\n这部分是 YOLOv3 相比 YOLOv2 的重大改进，他对每一个类别进行计算，\n总结来说就是要找出所有的正样本，对它们预测的每一个类别都进行一次对错检查，然后把这些检查结果全部累加。\n 通俗理解这个过程\n有一个正样本 Anchor：\n\n它对应的真实物体是一只“狗”。\n类别标签（p^\\hat{p}p^​）就是：[0, 0, 1, 0, ... 0]假设第 3 位是狗，假设一共有80个分类。\n模型预测（ppp）可能是：[0.1, 0.1, 0.8, 0.2, ... 0.05]。\n计算 Loss 时：\n\n第 3 位算一次 BCE(0.8,1)BCE(0.8, 1)BCE(0.8,1) —— “让你像狗，你做得还不够好，扣分。”\n其余 79 位分别算 BCE(pk,0)BCE(p_k, 0)BCE(pk​,0) —— “你虽然不是猫，但给了 0.1 的概率，也要扣一点分。”\n\n\n把这 80 个分数值全部加起来，就是这个 Anchor 的类别总损失。\n\n 视频中的损失函数\nloss=λcoord∑i=0S2∑j=0B1i,jobj⋅[(bx−b^x)2+(by−b^y)2+(bw−b^w)2+(bh−b^h)2](2−wi×hi)+∑i=0S2∑j=0B1i,jobj⋅[−log⁡(pc)+∑i=1nBCE(c^i,ci)]+λnoobj∑i=0S2∑j=0B1i,jnoobj⋅[−log⁡(1−pc)]\\begin{aligned}\nloss = &amp; \\lambda_{\\text{coord}} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{i,j}^{obj} \\cdot \\left[ (b_x - \\hat{b}_x)^2 + (b_y - \\hat{b}_y)^2 + (b_w - \\hat{b}_w)^2 + (b_h - \\hat{b}_h)^2 \\right] (2 - w_i \\times h_i) \\\\\n&amp; + \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{i,j}^{obj} \\cdot \\left[ -\\log(p_c) + \\sum_{i=1}^{n} BCE(\\hat{c}_i, c_i) \\right] \\\\\n&amp; + \\lambda_{\\text{noobj}} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{i,j}^{\\text{noobj}} \\cdot \\left[ -\\log(1 - p_c) \\right]\n\\end{aligned}\nloss=​λcoord​i=0∑S2​j=0∑B​1i,jobj​⋅[(bx​−b^x​)2+(by​−b^y​)2+(bw​−b^w​)2+(bh​−b^h​)2](2−wi​×hi​)+i=0∑S2​j=0∑B​1i,jobj​⋅[−log(pc​)+i=1∑n​BCE(c^i​,ci​)]+λnoobj​i=0∑S2​j=0∑B​1i,jnoobj​⋅[−log(1−pc​)]​\n 位置损失\nλcoord∑i=0S2∑j=0B1i,jobj⋅[(bx−b^x)2+(by−b^y)2+(bw−b^w)2+(bh−b^h)2](2−wi×hi)\\lambda_{\\text{coord}} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{i,j}^{obj} \\cdot \\left[ (b_x - \\hat{b}_x)^2 + (b_y - \\hat{b}_y)^2 + (b_w - \\hat{b}_w)^2 + (b_h - \\hat{b}_h)^2 \\right] (2 - w_i \\times h_i) \nλcoord​i=0∑S2​j=0∑B​1i,jobj​⋅[(bx​−b^x​)2+(by​−b^y​)2+(bw​−b^w​)2+(bh​−b^h​)2](2−wi​×hi​)\n这一行负责让预测框画得更准。\n\n(bx,by,bw,bh)(b_x, b_y, b_w, b_h)(bx​,by​,bw​,bh​)：模型预测的四个坐标值。\n(b^x,b^y,b^w,b^h)(\\hat{b}_x, \\hat{b}_y, \\hat{b}_w, \\hat{b}_h)(b^x​,b^y​,b^w​,b^h​)：真实目标的坐标值。\n(2−wi×hi)(2 - w_i \\times h_i)(2−wi​×hi​)：小目标增强权重。物体面积越小，这个值越大，从而补偿大框和小框在 MSE 损失上的不平衡。\nλcoord\\lambda_{\\text{coord}}λcoord​：坐标损失权重系数，用来提高定位的优先级。\n\n 正样本置信度与类别损失\n∑i=0S2∑j=0B1i,jobj⋅[−log⁡(pc)+∑i=1nBCE(c^i,ci)]\\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{i,j}^{obj} \\cdot \\left[ -\\log(p_c) + \\sum_{i=1}^{n} BCE(\\hat{c}_i, c_i) \\right]\ni=0∑S2​j=0∑B​1i,jobj​⋅[−log(pc​)+i=1∑n​BCE(c^i​,ci​)]\n这一行只针对正样本即包含物体的框的计算。\n\n−log⁡(pc)-\\log(p_c)−log(pc​)：物体存在的置信度损失。当预测概率 pcp_cpc​ 越接近 1，损失越小。\n∑i=1nBCE(c^i,ci)\\sum_{i=1}^{n} BCE(\\hat{c}_i, c_i)∑i=1n​BCE(c^i​,ci​)：分类损失。对 nnn 个类别中的每一个都进行二值交叉熵计算，支持多标签分类。\n\n 负样本置信度损失\nλnoobj∑i=0S2∑j=0B1i,jnoobj⋅[−log⁡(1−pc)]\\lambda_{\\text{noobj}} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{i,j}^{\\text{noobj}} \\cdot \\left[ -\\log(1 - p_c) \\right]\nλnoobj​i=0∑S2​j=0∑B​1i,jnoobj​⋅[−log(1−pc​)]\n这一行只针对负样本即背景计算。\n\n−log⁡(1−pc)-\\log(1 - p_c)−log(1−pc​)：背景的置信度损失。当预测概率 pcp_cpc​ 越接近 0，损失越小。\nλnoobj\\lambda_{\\text{noobj}}λnoobj​：背景惩罚系数。因为背景框远多于物体框，所以通常会设置一个较小的值来防止模型被背景淹没。\n\n 回望\nYOLOv3 是目标检测领域的经典之作，它在 YOLOv2 的基础上进行了大幅度改进，舍弃了之前的 Darknet-19，采用了拥有 53 层卷积层的 Darknet-53。它引入了大量的残差结构，有效解决了深层网络的梯度消失问题，使模型能够学到更复杂的特征。受 FPN 启发，YOLOv3 分别在 13×1313 \\times 1313×13、26×2626 \\times 2626×26 和 52×5252 \\times 5252×52 三个尺度进行预测。分类方式放弃了 Softmax，改用多个独立的 Logistic 分类器即BCE 损失，这使得 YOLOv3 能够支持多标签分类。\n 参考\n\n【YOLOv1、YOLOv2、YOLOv3目标检测算法原理与实战】https://www.bilibili.com/video/BV1WT421r72w\n\n","categories":["深度学习","YOLO"],"tags":["python","深度学习","CNN","YOLO"]},{"title":"模型评价指标","url":"/python/YOLO/evaluation_indicators/","content":" 什么是模型评价指标\n模型评价指标是用来定量评估机器学习模型性能的一套标准和度量方法。它们将模型的预测结果与真实情况进行比较，给出一个数值化的分数。\n 准确率\n简单来说，准确率回答了一个核心问题：“在模型做出的所有判断中，有多少次是猜对的？”\n在二分类问题中，我们通常使用混淆矩阵中的四个指标来定义准确率：\n\nTP (True Positive)：真正例，实际为正，模型也预测为正。\nTN (True Negative)：真负例，实际为负，模型也预测为负。\nFP (False Positive)：假正例，实际为负，模型却预测为正。\nFN (False Negative)：假负例，实际为正，模型却预测为负。\n\nAccuracy=TP+TNTP+TN+FP+FN=预测正确的样本数总样本数\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{\\text{预测正确的样本数}}{\\text{总样本数}}\nAccuracy=TP+TN+FP+FNTP+TN​=总样本数预测正确的样本数​\n三分类及其以上的也是可以的，我们关注的就是整理，其他的就是反例。\n 混淆矩阵\n混淆矩阵 就是一张“对比表”。它把模型预测的结果和实际的真实情况排在一起，让你一眼看出模型在哪些地方混淆了，而它的样子取决于你有多少个类别。\n无论是二分类还是多分类，混淆矩阵的结构通常如下：\n\n行：代表样本的真实类别。\n列：代表模型预测的类别。\n对角线：从左上角到右下角的格子，代表预测正确的数量。\n\n假设你有一个识别动物的模型，测试了 27 只动物，结果可能长这样：\n\n\n\n\n预测：猫\n预测：狗\n预测：兔\n总计（实际）\n\n\n\n\n实际：猫\n5 (TP)\n2\n1\n8\n\n\n实际：狗\n0\n6 (TP)\n0\n6\n\n\n实际：兔\n2\n1\n10 (TP)\n13\n\n\n总计（预测）\n7\n9\n11\n27\n\n\n\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ny_true = [0]*8 + [1]*6 + [2]*13\ny_pred = ([0]*5 + [1]*2 + [2]*1) + ([1]*6) + ([0]*2 + [1]*1 + [2]*10)\n\nprint(f&quot;实际标签: &#123;y_true&#125;&quot;)\nprint(f&quot;预测结果: &#123;y_pred&#125;&quot;)\ncm = confusion_matrix(y_true, y_pred)\nsns.heatmap(cm, annot=True, fmt=&#x27;d&#x27;, cmap=&#x27;Blues&#x27;)\n\n\n 精确率 / 查准率\nPrecision=TPTP+FP=预测对的正例所有预测为正例的样本数\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{\\text{预测对的正例}}{\\text{所有预测为正例的样本数}}\nPrecision=TP+FPTP​=所有预测为正例的样本数预测对的正例​\n它的核心问题在于，模型说“是”的时候，它有多可信？\n 召回率 / 查全率\nRecall=TPTP+FN=预测对的正例所有实际为正例的样本数\\text{Recall} = \\frac{TP}{TP + FN} = \\frac{\\text{预测对的正例}}{\\text{所有实际为正例的样本数}}\nRecall=TP+FNTP​=所有实际为正例的样本数预测对的正例​\n它关注的是模型发现正例的能力，或者说覆盖率。\n 精确率与召回率的进一步理解\n假设一个小镇上有 10个坏人 和 90个好人。\n 策略一：极端谨慎，没把握绝不动手\n在这个场景下，警察非常担心冤枉好人。他们只在掌握了百分之百证据的情况下才抓人。\n结果：警察只抓了 1个人，而这个人确实是坏人。\n\n精确率：1/1=100%1 / 1 = \\mathbf{100\\%}1/1=100%。因为抓的人里没有一个是冤枉的。\n召回率：1/10=10%1 / 10 = \\mathbf{10\\%}1/10=10%。虽然抓得准，但剩下的 9 个坏人还在外面逍遥法外。\n\n“虽然从不冤枉好人，但办事效率太低，坏人都漏掉了。”\n 策略二：宁可错杀一千，不可放过一个\n在这个场景下，警察为了彻底肃清坏人，决定把所有有嫌疑的人全部抓起来。\n结果：警察抓了 50个人。其中包含了全部 10个坏人，但也误抓了 40个好人。\n\n召回率：10/10=100%10 / 10 = \\mathbf{100\\%}10/10=100%。所有的坏人都被抓进了监狱，一个没漏。\n精确率：10/50=20%10 / 50 = \\mathbf{20\\%}10/50=20%。虽然坏人抓完了，但抓的人里有 80% 都是冤枉的好人。\n\n“虽然坏人一个没跑，但滥杀无辜，造成了大量的误报。”\n 精确率与召回率的相互制衡\n\n\n\n维度\n精确率高 / 召回率低\n精确率低 / 召回率高\n\n\n\n\n核心逻辑\n追求“真”，怕误报\n追求“全”，怕漏报\n\n\n模型表现\n表现得非常保守\n表现得非常激进\n\n\n反面后果\n漏网之鱼太多\n冤假错案太多\n\n\n\n在模型调整中，当你调低阈值时，你抓到的坏人会变多，但随之而来的误伤也会变多。这就是机器学习中著名的 P-R 权衡。\nP：Precision  R：Recall\n\n F1-score值\nF1-Score 使用的是调和平均数，它的公式是：\nF1=2×Precision×RecallPrecision+RecallF1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\nF1=2×Precision+RecallPrecision×Recall​\n调和平均数的特点： 它对“极小值”非常敏感。如果 Precision 或 Recall 其中一个非常低，整个 F1-Score 就会被拉得很低，取值范围是0~1。\n IOU交并比\nIoU 衡量的是模型预测的边界框与真实的边界框之间的重叠程度。\nIoU=Area of OverlapArea of UnionIoU = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}\nIoU=Area of UnionArea of Overlap​\n NMS非极大值抑制\n当你运行一个检测模型时，模型往往非常热心，会在同一个物体周围画出成百上千个重叠的候选框。NMS 的作用就是：在这一堆框里，只选出最准的那一个，把剩下的删掉。\n\n排序：将所有候选框按置信度得分从高到低排序。\n选中：挑选得分最高的框，把它作为最终结果保存。\n比较与删除：计算剩下的所有框与 AAA 的 IoU。如果某个框与 AAA 的 IoUIoUIoU 超过了预设的阈值（比如 0.5），就认为这个框和 AAA 预测的是同一个物体，直接把它删掉。\n循环：在剩下的框中继续找得分最高的，重复上述过程，直到处理完所有框。\n\n为什么不找一个置信度最高的的一个框就完事了呢，那是因为，我们在一张图片里有好多要检测的目标，如果只选择最大的那就只保留了一个，这是不合理的。\n这就是 NMS 算法设计的精妙之处：它不是简单的全场选最高，而是局部选最高。\n Confidence置信度\nConfidence=Pr(Object)×IoUpredtruth\\text{Confidence} = Pr(Object) \\times \\text{IoU}_{pred}^{truth}\nConfidence=Pr(Object)×IoUpredtruth​\nPr(Object)Pr(Object)Pr(Object)：是一个概率值，取值范围在 [0,1][0, 1][0,1] 之间。\n\n如果框内包含物体的中心点，Pr(Object)=1Pr(Object) = 1Pr(Object)=1。\n如果框内只有背景，没有任何物体中心点，Pr(Object)=0Pr(Object) = 0Pr(Object)=0。\n\nIoUpredtruth\\text{IoU}_{pred}^{truth}IoUpredtruth​：预测框与真实框之间的重叠程度。\n 特定类别得分\nScore=Pr(Object)×IoU×Pr(Classi∣Object)\\text{Score} = Pr(Object) \\times \\text{IoU} \\times Pr(Class_i | Object)\nScore=Pr(Object)×IoU×Pr(Classi​∣Object)\nPr(Classi∣Object)Pr(Class_i | Object)Pr(Classi​∣Object)：这是条件类别概率，即在已经确定有物体的前提下，它是狗的概率。\n最终得分：综合了有没有东西、框得准不准以及是什么东西这三个因素。\n PR 曲线\nPR 曲线 是衡量分类模型性能的另一把尺。它展示了在不同的置信度阈值下，精确率 和 召回率 之间的博弈关系。\n当我们使用模型预测时，通常会得到一个概率值。我们需要设定一个阈值来决定它是正还是负。\n\n如果阈值设为 0.9很严苛：只有极有把握的才算正例，精确率会很高，但会漏掉很多，召回率低。\n如果阈值设为 0.1很宽松：宁可错抓不可放过，召回率会很高，但误报也多，精确率低。\n\nPR 曲线就是把阈值从 0 变到 1 的过程中，所有的Recall与Precision坐标点连成的线。\n横轴是 Recall召回率，纵轴是 Precision精确率。它常是一条向右下方倾斜的曲线。当召回率上升时，精确率往往会下降。\n理想位置：曲线越靠近右上角（1, 1）越好。这意味着模型能同时保持高精确率和高召回率。\n mAP\nmAP: mean Average Precision\nAP: Average Precision， AP 是 PR 曲线下方的面积。\nm: mean\nmAP=1N∑i=1NAPi\\text{mAP} = \\frac{1}{N} \\sum_{i=1}^{N} AP_i\nmAP=N1​i=1∑N​APi​\nmAP 反映了模型的全能程度。如果一个模型只会认猫，不认识狗，那么它的 mAP 就会被拉低。mAP是衡量模型好坏的终极标准。它不仅看模型能不能分类正确，还看模型框得准不准。\n FPS\n在模型评价指标中，如果说 mAP 衡量的是模型聪不聪明，那么 FPS 衡量的就是模型快不快。FPS即每秒传输帧数，是衡量模型推理速度的核心指标。\n","categories":["深度学习","深度学习基础"],"tags":["python","深度学习","CNN","YOLO"]},{"title":"ultralytics YOLOv3","url":"/python/YOLO/YOLOv3_ultralytics/","content":" YOLOv3 ultralytics\n 安装ultralytics基础环境\nconda install -c conda-forge ultralytics\n\n 克隆YOLOv3代码\ngit clone https://github.com/ultralytics/yolov3\n\n\n 解决 git 443 报错\n推荐使用v2Ray进行代理，设置git的http及https代理：\ngit config --global http.proxy socks5://127.0.0.1:10808\ngit config --global https.proxy socks5://127.0.0.1:10808\n\n若使用v2Ray的话，其代理端口为10808，若为clash则应为7891。\n 安装缺少的依赖库\n\n当然你可以一个一个安装，但那样有点麻烦，请使用如下命令：\npip install -r .\\requirements.txt\n\n 权重文件\n请访问：https://gitcode.com/open-source-toolkit/b5126/\n其中包含了：\n\nyolov3.pt\nYOLOv3 的标准权重文件，适用于大多数目标检测任务。\nyolov3-spp.pt\nYOLOv3-SPP 的权重文件，通过空间金字塔池化（Spatial Pyramid Pooling）增强了模型的性能，特别适用于高分辨率图像的目标检测。\nyolov3-tiny.pt\nYOLOv3-Tiny 的权重文件，适用于资源受限的环境，如嵌入式设备或移动设备，具有较快的推理速度和较小的模型体积。\n\n 试运行\n首先修改权重文件路径\n\n运行detect.py\nFusing layers... \nyolov3 summary: 261 layers, 61922845 parameters, 0 gradients, 155.9 GFLOPs\nimage 1/2 D:\\PycharmProjects\\yolov3\\data\\images\\bus.jpg: 640x480 4 persons, 1 bicycle, 1 bus, 70.7ms\nimage 2/2 D:\\PycharmProjects\\yolov3\\data\\images\\zidane.jpg: 384x640 2 persons, 2 ties, 55.0ms\nSpeed: 1.0ms pre-process, 62.8ms inference, 25.5ms NMS per image at shape (1, 3, 640, 640)\nResults saved to runs\\detect\\exp3\n\n输出结果：\n\n\n效果惊艳，起初我观察输出时 4 persons, 1 bicycle, 1 bus, 70.7ms，我在想这个 1 bicycle 到底在哪里。\n我反复盯着图片看了好几遍：公交车很明显，人也数得清，可自行车呢？画面里没有完整的车轮，没有清晰的车架，甚至也没有骑车的人。那一刻我甚至怀疑，是不是模型误判了。\n直到我真正把 YOLO 的标注框图片打开，那辆自行车就在公交车的右上角，位于居民窗户围栏之中，一个几乎被遮挡、只露出局部轮廓的物体，被一个细小却坚定的框牢牢圈住。那一瞬间，我才理解You Only Look Once的真正含义…\n 项目结构\n 资源和工具\n\ndata：存放数据相关的配置文件比如类别名称、路径配置和图片测试数据。\nmodels：存放模型相关的配置文件，定义神经网络层级结构和核心代码。\nruns：结果文件夹。当你运行程序进行推理、训练或测试后，生成的图表、保存的检测结果都会放在这里。\nutils：工具代码库。存放一些辅助函数，比如计算损失的代码、在图片上画框框的代码等。\nVOCdevkit：数据集文件夹。通常是按照官方 VOC 标准存放的原始图片和标签文件。\nweight：权重文件夹。专门用来放训练好的模型参数（权重文件），没有它模型就无法工作。\n\n 视频与环境文件\n\nche.avi：一个测试视频。方便你写好程序后，直接运行来看看能不能识别出视频里的物体。\nrequirements.txt：环境配置文件。记录了运行这个项目需要安装哪些 Python 插件和库，以及对应的版本。\n\n Python 脚本部分（核心运行程序）\n\nDataset_partitioning.py：数据集划分代码。用来把你的图片随机分成“训练集”和“验证集”。\ndetect.py：检测推理代码。最常用的文件，运行它就可以调用模型去识别一张图片或一段视频。\nhubconf.py：PyTorch Hub 相关的脚本（用于方便地在其他地方调用这个模型）。\ntrain.py：模型训练代码。如果你想用自己的数据教模型认东西，就运行这个文件。\nval.py：模型测试/验证代码。用来跑分，看看训练出来的模型准确率（mAP）到底有多高。\n\n 超参数文件：hyp.scratch-high.yaml\nlr0 表示初始学习率，即训练刚开始时优化器使用的学习率。一般来说，SGD 常用 0.01，Adam 常用 0.001。学习率过大会导致训练震荡甚至发散，过小则收敛缓慢。\nlrf 表示最终学习率与初始学习率的比例，通常用于 OneCycle 学习率策略。训练后期的学习率等于 lr0 乘以 lrf，例如 lr0 为 0.01，lrf 为 0.1，则最终学习率为 0.001。这样可以在训练后期减小步长，使模型更稳定地收敛。\nmomentum 表示动量参数。在使用 SGD 时它是 momentum，在使用 Adam 时对应 beta1。动量的作用是保留历史梯度方向，减少参数更新的抖动，加快收敛速度。\nweight_decay 表示权重衰减系数，也就是 L2 正则化强度。它通过惩罚过大的权重来防止模型过拟合，0.0005 是目标检测中非常常见的经验值。\nwarmup_epochs 表示学习率预热的轮数，前若干个 epoch 内学习率会从较小值逐步增加到设定的初始学习率，主要目的是防止训练初期梯度过大导致不稳定。\nwarmup_momentum 表示在 warmup 阶段使用的初始动量值，随着 warmup 的结束逐渐过渡到正常的 momentum，有助于平滑训练初期的参数更新。\nwarmup_bias_lr 表示偏置参数在 warmup 阶段使用的学习率。通常会给 bias 一个相对更大的学习率，使模型能更快地学会目标的大致位置和存在性。\nbox 表示边界框回归损失的权重，用于控制模型对目标位置和大小回归精度的关注程度。该值越大，模型越重视框的位置准确性。\ncls 表示分类损失的权重，用于平衡多类别预测的重要性。如果类别数较多或类别区分困难，通常需要适当增大该值。\ncls_pw 表示分类损失中正样本的权重，主要用于缓解正负样本不平衡问题。值为 1 表示不额外加权。\nobj 表示目标置信度损失的权重，用于衡量模型判断“该位置是否存在目标”的能力，这是 YOLO 检测中非常核心的一项。\nobj_pw 表示目标置信度损失中正样本的权重，同样用于样本不平衡场景。\niou_t 表示训练时使用的 IoU 阈值，只有预测框与真实框的 IoU 大于该值时才会参与正样本训练。阈值较低时有利于提高召回率，较高时有利于提高定位精度。\nanchor_t 表示 anchor 匹配阈值，用于控制真实框与 anchor 在宽高比例上的匹配宽松程度。数值越大，允许匹配的 anchor 越多。\nanchors 表示每个输出层使用的 anchor 数量，该项被注释掉说明使用模型默认的 anchor 设置。\nfl_gamma 表示 Focal Loss 的 gamma 参数，当该值为 0 时表示不启用 Focal Loss。Focal Loss 主要用于缓解正负样本极度不平衡的问题。\nhsv_h 表示对图像色调的随机扰动幅度，用于增强模型对不同色彩变化的鲁棒性。\nhsv_s 表示对图像饱和度的随机扰动幅度，使模型适应颜色深浅变化。\nhsv_v 表示对图像亮度的随机扰动幅度，提高模型在不同光照条件下的泛化能力。\ndegrees 表示随机旋转角度的范围，当前为 0 表示不进行旋转增强。\ntranslate 表示图像随机平移的比例，0.1 表示在上下左右方向最多平移图像尺寸的 10%。\nscale 表示图像的随机缩放比例，数值越大，缩放范围越广，有助于增强尺度不变性。\nshear 表示错切变换的角度范围，当前为 0 表示不使用错切变换。\nperspective 表示透视变换的强度，通常用于模拟不同拍摄角度的效果。\nflipud 表示图像上下翻转的概率，0 表示不进行上下翻转。\nfliplr 表示图像左右翻转的概率，0.5 表示一半概率进行左右翻转，这是目标检测中非常常用的增强方式。\nmosaic 表示 Mosaic 数据增强的使用概率，1.0 表示始终使用。该方法将四张图片拼接成一张，对小目标检测和复杂场景效果显著。\nmixup 表示 MixUp 数据增强的概率，通过将两张图片和标签进行线性混合来增强模型的泛化能力。\ncopy_paste 表示 Copy-Paste 数据增强的概率，即从一张图中裁剪目标并粘贴到另一张图中，用于增加目标组合的多样性。\n no-aug / Object365/ scratch-high/med/low / VOC\n hyp.no-augment\nhyp.no-augment 非常特殊，它表示几乎关闭所有数据增强。mosaic、mixup、颜色扰动基本都不开。它主要有三种用途：第一，用来做消融实验，验证“数据增强到底有没有用”；第二，用在数据本身已经被严格对齐、增强会破坏语义的任务中比如工业缺陷、医学影像；第三，用于调试或快速验证训练流程是否正确。\n hyp.scratch-object365\nhyp.scratch-object365 是为 Object365 这种超大规模、类别极多的数据集设计的。它会更加重视分类稳定性，对 cls loss、正负样本平衡和 anchor 匹配都有特殊调优。这套参数一般不建议直接拿来用在普通自制数据集上。\n hyp.scratch\nscratch 的意思是“从零开始训练”，也就是不加载任何预训练权重，网络参数完全随机初始化。在这种情况下，训练对学习率、数据增强和 warmup 都非常敏感，所以官方给了多套经过验证的 hyp 配置。\n hyp.scratch-high\nhyp.scratch-high 是为大数据集 + 强数据增强准备的配置。它通常用于 COCO 这种规模在十万级以上的数据集，或者你有非常多样、复杂场景的数据。它的典型特征是数据增强非常激进，比如 mosaic、mixup、颜色扰动都开得比较大，学习率也相对偏高，模型泛化能力最强，但前期训练不稳定，对小数据集非常不友好。\n hyp.scratch-med\nhyp.scratch-med 是一个折中方案。数据增强和学习率都比 high 温和一些，适合中等规模数据集，比如几千到一两万张图片。很多人在自定义数据集时，其实用的就是这一套，只是没意识到。\n hyp.scratch-low\nhyp.scratch-low 是为小数据集准备的。它会明显减弱数据增强强度，学习率也更保守，避免模型在强增强下“学歪”。如果你的数据只有几百到一两千张，用 high 或 med 很容易出现 loss 震荡、mAP 不升甚至下降，这时 low 反而效果更好。\n hyp.scratch-voc\nhyp.scratch-voc。它是专门为 PASCAL VOC 数据集调的，而不是简单的 low 或 med。VOC 的特点是类别少、目标相对大、场景简单，所以这个配置通常会降低分类损失权重、调整 anchor 匹配策略，更偏向于“稳而准”。\n 最佳实践？\n90% 的自定义数据集，scratch-med 或 scratch-low 起步是最稳的，不要一上来就 high。\n 数据集分类文件及其下载脚本\n\n每一个 .yaml 文件都像是一份说明书，告诉模型：\n\n训练/验证数据在哪里：图片和标签存储在硬盘的哪个路径下。\n有多少个类别：例如模型需要识别 80 种物体还是 10 种。\n类别的名字是什么：例如 0: person, 1: bicycle 等。\n\n# 路径设置\npath: ../datasets/coco128  # 数据集根目录\ntrain: images/train2017    # 训练集图片路径\nval: images/train2017      # 验证集图片路径\n\n# 类别信息\nnc: 80  # 类别数量 (number of classes)\nnames:\n  0: person\n  1: bicycle\n  2: car\n\n labelimg标注软件\n开源地址：https://github.com/HumanSignal/labelImg\n\nLabelImg is now part of the Label Studio community. The popular image annotation tool created by Tzutalin is no longer actively being developed, but you can check out Label Studio, the open source data labeling tool for images, text, hypertext, audio, video and time-series data.\nLabelImg 现已成为 Label Studio 社区的一部分。Tzutalin 开发的流行图像注释工具已不再积极开发，但你可以看看 Label Studio，这是一个开源的数据标注工具，用于图像、文本、超文本、音频、视频和时间序列数据。\n\nLabel Studio：https://labelstud.io/ （Open Source Data Labeling Platform）\n开源地址：https://github.com/HumanSignal/label-studio/\nlabelimg最终版本为：1.8.1 / released this Dec 3, 2018 https://github.com/tzutalin/labelImg/files/2638199/windows_v1.8.1.zip\n\n推荐使用VOC格式的xml信息。\n VOC2007\n源：http://host.robots.ox.ac.uk/pascal/VOC/voc2007 但是我访问这个时候被防火墙拦截了。\n源码中的：\n\n训练集与验证集：https://github.com/ultralytics/assets/releases/download/v0.0.0/VOCtrainval_06-Nov-2007.zip\n测试集：https://github.com/ultralytics/assets/releases/download/v0.0.0/VOCtest_06-Nov-2007.zip\n\n 将VOC格式转为YOLO格式\n Data_Trans脚本\n请访问开源脚本：https://github.com/JieZzzoo/Data_Trans\n使用说明：https://blog.csdn.net/Thebest_jack/article/details/125637099\n\n          \n          蓝胖胖\n          感谢作者开源\n          \n这个脚本可以将VOC格式转为YOLO格式，不过他把数据集索引写在了test、train和val的文本文件里了。\n不过他这个提取脚本有点小bug，后续我将会制作一个完整的脚本。\n如果要产生图片（训练、验证、测试）和标签（训练、验证、测试）那样的结构，需要进一步改写，因此我设计了这样的脚本：imgsWlabels\n imgsWlabels脚本\nimgsWlabels是我自创的一个提取与整理的脚本，如用方式如下，首先你需要把Data_Trans中的的Imagesets、labels以及JPEGImages复制下来放到imgsWlabels的pre目录下，形如：\n\nimgsWlabels有两种模式，分别是“train与val”模式和”trainval“模式。\n train与val模式\n在这种模式下，数据集被分为三个部分：训练集、验证集、测试集，这种模式的特点是：训练集和验证集是分开的，避免数据泄露，确保模型的泛化能力。\n trainval模式\n在 trainval 模式下，训练集和验证集被合并为一个整体的数据集，模型在整个 trainval 数据集上进行训练和验证。这种模式通常用于特定的任务，例如数据量不大时，可以通过合并训练集和验证集来提升训练集的多样性和大小。\n 代码\n# 划分数据集模式，0为train与val模式，1为trainval模式\nsplit_mode = 0\n\n# ----\nimport os\nimport time\nimport shutil\nfrom tqdm import tqdm\n\nroot_dir = &#x27;pre&#x27;\nroot_img_dir = os.path.join(root_dir, &#x27;JPEGImages&#x27;)\nroot_label_dir = os.path.join(root_dir, &#x27;labels&#x27;)\nindexes = os.path.join(root_dir, &#x27;Imagesets&#x27;)\ntrain_index = os.path.join(indexes, &#x27;train.txt&#x27;)\nval_index = os.path.join(indexes, &#x27;val.txt&#x27;)\ntrainval_index = os.path.join(indexes, &#x27;trainval.txt&#x27;)\ntest_index = os.path.join(indexes, &#x27;test.txt&#x27;)\n\n\ndef read_file_return_list(file_path):\n    try:\n        with open(file_path, &#x27;r&#x27;) as f:\n            lines = f.readlines()\n            return [line.strip() for line in lines]\n    except FileNotFoundError:\n        print(&#x27;index file not found&#x27;)\n        exit()\n    except Exception as e:\n        print(e)\n        exit()\n\n\ntrain_indexes_list = read_file_return_list(train_index)\nval_indexes_list = read_file_return_list(val_index)\ntrainval_indexes_list = read_file_return_list(trainval_index)\ntest_indexes_list = read_file_return_list(test_index)\n\n\ndef copy_file(src_file, dst_dir):\n    try:\n        os.makedirs(dst_dir, exist_ok=True)\n        shutil.copy(src_file, dst_dir)\n        return True\n    except Exception as e:\n        print(e)\n        return False\n\n\ndef cp_file_in_list(src_dir, dst_dir, index_list, target_ext=&#x27;.jpg&#x27;, msg=&#x27;copy file...&#x27;):\n    if not os.path.exists(src_dir):\n        print(&#x27;Source directory does not exist!&#x27;)\n        return\n    if not os.path.exists(dst_dir):\n        os.makedirs(dst_dir)\n\n    with tqdm(index_list, desc=msg, unit=&#x27;file&#x27;, total=len(index_list), dynamic_ncols=True,\n              ncols=100, mininterval=0.1,\n              bar_format=&quot;&#123;l_bar&#125;&#123;bar&#125;| &#123;n_fmt&#125;/&#123;total_fmt&#125; [&#123;elapsed&#125; &lt; &#123;remaining&#125;, &#123;rate_fmt&#125;]&quot;) as pbar:\n        for index in pbar:\n            src_file = os.path.join(src_dir, index + target_ext)\n            if copy_file(src_file, dst_dir):\n                pbar.set_postfix(file=index, color=&#x27;green&#x27;)\n            else:\n                pbar.set_postfix(file=index, color=&#x27;red&#x27;)\n            pbar.update(1)\n\n\ndef boot_split():\n    if split_mode == 0:\n        # train与val模式\n        cp_file_in_list(root_img_dir, &#x27;images/train&#x27;, train_indexes_list, msg=&#x27;copy train file...&#x27;)\n        cp_file_in_list(root_img_dir, &#x27;images/val&#x27;, val_indexes_list, msg=&#x27;copy val file...&#x27;)\n        cp_file_in_list(root_img_dir, &#x27;images/test&#x27;, test_indexes_list, msg=&#x27;copy test file...&#x27;)\n\n        cp_file_in_list(root_label_dir, &#x27;labels/train&#x27;, train_indexes_list, target_ext=&#x27;.txt&#x27;, msg=&#x27;copy train label...&#x27;)\n        cp_file_in_list(root_label_dir, &#x27;labels/val&#x27;, val_indexes_list, target_ext=&#x27;.txt&#x27;, msg=&#x27;copy val label...&#x27;)\n        cp_file_in_list(root_label_dir, &#x27;labels/test&#x27;, test_indexes_list, target_ext=&#x27;.txt&#x27;, msg=&#x27;copy test label...&#x27;)\n    elif split_mode == 1:\n        # trainval模式\n        cp_file_in_list(root_img_dir, &#x27;images/trainval&#x27;, trainval_indexes_list, msg=&#x27;copy trainval file...&#x27;)\n        cp_file_in_list(root_img_dir, &#x27;images/test&#x27;, test_indexes_list, msg=&#x27;copy test file...&#x27;)\n\n        cp_file_in_list(root_label_dir, &#x27;labels/trainval&#x27;, trainval_indexes_list, target_ext=&#x27;.txt&#x27;, msg=&#x27;copy trainval label...&#x27;)\n        cp_file_in_list(root_label_dir, &#x27;labels/test&#x27;, test_indexes_list, target_ext=&#x27;.txt&#x27;, msg=&#x27;copy test label...&#x27;)\n\n\n# ----\nif __name__ == &#x27;__main__&#x27;:\n    start_time = time.time()\n    boot_split()\n    print(&#x27;Time used: %.2f&#x27; % (time.time() - start_time))\n\n可以根据需要的模式进行修改运行模式。\n执行后：\n\n会生成：\n\n 如何训练模型\n一般训练需要修改这几个参数就足够了\n\n设置好数据集路径：\n\n修改类别数：\n\n最好根据自己的数据集设置检测框那样最好。\n开始训练：\n\n\n 参数详解\n rect矩形训练\n在训练时，按照图片的原始宽高比进行分组，并尽量减少 padding，而不是把所有图片强行 resize 成正方形。\n resume断点续训\n当训练被中断比如关机、Ctrl+C、显存不足或崩溃后， 从上一次保存的 checkpoint 继续训练，而不是从头开始。\n 如何上采样？\n[-1, 1, nn.Upsample, [None, 2, &quot;nearest&quot;]],\n\n\n\n\n参数\n解释\n\n\n\n\nNone\n不指定目标尺寸\n\n\n2\nscale_factor=2，宽和高放大 2 倍\n\n\n&quot;nearest&quot;\n使用 最近邻插值\n\n\n\n 为什么yaml定义结构中的输出通道并不是255？（对于Coco）\nhead: [\n    [-1, 1, Bottleneck, [1024, False]],\n    [-1, 1, Conv, [512, 1, 1]],\n    [-1, 1, Conv, [1024, 3, 1]],\n    [-1, 1, Conv, [512, 1, 1]],\n    [-1, 1, Conv, [1024, 3, 1]], # 15 (P5/32-large)\n\n    [-2, 1, Conv, [256, 1, 1]],\n    [-1, 1, nn.Upsample, [None, 2, &quot;nearest&quot;]],\n    [[-1, 8], 1, Concat, [1]], # cat backbone P4\n    [-1, 1, Bottleneck, [512, False]],\n    [-1, 1, Bottleneck, [512, False]],\n    [-1, 1, Conv, [256, 1, 1]],\n    [-1, 1, Conv, [512, 3, 1]], # 22 (P4/16-medium)\n\n    [-2, 1, Conv, [128, 1, 1]],\n    [-1, 1, nn.Upsample, [None, 2, &quot;nearest&quot;]],\n    [[-1, 6], 1, Concat, [1]], # cat backbone P3\n    [-1, 1, Bottleneck, [256, False]],\n    [-1, 2, Bottleneck, [256, False]], # 27 (P3/8-small)\n\n    [[27, 22, 15], 1, Detect, [nc, anchors]], # Detect(P3, P4, P5)\n  ]\n\n因为 YAML 里定义的是特征提取与融合过程中的中间特征通道数，真正的 255 通道是在 Detect 模块内部，通过 1×1 卷积动态生成的，而不是在 YAML 中显式写出来的。\n\n 源码种这样写到…\nself.nc = nc  # number of classes\nself.no = nc + 5  # number of outputs per anchor\nself.nl = len(anchors)  # number of detection layers\nself.na = len(anchors[0]) // 2  # number of anchors\nself.grid = [torch.empty(0) for _ in range(self.nl)]  # init grid\nself.anchor_grid = [torch.empty(0) for _ in range(self.nl)]  # init anchor grid\nself.register_buffer(&quot;anchors&quot;, torch.tensor(anchors).float().view(self.nl, -1, 2))  # shape(nl,na,2)\nself.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv\n\n\n255 = 3 × (80 + 5)\n255 与 nc 强相关，nc 改了，输出通道立刻变，YAML 不可能写死 255。\n","categories":["深度学习","YOLO","项目与实战"],"tags":["python","深度学习","CNN","YOLO"]},{"title":"YOLO系列","url":"/python/YOLO/index/","content":"\n          \n          YOLO-V1\n          一切的开始\n          \n          \n          YOLO-V2\n          一次关键的且天才的改进\n          \n          \n          YOLO-V3\n          Joseph Redmon在YOLO的最后一舞\n          \n","categories":["深度学习","YOLO"],"tags":["YOLO"]}]