[{"title":"起点，但不止于起点！","url":"/hello/","content":"# 领航\n首先感谢 Github 提供的 pages 服务，其次感谢制作 Hexo 框架全体工作人员和 aurora 主题作者将我带入绚丽多彩的博客世界，最后我还要感谢目前博客使用的主题 Shoka 的制作者，在这里 Karry 献上最崇高的敬意！\n# 简单总结\n接下来会使用这个博客分享各种生活趣事、代码经验以及学科题目解答，敬请期待！\nwelcome\n\n\n2023 年 3 月 17 日 完成 Hexo+GitHub 网页搭建\n欢迎访问 Karry.Liu 的个人博客\nQQ：735690757\nWeChat：Sa9329Mxz\ngmail：735690757carry@gmail.com\n\n\n","categories":["初来乍到"],"tags":["欢迎"]},{"title":"考试中有关广义表的两个常用函数的解","url":"/DSLearnNote/GeneralizedLists/","content":"# 认识广义表\n广义表是线性表的推广，与线性表不同的是，线性表中的每一个数据元素都属于同一数据对象。\n广义表可以表示为：\n\n空表：()\n表头：(表头)\n表头 + 表尾：(表头，表尾)\n\n实际上这就是他的基本结构，而对于其中的元素来说，它可以是表，也可以是元素，这就是广义！\n# 取头 Head ()\nHead 是取头操作，他拿的是一个元素或者一个表\n# 取尾 Tail ()\nTail 是取尾操作，他拿到的必是一个表\n# 巧记\n对于蟒蛇（Python）来说，头只有一个，而它的尾巴很长。\n所以，取头 Head 元素比较单一，取尾 Tail 往往比较长。\n# 练习\n\nA=（a，b）\nB=（A，A）\nC=（a，（b，A），B）\n对于操作：Tail（Head（Tail（C）））的结果是什么？\n\n# 解答\nTail（C）尾巴长：（a，（b，A），B ）\nTail（C）=（（b，A），B）\nHead（Tail（C））头短：（（b，A），B）\nHead（Tail（C））=（b，A）\nTail（Head（Tail（C）））尾巴长：（b，A）\nTail（Head（Tail（C）））=（A）\n尾巴长必是一个表，头短可能是一个表也可能是一个元素\n","categories":["DS"],"tags":["数据结构"]},{"title":"数据结构代码题速记","url":"/DSLearnNote/codeTM/","content":"# 线性表方向\n# 链表的合并\n\n设计实现将两个带有头节点的有序链表合并为一个新的有序链表。\n\nvoid marge(LNode *A,LNode *B,LNode *C)&#123;    LNode p = A->next;    LNode q = B->next;    LNode r;    r->next = null;    C = A;    r=c;    while(p != null||q != null)&#123;        if(p->data &lt;= q->data)&#123;            r->next = p;            p = p->next;            r = r->next;        &#125; else&#123;            r->next = q;            q = q->next;            r = r->next;        &#125;    &#125;    if(p) r = p;    if(q) r = q;&#125;# 链表的逆置\n\n设计实现将无头节点链表进行逆置\n\nvoid invert(LinkList &amp;L)&#123;    p=L;    q=p->next;    while(p)&#123;        r=q->next;        q->next=p;        p=q;        q=r;    &#125;    L->next=null;    L=p;&#125;# 顺序表的逆置\n\n设计一个算法，将顺序表 L 所有的元素逆置，要求算法效率尽可能地高。\n\nvoid reverse(Sqlist &amp;L)&#123;    ElemType temp;    for(int i=0; i&lt;L.length/2; i++)&#123;        temp = L[i];        L.data[i]=L.data[L.length-i-1];        L.data[L.length-i-1] = temp;    &#125;&#125;\n一种” 就地 “思想\n\n# 统计单链表 HL 中的值等于 x 的个数\nint countX(LNode *HL, ElemType x)&#123;    int countNum = 0;    LNode p = HL->next;    while(p)&#123;        if(p->data == x)&#123;            countNum++;        &#125;        p = p->next;    &#125;    return countNum;    &#125;# HL 是单链表的头指针，删除头节点\nElemType DeleFront(LNode *&amp;HL)&#123;    if(HL == null) exit(1);    LNode *p = HL;    ElemType con = p->data;    p->next =  p->netx->next;    return com;&#125;# 判断单链表是否中心对称\n\n\n链表计数\n动态开辟数组（逻辑栈）\n奇数个要跳过最中心的那个元素\n链表向后，栈向下依次检查\n结束前记得 free 栈\n\n\nbool isDC(LinkList *L) &#123;    if (L == NULL || L->next == NULL) return true;    // 计算链表长度    int count = 0;    LinkList *p = L;    while (p) &#123;        count++;        p = p->next;    &#125;    // 动态分配栈空间    int *stack = (int *)malloc((count / 2) * sizeof(int));    if (stack == NULL) return false;    p = L;    int top = -1;    for (int i = 0; i &lt; count / 2; i++) &#123;        stack[++top] = p->data;        p = p->next;    &#125;    // 如果链表长度为奇数，跳过中间的节点    if (count % 2 != 0) p = p->next;    // 开始比较    while (p) &#123;        if (p->data != stack[top--]) &#123;            free(stack);            return false;        &#125;        p = p->next;    &#125;    free(stack);    return true;&#125;# 判断单链表是否递增\nbool isIncrease(LinkList *l)&#123;    LinkList *p = l;    int nowData = p->data;    p = p->next;    while(p)&#123;        if(p->data>nowData)&#123;            nowData = p->data;            p = p->next;        &#125;else&#123;            return false;        &#125;    &#125;    return true;&#125;# 链表 A，链表 B，求 A 与 B 的交集生成链表 C\nbool isExistence(LinkList *L,Elemtype goData)&#123;    if(L==null) return true;    LinkList *p = L;    while(p)&#123;        if(p->data == goData) return false;        p=p->next;    &#125;    return true;&#125;void intersectionAB(LinkList *A,LinkList *B,LinkList *&amp;C)&#123;    LinkList *a=A;    LinkList *b=B;    LinkList *c=C;    LinkList go;    LinkList *goP=go;        while(a)&#123;        while(b)&#123;            if(a->data==b->data&amp;&amp;isExistence(goP))&#123;                c = (LinkList*)malloc(sizeof(LinkList));                go = (LinkList*)malloc(sizeof(LinkList));                c->data = b->data;                c = c->next;                goP = goP->next;            &#125;            b = b->next;        &#125;        a = a->next;        b = B;    &#125;&#125;# 链式有序归并\nvoid mergelkList(LinkList *a,LinkList *b,LinkList &amp;*c)&#123;    LinkList *p = a;    LinkList *q = b;    LinkList *temp = (Linklist*)malloc(sizeof(LinkList));    Linklist *r = temp;    while(p != NULL &amp;&amp; q != NULL)&#123;        if(p->data &lt; q->data)&#123;            r->next = (Linklist*)malloc(sizeof(LinkList));            r->next->data = p->data;            r = r->next;            p = p->next;        &#125;else&#123;            r->next = (Linklist*)malloc(sizeof(LinkList));            r->next->data = q->data;            r = r->next;            q = q->next;        &#125;    &#125;    while(p)&#123;        r->next = (Linklist*)malloc(sizeof(LinkList));        r->next->data = p->data;        r = r->next;        p = p->next;    &#125;    while(q)&#123;         r->next = (Linklist*)malloc(sizeof(LinkList));         r->next->data = q->data;         r = r->next;         q = q->next;    &#125;    r->next = NULL;    c = temp->next;&#125;# 串方向\n# 在顺序存储结构上实现求子串\n\n\n# 堆方向\n# 已知堆，新加入一个底部元素重新调整成堆\nvoid adjustheap(int r[],int n)&#123;    int j = n;    int i = j/2;    int temp = r[j-1];    while(i>=1)&#123;        if(temp>=r[i-1])&#123;            break;        &#125;else&#123;            r[j-1] = r[i-1];            j = i;            i = i/2;        &#125;    &#125;    r[j-1] = temp;&#125;# 设计单链表中值相同的多余节点\n\n这段代码可能存在内存泄漏的风险，应对考试应该还是没问题的\n\nvoid deleteCon(LinkList *&amp;L)&#123;    if (!L) return;    LinkList *p = L;    LinkList *q;    LinkList *r;    int num = 0;    while(p)&#123;        num++;        p = p->next;    &#125;    p = L;    int* nodeHave = malloc(sizeof(int)*num);    int now=1;    nodeHave[0]=p->data;    q=p;    p=p->next;    while(p)&#123;        bool found = false;        for(int i=0;i&lt;now;i++)&#123;            if(nodeHave[i] == p->data) &#123;                q->next=q->next->next;                q=q->next;                p=p->next;                r=p;                free(r);                found = true;            &#125;        &#125;        if(!found)&#123;            nodeHave[now++]=p->data;        \tp=p->next;        \tq=q->next;        &#125;     &#125; &#125;# 树方向\n# 二叉树求树高\nint getDeep(BTree b)&#123;    int HL,HR;    HL = HR = 0;    HL = getDeep(b->Lchild);    HR = getDeep(b->Rchild);    return HL > HR ? (HL+1) : (HR+1);    // 此处 return 就是谁大就返回谁加一&#125;# 左孩子友兄弟表示法，求原树高\nint hight(BTree b)&#123;    int HL,HR;    HL = HR = 0;    if(b == NULL) return 0;    HL = high(b->Lchild);    HR = high(b->Rchild);    if(HL+1 > HR)&#123;        return HL + 1;    &#125; else&#123;        return HR;    &#125;    &#125;# 二叉树求树宽\nint count[100];int max = -1;void width(BTree t,int k)&#123;    if(t = NULL) return;    count[k] ++;    if(max &lt; count[k]) max = count[k];    width(t->lchild, k+1);    width(t->rchild, k+1);&#125;# 链式存储结构建立二叉树\ntypedef char datatype;typedef struct node&#123;    datatype data;    struct node *lchild,*rchild;&#125;btreevoid createBtree(btree *&amp;bt)&#123;    datatype zData;    scanf(\"%c\",&amp;zData);    if(zData == '#') &#123;        bt=null;        return;    &#125;    bt = (btree*)malloc(sizeof(btree))    bt->data=zData;    createBtree(bt->lchild);    createBtree(bt->rchild);&#125;# 判断二叉树是否为排序二叉树 / 二叉搜索树\n\n判断一棵二叉树是否为排序二叉树，可以通过中序遍历来实现。在 BST 中，中序遍历的结果应该是一个严格递增的序列。如果遍历过程中发现任何一个节点的值不大于前一个节点的值，则该树不是 BST。\n\nint minnum = INT_MIN;int isFirstNode = 1;      // 标记是否是第一个节点 (最左侧的节点)int isBST(BTree *bt)&#123;    if(bt==null)&#123;        return 1;// 空树是 BST    &#125;    if(!isBST(bt->lchild))&#123;        return 0;    &#125;    if(!isFirstNode &amp;&amp; bt->data&lt;minnum)&#123;        return 0;    &#125;    isFirstNode =0;    minnum = bt->data;    return isBST(bt->rchild);    &#125;# 求节点在二叉排序树，关键字是 x 的层数\nint layer = 0;void `findXCountLayer(BTree *b,int x)&#123;    if(b!=NULL)&#123;        layer++;        if(b->data == x)&#123;            return;        &#125;else if(x > b->data)&#123;            findXCountLayer(b->rchild,x);        &#125;else&#123;            findXCountLayer(b->lchild,x)        &#125;    &#125;&#125;# 统计二叉树节点个数\nint countNode(BTree *b)&#123;    if(b == NULL)&#123;        return 0;    &#125;    return countNode(b->lchild) + countNode(b->rchild) + 1;&#125;# 计算二叉树中所有节点之和\nint sum = 0;void bTreeSum(BTree *b)&#123;    bTreeSum(b->lchild);    sum += b->data;    bTreeSum(b->rchild);&#125;# 排序方向\n# 快速排序\n# 快速排序分区函数\nint Partition(int a[],int low,int high)&#123;    int flag = a[low];    while(low &lt; high)&#123;        while(low &lt; high &amp;&amp; flag &lt;= a[high]) --high;        a[low] = a[high];        while(low &lt; high &amp;&amp; flag > a[low]) ++low;        a[high] = a[low];    &#125;    a[low] = flag;    return low;&#125;# 快速排序递归函数\nvoid QuickSort(int a[],int low,int high)&#123;    if(low &lt; high)&#123;        int pivo = Partition(a,low,high);        QuickSort(a,low,pivo-1);        QuickSort(apivo+1,high);    &#125;&#125;# 链式结构的简单选择排序\nvoid easySelectSort(LinkList *l)&#123;    if(l == NULL) return;    int min = MAX_INT;    int temp;    LinkList *p = l;    LinkList *r = p;    while(p)&#123;        min = p->data;        LinkList *q = p;        while(q)&#123;            if(q->data &lt; min)&#123;                r=q;                min = r->data;            &#125;            q = q->next;        &#125;        if (r != p) &#123;        \ttemp = r->data;        \tr->data = p->data;        \tp->data = temp;        &#125;        p = p->next;    &#125;&#125;# 综合代码题目\n# 快速排序分区函数 + 顺序表移动\n\n设计一个算法，调整数组 a [] 中的元素并返回分界值，使所有小于 x 的元素都出现在其左边（a [1…i]），所有大于 x 的元素都出现在其右边（a [i+1…n]）。\n\n不严谨的代码int div(int a[],intx)&#123;    int rear = a.length;    a[rear] = x;    int start = 0;    while(start &lt; rear)&#123;        while(start&lt;rear &amp;&amp; x>=a[start]) rear--;        a[rear] = a[start];        while(start&lt;rear &amp;&amp; x&lt;a[rear]) start++;        a[strat] = a[rear];    &#125;    int i = start;    for(int j=i; j>0; j--)&#123;        a[i]=a[i-1];    &#125;    return i;&#125;\n实际上，上述代码有一些不严谨的部分，比如，可能造成数组越界。\n但仍然可以帮助我们拿下绝大部分的分数。\n\n相同逻辑更严谨的代码int div(int a[],int h,int x)&#123;    int i,j,t;    i = 1;    j = h;    while(i&lt;j)&#123;        // 找到第一个小于等于 x 的元素        while(i&lt;j &amp;&amp; a[j]>=x) j--;        // 找到第一个大于 x 的元素        while(i&lt;j &amp;&amp; a[i]&lt;=x) i++;        if(i&lt;j)&#123;            // 交换元素            t = a[i];            a[i] = a[j];            a[j] = t;        &#125;    &#125;    // 这里可能会出现 i 跨越 j 的情况，这里做下判断    // 如果 i 位置的数仍然比 x 要小，那么就可以返回 i    // 反之就是 i 跨越了 j，而且最多只会跨越一步，返回 i-1    if(a[i]&lt;x) return i;    else return i-1;&#125;\n上述代码体现就是一种类似 “就地算法” 的思想，无需借助辅助空间。\n找到第一个大于 x 与小于 x 的数，直接原地交换位置，然后继续寻找。\n\n# 将奇数转移到偶数之前\nvoid quickPass(int r[],int s,int t)&#123;    int i = s;    int j = t;    int x = r[s];    while(i &lt; j)&#123;        while(i &lt; j &amp;&amp; r[j]%2 == 1)&#123;            j--;        &#125;        if(i &lt; j)&#123;            r[i] = r[j];            i++;        &#125;        while(i &lt; j &amp;&amp; r[i]%2 == 0)&#123;            i++;        &#125;        if(i &lt; j)&#123;            r[j] = r[i];            j++;        &#125;    &#125;    r[i] = x;&#125;# S 是栈，Q 是队列，将队列中的元素逆置\nvoid Inverser(Stack &amp;S,Queue &amp;Q)&#123;    int x;    while(!QueueEmpty(Q))&#123;        x = DeQueue(Q);        Push(S,x);    &#125;    while(!StackEmpty(S))&#123;        Pop(S,x);        EnQueue(Q,x);    &#125;&#125;","categories":["DS"],"tags":["数据结构"]},{"title":"Dubbo快速入门","url":"/Java/DubboStart/","content":"# RPC 协议远程调用的几种实现\n# Dubbo 简介\nDubbo 是阿里巴巴公司开源的一个高性能、轻量级的 Java RPC 框架。\n致力于高性能透明化的透明化的 RPC 原创服务调用方案，以及 SOA 服务治理方案。\n\n面向接口的远程方法调用（RPC）：像调用本地方法一样调用远程服务，对开发者透明，降低了分布式服务调用的复杂度。\n智能容错和负载均衡：提供了多种容错策略（如失败自动切换、失败安全等）和内置的负载均衡算法（如随机、轮询、最少活跃调用数等），保证了服务的高可用性。\n服务自动注册与发现：服务提供者向注册中心注册自己的服务，消费者从注册中心发现服务地址列表，并能感知服务的上下线，实现软负载。\n高度可扩展性：几乎所有组件（如协议、序列化、传输、注册中心等）都是可插拔的，允许用户根据自身需求进行定制和扩展。\n运行期流量调度：可以在后台进行路由规则、配置规则的调整，实现灰度发布、权重路由等精细化的流量控制。\n可视化服务治理：提供丰富的服务治理功能，如服务查询、服务测试、服务 Mock、依赖分析、健康度检查等，通常通过 Dubbo Admin 控制台进行操作。\n\nDubbo 的架构主要包含以下几个角色：\n\nProvider：服务提供者。发布服务到注册中心。\nConsumer：服务消费者。从注册中心订阅服务，并调用提供者。\nRegistry：注册中心。负责服务的注册与发现。\nMonitor：监控中心。统计服务的调用次数和调用时间等监控信息。\nContainer：服务运行容器。负责启动、加载、运行服务提供者。\n\n调用流程：\n\n启动阶段：服务容器启动、加载并运行 Provider。Provider 在启动时，会向 Registry 注册自己提供的服务。\n订阅阶段：Consumer 在启动时，向 Registry 订阅自己所需要服务的地址列表。Registry 会将提供者地址列表返回给 Consumer，同时 Consumer 会与 Registry 建立动态监听，以便感知服务变化。\n调用阶段：\n\nConsumer 根据负载均衡算法，从本地缓存的服务地址列表中，选择一个 Provider 进行调用。\n调用时，Consumer 的接口代理会将调用信息（接口名、方法名、参数等）进行序列化，通过网络传输给 Provider。\nProvider 收到请求后，反序列化数据，通过反射调用本地实现，并将结果返回给 Consumer。\n\n\n监控：Consumer 和 Provider 在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到 Monitor。\n\n# 实现 1 传统 RPC ：Dubbo + Protobuf\n","categories":["Dubbo"],"tags":["java","微服务"]},{"title":"SpringCloud学习记录","url":"/Java/SpringCloudSTU/","content":"# SpringCloud\n# 从单体到集群再到分布式\n早期阶段，单体架构是主流选择，所有功能模块打包在一个应用中，开发简单直接，但是随着业务增长，代码变得臃肿，难以扩展特定功能模块，技术栈单一，难以采用新技术。\n为了应对单体架构的性能瓶颈和高可用需求，集群架构应运而生。\n实现方式：\n\n水平扩展：部署多个相同的单体应用实例\n通过负载均衡器 (Nginx、F5 等) 分配请求\n共享数据库或数据库主从复制\n\n但是仍然有缺陷，比如应用本身仍然是单体，业务复杂时扩展不灵活。\n此时分布式架构与微服务应运而生，分布式架构通过将系统拆分为多个服务来解决上述问题。\n本次学习使用尚硅谷 b 站开放课堂：https://www.bilibili.com/video/BV1UJc2ezEFU\n框架（组件）学习与本套课程高度重合，但并不是课程资料的再复写。\n相关技术：\n\nNacos（注册中心、配置中心）来自 Spring Cloud Alibaba\nOpenFegin（远程调用）来自 Spring Cloud 官方\nSentinel（异常处理、流控规则、熔断规则）来自 Spring Cloud Alibaba\nGateway（路由、断言、过滤）来自 Spring Cloud 官方\nSeata（分布式事务）来自 Spring Cloud Alibaba\n\n# Nacos\n\n# 注册中心\n# 服务注册\n首先进行依赖导入\n&lt;!--    nacos 配置中心、注册中心    -->&lt;dependency>    &lt;groupId>com.alibaba.cloud&lt;/groupId>    &lt;artifactId>spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId>&lt;/dependency>使用 docker 或者直接运行的方式启动 Nacos\n在 Windows 平台直接运行下使用命令：startup.cmd -m standalone（standalone 为使用单机模式）\n在不同的服务下编写配置，如订单服务和产品服务：\nspring:  cloud:    nacos:      server-addr: 127.0.0.1:8848  application:    name: service-orderserver:  port: 8080spring:  cloud:    nacos:      server-addr: 127.0.0.1:8848  application:    name: service-productserver:  port: 9000其中 nacos.server-addr 为 nacos 服务的地址为 127.0.0.1:8848（本地测试）\n访问 http://localhost:8848/nacos，在服务管理 - 服务列表可以看到现在已经注册上的服务\n\n\n\n服务名\n分组名称\n集群数目\n实例数\n健康实例数\n触发保护阈值\n操作\n\n\n\n\nservice-order\nDEFAULT_GROUP\n1\n2\n2\nfalse\n详情 | 示例代码 | 订阅者 | 删除\n\n\nservice-product\nDEFAULT_GROUP\n1\n3\n3\nfalse\n详情 | 示例代码 | 订阅者 | 删除\n\n\n\n# 服务发现\n由于使用了 Nacos，服务发现方法的调用存在两套标准，分别是 Spring Cloud 的 DiscoveryClient 和 Nacos 的 NacosServiceDiscovery\n@ResourceDiscoveryClient discoveryClient;@ResourceNacosServiceDiscovery nacosServiceDiscovery;下面为测试代码：\n/** * spring 标准 discovery 使用 DiscoveryClient */@Testpublic void testDiscoveryClient() &#123;    for (String service : discoveryClient.getServices()) &#123;        /*            循环输出服务列表（服务名）                service-order                service-product        */        System.out.println(service);        // 获取所有实例、输出 IP 与端口号        List&lt;ServiceInstance> instances = discoveryClient.getInstances(service);        for (ServiceInstance instance : instances) &#123;            System.out.println(instance.getHost() + \":\" + instance.getPort());        &#125;    &#125;&#125;/** * nacos 标准 discovery 使用 NacosServiceDiscovery */@Testpublic void testNacosServiceDiscovery() throws NacosException &#123;    for (String service : nacosServiceDiscovery.getServices()) &#123;        // 输出服务列表        System.out.println(service);        // 获取所有实例、输出 IP 与端口号        List&lt;ServiceInstance> instances = nacosServiceDiscovery.getInstances(service);        for (ServiceInstance instance : instances) &#123;            System.out.println(instance.getHost() + \":\" + instance.getPort());        &#125;    &#125;&#125;输出：\nservice-order192.168.25.1:8080192.168.25.1:8001service-product192.168.25.1:9000192.168.25.1:9002192.168.25.1:9001如果并没有如此多的输出可能是只启动了两个后端服务，还需要多启动几个来模拟分布式。\n其次，在实际应用中，这个一般会被进一步封装，其发现的过程是自动进行的。\n# 初见，远程调用\n现在有一个实例，我们需要一个下单功能，当用户下单后对其商品进行结算这里我们对一些数据做出模拟。\n订单实体：\n@Datapublic class Order &#123;    private Long id;    private BigDecimal totalAmount;    private Long userId;    private String nickName;    private String address;    private List&lt;Object> product;&#125;商品实体：\n@Data\npublic class Product &#123;\n    private Long id;\n    private BigDecimal price;\n    private String productName;\n    private int num;\n&#125;\n\n其次就是相应和 Controller 与 Service 代码，其较为简单不在此处详细展开，不过我想说一下订单部分的 Service：\n@Servicepublic class OrderServiceImpl implements OrderService &#123;    @Override    public Order createOrder(Long productId, Long userId) &#123;        Order order = new Order();        order.setId(1L);        // TODO 需要计算        order.setTotalAmount(new BigDecimal(\"0\"));        order.setUserId(userId);        order.setNickName(\"Karry.Liu\");        order.setAddress(\"北极\");        // TODO 需要远程查询        order.setProduct(null);        return order;    &#125;&#125;由于 Order 与 Product 分别位于两个服务之中，其详细的金额 Amount 与产品详情列表 Product List 我们目前似乎无法获取，那我们应该怎么办呢？这个我们暂时按下不表，我们现在需要解决一个更加棘手的问题。\n现在我们有如下项目结构（简略版）\n- cloud-demo(基座项目)|| - services(服务层)| | | |  - service-order(订单服务)(包含订单实体bean、服务service和控制controller)| | | |  - service-product(商品服务)(包含商品实体bean、服务service和控制controller)订单服务无法使用商品bean，反之商品服务无法使用订单bean，因为其每个服务均为独立的项目。当 Order 服务需要 Product 服务时，其在 Order 的代码内一定会存在与 Product 相关的关键字，特别地，由于两个服务之间项链紧密，在 Product 的代码内也许也会出现 Order 相关的关键字。可是两套服务分别维护着自己的 bean（实体对象 / 实体类），在不同的服务之间甚至没有办法使用对方的实体类。\n解决方案也很简单，将商品与订单的 Bean 剥离出来，形成一个独立的项目，与 services 等价地位，并在 services 添加 model 依赖。\n&lt;!--    模型依赖    -->&lt;dependency>    &lt;groupId>com.KarryCode&lt;/groupId>    &lt;artifactId>model&lt;/artifactId>    &lt;version>0.0.1-SNAPSHOT&lt;/version>&lt;/dependency>现在的结构为：\n- cloud-demo(基座项目)|| - model(模型层)(包含订单实体bean与商品实体bean)|| - services(服务层)| | | |  - service-order(订单服务)(包含服务service和控制controller)| | | |  - service-product(商品服务)(包含服务service和控制controller)至此，服务之间的实体使用已经被打通。\n其次编写远程访问请求模板类 RestTemplate，由于 RestTemplate 是线程安全的，我们可以这样写：\n@Configurationpublic class ProductServiceConfig &#123;    @Bean    public RestTemplate restTemplate() &#123;        return new RestTemplate();    &#125;&#125;// 上拉使他成为一个 Bean其次编写远程访问方法：\nprivate Product getProductFromRemote(Long productId) &#123;    // 获取商品服务所在的所有机器 IP + 端口    List&lt;ServiceInstance> instances = discoveryClient.getInstances(\"service-product\");    // 获取第一个机器（简单版）    ServiceInstance serviceInstance = instances.get(0);    // 拼接请求地址 http://192.168.25.1:9000/product/100    String url = \"http://\" + serviceInstance.getHost() + \":\" + serviceInstance.getPort() + \"/product/\" + productId;    log.info(\"远程请求: &#123;&#125;\", url);    // 发送请求（远程）    return restTemplate.getForObject(url, Product.class);&#125;补充 createOrder 方法\npublic Order createOrder(Long productId, Long userId) &#123;    // 此处提前远程查询    Product productFromRemote = getProductFromRemote(productId);    Order order = new Order();    order.setId(1L);    // 计算    order.setTotalAmount(productFromRemote.getPrice().multiply(new BigDecimal(productFromRemote.getNum())));    order.setUserId(userId);    order.setNickName(\"Karry.Liu\");    order.setAddress(\"北极\");    // 远程查询的结果    order.setProduct(List.of(productFromRemote));    return order;&#125;此时双端服务已经打通了，注意到日志：2025-06-24T22:21:11.646+08:00  INFO 3496 — [service-order] [nio-8080-exec-1] c.K.service.impl.OrderServiceImpl        : 远程请求: http://192.168.25.1:9000/product/100\n&#123;  \"id\": 1,  \"totalAmount\": 198,  \"userId\": 2,  \"nickName\": \"Karry.Liu\",  \"address\": \"北极\",  \"product\": [    &#123;      \"id\": 100,      \"price\": 99,      \"productName\": \"IPhone-100\",      \"num\": 2    &#125;  ]&#125;此时我们还有两个问题，一是我们每次只取了第一个服务器，二是这样写太复杂，没有实现负载均衡。\n针对此第一个问题将在下一小节中解决，第二个问题将在下一个组件中解决。\n# 实现负载均衡 APIs\n首先是使用复杂一点的方式，后面将会介绍使用注解的形式。\n先引入负载均衡环境依赖。\n&lt;!--    负载均衡    -->&lt;dependency>    &lt;groupId>org.springframework.cloud&lt;/groupId>    &lt;artifactId>spring-cloud-starter-loadbalancer&lt;/artifactId>&lt;/dependency># 基于方法的负载均衡\n将 getProductFromRemote 方法改造为 getProductFromRemoteLoadBalancing：\nprivate Product getProductFromRemoteLoadBalancing(Long productId) &#123;    // 获取商品服务所在机器 (负载均衡)    ServiceInstance chooseLoadBalancing = loadBalancerClient.choose(\"service-product\");    // 拼接请求地址 http://192.168.25.1:9000/product/100    log.info(\"服务地址（uri）: &#123;&#125;\", chooseLoadBalancing.getUri());//http://192.168.25.1:9000    String url = \"http://\" + chooseLoadBalancing.getHost() + \":\" + chooseLoadBalancing.getPort() + \"/product/\" + productId;    log.info(\"远程请求: &#123;&#125;\", url);    // 发送请求（远程）    return restTemplate.getForObject(url, Product.class);&#125;多次请求 http://localhost:8080/create 后观察日志：\n服务地址（uri）: http://192.168.25.1:9001远程请求: http://192.168.25.1:9001/product/100服务地址（uri）: http://192.168.25.1:9002远程请求: http://192.168.25.1:9002/product/100服务地址（uri）: http://192.168.25.1:9000远程请求: http://192.168.25.1:9000/product/100可以观察到其负载均衡的使用了不同的端口，下面将介绍注解的形式。\n# 基于注解的负载均衡\n还记得我们之前使用的 RestTemplate 嘛？\n@Resourceprivate RestTemplate restTemplate;我们可以观察到，无论哪种方法，最终都会是去使用 restTemplate.getForObject (…) 这个方法，如果这个方法自己就可以进行负载均衡呢？我们是不是可以少些一点代码？\n改造 ProductServiceConfig 配置类：\n@Configurationpublic class ProductServiceConfig &#123;    @LoadBalanced// 使用负载均衡    @Bean    public RestTemplate restTemplate() &#123;        return new RestTemplate();    &#125;&#125;将 getProductFromRemoteLoadBalancing 方法改造为 getProductFromRemoteLoadBalancingWithAnnotation：\nprivate Product getProductFromRemoteLoadBalancingWithAnnotation(Long productId) &#123;    String url = \"http://service-product/product/\" + productId;    // 发送请求（远程）    return restTemplate.getForObject(url, Product.class);&#125;注意到我们的 url 中出现了 service-product，在由于 restTemplate 被追加了 @LoadBalanced 注解，使得整个 restTemplate 自带有负载均衡的能力，url 传过去的时候 service-product 会被自动替换为 IP+HOST 的形式，替换的结果符合负载均衡。\n# 配置中心\n# 基本用法\n引入依赖\n&lt;!--    配置中心    -->&lt;dependency>    &lt;groupId>com.alibaba.cloud&lt;/groupId>    &lt;artifactId>spring-cloud-starter-alibaba-nacos-config&lt;/artifactId>&lt;/dependency>书写导入配置：nacos:service-order.yml\nspring:  config:    import: nacos:service-order.yml  cloud:    nacos:      server-addr: 127.0.0.1:8848  application:    name: service-orderserver:  port: 8080在 nacos 中设置名为 service-order.yml\norder:  timeout: 120s  autoConfirm: 7d在 controller 加入 @RefreshScope 自动刷新注解\n@Value(\"$&#123;order.timeout&#125;\")private String orderTimeout;@Value(\"$&#123;order.auto-confirm&#125;\")private String orderAutoConfirm;// 获取配置信息@GetMapping(\"/getConfig\")public String getConfig() &#123;    return \"order.timeout:\" + orderTimeout + \" order.autoConfirm:\" + orderAutoConfirm;&#125;访问后得到回应：order.timeout:120s order.autoConfirm:7d\n但是配置中心的依赖导入方法具有广播性，有可能出现其他服务的无法启动问题，因此可以在 yml 中加入：\nspring:  cloud:    nacos:      config:        import-check:          enabled: false来禁用检查。\n# 无感动态刷新\n手动配置很麻烦，通常使用统一导入的方法来实现，创建 OrderProperties：\n@Data@Component// 批量获取配置，无需使用 @RefreshScope 即可自动刷新@ConfigurationProperties(prefix = \"order\")public class OrderProperties &#123;    String timeout;    String autoConfirm;&#125;@ConfigurationProperties (prefix = “order”) 中 prefix = &quot;order&quot; 获取前缀为 order 的配置，有了 ConfigurationProperties 无需使用 @RefreshScope 即可自动刷新。\n改造 OrderController：\n@Autowiredprivate OrderProperties orderProperties;// 获取配置信息@GetMapping(\"/getConfig\")public String getConfig() &#123;    return \"order.timeout:\" + orderProperties.getTimeout() + \" order.autoConfirm:\" + orderProperties.getAutoConfirm();&#125;# 本地配置与 Nacos 配置冲突时\n当本地配置与 Nacos 配置冲突时，优先以 Nacos 中的配置中心为准。\n即先导入优先，外部优先。\n当：import: nacos:service-order.yml,nacos:common.yml 出现时，仍然是优先以第一次出现的 nacos:service-order.yml 为准。\n# 数据隔离\n当出现不同环境需要不同配置时，比如 dev 环境、test 环境和 prod 环境，分别需要不同的配置，我们该如何组织？\n首先 Nacos 提供了：命名空间 - 组织 - 配置单元的模式，命名空间可以对应到 dev 环境、test 环境和 prod 环境等，组开源对应到不同的微服务比如商品微服务、用户微服务等，配置单元即为具体的详细配置。\n首先在 Nacos 命名空间、组织与配置。\n\n这里已经创建了 dev 环境、test 环境和 prod 环境，下面创建详细的组与配置\n\n然后改造 yml 配置文件\nspring:  profiles:    active: dev  cloud:    nacos:      server-addr: 127.0.0.1:8848      config:        namespace: $&#123;spring.profiles.active:dev&#125;  application:    name: service-orderserver:  port: 8080---spring:  config:    activate:      on-profile: dev    import:      - nacos:common.yml?group=order      - nacos:database.yml?group=order---spring:  config:    activate:      on-profile: test    import:      - nacos:common.yml?group=order      - nacos:database.yml?group=order---spring:  config:    activate:      on-profile: prod    import:      - nacos:common.yml?group=order      - nacos:database.yml?group=order\n具体地：\nnamespace: $&#123;spring.profiles.active:dev&#125;这里主要负责的时从项目到 Nacos 时我们应该选择哪套命名空间，是 Nacos 的命名空间\n而对于：\nprofiles:  active: dev主要负责的是要激活哪套分片配置，是 on-profile: dev 还是 on-profile: test，还是 on-profile: prod，这里指的是项目的配置分片\n此时只需要切换不同的 active: dev，即可完成不同环境的配置切换\n# Nacos 总结\n\n来自尚硅谷课堂：https://www.bilibili.com/video/BV1UJc2ezEFU\n# OpenFeign\nOpenFeign 是 Spring Cloud 生态系统中的一个重要组件。Spring Cloud 是一个基于 Spring Boot 实现的分布式系统开发工具，它提供了一系列的工具来简化分布式系统开发，包括服务注册与发现、配置中心、断路器等功能，OpenFeign 默认集成了 Ribbon 负载均衡器。\n# 远程调用\n# 导入注解\n&lt;!--    openfeign 远程调用    -->&lt;dependency>    &lt;groupId>org.springframework.cloud&lt;/groupId>    &lt;artifactId>spring-cloud-starter-openfeign&lt;/artifactId>&lt;/dependency># 创建 feign 包与 XXXClient 接口\n@FeignClient(value = \"service-product\")public interface ProductFeignClient &#123;    //mvc 注解使用的两套逻辑，放在 Controller 上是接收请求，放在 FeignClient 上是发送请求    @GetMapping(\"/product/&#123;productId&#125;\")    Product getProductById(@PathVariable String productId);&#125;这里的 value = &quot;service-product&quot; 指的是给那个微服务发送请求，这里还标注了 GetMapping，指的是给那个微服务发送请求的路径是什么，其明确了接口。\n# 改造订单创建方法\n@Override    public Order createOrder(Long productId, Long userId) &#123;//        Product productFromRemote = getProductFromRemoteLoadBalancingWithAnnotation(productId);        Product productFromRemote = productFeignClient.getProductById(productId);        Order order = new Order();        order.setId(1L);        // TODO 需要计算        order.setTotalAmount(productFromRemote.getPrice().multiply(new BigDecimal(productFromRemote.getNum())));        order.setUserId(userId);        order.setNickName(\"Karry.Liu\");        order.setAddress(\"北极\");        // TODO 需要远程查询        order.setProduct(List.of(productFromRemote));        return order;    &#125;# 也可以向外部（第三方）做出请求\n比如下面是一个针对天气数据接口的代码实现\n@FeignClient(value = \"weather-client\", url = \"http://apis.juhe.cn\")public interface WeatherFeignClient &#123;    @GetMapping(\"/simpleWeather/query\")    String getWeatherByCityId(@RequestParam(\"city\") String city,                              @RequestParam(\"key\") String key);    default String getEncodedCity(String city) throws Exception &#123;        return URLEncoder.encode(city, StandardCharsets.UTF_8);    &#125;&#125;# 小技巧（懒狗模式）\n如果是要访问自己的设计的接口，着通常是业务接口，比如一个订单服务要访问商品服务，最简单的方式就是将，商品的 Controller 下的接口找到，然后直接复制方法名（以接口的方式进行复制就可以），然后把他放在地点服务的 OpenFegin 接口下就可以了，值得注意的是，OpenFegin 已经帮我们做好了负载均衡，相比上述 Nacos 中那种请求模板方便多了。\n具体地如下面的操作所示：\n\n\n找到商品服务接口的代码\n@Slf4j@RestControllerpublic class ProductController &#123;    @Autowired    private ProductService productService;    @GetMapping(\"/product/&#123;productId&#125;\")    public Product getProduct(@PathVariable Long productId) &#123;        log.info(\"查询商品信息: &#123;&#125;\", productId);        return productService.getProductById(productId);    &#125;&#125;\n\n就像接口的方式去复制代码\n@GetMapping(\"/product/&#123;productId&#125;\")public Product getProduct(@PathVariable Long productId)\n\n粘贴到订单服务之中\n@FeignClient(value = \"service-product\")public interface ProductFeignClient &#123;    //mvc 注解使用的两套逻辑，放在 Controller 上是接收请求，放在 FeignClient 上是发送请求    @GetMapping(\"/product/&#123;productId&#125;\")    Product getProductById(@PathVariable Long productId);        @GetMapping(\"/product/&#123;productId&#125;\")    public Product getProduct(@PathVariable Long productId);&#125;可以观察到，粘贴到 ProductFeignClient 中的代码与我们之前自己定义的代码片段基本是完全一致，所以这是一个非常好用的小技巧。\n\n\n# 面试题\n问：客户端的负载均衡和服务端负载均衡有不用？\n答：首先对于客户端的负载均衡来说，客户端服务会先去访问注册中心，首先获取到一些地址，然后选择一个地址，最后发起调用，这个过程完全是发生在客户端方面的，但是对于服务端的负载均衡来说，这个服务端只对外部暴露一个服务接口，那么所有的请求都需要通过这个唯一的接口来访问。然而，在这个接口背后有着一套负载均衡的逻辑，接口背后运行着许多的服务，而具体使用哪个，由不同的负载均衡算法决定，这一过程放生在服务端。\n# 日志\n可以在 Configuration 类下加入以下代码：\n@BeanLogger.Level feignLoggerLevel() &#123;    return Logger.Level.FULL;&#125;在 yaml 下加入远程调用包的日志配置：\nlogging:  level:    com.KarryCode.feign: debug再次请求即可观察到日志输出，可以看到是怎么请求的。\n# 超时控制\n分别有连接超时和读取超时，通过其源码实现可以观察到，连接超时是 10 秒，而读取超时是 60 秒。我们仍然可以通过配置来修改，这个具体的时间。\n\n这里说一个题外话，我们可以使用多个配置文件来让其生效，比如我这里有一个 application-feign.yml，我们还有一个主文件 application.yml，如果我们想让 application-feign.yml 生效的话，我们可以在主配置文件加上这样的一句话：include: feign\nspring:  profiles:    active: dev    include: feign\n其配置方式如下所示：\nspring:  cloud:    openfeign:      client:        config:          default:            connectTimeout: 3000            readTimeout: 5000            logger-level: full# 重试机制\n默认情况下，OpenFeign 的重试策略是从不重试（是的没错），我们可以手动启动这个重试策略，其具体的有：我们可以设计间隔 100 毫秒，最大间隔 1 秒。最大尝试 5 次。类似计网里面的退避算法\n同样在相应的配置下面加入这样的代码：\n@BeanRetryer feignRetryer() &#123;    return new Retryer.Default();&#125;这是一个默认的重试器，其重试规则正如我上面所说那样：我们可以设计间隔 100 毫秒，最大间隔 1 秒。最大尝试 5 次。\n# 拦截器\n我们可以创建一个请求拦截器：\n@Componentpublic class XTokenRequestInterceptor implements RequestInterceptor &#123;    @Override    public void apply(RequestTemplate requestTemplate) &#123;        System.out.println(\"拦截器启动\");        requestTemplate.header(\"X-Token\", UUID.randomUUID().toString());    &#125;&#125;然后再接收请求时，我们可以解析一下：\n@GetMapping(\"/product/&#123;productId&#125;\")public Product getProduct(@PathVariable Long productId, HttpServletRequest request) &#123;    System.out.println(\"token:\" + request.getHeader(\"X-Token\"));    log.info(\"查询商品信息: &#123;&#125;\", productId);    return productService.getProductById(productId);&#125;可以看到 Token 的输出：token:ebf2c253-7e63-483a-92bc-fc9d1819a418。\n# Fallback 兜底返回\n兜底返回机制需要配合我们还没有学到的 sentinel 框架，首先引入依赖：\n&lt;!--    sentinel    -->&lt;dependency>    &lt;groupId>com.alibaba.cloud&lt;/groupId>    &lt;artifactId>spring-cloud-starter-alibaba-sentinel&lt;/artifactId>&lt;/dependency>加入配置：\nfeign:  sentinel:    enabled: true在服务发起端加入兜底策略：\n@Componentpublic class ProductFeignClientFallback implements ProductFeignClient &#123;    @Override    public Product getProductById(Long productId) &#123;        Product product = new Product();        product.setId(productId);        product.setPrice(new BigDecimal(\"0\"));        product.setProductName(\"不到啊-\" + productId);        product.setNum(2);        return product;    &#125;&#125;关闭 Product 服务，只保留 Order 服务，可以看到返回了兜底数据：\n&#123;  \"id\": 1,  \"totalAmount\": 0,  \"userId\": 777,  \"nickName\": \"Karry.Liu\",  \"address\": \"北极\",  \"product\": [    &#123;      \"id\": 888,      \"price\": 0,      \"productName\": \"不到啊-888\",      \"num\": 2    &#125;  ]&#125;# Sentinel\n\n\n# 基础场景\n@SentinelResource(value = \"createOrder\")添加上此注解，流量即被监控。\n\n添加流控规则：\n\n每秒只放行一次请求，如果你请求多了就会被打回。\nFail to send:http://localhost:8081/create?productId=777&amp;userId=1Blocked by Sentinel (flow limiting)这是一个默认错误页面，我们能不能返回一个高度自定义的数据呢？\n但是可以的！这涉及到 Sentinal 的异常处理机制。\n# 异常处理\n\n\nFlow Exception：流控异常\nParam Flow Exception：热点参数异常\nDegrade Exception：熔断降级异常\nAuthority Exception：权限控制类异常\nSystem Block Exception：系统阻塞异常\n\n上述说的问题，我们需要自定义一个异常。\n@Componentpublic class MyBlockException implements BlockExceptionHandler &#123;    private ObjectMapper objectMapper = new ObjectMapper();    @Override    public void handle(HttpServletRequest httpServletRequest,                       HttpServletResponse httpServletResponse,                       String resourceName, BlockException e) throws Exception &#123;        R error = R.error(500, resourceName + \"被限流了\", e.getMessage());        httpServletResponse.setContentType(\"application/json;charset=utf-8\");        PrintWriter writer = httpServletResponse.getWriter();        String json = objectMapper.writeValueAsString(error);        writer.write(json);    &#125;&#125;\n这个时候，如果你给 createOrder 方法添加流控规则的话，你会发现这个我们之前定义的 json 不生效了。\nFail to send:http://localhost:8081/create?productId=777&amp;userId=1&lt;html>&lt;body>&lt;h1>Whitelabel Error Page&lt;/h1>&lt;p>This application has no explicit mapping for /error, so you are seeing this as a fallback.&lt;/p>&lt;div id='created'>Wed Nov 05 16:15:00 CST 2025&lt;/div>&lt;div>There was an unexpected error (type=Internal Server Error, status=500).&lt;/div>&lt;/body>&lt;/html>这是因为它的异常类型不同，才导致的这样的结果。\n# 流控规则\n阈值类型：\n\nQPS：每秒请求数\n并发线程数\n\n我们优先推荐使用 QPS。\n是否集群：\n\n单机均摊\n总体阈值\n\n流控模式：\n\n直接模式\n关联模式\n链路模式\n\n流控效果\n\n快速失败\nWarm Up\n排队等待\n\n\n\n# 熔断降级\n\n# Gateway\n# 参考文献\n\n【尚硅谷 SpringCloud 速成】https://www.bilibili.com/video/BV1UJc2ezEFU\n\n","categories":["SpringCloud"],"tags":["java","微服务"]},{"title":"文章翻译","url":"/English/shEnglish/","content":"# 英语文章翻译\n# C1P1\nIt’s hard to imagine meeting someone for the first time and not exchanging any personal information.\n\n很难想象，与某人第一次见面却没有交换任何个人信息。\n\nAt the very least, you offer your name and a few important facts - perhaps age, occupation,reason for joining a certain organization, or reason for attending a certain class.\n\n至少你会提供你的名字和一些重要的个人情况比如年龄、职业、加入某个组织或者参加某个课程的原因。\n\nAs friendships develop, however, the answer to the question “Who are you?” becomes more complex.\n\n然而随着友谊的发展，“你是谁” 这个问题的答案会变得越来越复杂\n\nOur identities start to form when we are children and continue to grow,solidify, and even change as we mature.\n\n我们的身份认同从孩童时代开始形成，并随着我们的成长而不断发展和巩固，甚至会随着我们的成熟而发生变化。\n\nA person’s identity is actually made up of many different aspects, some broad and some narrow.\n\n一个人的身份实际上由许多不同的方面构成，有些方面宽泛，有些方面则具体。\n\nFor instance, you might identify with the broad categories of “German,”“male,” and “student” as well as the narrower ones of&quot;violinist,&quot;“left-handed person,” and “brother of Anna.”\n\n例如，你可能会认同 “德国人”、“男性” 和 “学生” 这些宽泛的类别，以及 “小提琴手”、“左撇子” 和 “安娜的哥哥” 这些更具体的类别\n\nIdentity traits can be ascribed, achieved or chosen.\n\n身份特质可以是赋予的、获得的或选择的。\n\nAn ascribed trait is one that you are born with; examples include your ethnicity, your birthplace, and being the child and possibly the sibling of certain people.\n\n被赋予的是你与生俱来的特质；例如，你的种族、出生地，以及作为某些人的子女甚至可能是兄弟姐妹。\n\nAn achieved trait is one you work for, such as being a university graduate or the employee of a certain company. An identity such as a club membership or affiliation with a political party is chosen.\n\n获得性特质是你努力追求的，比如大学毕业或成为某公司的员工。而身份则是你选择的，比如加入某个俱乐部或成为某个政党的成员。\n\nHowever, traits are not always so easy to categorize.\n\n然而，特征并不总是那么容易归类。\n\nIs speaking your native language, for example, ascribed (because you were born into the family and country where that language was spoken), achieved (because you studied the language and became more proficient), or even chosen (if you grew up in a multilingual country, but preferred one language over another)?\n\n例如，说你的母语，是因为你出生在讲这种语言的家庭和国家，所以被赋予了这种语言？还是因为你学习了这种语言并变得更加熟练，所以掌握了这种语言？或者，如果你在一个多语言国家长大，但更喜欢一种语言而不是另一种语言，那么你是主动选择了这种语言？\n\nOur identities are important not only because they shape our belief in who we are, but also because they impact how others treat us.\n\n我们的身份之所以重要，不仅是因为它们塑造了我们对自我身份的认知，更是因为它们影响着他人对我们的态度。\n\nAlthough traits can be positive (intelligent; loyal) or negative (stubborn;criminal), people are more affected by how similar or different their traits are compared to those of other people.\n\n尽管特质可以是积极的（如聪明、忠诚）或消极的（如固执、犯罪），但人们更容易受到自己特质与他人特质相似或相异程度的影响。\n\nFor example, if you are a fan of the Falcons sports team, you have something in common with other Falcons fans.\n\n例如，如果你是猎鹰队的球迷，那么你和其他猎鹰队的球迷就有共同之处。\n\nThe next time you go to an event or social gathering, watch how people who are strangers at first try to find something in common with the people they meet-perhaps a shared hometown,a similar occupation or hobby, or even the same opinion about the weather that day or a current event\n\n下次你去参加活动或社交聚会时，观察一下那些起初是陌生人的人是如何试图找到与他们遇到的人的共同点的 —— 也许是共同的家乡、相似的职业或爱好，甚至是当天对天气或当前事件的相同看法\n\nPeople don’t just define themselves as who they are, however; theyalso define themselves as who they are not.\n\n然而，人们不仅仅定义自己是谁，他们还定义自己不是谁。\n\nThat is to say, they aren’t just fans of the Springfield High School basketball team; they are also not fans of the Pleasant Valley High School basketball team.\n\n也就是说，他们不仅是斯普林菲尔德高中篮球队的粉丝，同时也不是普莱森特谷高中篮球队的粉丝\n\nA friendly rivalry between two sports teams isn’t necessarily a bad thing, but when rivalries are taken too far or tensions arise over differences about larger social issues, the consequences can be more serious.\n\n两支运动队之间友好的竞争并不一定是坏事，但当竞争过度或因更大的社会问题上的分歧而引发紧张局势时，后果可能会更加严重。\n\nInterestingly, groups that have a lot in common sometimes form the most intense separate identities.\n\n有趣的是，有很多共同点的群体有时会形成最强烈的独立身份。\n\nTo someone who doesn’t use a computer at all, they might all seem very similar.\n\n对于一个根本不使用电脑的人来说，它们可能看起来都很相似。\n\nHowever, debates over the best brands of laptop can become quite heated.\n\n然而，关于笔记本电脑最佳品牌的争论可能会变得相当激烈。\n\nPeople form different groups over whether they preferred a book or movie adaptation; which brand of cell phone they prefer; which leader in the same political party they support.\n\n人们会因为喜欢某本书或某个电影的改编、某个手机品牌、或因支持同一党内领导人而形成不同群体。\n\nStates or cities that are near each other can be stronger rivals than those separated by greater distances.\n\n相距较近的州或城市可能比相距较远的州或市更强大。\n\nRather than confirming the positive effects of social identity, these rivalries can make people feel insecure, threatened, angry, or even fearful.\n\n这些竞争非但没有证实社会认同的积极影响，反而会让人们感到不安全、威胁、愤怒，甚至恐惧。\n\nThe challenge, then, for both leaders and all of us in society is to foster the positive effects of group membership while avoiding the negative ones.\n\n因此，对于领导者和我们社会中的所有人来说，挑战在于培养群体成员的积极影响，同时避免消极影响。\n\n# C1P2\nYou know the old saying: You can’t teach an old dog new tricks.\n\n你知道一句老话：老狗学不了新把戏。\n\nIt’s no surprise that we tend to believe that a person’s personality is stable.\n\n我们倾向于相信一个人的性格是稳定的，这并不奇怪。\n\nPeople might disagree about whether someone is born with a certain personality or develops a personality while growing up,but it’s commonly accepted that someone’s personality will be much the same at age 50 as it was at age 20.\n\n人们可能不同意一个人是天生具有某种性格，还是在成长过程中发展出某种性格，但人们普遍认为，一个人的性格在 50 岁时与 20 岁时基本相同。\n\nBoth in our personal lives and our work lives, we’re told that we need to accept people the way they are and to learn to get along with other people even when they’re difficult.\n\n在我们的个人生活和工作生活中，我们被告知，我们需要接受人们的现状，学会与他人相处，即使他们很难相处。\n\nAfter all, they’re never going to change.\n\n毕竟，他们永远不会改变。\n\nNew evidence,however, suggests that this isn’t true.\n\n然而，新的证据表明，事实并非如此。\n\nPublished in the journal Psychology and Aging, a comprehensive study by four psychologists examined a group of Scottish volunteers over a period of 63years, making it the longest study of its type ever done.\n\n发表在《心理学与衰老》杂志上的一项由四位心理学家进行的综合研究对一组苏格兰志愿者进行了为期 63 年的调查，使其成为有史以来最长的同类研究。\n\nAnd what they found was unexpected: namely, no correlation at all between the participants’ scores on personality tests when they were 14 years old and the same tests when they were 77 years old.\n\n他们的发现出乎意料：即参与者 14 岁时的性格测试得分与 77 岁时的相同测试得分之间根本没有相关性。\n\nThe test examined six areas: self-confidence,perseverance, stability of moods, conscientiousness,originality, and desire to learn.\n\n该测试考察了六个方面：自信、毅力、情绪稳定性、责任心、原创性和学习欲望。\n\nThe original study involved 1,208 children, and 174 of them were available for the follow-up study six decades later.\n\n最初的研究涉及 1208 名儿童，其中 174 名儿童在 60 年后可用于后续研究。\n\nBecause it’s not reliable to have people rate themselves, the participants were evaluated in these categories by other people-by teachers when they were 14, and by friends or relatives when they were 77.\n\n因为让人们对自己进行评分是不可靠的，所以参与者在 14 岁时由其他人进行评估，在 77 岁时由朋友或亲戚进行评估。\n\nThey were also tested for intelligence and general well-being.\n\n他们还接受了智力和总体小方格状况的测试。\n\nThe researchers were surprised to find that none of the ratings matched up with each other over the years.\n\n研究人员惊讶地发现，多年来，这些评级都不匹配。\n\nEarlier studies and tests produced somewhat different outcomes.\n\n早期的研究和测试产生了一些不同的结果。\n\nResearch suggested a few character traits had a low correlation over time and others had a modest correlation.\n\n研究表明，随着时间的推移，一些性格特征的相关性较低，而另一些则相关性适中。\n\nThe Scottish study, although smaller in scope because it involved fewer participants, measured them over a much longer period of time.\n\n苏格兰的这项研究尽管规模不大，但其时间跨度更长。\n\nThis led there searchers to conclude that personality shifts are more likely to occur over long periods of time.\n\n这导致研究人员得出结论，人格转变更有可能在很长一段时间内发生。\n\nNow, it’s not a perfect study,of course; such a thing is rare, if not impossible, with human beings and personality.\n\n当然，这并非一项完美的研究；对于人类和性格而言，这样的研究实属罕见，甚至可以说是不可能的。\n\nFor instance, the people who did the ratings in 1950 were not the same people who did the ratings in 2012, and this could have caused some difference.\n\n例如，1950 年进行评级的人与 2012 年进行评级的人并非同一拨人，这可能会造成一些差异。\n\nIt’s difficult for a study on something as broad as identity and personality to take all the variables into consideration.\n\n对于像身份和性格这样广泛的研究课题来说，要将所有变量都考虑在内是很困难的。\n\nHowever, the results are still significant, and they have interesting implications.\n\n然而，结果仍然是重要的，他们有有趣的启示。\n\nLet’s consider some of those implications for a moment. What does it all mean? And is it only of academic interest, or can you yourself apply this knowledge to your own life?\n\n让我们考虑一下其中的一些含义。这一切意味着什么？这仅仅是学术上的兴趣，还是你能把这些知识应用到你自己的生活中？\n\nFor one thing,it should give you a new way to think about other people.\n\n首先，它应该给你一种思考他人的新方式。\n\nFor example, say you’re contacted on social media by someone you knew in school years ago. If you didn’t like the person at that time, you might be tempted to refuse the connection.\n\n例如，假设你在社交媒体上被你多年前在学校认识的人联系。如果你当时不喜欢这个人，你可能会想要拒绝这段关系。\n\nIf you didn’t like each other then, after all, why would you like each other now?\n\n毕竟，如果你们当时不喜欢对方，为什么现在会喜欢对方呢？\n\nBut if it’s true that personality can change, then there’s a reason to give that person another chance.\n\n但如果性格真的可以改变，那就有理由再给那个人一次机会。\n\nHe or she might be very different now -and you might beto0.\n\n他或她现在可能和以前大不相同了 —— 你也可能是。\n\nYou might also have more reasonable expectations of old childhood friends who reconnect after many years.\n\n你也可能对多年后重新联系的童年老朋友有更合理的期望。\n\nIf you know their personalities (and yours) could have changed over the years, you’ll be less disappointed if your friendship isn’t as deep now as it was before.\n\n如果你知道他们（和你自己）的性格可能会随着时间的推移而改变，那么如果你们的友谊没有以前那么深厚，你就不会那么失望了。\n\nRather than feel frustrated with yourselves, the two of you can accept that you have changed.\n\n与其对自己感到沮丧，不如接受自己已经改变的事实。\n\nThe study has implications for the workplace too.\n\n这项研究对工作场所也有影响。\n\nPersonality forms a large part of a worker’s suitability for a job, both in dealing with co-workers and in dealing with clients.\n\n性格在很大程度上决定了一个人是否适合一份工作，无论是在与同事打交道还是与客户打交道时。\n\nIf a person has a personality trait that interferes with work-say he argues with customers or she misses deadlines-it’s important for managers to know that these traits can change.\n\n如果一个人的性格特征会干扰工作，比如他会和客户争吵，或者她错过了截止日期，那么管理者就应该知道这些性格特征是可以改变的。\n\nIt’s usually cheaper to train a current employee than to let that person go and hire a replacement.\n\n通常来说，培训一名现有员工要比让他离开并雇佣一个替代者便宜得多。\n\nEven employees who aren’t experiencing problems can be trained to be even better and more effective in terms of personality.\n\n即使没有遇到问题的员工也可以通过培训，在个性方面表现得更好、更有效。\n\nThis will help ensure that people continue to get along with one another.\n\n这将有助于确保人们继续彼此相处。\n\nFinally, there are personal implications.\n\n最后，还有个人影响。\n\nIf you’re the sort of person who says things like &quot;I have a quick temper&quot;or “My problem is I can’t help procrastinating” or &quot;I’ve always been too sensitive, and I blame myself when ever something goes wrong,&quot;it should be good news to know that these personality traits are not ones you have to keep.\n\n如果你是那种会说 “我脾气暴躁” 或 “我的问题是我总是忍不住拖延” 或 “我总是太敏感，一旦出了问题我就会责怪自己” 的人，那么知道这些性格特征并不是你必须保留的，这应该是个好消息。\n\nAlthough some therapists do good work helping patients accept themselves as they are,to build self-esteem,wouldn’t it be more beneficial to eliminate negative personality traits than to learn to accept them?\n\n尽管一些治疗师在帮助病人接受自己的本来面目、建立自尊方面做得很好，但消除消极的人格特征不是比学会接受它们更有益吗？\n\nKnowing that you can change is the first stage in learning how to change.\n\n知道自己可以改变是学习如何改变的第一步。\n\nThen you can look forward to saying things like,“I used to be too sensitive, but I’m not anymore”;or look forward to a time when,as we might start saying, you can learn some new tricks.\n\n然后你就可以期待说这样的话：“我以前太敏感了，但现在不会了。” 或者期待有一天，就像我们开始说的，你可以学到一些新技巧。\n\n# C2P1\nOh, no! You dropped the cup, and it smashed! Time to throw it away and buy a new one.\n\n哦，不！你把杯子掉在地上，打碎了！是时候扔掉它，买一个新的了。\n\nUnless, perhaps, you are a fan of the Japanese art of kintsugi or kintsukuroi–roughly translated,&quot;to mend with gold.&quot;This is the practice among certain craftsmen of mending the broken pieces of pottery, such as a plate, a cup, or a bowl, with gold (or similar) lacquer.\n\n除非你是日本 kintsugi 或 kintsukuroi 艺术的粉丝。这是某些工匠用金（或类似的）漆修补破碎的陶器碎片的做法，例如盘子，杯子或碗。\n\nThe gold is used to glue the pieces back together\n\n金子是用来把碎片粘在一起的\n\nIf small pieces are missing, they can be created out of gold, or a piece from a different bowl or plate can be used instead.\n\n如果少了一小块，可以用黄金制作，或者用另一个碗或盘子里的小块代替。\n\nThe repaired product’s value is not reduced, though-it is actually enhanced.\n\n修复后的产品的价值并没有减少，反而增加了。\n\nIt is believed to become more beautiful because it was broken.\n\n人们认为它因为被打破而变得更加美丽。\n\nPieces of kintsugi pottery can be enormously expensive and are featured in museum exhibits in Japan and overseas.\n\n金杉陶器非常昂贵，在日本和海外的博物馆都有展出。\n\nThese days you can even see machine-made ceramics with gold designs on them that look as if they are kintsugi,even though the original was actually never broken.\n\n如今，你甚至可以看到机器制作的陶瓷，上面有金色的图案，看起来像是金杉，尽管原来的金杉实际上从未破损过。\n\nBut the mended patterns have become so trendy that people want to imitate them.\n\n但是缝补的图案变得如此流行，以至于人们都想模仿它们。\n\nThere’s a story or legend behind the practice - which may or may not be historically accurate, but beautifully illustrates the concept.\n\n这种做法背后有一个故事或传说 —— 可能与历史不相符，但却很好地说明了这一概念。\n\nMore than five hundred years ago, there lived a military ruler in Japan, who owned a bowl he especially loved.\n\n五百多年前，日本有一位军事统治者，他有一只碗，他特别喜欢。\n\nOne day while he was entertaining some guests, his servant dropped the bowl, and it broke into five pieces.\n\n一天，当他招待客人时，他的仆人把碗掉在地上，摔成了五片。\n\nKnowing the leader’s bad temper, his guests worried that he would punish the servant\n\n客人们知道首领的坏脾气，担心他会惩罚仆人\n\nHowever, one of the guests made up an amusing poem about the incident\n\n然而，其中一位客人就这件事编了一首有趣的诗\n\nEverybody laughed, including the ruler.\n\n每个人都笑了，包括统治者。\n\nWhen he relaxed, he was able to see that the bowl’s beauty had not been destroyed by the accident.\n\n当他放松下来时，他能看到碗的美丽并没有被事故破坏。\n\nInstead,because the vessel could be repaired, the ruler now had a new appreciation for its strength and ability to survive.\n\n相反，因为容器可以修复，统治者现在对它的力量和生存能力有了新的认识。\n\nIn fact, according to the story, the true life of the bowl began the moment it was dropped.\n\n事实上，根据这个故事，这个碗的真正生命从它被扔下的那一刻就开始了。\n\nIf this seems a hard notion to understand, then consider it in light of another Japanese philosophy, that of wabi-sabi.\n\n如果这似乎是一个难以理解的概念，那么请根据另一种日本哲学来考虑它，那就是 wabi-sabi。\n\nThis is harder to translate into English, but it refers to the combination of three beliefs that nothing is permanent, nothing is finished, and nothing is perfect.\n\n这句话很难翻译成英语，但它指的是三个信念的结合：没有什么是永恒的，没有什么是完成的，没有什么是完美的。\n\nApplied to arts and crafts, it explains why the Japanese traditionally value handmade objects.\n\n应用于工艺品，它解释了为什么日本传统上重视手工制品。\n\nEven though they look less perfect than those made by machine, it is actually this imperfection that makes them beautiful\n\n尽管它们看起来没有机器制作的那么完美，但实际上正是这种不完美让它们变得美丽\n\nIn fact, artists who value the wabi-sabi aesthetic create works that are deliberately imperfect, such as a bowl that isn’t entirely round or a vase with a thumbprint visible in the clay.\n\n事实上，看重 wabi-sabi 美学的艺术家会刻意创造出不完美的作品，比如一个不全圆的碗，或者一个在粘土上可以看到拇指指纹的花瓶。\n\nRough surfaces, instead of ones smoothed by machines, are common in wabi-sabi ceramics, and often the pieces are not glazed or colored.\n\n粗糙的表面，而不是机器光滑的表面，在 wabi-sabi 陶瓷中很常见，而且这些碎片通常没有上釉或着色。\n\nIt’s not just Japan that has such a tradition, however\n\n然而，并非只有日本有这样的传统\n\nA similar idea can be found in Iran, among the makers of Persian rugs.\n\n在伊朗的波斯地毯制造商中也可以找到类似的想法。\n\nTradition has it that those who weave carpets will deliberately include one small flaw,as recognition of the fact that nothing can be perfect.\n\n传统上，编织地毯的人会故意留下一个小瑕疵，因为他们意识到没有什么是完美的。\n\nThe intentional mistake reminds them to be modest about their work.\n\n故意的错误提醒他们对自己的工作要谦虚。\n\nSimilarly, some early American settlers known as the Puritans included a “humility square” when they sewed a quilt -one square that didn’t match the rest of the blanket.\n\n类似地，一些被称为清教徒的早期美国定居者在缝制被子时也会附上一块 “谦卑的方巾”—— 这块方巾与毯子的其他部分不匹配。\n\nSome Native American bead workers would include an intentional “mistake bead” for the same reason.\n\n出于同样的原因，一些印第安人的头饰工人会故意加上一个 “错误的头饰”。\n\nSuch practices have also been reported among Amish furniture makers in the United States and some forms of Islamic art - although careful work by sociologists and historians suggests that these stories are actually not true, but rather a romanticized version of their art or a misunderstanding of a tradition.\n\n在美国的阿米什家具制造商和某些形式的伊斯兰艺术中也有类似的报道 —— 尽管社会学家和历史学家的仔细研究表明，这些故事实际上不是真实的，而是他们艺术的浪漫化版本，或者是对传统的误解。\n\nTrue or not,however, these cultural practices teach us not only about art but about life, and the importance of not only accepting, but actually celebrating, our imperfections.\n\n无论真假，这些文化实践不仅教会了我们艺术，也教会了我们生活，教会了我们不仅要接受，而且要赞美我们的不完美的重要性\n\nThat doesn’t mean we shouldn’t care about making mistakes; but for many people, worrying about small imperfections keeps them from finishing a project or appreciating one they have finished.\n\n这并不意味着我们不应该在意犯错误；但对很多人来说，担心小瑕疵让他们无法完成一个项目，也无法欣赏他们已经完成的项目。\n\nPeople who are “perfectionists” can feel insecure and anxious about the art they create, which makes it harder for them to enjoy what they do.\n\n“完美主义者” 会对自己创造的艺术感到不安和焦虑，这使得他们更难享受自己所做的事情。\n\nThe concept can even be applied more broadly than just to art, however.\n\n然而，这个概念甚至可以应用于更广泛的领域，而不仅仅是艺术。\n\nConsider yourself, for example. Do you have any imperfections - anything from physical scars to personal habits?\n\n以你自己为例。你有什么不完美的地方吗 —— 从身体上的伤疤到个人习惯？\n\nWhat if, instead of considering these to be flaws, you could appreciate them as part of what makes you a beautiful person?\n\n如果你不认为这些是缺点，而是把它们看作是让你成为一个美丽的人的一部分呢？\n\n","categories":["English"],"tags":["英语"]},{"title":"英语笔记","url":"/English/xbbEnglish/","content":"# 领航\n\n核心内容来自颉斌斌老师\n\n# 阅读\n# 常考的隐含假设\n\n\n\n要素\n隐含假设\n\n\n\n\n做了什么\n论述的主体有能力 / 条件可以做\n\n\n建议 / 应该去做一件事情\n这件事还没有做过，作者认为这件事合理\n\n\n为什么 + 观点\n观点成立\n\n\n一个措施有利，应该去做\n同时不会有更大的弊端\n\n\n一个措施不利，应该去做\n同时不会出现更大的利\n\n\n禁止做一件的事情\n这件事已经被做过\n\n\n\n# 选项比错的依据\n\n答案已知且唯一\n选项具有有限性（4 个）\n\n比出一个最佳选择即可，放弃求证每日一个选项的正确于错误\n但，什么是最佳？\n实际上，找对很难， 比错比比对更加容易！\n选项比错是高效的做法\n# 比错的逻辑\n依据精准定位，评估一个出错率最小的选项\n比选项👉相对性👉借助其他选项\n比错👉 比较四个选项错误率的大小\n出错率最小不是完全正确，放弃追求完全正确的执念，转变为 “接受最佳的选项” 的新认识。\n\nWhen you have eliminated the impossible ，whatever remains，however improbable，must be the truth.\n\n# 精准定位 + 选项比错\n# 定位不到的解题思路\n\n反向定位（判断选项信息正误）\n取反 / 取非\n原命题转逆否命题\n主题主旨\n\n# 写作 (大作文)\n# 图画作文\n# 行文框架\n# 第一段（2-3 句）\n\n引出主题\n描述图片\n评价图片（可有可无）\n\n# 第二段（5 句）\n\n重申主题 + 分析影响\n影响 1 + 研究调查\n影响 2 + 名人名言\n\n# 第三段\n\n升华主题\n建议措施\n展望未来（可有可无）\n\n# 引出主题的句子（选一个即可）\nThe animated cartoon reminds its readers of the significance of  (主题词).\n\n看到这副生动的漫画，人们想到了（主题词）的重要性\n\nNothing can get us denying that (主题词), under any circumstances, tends to be of great/enormous importance.\n\n不可否认，在任何情况下，（主题词）往往很重要\n\nYou will not miss thinking that (主题词) is playing an indispensable role in our life and work.\n\n你一定会想到（主题词）在我们的生活和工作中的起着不可或缺的作用（背）\n\n# 描述图片的句子\nThe picture is mainly related to the case that (图片主要人物 / 事物) + （doing/done…）, which can be a vivid expression of the topic.\n\n这一图片主要和… 有关，该图是对主题词的生动表达（背）\n\n# 评价图片（可有可无）\nWhat the drawer tries to convey has profound practical significance.\n\n画家试图传达的信息具有深刻的现实意义。(可以背一下)\n\nSimple as the picture looks, its implicit meaning should be thought–provoking.\n\n尽管图片简单，但其内涵发人深省。\n\nThe cartoon, at first glance, seems to be simple, but the deep meaning behind it is worth pondering.\n\n这幅漫画乍一看似乎很简单，但其深层含义值得深思。\n\n# 重申主题 + 分析影响\nConsidering the significance of（主题词）, it is necessary to point out the far-reaching consequences behind it.\n\n考虑到 (主题词) 的重要性，有必要指出其背后意义深远的影响。\n\nThere seems to be more than one direct or indirect effect involved in (主 题 词) , which none of us can fail to notice.\n\n似乎不止一个直接或间接的影响与 (主题词) 有关，我们无法忽视这些影响。(背)\n\nThe effects that (主题词) can exert are obvious and within easy reach.\n\n(主题词) 产生的影响显而易见，也是不难找到的。\n\n# 影响 1 + 调查研究\n# 万能理由先行\n\n\n成长 / 成功 / 发展\n\n个人话题：the growth /success/prospect of individual\n社会话题：the development of economy /society/culture /legal system /enviroment\n\n\n\n知识 / 阅历 / 机会\n\nacquire adequate knowledge and experience\n\n\n\n效率 / 便利 / 时间\n\nefficiency / convenience\n\n\n\n健康（身体和心理） / 压力 / 愉悦\n\nphysical health / mental health\npressure / stress / stain\nbring a lot of mental and physical pleasure\n\n\n\n# 分析影响\nIt is generally recognized that nothing is more important than (主题词) for (四大万能理由) .\n\n人们普遍认为 (主题词) 对 (四大万能理由) 最重要。（背）\n\n# 调查研究\nDepending upon a rough estimate of the relevant department, nearly 86.2% of the respondents hold the same view.\n\n根据有关部门的粗略估计，近 86.2% 的受访者持相同观点。\nrelevant department 可以替换为具体部门\n\nA recent Internet questionnaire of the relevant department indicates that approximately 86.2% of the respondents hold the same view.\n\n有关部门最近的一份互联网问卷显示，约 86.2% 的受访者持相同观点。（背）\n\n# 影响 2 + 名人名言\n# 分析影响\n(主题词) contributes to/is conducive to/is responsible for (四大万能理由) .\n\n(主题词) 有助于 (四大万能理由) 。\n\n# 名人名言\nAs Envduct Karryguess, director of community service at LA College in LitLingo, puts it, “(万能理由) , needless to say,  enormous importance for us.”\n# 升华主题\nAll these factors support a rational conclusion that great importance should be attached to (主题词)\n\n所有这些因素都支持一个合理的结论，即应该对 (主题词) 给予重视。\n\n# 建议措施\nEveryone should cultivate a better awareness of it and take practical actions.\n\n每个人都应更好地意识到这一点，并采取实际行动。\n\nStrict laws and disciplines can guarantee the smooth running of the society\n\n严格的法律和纪律可以保证社会的平稳运行。\n\n# 展望未来（可有可无）\nOnly in this way would our life become more real and meaningful\n\n只有这样，我们的生活才能变得更加真实和富有意义。\n\n# 原型\n\tYou will not miss thinking that (主题词) is playing an indispensable role in our life   and work.The picture is mainly related to the case that ( 图片主要人物/事物 ) + （doing/done....）, which can be a vivid expression of the topic.\n\tThere seems to be more than one direct or indirect effect involved in ( 主 题 词 ) , which none of us can fail to notice.It is generally recognized that nothing is more important than (主题词) for (四大万能理由) .A recent Internet questionnaire of the relevant department indicates that approximately 86.2% of the respondents hold the same view. (主题词) contributes to (四大万能理由) .As Envduct Karryguess, director of community service at LA College in LitLingo, puts it, “ (万能理由) , needless to say,  enormous importance for us.”\n\tAll these factors support a rational conclusion that great importance should be attached to(主题词). Everyone should cultivate a better awareness of it and take practical actions.\n\n原型净约 136 词\n# 试写（2005. PartB. 52.）\n\tYou will not miss thinking that concern elderly is palying an indispensable role in our life and work. The picture is mainly related to the case that four children would not like to take care their father.\n\tThere seems to be more than one direct or indirect effect involed in concern elderly, which none of us can fail to notice, It is generally recognized that nothing is more important than concern elderly for bring a lot of mental and physical pleasure. A recent Internet questionnaire of the relevant department indicates that approximately 82.6% of the respondents hold the same view. concern elderly contributes to a happy old age. As Envduc Karry, director of community service at LA College in LitLingo, put it,&quot;A happy old age ,needless to say, enormous importance for elderly.&quot;\n\tAll these factors support a rational conclusion that great importance should be attached to concern elderly. Everyone should cultivate a better awareness of it and take pratical actions.\n\n试写仅为作者的练习，不是范文！不是范文！不是范文！可能不会拿到很高的分数，仅供参考。\n共 161 词，虽然针对 2005 年的要求有点捉襟见肘，但对于新标准来说还是游刃有余的。\n# 图表作文\n# 行文框架\n# 第一段（2-3 句）\n\n引出主题\n描述图表\n评价图表（可有可无）\n\n# 第二段（5 句）\n\n重申主题 + 分析原因\n原因 1 + 研究调查\n原因 2 + 名人名言\n\n# 第三段\n\n升华主题\n建议措施\n展望未来（可有可无）\n\n# 引出主题\n（主题）is clearly shown in the picture.\n\n在图表中清楚的表示了（主题）\n\n（主题）is what the chart tries to convey.\n\n(主题) 是图表想要传达的信息\n\nThe chart,in an obvious way, speaks volumes about the phenomenon that (主题).   （背）\n\n这张图表显然表明了一个现象，即（主题）\n\nThe chart above demonstrates clearly that some changes have taken place in terms of（数据标题）\n\n上图清楚地表明，在（数据标题）方面已经发生了一些变化。\n\n# 描述图表\n# 饼状图 / 表格\nX accounts for the highest percentage at x% of （主题词）,followed by B at y% and C ,z%. At the bottom of the chart, k% is regarded as D.\n\nX 占了（主题词）的最高比例即 x%，紧接着是 B 占了 y%，C 占了 z%。D 占了图的最少的一部分，是 k%。\n\n# 柱状图 / 折线图\nThere was a （dramatic increase/a steep drop） in the number of (一个东西) from （数据）in （时间）to（数据） in （时间）.\n\n(一个东西的) 数据出现了急剧的上升 / 严重的的下降，从（时间）的（数据）到（时间）的（数据）。\n\n# 重申主题 + 分析原因\nThe reasons that caused the phenomenon are obvious and within easy reach.\n\n导致这一现象的原因是显而易见的。\n\nAfter observation and consideration, some reasons of （主题词） can be highlight as follows.\n\n经过观察和考虑，将一些原因列举如下。（背）\n\n# 原因 1 + 研究调查\n同图画作文\n# 原因 2 + 名人名言\n同图画作文\n# 升华主题\n同图画作文\n# 建议措施\n同图画作文\n# 原型\n\tThe chart,in an obvious way, speaks volumes about the phenomenon that (主题).X accounts for the highest percentage at x% of （主题词）,followed by B at y% and C ,z%. At the bottom of the chart, k% is regarded as D.\n\tAfter observation and consideration, some reasons of （主题词） can be highlight as follows.It is generally recognized that nothing is more important than (主题词) for (四大万能理由) .A recent Internet questionnaire of the relevant department indicates that approximately 86.2% of the respondents hold the same view. (主题词) contributes to (四大万能理由) .As Envduct Karryguess, director of community service at LA College in LitLingo, puts it, “ (万能理由) , needless to say,  enormous importance for us.”\n\tAll these factors support a rational conclusion that great importance should be attached to(主题词). Everyone should cultivate a better awareness of it and take practical actions.\n\n模板净含约 120 词\n# 文字作文\n# 行文框架\n# 第一段\n\n引出主题\n陈述作者观点和态度\n表明自己的观点\n\n# 第二段\n\n重申主题 + 分析原因 / 影响\n原因 1 / 影响 1 + 例子（调查研究）\n原因 2 / 影响 2 + 例子（名人名言）\n\n# 第三段\n\n升华主题\n建议措施\n展望未来（可有可无）\n\n# 引出主题\nThe phenomenon mentioned in the above excerpt that （主题）is receiving more and more attention and hot discussion.\n\n上述摘录中提到的现象（主题）正受到越来越多的关注并成为热点讨论话题。\n\n# 陈述作者观点和态度\nAs is presented in the excerpt , the author believes that… is of graet significance for …\n\n如摘录所示，作者认为… 对… 具有重要意义\n\n# 表明自己的观点\nAs far as this question is concerned, I support the author’s view.\n\n关于这个问题，我支持作者的观点。\n\n# 原因 1 + 研究调查\n同图画作文\n# 原因 2 + 名人名言\n同图画作文\n# 升华主题\n同图画作文\n# 建议措施\n同图画作文\n# 写作 (小作文)\n# 书信\n# 格式\nDear_______,（&lt;-逗号）\n\t\n\t第一段_____________________________________________\n\t第二段_____________________________________________\n\t第三段_____________________________________________\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t（逗号-&gt;）\tyour sincerely，\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tLi Ming\n\n# 行文结构\n# 前置\n称呼\n# 第一段\n\n表明身份（可有可无）+ 表明主题（重要）\n\n# 第二段\n\n展开主题 + 两点论\n\n# 第三段\n\n重申主题 + 寒暄（可有可无）\n\n# 尾部\n落款\n# 万能身份\nI am president of the Students’ Union\n# 表明主题\nI am writing the letter to you for the purpose of (目的，从”directions“中汲取。doing sth)\n# 展开主题 + 两点论\n参考万能主题\n# 万能结尾\nPlease accept my sincere gratitude to you for reading the content above. If you need to know more about it, please contact me.\n# 模板\nDear_______,\n\tI am president of the Students' Union)(此处视情况变换身份). I am writing the letter to you for the purpose of (目的，从”directions“中汲取。doing sth).\n\tOf all the points concerned. I'd like to name the most significant ones as follows. First and foremost,______(要点一).Additionally,_______(要点二).(此处可再加一个要点&quot;Not but not least&quot;)\n\tplease accept my sincere gratitude to you for reading the content above. If you need to know more about it, please contact me.\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tYours sincerely,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(name)\n\n# 通知\n# 备忘录\n# 翻译\n# 文章一\n# 原文\n\tWe tend to think that friends and family members are our biggest source of connection, laughter and warmth. While that may well be true, researchers have also recently found that interacting with strangers actually brings a boost in mood and feelings of belonging that we didn’t expect.\n\tIn one series of studies, researchers instructed Chicago-area commuters using public transportation to strike up a conversation with someone near them. On average, participants who followed this instruction felt better than those who had been told to stand or sit in silence. The researchers also argued that when we shy away from casual interactions with strangers, it is often due to a misplaced anxiety that they might not want to talk to us. Much of that time, however, this belief is false. As it turns out, many people are actually perfectly willing to talk and may even be flattered to receive your attention.\n\n# 逐句翻译\nWe tend to think that friends and family members are our biggest source of connection, laughter and warmth.\n\n我们往往会认为朋友和家庭成员是我们情感联系、欢声笑语以及幸福温暖的最大来源。\n\nWhile that may well be true, researchers have also recently found that interacting with strangers actually brings a boost in mood and feelings of belonging that we didn’t expect.\n\n尽管那很肯能是真的，但最近研究者们发现与陌生人交往实际上会带来我们没有想到的情绪上的增长以及感觉上的促进（让人心情愉悦、有归属感）\n\nIn one series of studies, researchers instructed Chicago-area commuters using public transportation to strike up a conversation with someone near them\n\n在一系列研究中，研究者们让 Chicago 区域使用公共交通的通勤人员去努力和他们身边的人产生交流\n\nOn average, participants who followed this instruction felt better than those who had been told to stand or sit in silence.\n\n通常来讲，相较于被告知在安静中站着或者坐着的来说，参与者们跟随着这个指示来做会感觉到更好\n\nThe researchers also argued that when we shy away from casual interactions with strangers, it is often due to a misplaced anxiety that they might not want to talk to us.\n\n研究者们也认为，当我们在与陌生人随意的交谈而感到害羞跑开时，那通常是因为不合时宜的焦虑，那就是他们也许不想和我们讲话\n\nMuch of that time, however, this belief is false.\n\n然而，大多数情况下，这种想法是错误的\n\nAs it turns out, many people are actually perfectly willing to talk and may even be flattered to receive your attention.\n\n结果是，许多人实际上非常愿意交谈，甚至会因为得到你的关注而感到荣幸。\n\n# 词汇\n\n\n\n单词\n释义\n\n\n\n\nflattered\nadj.（因受重视而）感到满意的，觉得荣幸的；（在正式场合表示感谢）不胜荣幸的，深感荣幸的 v. 奉承；自命不凡；使显得漂亮，使显得出色（flattered 的过去式和过去分词）\n\n\nmisplaced\nadj. 不合时宜的；（情感）寄托错的；放错位置的；暂时丢失的v. 乱放（而找不到），暂时丢失；错放（misplace 的过去式和过去分词）\n\n\nargued\nv. 争论；争吵；争辩；论证；说理；显示出，表明（argue 的过去式和过去分词）\n\n\ncommuters\nn. 通勤者；每日往返上班者（commuter 的复数）\n\n\nreceive\nv. 得到，收到；遭受，经受（特定待遇）；对…… 作出反应；接待，招待；接收（某人为成员）；接收，收听（信号）；（通过无线电）听到；购买，窝藏（赃物）；接（球）；领受（圣餐面包或葡萄酒）；接受（治疗）；形成（看法，印象）；容纳，承接\n\n\n\n# 文章二\n# 原文\n\tIn the late 18th century, William Wordsworth became famous for his poems about nature.And he was one of the founders of a movement called Romanticism, which celebrated the wonders of the natural world.\n\tPoetry is powerful. Its energy and rhythm can capture a reader, transport them to another world and make then see things differently. Through carefully selected words and phrases, poems can be dramatic, funny, beautiful, moving and inspiring.\n\tNo one knows for sure when poetry began but it has been around for thousands of years,even before people could write. It was a way to tell stories and pass down history. It is closely related to song and even when written it is usually created to be performed out loud. Poems really come to life when they are recited. This can also help with understanding them too, because the rhythm and sounds of the words become clearer\n\n# 逐句翻译\nIn the late 18th century, William Wordsworth became famous for his poems about nature.\n\n在 18 世纪后期，William Wordsworth 因他的有关自然的诗集而远近闻名。\n\nAnd he was one of the founders of a movement called Romanticism, which celebrated the wonders of the natural world.\n\n他是万千浪漫主义运动奠基者的一员，专为自然世界而歌颂。\n\nPoetry is powerful.\n\n诗歌满含力量。\n\nIts energy and rhythm can capture a reader, transport them to another world and make then see things differently.\n\n这种能量和神话能够俘获读者芳心，带领他们进入另外一个世界，使之他们以不同的眼光看待世界。\n\nThrough carefully selected words and phrases, poems can be dramatic, funny, beautiful, moving and inspiring.\n\n通过在词汇和语段上的精挑细选，诗歌可以表现出戏剧的，有趣的、华丽的、感动的以及鼓舞人心的情感。\n\nNo one knows for sure when poetry began but it has been around for thousands of years,even before people could write.\n\n没有人确切地知道诗歌始于何时，但其确实萦绕千年之久，甚至在人们学会书写之前就已经出现了。\n\nIt was a way to tell stories and pass down history.\n\n这曾经是讲述故事和传承过去历史的一种方式\n\nIt is closely related to song and even when written it is usually created to be performed out loud.\n\n他与歌曲密切相关，即使写成它，它通常也是为了大声表演而创作的。\n\nPoems really come to life when they are recited.\n\n诗歌吟诵时才正真背赋予了生命。\n\nThis can also help with understanding them too, because the rhythm and sounds of the words become clearer\n\n这也有助于理解它们，因为单词的节奏和声音变得更加清晰\n\n# 文章三\n# 原文\n\tWith the smell of coffee and fresh bread floating in the air, stalls bursting with colorful vegetables and tempting cheeses, and the buzz of friendly chats, farmers’ markets are a feast for the senses. They also provide an opportunity to talk to the people responsible for growing or raising your food, support your local economy and pick up fresh seasonal produce —— all at the same time.\n\n\tFarmers’ markets are usually weekly or monthly events, most often with outdoor stalls, which allow farmers or producers to sell their food directly to customers. The size or regularity of markets can vary from season to season, depending on the area's agricultural calendar, and you’re likely to find different produce on sale at different times of the year. By cutting out the middlemen, the farmers secure more profit for their produce. Shoppers also benefit from seeing exactly where-and to who- their money is going.\n\n# 逐句翻译\nWith the smell of coffee and fresh bread floating in the air, stalls bursting with colorful vegetables and tempting cheeses, and the buzz of friendly chats, farmers’ markets are a feast for the senses.\n\n伴随着漂浮在空气中的咖啡和新鲜面包香气，摊位上摆放着五颜六色的水果和楚楚可人的奶酪，以及友好的聊天嘈杂而又喧闹着，农贸市场上显现出一幅美好的景象（感官盛宴）。\n\nThey also provide an opportunity to talk to the people responsible for growing or raising your food, support your local economy and pick up fresh seasonal produce— all at the same time.\n\n这也为你提供了一种既能去人直面交流的机会，而这个人正是负责你食物的播种与采集的人，也能去支持本地经济，与此同时挑选新鲜的季节性产品的 —— 这一切同时发生。\n\nFarmers’ markets are usually weekly or monthly events, most often with outdoor stalls, which allow farmers or producers to sell their food directly to customers.\n\n农贸市场通常是每周或者每月举行，最常见的是户外摊位，届时允许农民或者生产者去直接面向顾客售卖他们的商品。\n\nThe size or regularity of markets can vary from season to season, depending on the area’s agricultural calendar, and you’re likely to find different produce on sale at different times of the year.\n\n市场的规模或者是规律很大程度上是适时而异，依托于当地农历，你很可能会发现与众不同的产品售卖于一年的不同时间。\n\nBy cutting out the middlemen, the farmers secure more profit for their produce.\n\n鉴于没有中间商赚差价，因此农民们可以获得来自于他们产品的更多利润。\n\nShoppers also benefit from seeing exactly where — and to who — their money is going.\n\n购买者也受益于他们可以更加准确的认识到，他们的钱支付给谁并流向何方。\n\n# 词汇\n\n\n\n单词\n释义\n\n\n\n\nstalls\nn. 正厅前排座位（stall 的复数形式） v. 使（汽车等）抛锚（stall 的第三人称单数形式）\n\n\nbursting\nadj. 充满的；渴望的v. 爆炸；猛冲；涨满；充满感情；突然爆发（感情）；猛然打开；决堤；突然开始活跃（burst 的现在分词）\n\n\ntempting\nadj. 诱人的，吸引人的v. 引诱，诱惑；怂恿，利诱；冒…… 的风险（tempt 的现在分词）\n\n\nbuzz\nv. 发嗡嗡声，发蜂鸣声；匆忙走动；充满嘈杂声；充满想法；繁忙，充满活力；&lt;非正式&gt; 给（某人）打电话；&lt; 非正式 &gt;（飞机）低空飞过；（用蜂鸣器）呼叫n. 嗡嗡声，蜂鸣声；叽叽喳喳声，嘈杂声；&lt;非正式&gt; 兴奋，快乐；热闹有趣的气氛，时尚氛围；&lt; 非正式 &gt; 电话；&lt; 非正式 &gt; 流言，传闻adj. （词语、想法或活动）时髦的\n\n\nfeast\nn. 宴会，筵席；盛会，特别的享受；宗教节日v. 饱餐，尽情享用；宴请，设宴招待（某人）\n\n\nregularity\nn. 规律性，经常性；匀称，端正；有规则的东西，有规律的事物\n\n\nmiddlemen\nn. 中间商；经纪商\n\n\nsecure\nadj. 稳固的，可靠的；严密把守的，牢固的；安全的，稳妥的；（对自己和自己的能力）有自信的；感到有保障的，没有顾虑的；固定住的，系牢的；秘密的v. （尤指经过努力而）获得，得到；使安全，保护；缚牢，将（某物）固定；确保，保证；为（债务或贷款）作抵押，作保；（外科）压迫（血管）止血；停止工作；船抛锚\n\n\n\n# 完型\n# 完型理论\n\n其本质上仍然是比错\n亟需打破传统思维，摒弃完美理解\n追求文章内部的一致性\n主旨一致\n\n# 局部一致性 + 全局一致性\n\n上下文语义一致\n注意逻辑合理（逻辑改变方向还是保持方向？）（二分法）\n\n","categories":["English"],"tags":["英语"]},{"title":"Java高级特性1","url":"/Java/javaAd01/","content":"# List 与 Set\n本篇内容意在总结课上所学的知识并加以巩固，如有错误请立即联系我，谢谢！\n\n\n关于 Collection\n掌握 List 部分内容\n掌握 Set 部分内容\n迭代器\nWeChat：a735690757\ngmail：735690757carry@gmail.com\n\n\n# 1，概述\n在 Java 中，List 和 Set 接口都是继承自 Collection 接口。Collection 接口定义了一组通用的集合操作方法，而 List 和 Set 则分别扩展了 Collection 接口，以提供特定的集合行为。\n# 2，Collection\n在 Java 中，Collection 接口是一个集合框架的根接口，它定义了一组通用的集合操作方法，可以用于处理各种类型的集合。Collection 接口提供了一些常用的方法，如添加元素、删除元素、检查元素是否存在、迭代集合中的元素等。它是 Java 集合框架中最基本的接口，所有其他的集合接口都是扩展自 Collection 接口。\nCollection 接口定义了如下方法（常用、不完全）：\n\n\nboolean add (E e): 将元素添加到集合中，并返回是否添加成功。\nboolean addAll (Collection&lt;? extends E&gt; c): 将指定集合中的所有元素添加到该集合中，并返回是否添加成功。\nboolean remove (Object o): 从集合中删除指定元素，并返回是否删除成功。\nvoid clear (): 清空集合中的所有元素。\nboolean isEmpty (): 检查集合是否为空。\nint size (): 返回集合中元素的数量。\n\n\n# 3，List\nJava 中的 List 是一种常见的集合类，用于存储一组元素。List 是一个接口，它有多个实现类，如 ArrayList、LinkedList 等。\n！重要！\n\n\n允许存储重复的元素。\n\n\n元素按照插入顺序有序存储。\n\n\n可以通过索引访问 List 中的元素，索引从 0 开始。\n\n\nList 可以动态调整大小，即添加或删除元素。\n有序，可重复\n\n\n# 3.1 ArrayList\nList 的常用方法（可能不完全）：（类似数组）\n\nadd (element)：向 List 末尾添加元素。\nadd (index, element)：在指定索引位置插入元素。\nremove (index)：删除指定索引位置的元素。\nget (index)：返回指定索引位置的元素。\nset (index, element)：替换指定索引位置的元素。\nsize ()：返回 List 的大小。\ncontains (element)：判断 List 是否包含指定元素。\nindexOf (element)：返回指定元素在 List 中第一次出现的索引位置。\nclear ()：清空 List 中的所有元素。\n\nimport java.util.ArrayList;import java.util.List;public class ListExample &#123;    public static void main(String[] args) &#123;        List&lt;String> list = new ArrayList&lt;>();        list.add(\"apple\");        list.add(\"banana\");        list.add(\"orange\");        System.out.println(list); // 输出：[apple, banana, orange]        list.remove(1);        System.out.println(list); // 输出：[apple, orange]        String fruit = list.get(1);        System.out.println(fruit); // 输出：orange        list.set(1, \"grape\");        System.out.println(list); // 输出：[apple, grape]    &#125;&#125;以上代码由 [ChatGPT Mar 14 Version] 生成。\npackage edu.beihua.KarryCode.listEX001.test;import edu.beihua.KarryCode.listEX001.entity.news;import java.util.ArrayList;public class test_for_ArrayList &#123;    public static void main(String[] args) &#123;        /*        * add ()     ---- 增加对象        * get ()     ---- 获取对象        * 非泛型返回 Object        * isEmpty () ---- 判空        * clear ()   ---- 清除        * iterator ()---- 迭代器        * toArray () ---- 转为数组        * */        news newslkr = new news(1,\"timu1\",\"lkr\");        news newsffl = new news(2,\"timu2\",\"ffl\");        news newslqr = new news(3,\"timu3\",\"lqr\");        news imnews = new news(4,\"hexin\",\"!!!!!\");        ArrayList&lt;news> arrayList = new ArrayList&lt;news>();        arrayList.add(newslkr);                                     // 普通追加        arrayList.add(newsffl);        arrayList.add(newslqr);         arrayList.add(0,imnews);                              // 标记追加        System.out.println(\"______________标题数目输出______________\");        System.out.println(\"新闻有\"+arrayList.size()+\"条\");        System.out.println(\"______________标题输出______________\");        for (int i=0;i&lt;arrayList.size();i++)&#123;            System.out.println(arrayList.get(i).getTitle());          // 不适用非泛型，可以使用（Object）强制转换，（news）进行匹配        &#125;        System.out.println(\"______________标题输出____________\");        for(Object obj:arrayList)&#123;                                  //** 增强形态的 For**            news newst = (news) obj;            System.out.println(newst.getTitle());        &#125;        System.out.println(\"______________标题输出____________\");        for(news news:arrayList)&#123;                                  //** 增强形态的 For**|| 最终版：面向对象            System.out.println(news.getTitle());        &#125;        System.out.println(\"________________标题判断__________\");        System.out.println(arrayList.contains(newslqr));        arrayList.remove(3);        System.out.println(\"______________标题输出____________\");        for(news news:arrayList)&#123;                                  //** 增强形态的 For**|| 最终版：面向对象            System.out.println(news.getTitle());        &#125;        System.out.println(\"______________标题判断__________\");        System.out.println(arrayList.contains(newslqr));        System.out.println(\"________________清空_______________\");        arrayList.clear();        System.out.println(arrayList.isEmpty());    &#125;&#125;以上代码为上课所写（丑丑的 www）。\n# 3.2 LinkedList\nLinkedList 具有以下特点：\n\n随机访问元素效率较低，因为需要遍历链表。\n在链表的开头或结尾插入、删除元素的效率较高。\n占用的内存空间相对较小。\n\n有趣的方法：\n\n\ngetFirst ()：返回链表的第一个元素。\ngetLast ()：返回链表的最后一个元素。\nremoveFirst ()：删除链表的第一个元素。\nremoveLast ()：删除链表的最后一个元素。\n\n\nimport java.util.LinkedList;import java.util.List;public class LinkedListExample &#123;    public static void main(String[] args) &#123;        List&lt;String> list = new LinkedList&lt;>();        list.add(\"apple\");        list.add(\"banana\");        list.add(\"orange\");        System.out.println(list); // 输出：[apple, banana, orange]        list.remove(1);        System.out.println(list); // 输出：[apple, orange]        String fruit = list.get(1);        System.out.println(fruit); // 输出：orange        list.set(1, \"grape\");        System.out.println(list); // 输出：[apple, grape]    &#125;&#125;以上代码由 [ChatGPT Mar 14 Version] 生成。\npackage edu.beihua.KarryCode.listEX001.test;import edu.beihua.KarryCode.listEX001.entity.news;import java.util.Iterator;import java.util.LinkedList;public class test_for_LinkList &#123;    public static void main(String[] args) &#123;        news newslkr = new news(1,\"timu1\",\"lkr\");        news newsffl = new news(2,\"timu2\",\"ffl\");        news newslqr = new news(3,\"timu3\",\"lqr\");        news imnews = new news(4,\"hexin\",\"!!!!!\");        LinkedList list = new LinkedList();         // 父父 new 子这种只能使用父子公用的方法        list.add(newslkr);                                     // 普通追加        list.add(newsffl);        list.add(newslqr);        list.remove(0);        list.add(0,imnews);        System.out.println(\"______________内容输出____________\");        for(Object obj:list)&#123;            news s = (news) obj;            System.out.println(s.getTitle());        &#125;        news tyu = (news)list.getFirst();        System.out.println(tyu.getNo());        news imnews666 = new news(10,\"hexin66666\",\"!!66666!!!\");        list.addFirst(imnews666);        System.out.println(\"______________内容输出____________\");        for(Object obj:list)&#123;            news s = (news) obj;            System.out.println(s.getTitle());        &#125;        news re = (news)list.removeLast();        System.out.println(re.getData());        System.out.println(list.size());        Iterator iterator = list.iterator();        System.out.println(\"_________________________\");        while (iterator.hasNext())&#123;            news news = (news) iterator.next();            System.out.println(news.getTitle());        &#125;    &#125;&#125;# 4，Set\n在 Java 中，Set 是一种集合（Collection）类型，它是一组不允许包含重复元素的对象。Set 是通过哈希表（hash table）实现的，因此它没有顺序，也不能通过索引访问其中的元素。Set 接口继承自 Collection 接口，并添加了一些独有的方法。\n无序，唯一\nJava 中常用的 Set 类有以下几种：\n\nHashSet：基于哈希表实现，具有良好的插入和查询性能，但不保证元素的顺序。\nTreeSet：基于红黑树实现，元素按照自然顺序排序或指定的 Comparator 顺序排序。\nLinkedHashSet：基于哈希表和链表实现，保证元素按照插入顺序排列。\n\nSet 的常用方法包括：\n\nadd (element)：向集合中添加元素。\nremove (element)：从集合中删除指定元素。\ncontains (element)：判断集合中是否包含指定元素。\nsize ()：返回集合中元素的数量。\nisEmpty ()：判断集合是否为空。\nclear ()：清空集合中的所有元素。\n\n例如，以下是使用 HashSet 和 TreeSet 实现的 Set 的示例：\nimport java.util.HashSet;\nimport java.util.Set;\nimport java.util.TreeSet;\n\npublic class SetExample &#123;\n    public static void main(String[] args) &#123;\n        Set&lt;String&gt; hashSet = new HashSet&lt;&gt;();\n        hashSet.add(&quot;apple&quot;);\n        hashSet.add(&quot;banana&quot;);\n        hashSet.add(&quot;orange&quot;);\n\n        System.out.println(hashSet); // 输出：[orange, banana, apple]\n\n        hashSet.remove(&quot;banana&quot;);\n        System.out.println(hashSet); // 输出：[orange, apple]\n\n        System.out.println(hashSet.contains(&quot;orange&quot;)); // 输出：true\n\n        Set&lt;String&gt; treeSet = new TreeSet&lt;&gt;();\n        treeSet.add(&quot;apple&quot;);\n        treeSet.add(&quot;banana&quot;);\n        treeSet.add(&quot;orange&quot;);\n\n        System.out.println(treeSet); // 输出：[apple, banana, orange]\n\n        treeSet.remove(&quot;banana&quot;);\n        System.out.println(treeSet); // 输出：[apple, orange]\n\n        System.out.println(treeSet.contains(&quot;orange&quot;)); // 输出：true\n    &#125;\n&#125;\n\n\n以上代码由 [ChatGPT Mar 14 Version] 生成。\n# 4.1 HashSet\n在 Java 中，HashSet 是一种基于哈希表实现的 Set 集合，它不保证元素的顺序，但是可以快速地插入和查找元素。HashSet 使用哈希函数将元素映射到哈希表中的桶（bucket）中，桶是一个链表或树结构，用于解决哈希冲突（即不同元素映射到同一个桶中的情况）。\nHashSet 的特点包括：\n\n不保证元素的顺序。\n不允许集合中存在重复元素。\n允许 null 元素。\n\npackage edu.beihua.KarryCode.listEX001.test;import edu.beihua.KarryCode.listEX001.entity.news;import java.util.HashSet;import java.util.Iterator;import java.util.Objects;import java.util.Set;public class test_foe_HashSet &#123;    public static void main(String[] args) &#123;        /*        * 无论什么 List 还是 Set 均为 Cll... 的继承类，其均具有父类的方法        * Set 中存放的是对象的引用，相同的引用是互斥的只能添加一次        * equals () 方法可以被重写        * Set 是无序的，不存在 Function（index，XXX）类似如此的有序号的的方法，且只能使用增强型的 for 进行循环输出 */        Set set = new HashSet();        news newslkr = new news(1,\"timu1\",\"lkr\");        news newsffl = new news(2,\"timu2\",\"ffl\");        news newslqr = new news(3,\"timu3\",\"lqr\");        set.add(newslkr);                                     // 普通追加        set.add(newsffl);        set.add(newslqr);        for(Object obj:set)&#123;            news test = (news) obj;            System.out.println(test.getData());        &#125;        Iterator iterator = set.iterator();        while (iterator.hasNext())&#123;            news test = (news) iterator.next();            System.out.println(test.getData());        &#125;    &#125;&#125;# 5，Iterator 迭代器\n在 Java 中，Iterator 是一个用于遍历集合（Collection）元素的接口，它提供了一种统一的访问集合中元素的方式。通过 Iterator，我们可以遍历集合中的每个元素，而不需要知道集合的具体实现方式。\nIterator 接口包含了以下方法：\n\nhasNext ()：判断集合中是否还有下一个元素。\nnext ()：返回集合中的下一个元素。\nremove ()：从集合中移除通过 next () 方法返回的最后一个元素。\n\nIterator 的工作原理是，首先通过集合的 iterator () 方法获得一个 Iterator 对象，然后使用 hasNext () 和 next () 方法遍历集合中的元素，最后使用 remove () 方法从集合中移除元素。\n在 4.1HashSet 中用到了本迭代器，以下为关键语句：\nIterator iterator = set.iterator();        while (iterator.hasNext())&#123;            news test = (news) iterator.next();            System.out.println(test.getData());        &#125;","categories":["java高级特性"],"tags":["java"]},{"title":"MongoDB的安装与初步使用（Windows平台）","url":"/MongoDB/MongoDB01/","content":"# 初见 MongoDB\n# 什么是 MongoDB？\nMongoDB 是一个开源的、面向文档的 NoSQL 数据库管理系统。它与传统的关系型数据库（如 MySQL、PostgreSQL）不同，因为它不使用表格来存储数据，而是使用一种称为 &quot;文档&quot; 的数据结构来组织和存储数据。每个文档是一个包含键值对的数据结构，类似于 JSON 格式，这使得 MongoDB 非常适合存储具有不同结构的数据。\n\n# MongoDB 的安装与服务启动\n# 下载 MongoDB 安装包\n访问 https://www.mongodb.com/download-center#community\n 或者点击下方连接\n\n          \n          MogoDB\n          官方下载地址\n          \n下载载最新版本的 MongoDB 数据库。\n# MongoDB 安装\n\n双击刚刚下载的安装文件 (mongodb-XXXX-XXXX-signed.msi) 启动安装程序。\n单击【Next】按钮，进入 “End-User License Agreement” 界面\n勾选 “I accept the terms in the License Agreement” 选项，单击【Next】按钮进入 “Choose Setup Type” 界面，该界面中可选择安装类型\n\nComplete。此类型将安装所有程序功能，需占用较多的磁盘空间，建议大多数用户使用。\nCustom。此类型允许用户自行选择要安装的程序功能及安装位置，建议高级用户使用。\n\nMongoDB Compass 是 MongoDB 数据库的 GUI 管理系统，默认会选择安装，但是安装速度非常慢。\n\n值得一提的是，MongoDB 默认会将创建的数据库文件存储在 db 目录下，但是这个目录不会被主动创建，用户需要在 MongoDB 安装完成后手动创建 db 目录。在 “C:\\Program Files\\MongoDB\\Server\\4.0\\data\\” 目录下创建一个文件夹 db\n# 配置\n同样的我们也可以将 bin 目录配置到环境变量的 Path 中\n# 启动 MongoDB 服务（启动与停止）\n进入如图所示的目录输入 net start MongoDB 启动服务，相应的输入 net stop MongoDB 则为停止服务\n\n# MongoDB 基本命令\n在 cmd 中输入 Mongo 进入 Mongo 的交互界面\n创建数据库:\n创建数据库use mydb展示数据库:\n展示据库show dbs删除数据库:\n删除数据库db.dropDatabase()创建集合：\n创建集合db.createCollection(\"myCollection\")插入数据：\n插入db.myCollection.insert(&#123;\"_id\":1,\"name\":\"诗岸梦行舟\"&#125;)删除：\n删除db.myCollection.remove(&#123;\"_id\":1&#125;)更新：\n更新db.myCollection.update(&#123;\"_id\":1&#125;,&#123;$set&#123;\"name\":\"Karry.Liu\"&#125;&#125;)查询所有：\n删除db.myCollection.find()指定查询：\n删除db.myCollection.find(&#123;\"_id\":1&#125;)","categories":["MongoDB"],"tags":["MongoDB"]},{"title":"初见Linux","url":"/Linux/Linux01/","content":"# 初见 Linux\n首先安装什么的就不讲了，这里先讲一下基础命令\n# 简单命令\n# who 命令\n[karry@localhost ~]$ who am i\n[karry@localhost ~]$ who am i\nkarry    pts/0        2023-09-01 10:26 (laptop-karry1107)\n\n# echo 命令\n这个命令是将内容输出到屏幕上\n[karry@localhost ~]$ echo Hello Karry.Liu\n[karry@localhost ~]$ echo Hello Karry.Liu\nHello Karry.Liu\n\n# date 命令\n[karry@localhost ~]$ date\n[karry@localhost ~]$ date\n2023 年 09 月 01 日 星期五 10:32:23 PDT\n\n# cal 命令\n[karry@localhost ~]$ cal 9 2023\n[karry@localhost ~]$ cal 9 2023\n 九月 2023\n 日 一 二 三 四 五 六\n 1  2\n3  4  5  6  7  8  9\n10 11 12 13 14 15 16\n17 18 19 20 21 22 23\n24 25 26 27 28 29 30\n\n# 基础命令\n# 基础文件操作命令\n# 展示文件夹中的内容\n[karry@localhost ~]$ ls\n[karry@localhost ~]$ ls\nDesktop  Documents  Downloads  Music  Pictures  Public  Templates  Videos\n\n# 进入 / 退出文件夹\n[karry@localhost ~]$ cd Desktop\n[karry@localhost ~]$ cd Desktop\n[karry@localhost Desktop]$\n\n# 创建文件夹\n[karry@localhost Desktop]$ mkdir LinuxHello\n[karry@localhost Desktop]$ ls\nfirefox.desktop  myFile  test\n[karry@localhost Desktop]$ mkdir LinuxHello\n[karry@localhost Desktop]$ ls\nfirefox.desktop  LiunxHello  myFile  test\n\n# 创建一个文件\n[karry@localhost LinuxHello]$ touch fistText.txt\n[karry@localhost Desktop]$ ls\nfirefox.desktop  LinuxHello  myFile  test\n[karry@localhost Desktop]$ cd LinuxHello\n[karry@localhost LinuxHello]$ touch fistText.txt\n[karry@localhost LinuxHello]$ ls\nfistText.txt\n\n# 编辑文件\n[karry@localhost LinuxHello]$ touch fistText.txt\n~\n~\n~\n“fistText.txt” 0L, 0C\n\n按   i        进入插入模式\n\n~\n~\n~\n– 插入 –\n\n现在可以编辑文件了！\n\nhi hi，这是我第一次学习 Linux！！！\n我的名字是诗岸梦行舟\n或者是 Karry.Liu\n 让我们共同努力吧！！\n~\n~                                                                                                                                                                                                                                                                                                                                                                       – 插入 –\n\n按   Esc        退出插入模式\n按   :+  w       保存刚才所编辑的文件\n\nhi hi，这是我第一次学习 Linux！！！\n我的名字是诗岸梦行舟\n或者是 Karry.Liu\n 让我们共同努力吧！！\n~\n“fistText.txt” 4L, 128C 已写入\n\n最后按 :+ q   退出 vi 编辑器\n\n~\n~\n:q\n\n# 读文件内容\n[karry@localhost LinuxHello]$ cat fistText.txt\n[karry@localhost LinuxHello]$ cat fistText.txt\nhi hi，这是我第一次学习 Linux！！！\n我的名字是诗岸梦行舟\n或者是 Karry.Liu\n 让我们共同努力吧！！\n[karry@localhost LinuxHello]$\n\n# 使用 g++ 编译程序\n\n\n如果你还没有安装 g++ 编译器，请先安装\nsudo yum install gcc-c++ make安装需要一定时间，请耐心等待！\n\n\n进入指定目录，创建 cpp 文件。\n[karry@localhost C++]$ touch firstApp.cpp\n\n使用 vi 指令编辑文件\n[karry@localhost C++]$ vi firstApp.cpp\n\n按 i 进入插入模式，编辑程序\n#include&lt;iostream>using namespace std;int main()&#123;        int a=1;        int b=2;        int c=a+b;        cout&lt;&lt;\"计算结果为：\"&lt;&lt;a&lt;&lt;\"+\"&lt;&lt;b&lt;&lt;\"=\"&lt;&lt;c&lt;&lt;endl;        return 0;&#125;\n\n按   Esc        退出插入模式，按   :+  w       保存刚才所编辑的文件，最后按 :+ q   退出 vi 编辑器\n\n\n执行命令编译程序\n[karry@localhost C++]$ g++ -o firstAppCompile firstApp.cpp\n\n运行输出结果\n[karry@localhost C++]$ ./firstAppCompile\n[karry@localhost C++]$ ./firstAppCompile\n 计算结果为：1+2=3\n[karry@localhost C++]$\n\n\n\n","categories":["Linux"],"tags":["Linux"]},{"title":"操作系统知识点阶段总结","url":"/Operate-system/01OS/","content":"# 引论\n# 操作系统是什么？\n操作系统是计算机系统中的核心软件之一，它是位于硬件和应用程序之间的一层软件，负责管理和控制计算机的硬件资源，并为应用程序提供一个运行环境。操作系统充当计算机系统的管理者，协调各种硬件和软件资源的分配和调度，以使计算机能够高效、可靠地运行。\n# 为什么要操作系统？\n操作系统的存在是为了解决计算机资源管理和用户程序执行的复杂性。它提供了一种抽象层，使应用程序开发人员不必直接与底层硬件进行交互，从而简化了应用程序的开发和维护。此外，操作系统还负责处理多任务管理、内存管理、文件系统管理、用户接口等，为用户和应用程序提供了一个友好且高效的计算环境。\n# 操作系统的特征是什么？\n操作系统具有以下几个主要特征：\n\n\n并发（Concurrency）： 能够同时处理多个任务或程序，使多个程序可以在同一台计算机上交替执行。\n（并发：同一时间段 并行：同一时刻）\n\n\n共享（Sharing）： 多个用户和应用程序可以同时访问计算机的资源，如内存、处理器、文件等。\n操作系统的最基本特征：并发与共享\n\n\n虚拟（Virtualization）： 操作系统可以为每个应用程序提供一种虚拟的环境，使其感觉自己独占了计算机资源。\n\n\n异步（asynchronous）： 异步特性指的是操作系统允许某些任务在进行的过程中，不必等待前一个任务的完成，而可以继续执行其他任务。（存在不确定性）\n✔️操作系统的基本特征：并发、共享、虚拟、异步\n\n\n抽象（Abstraction）： 操作系统通过抽象化硬件和软件资源，为应用程序提供一个更简单、一致的编程接口。\n\n\n持久性（Persistence）： 数据和程序可以被存储在持久性存储设备中，并在计算机关闭后保留下来。\n\n\n处理器管理（Processor Management）： 分配和管理处理器的时间片，以便多个任务可以轮流执行。\n\n\n内存管理（Memory Management）： 管理计算机的内存资源，包括分配、释放、虚拟内存等。\n\n\n文件系统管理（File System Management）： 管理文件的存储、组织、检索和保护。\n\n\n设备管理（Device Management）： 管理输入输出设备，使应用程序能够与设备进行交互。\n\n\n# 实时操作系统和分时操作系统？\n实时操作系统（Real-time Operating System，RTOS）和分时操作系统（Time-sharing Operating System）是两种不同类型的操作系统，用于满足不同应用场景下的需求。\n# 实时操作系统（RTOS）：\n实时操作系统是专门设计用于处理实时任务的操作系统。实时任务是具有严格时间要求的任务，可以分为硬实时和软实时任务。硬实时任务要求任务必须在严格的时间限制内完成，否则会导致系统错误。软实时任务也有时间要求，但对于这类任务，如果错过了截止日期，系统不会崩溃，但会影响任务的结果的实用性。\nRTOS 致力于确保任务能够按照特定的时间要求得到执行。它通常采用优先级调度策略，确保高优先级的实时任务能够在预定的时间内得到执行。\n# 分时操作系统（Time-sharing Operating System）：\n分时操作系统旨在支持多用户的共享计算机系统。它允许多个用户通过终端或其他用户界面同时访问系统，每个用户似乎都在独占地使用计算机资源。分时操作系统通过分配时间片（时间片轮转）来在多个任务之间切换，每个任务在时间片结束前都能得到一段时间的执行。\n分时操作系统的目标是实现多任务的并发执行，使用户能够在几乎同时使用计算机资源，这样可以提高资源利用率和用户体验。\n分时操作系统特征：多路性、独立性、及时性、交互性。详细解释：P10\n# 单道批处理系统和多道批处理系统？\n单道批处理系统（Single Batch Processing System）和多道批处理系统（Multi-Batch Processing System）是两种不同的操作系统工作方式，用于管理和执行计算机中的多个任务。\n# 单道批处理系统：\n单道批处理系统是早期计算机操作系统的一种。在这种系统中，计算机只能处理一道程序，也就是一个任务，每次只有一个任务在运行。当一个任务执行完成后，才能加载并执行下一个任务。这种系统对于用户和程序员来说，操作不够灵活，因为他们必须等待当前任务执行完成才能继续工作。\n💔单道批处理系统的缺点：系统中的的资源得不到充分利用！\n✍️注意注意！！！单道批处理系统无并发，不能称之为 OS！！！\n# 多道批处理系统：\n多道批处理系统是在单道批处理系统的基础上发展而来的。这种系统允许计算机在内存中同时加载和管理多个任务，而不需要等待前一个任务的完成。多道批处理系统将内存分割成多个区域，每个区域可以加载一个任务的代码和数据。操作系统会在任务之间进行切换，以实现多个任务的并发执行。这种方式提高了计算机的资源利用率和效率。\n# OS 的类型？\n\n多道批处理系统（Multi-programming Batch System）： 多道批处理系统是一种操作系统，允许多个任务（作业）在内存中同时存在，但每个任务的执行是按照一定的调度策略轮流进行的。它旨在提高计算机的资源利用率，通过在任务之间切换以避免 CPU 空闲。每个任务通常独立运行，不需要用户干预。这种系统适用于大量任务需要批量处理，例如批量数据处理任务。\n分时系统（Time-sharing System）： 分时系统允许多个用户通过终端或用户界面同时访问计算机，每个用户似乎都在独占地使用计算机资源。系统通过快速的任务切换（时间片轮转）实现用户之间的并发执行，每个用户能够迅速交互并使用计算机。分时系统适用于多用户、交互式的环境，如图形用户界面和终端。\n实时系统（Real-time System）： 实时系统是专门设计用于处理实时任务的操作系统。实时任务具有严格的时间要求，可以分为硬实时和软实时。硬实时任务要求任务必须在严格的时间限制内完成，否则会导致系统错误。软实时任务也有时间要求，但错过截止日期不会导致系统错误。实时系统旨在确保任务能够按照特定的时间要求得到执行，适用于需要在严格时间约束下运行的应用，如飞行控制系统和医疗设备。\n\n# 操作系统能干什么？\n操作系统的主要任务包括：\n\n管理计算机的硬件资源，如处理器、内存、硬盘、网络接口等。\n提供多任务管理，使多个应用程序可以同时运行。\n提供虚拟化，将物理资源抽象为多个虚拟资源，提供更好的资源利用率。\n管理文件系统，让用户能够创建、存储、组织和检索文件。\n处理输入输出，使用户和应用程序能够与外部设备进行交互。\n提供用户界面，让用户能够与计算机进行交互，如命令行界面或图形用户界面。\n\n# 操作系统有哪些？\n常见的操作系统包括：\n\nWindows：微软开发的操作系统系列，如 Windows 10、Windows 11 等。\nmacOS：苹果公司开发的操作系统，用于 Mac 电脑。\nLinux：一种开源的 Unix-like 操作系统，有许多不同的发行版，如 Ubuntu、Fedora、Debian 等。\nUnix：一种经典的多用户多任务操作系统，影响了许多其他操作系统的设计。\nAndroid：基于 Linux 内核的移动设备操作系统，由谷歌开发。\niOS：苹果公司用于 iPhone 和 iPad 等移动设备的操作系统。\nOpenHarmony：分布式操作系统，由华为公司开发。\n\n# 操作系统的新发展？\n操作系统领域一直在不断发展演进。一些新的趋势和发展包括：\n\n云操作系统： 针对云计算环境的操作系统，如 Google 的 Chrome OS 和微软的 Azure Sphere。\n嵌入式操作系统： 用于嵌入式系统，如物联网设备和嵌入式控制器的操作系统，如 FreeRTOS 和 Zephyr。\n容器化和微服务： 使用容器技术（如 Docker）和微服务架构的操作系统，以支持更高效的应用程序部署和管理。\n实时操作系统（RTOS）： 针对实时应用程序的操作系统，要求任务能够在严格的时间限制内得到执行。\n量子操作系统： 随着量子计算的发展，涉及管理量子资源和运行量子算法的操作系统正在探索中。\n\n这些都只是操作系统领域的一部分发展趋势，操作系统将继续适应新的硬件和应用场景，以满足不断变化的需求。\n# 简单总结！\n操作系统的定义可归纳为：操作系统是控制和管理计算机系统内各种硬件和软件资源、合理组织计算机工作流程的系统软件 (或程序集合)，是用户与计算机之间的接口。\n操作系统是什么？是核心系统软件\n操作系统管什么？控制和管理系统内各资源\n操作系统有何用？扩充硬件功能，方便用户使用\n# 练习\n\n\n一个作业第一次执行时用了 5 分钟，而第二次执行时用了 6 分钟，这说明了操作系统的  特点。\n\n并发\n共享\n虚拟\n异步\n\n解释\n\n异步存在时间不确定性\n\n\n\n\n操作系统的最基本的两个特征是资源共享和  。\n\n多道程序设计\n程序的并发执行\n中断\n程序顺序执行\n\n\n\n单道批处理系统的主要缺点是  。\n\nCPU 利用率不高\n失去了交互性\n不具备并行性\n以上都不是\n\n\n\n解释\n\n实际上 ABC 都是他的缺点，但是 CPU / 资源利用率不高是他的主要缺点\n\n\n\n\n采用多道程序设计的系统中，系统中的程序道数越多，系统的效率越高。\n\n\n通常将 CPU 模式分为内核态 (核心态) 和用户态，这样做的目的是为了提高运行速度。\n\n\n操作系统内核能使用特权指令。\n\n\n解析\n\n程序道数多多会导致每个程序分得的内存不够，很多程序所需的数据和代码要临时从磁盘调入内存系统会频繁的进行 I/O，使得系统效率下降！\n是为了提高安全性 ==（双重工作模式 P20）==\n确实对\n\n\n# 进程\n# 进程的几个基本状态\n\n就绪状态（Ready）： 进程已获得除处理器外的所需资源，等待分配处理器资源；只要分配了处理器进程就可执行。就绪进程可以按多个优先级来划分队列。例如，当一个进程由于时间片用完而进入就绪状态时，排入低优先级队列；当进程由 I/O 操作完成而进入就绪状态时，排入高优先级队列.\n运行状态 (Running)： 进程占用处理器资源；处于此状态的进程的数目小于等于处理器的数目。在没有其他进程可以执行时 (如所有进程都在阻塞状态), 通常会自动执行系统的空闲进程.\n阻塞状态 (Blocked)： 由于进程等待某种条件（如 I/O 操作或进程同步）, 在条件满足之前无法继续执行。该事件发生前即使把处理机分配给该进程，也无法运行.\n\n\n# 名词解释\n挂起： 从内存挂至外存\n时间片： 程序执行的一段时间\n阻塞： 发生 I/O 或者其他事件进入阻塞状态\n# 扩展状态（考研）\n\n# 练习\n\n\n当  时，进程从执行状态转变为就绪状态。\n\n进程被调度程序选中\n时间片到\n等待某一事件\n等待的事件发生\n\n\n\n在进程状态转换时，下列  转换是不可能发生的。\n\n就绪态 一 &gt; 运行态\n运行态 一 &gt; 就绪态\n运行态 一 &gt; 阻塞态\n阻塞态 一 &gt; 运行态\n\n\n\n进程和程序的本质区别是 \n\n前者是动态的，后者是静态的\n前者存储在内存，后者存储在外存\n前者在一个文件中，后者在多个文件中\n前者分时使用 CPU，后者独占 CPU\n\n\n\n程序运行时独占系统资源，只有程序本身能改变系统资源状态，这是指 \n\n程序顺序执行的再现性\n并发程序失去再现性\n并发程序失夫封闭性\n程序顺序执行时的封闭性\n\n\n\n不同的进程必然对应不同的程序。\n\n\n进程状态的转换是由操作系统完成的，对用户是透明的。\n\n\n# 进程控制块（Process Control Block，PCB）\n每个正在运行或等待运行的进程都有一个对应的 PCB，它包含了进程的各种属性、状态以及与其相关的控制信息。PCB 是操作系统内部用于实现进程管理的重要数据结构之一。\n\n进程状态（Process State）：表示进程的当前状态，如运行、就绪、阻塞等。操作系统根据进程状态来进行调度和管理。\n程序计数器（Program Counter）：指向进程当前执行的指令的地址，用于恢复进程的执行状态。\n寄存器（Registers）：保存进程的寄存器值，包括通用寄存器、程序状态寄存器等。\n进程优先级（Process Priority）：用于调度器决定哪个进程将获得 CPU 执行时间。\n进程标识符（Process ID）：唯一标识一个进程的数字或字符串。\n进程所拥有的资源信息：如打开的文件列表、分配的内存空间等。\n进程的父子关系：记录进程之间的层次结构，用于实现进程间的通信和协作。\n进程的各种统计信息：如运行时间、等待时间等，用于性能分析和调优。\n\nPCB 的存在使得操作系统可以高效地进行进程的切换、调度、挂起、恢复等操作。当操作系统需要切换到另一个进程时，它可以保存当前进程的状态信息到其对应的 PCB，然后加载新进程的状态信息，从而实现进程切换。这种切换是操作系统多任务处理的基础，使得多个进程可以在单个 CPU 上共享时间，并且实现了对系统资源的合理分配和利用。\n# PCB 组织方式\n\n\n\n# 练习\n\n\n在 PCB 中可以直接或间接找到有关该进程的所有信息。\n\n\n进程由 PCB 和其执行的程序、数据所组成\n\n\n","categories":["操作系统"],"tags":["OS"]},{"title":"进程与线程","url":"/Operate-system/02OS/","content":"# 什么是进程与线程\n进程（Process）和线程（Thread）是操作系统中的两个重要概念，用于管理和执行程序的执行单元。它们在多任务处理和并发执行中起着关键作用。\n# 进程\n进程是计算机系统中运行的程序的实例。每个进程都有自己的内存空间、代码和数据，以及与其他进程隔离的资源。进程可以看作是一个独立的执行环境，可以执行自己的任务。每个进程都有一个唯一的进程标识符（PID），用于区分和管理不同的进程。\n一个进程可以包含多个线程，这些线程共享同一个进程的资源，如内存空间、文件句柄等。不同进程之间的通信相对复杂，通常需要使用进程间通信（IPC）机制，如管道、消息队列、共享内存等。\n# 线程\n线程是进程内的执行单元，一个进程可以包含多个线程。线程共享同一个进程的代码和数据，但每个线程拥有自己的栈空间和程序计数器。因为线程共享相同的内存空间，它们之间的通信和数据共享更加方便，但也需要适当的同步控制来避免竞态条件和数据不一致问题。\n多线程的使用可以实现并发执行，提高程序的响应速度和资源利用率。常见的线程使用场景包括图形界面应用程序中的响应性、多媒体处理、网络服务器等。\n# 程序并发执行的特征\n\n间断性：并发程序之间相互制约\n失去封闭性：多个程序共享全机资源，执行状态收外界因素影响\n不可在现性：程序经过多次执行后，虽然其执行时的环境和初始条件都相同，但得到的结果却各不相同\n\n# 进程的特性\n\n动态性（最基本的特征）\n并发性\n独立性\n异步性：不可预知的速度\n\n# 进程控制\n一般包括以下几个方面：\n\n进程创建\n进程终止\n进程阻塞与唤醒\n进程挂起与激活\n\n# 进程创建\nUNIX 下的进程创建 ——fork ()\n# 消息机制\n直接通信方式一一消息缓冲队列\n这是指发送进程利用 OS 所提供的发送命令，直接把消息发送给目标进程。此时，要求发送进程和接收进程都以显式方式提供对方的标识符。通常，系统提供下述两条通信命令 (原语):\nSend (Receiver, message): 发送一个消息给 Receiver\nReceive (Sender,message): 接收 Sender 发来的消息\n例如，原语 Send (P，m) 表示将消息 m, 发送给接收进程 P; 而原语 Receive (P，m) 则表示接收由 P 发来的消息 m。\n# 进程通信\n进程通信实例 ------ 管道通信方式 Pipe\n\nwrite (fd [1],buf,size); 将 buf 中长为 size 字符的消息送入 fd [1] 口\n\n\nread (fd [0], buf’,size); 从 fd [O] 口读出 size 个字符置于 buf 中\n\n# 进程与线程的比较\n# 关于调度的基本单位\n在传统 OS 中，拥有资源、独立调度和分配的基本单位都是进程。\n在引入线程的 OS 中，线程作为调度和分派的基本单位，进程作为拥有资源的基本单位。\n在同一进程中，线程的切换不会引起进程切换，在由一个进程中的线程切换到另一个进程中的线程时，将会引起进程切换。\n# 并发性\n在引入线程的操作系统中，不仅进程之间可以并发执行，而且在一个进程中的多个线程之间，也可并发执行\n# 拥有资源\n进程是系统中拥有资源的一个基本单位，它可以拥有资源\n线程本身不拥有系统资源，仅有一点保证独立运行的资源\n允许多个线程共享其隶属进程所拥有的资源\n# 独立性\n同一进程中的不同线程之间的独立性要比不同进程之间的独立性低得多\n# 开销\n在创建或撤消进程时，OS 所付出的开销将显著大于创建或撤消线程时的开销\n线程切换的代价远低于进程切换的代价。\n同一进程中的多个线程之间的同步和通信也比进程的简单\n# 支持多处理机系统\n","categories":["操作系统"],"tags":["OS"]},{"title":"MongoDB初步使用","url":"/MongoDB/MongoDB02/","content":"# MongoDB 命令\n# 创建用户（可读可写）\n# 创建\ndb.createUser(&#123;user:\"lkr40\",pwd:\"123456\",roles:[\"readWrite\"]&#125;)# 检验\ndb.auth(\"lkr40\",\"123456\")\ndb.createUser({user:“lkr40”,pwd:“123456”,roles:[“readWrite”]})\nSuccessfully added user: { “user” : “lkr40”, “roles” : [ “readWrite” ] }\ndb.auth(“lkr40”,“123456”)\n1\n\n# 登录\nmongo 127.0.0.1:27017/mymongo -u lkr40 -p 123456# 建立 / 使用数据库\nuse mymongo\nuse mymongo\nswitched to db mymongo\n\n# 查询当前数据库\ndb\ndb\nmymongo\n\n# 删除数据库\ndb.dropDatabase()# 查询时间\nDate()\nDate()\nWed Sep 06 2023 10:21:36 GMT+0800\n\n# 创建集合\n集合中可以包含子集合\n# 显式创建（t）\ndb.createCollection(\"t\")\ndb.createCollection(“t”)\n\n# 隐式创建（t1）\ndb.t1.insert(&#123;&quot;age&quot;:18&#125;)\n\n\ndb.t1.insert({“age”:18})\nWriteResult({ “nInserted” : 1 })\nshow collections\nt\nt1\n\n# 展示集合\nshow collections\nshow collections\nt\nt1\n\n# 删除集合\ndb.t.drop()\ndb.t.drop()\ntrue\n\n# 查询文档\ndb.t1.find()\ndb.t1.find()\n\n# 条件查询\n# 等于\ndb.t.find(&#123;&quot;x&quot;:1&#125;)\n\n\ndb.t.find({“x”:1})\n\n# 小于\ndb.t.find(&#123;\"x\":&#123;$lte : 5&#125;&#125;)# 大于\ndb.t.find(&#123;\"x\":&#123;$lte : 5&#125;&#125;)# 在之中（包含）\ndb.t.find(&#123;\"x\":&#123;$in : [1,3,5]&#125;&#125;)# 不在其中（不包含）\ndb.t.find(&#123;\"x\":&#123;$nin : [1,3,5]&#125;&#125;)# 插入文档\n# 单一插入\ndb.t1.insert(&#123;\"_id\":\"1\",\"age\":23&#125;)\ndb.t1.insert({&quot;_id&quot;:“1”,“age”:23})\nWriteResult({ “nInserted” : 1 })\ndb.t1.find()\n\n\n# 批量插入（单语句）\ndb.t1.insertMany([&#123;\"_id\":2,\"age\":65&#125;,&#123;\"_id\":3,\"age\":45&#125;])\ndb.t1.insertMany([{&quot;_id&quot;:2,“age”:65},{&quot;_id&quot;:3,“age”:45}])\n{ “acknowledged” : true, “insertedIds” : [ 2, 3 ] }\ndb.t1.find()\n{ “_id” : ObjectId(“64f7ec7bff2142d2a2445360”), “age” : 18 }\n{ “_id” : “1”, “age” : 33 }\n\n\n# 循环插入（for）\nfor(i=1;i&lt;9;i++) db.t.insert(&#123;\"x\":i&#125;)\nfor(i=1;i&lt;9;i++) db.t.insert({“x”:i})\nWriteResult({ “nInserted” : 1 })\ndb.t.find()\n{ “_id” : ObjectId(“64f7f05bff2142d2a2445361”), “x” : 1 }\n{ “_id” : ObjectId(“64f7f05cff2142d2a2445362”), “x” : 2 }\n{ “_id” : ObjectId(“64f7f05cff2142d2a2445363”), “x” : 3 }\n{ “_id” : ObjectId(“64f7f05cff2142d2a2445364”), “x” : 4 }\n{ “_id” : ObjectId(“64f7f05cff2142d2a2445365”), “x” : 5 }\n{ “_id” : ObjectId(“64f7f05cff2142d2a2445366”), “x” : 6 }\n\n\n# 同 id 插入异常\n\ndb.t1.insert({&quot;_id&quot;:“1”,“age”:23})\nWriteResult({\n“nInserted” : 0,\n“writeError” : {\n“code” : 11000,\n“errmsg” : “E11000 duplicate key error collection: mymongo.t1 index: id dup key: { _id: “1” }”\n}\n})\n\n# 修改（输入全部属性 / 替换）\ndb.t1.save(&#123;\"_id\":\"1\",\"age\":33&#125;)\ndb.t1.save({&quot;_id&quot;:“1”,“age”:33})\nWriteResult({ “nMatched” : 1, “nUpserted” : 0, “nModified” : 1 })\ndb.t1.find()\n\n\n# 更新\ndb.t1.update(&#123;\"age\":65&#125;,&#123;$set:&#123;\"age\":100&#125;&#125;)\ndb.t1.update({“age”:65},{$set:{“age”:100}})\nWriteResult({ “nMatched” : 1, “nUpserted” : 0, “nModified” : 1 })\ndb.t1.find()\n{ “_id” : ObjectId(“64f7ec7bff2142d2a2445360”), “age” : 18 }\n{ “_id” : “1”, “age” : 33 }\n\n\n# 删除\n# 批量删除\ndb.t1.remove(&#123;\"age\":33&#125;)\ndb.t1.remove({“age”:33})\nWriteResult({ “nRemoved” : 1 })\ndb.t1.find()\n{ “_id” : ObjectId(“64f7ec7bff2142d2a2445360”), “age” : 18 }\n\n\n# 单一删除\ndb.t1.remove(&#123;\"age\":45&#125;,&#123;justOne:1&#125;)\ndb.t1.remove({“age”:45},{justOne:1})\nWriteResult({ “nRemoved” : 1 })\ndb.t1.find()\n{ “_id” : ObjectId(“64f7ec7bff2142d2a2445360”), “age” : 18 }\n{ “_id” : 2, “age” : 100 }\n\n\n# Capped 限制\n# 创建限制\ndb.createCollection(\"t2\",&#123;capped:true,size:3,max:8&#125;)db.t2.isCapped()# 检验\nfor(i=1;i&lt;9;i++) db.t2.insert(&#123;\"x\":i&#125;)\ndb.t2.find();\n{ “_id” : ObjectId(“64f7f23fff2142d2a244536a”), “x” : 2 }\n{ “_id” : ObjectId(“64f7f23fff2142d2a244536b”), “x” : 3 }\n{ “_id” : ObjectId(“64f7f23fff2142d2a244536c”), “x” : 4 }\n{ “_id” : ObjectId(“64f7f23fff2142d2a244536d”), “x” : 5 }\n{ “_id” : ObjectId(“64f7f23fff2142d2a244536e”), “x” : 6 }\n\n\n# 结论\n前面的数据被顶掉了\n# pretty 美化输出\ndb.t3.insert(&#123;\"_id\":1,\"phone\":&#123;\"hemophone\":\"8617118\",\"mobilephone\":\"18643079329\"&#125;&#125;)\ndb.t3.find().pretty();\n{\n“_id” : 1,\n“phone” : {\n“hemophone” : “8617118”,\n“mobilephone” : “18643079329”\n}\n}\n\n# 管道机制\n\n","categories":["MongoDB"],"tags":["MongoDB"]},{"title":"MongoDB初步使用","url":"/MongoDB/MongoDB03/","content":"# MongoDB 约束命令\n# 输出行数限制\ndb.t4.aggregate(&#123;$limit:4&#125;)# xx 升序 / 降序\ndb.t4.aggregate([&#123;$sort:&#123;price:-1&#125;&#125;])1：升序\n2：降序\n# MapReduce\nMapReduce 是一种用于分布式计算的编程模型和处理大规模数据集的方法。\nMapReduce 模型的基本思想是将大规模数据集分成小块，然后并行处理这些小块数据以生成中间结果。\n\nMap 阶段（映射阶段）：在这个阶段，原始数据被映射成键 - 值对的形式。每个映射操作都是独立的，可以在不同的计算节点上并行执行。Map 操作通常用于筛选、过滤、排序和转换数据。\nReduce 阶段（归约阶段）：在 Map 阶段之后，所有的中间结果按键分组，并将每个组的数据传递给 Reduce 函数进行聚合和处理。Reduce 操作通常用于对数据进行汇总、计数、计算统计信息等操作。\n\n//MapReduce//map映射var map = function()&#123;    emit(this.type,this.name);&#125;//reduce减少并以“，”分割var reduce = function(key,values)&#123;    return values.join(',');&#125;//选项设置输出var opt=&#123;out:\"name_list\"&#125;db.t4.mapReduce(map,reduce,opt)\n[\n{\n“_id”: “电子设备”,\n“value”: “logi”\n},\n{\n“_id”: “服装”,\n“value”: “安踏，lining”\n},\n{\n“_id”: “电子通信”,\n“value”: “oppo,vivo,huawei,iphone8”\n},\n{\n“_id”: “饮品”,\n“value”: “康师傅”\n}\n]\n\n","categories":["MongoDB"],"tags":["MongoDB"]},{"title":"进程同步问题","url":"/Operate-system/03OS/","content":"# 生产者 - 消费者问题\n在进程同步中第一个问题就是生产者 - 消费者问题，首先我们利用记录型信号来分析这个问题。\n好好好，我们直接伪代码解析！\n解释在行代码的上方\n详细伪代码//in 代表下一个要写入的位置，out 代表要读取的位置int in = 0,out = 0;// 用于存储生产者生产的容器，可以理解为缓冲区item buffer[n];// 前两行不是重点，下面才是核心！！//mutex 用于互斥访问共享资源，初值必须为 1（可以理解为锁）//empty 代表中转的容量，初值为最大承载容量，题里会给（可以是各种容器）//full 是生产出的一种逻辑 / 实体的物质// 总之不管是什么他是生产者生产出来的，一开始还没生产，full 初值为 0semaphore mutex = 1,empty = n,full = 0;// 生产者逻辑void producer()&#123;    //do-while 死循环，不断生产，配合消费者模拟程序并发    do&#123;        // 生产一个产品        produce an item nextp;        ...        //wait 什么什么就是什么什么 “--”，比如这里 wait (empty) 就是 empty--        // 相当于空位置减一，申请一个缓冲区        wait(empty);        // 相当于加锁，申请缓冲区的使用权        wait(mutex);        // 将产品放入缓冲区之中        buffer[in] = nextp;        // 下一个缓冲区的地址        in = (in + 1) % n;        //signal 什么什么就是什么什么 “++”，比如这里 signal (mutex) 就是 empty++        // 解锁，相当于释放权限        signal(mutex);        // 生产的东西数量加 1，也就是释放缓冲区        signal(full);    &#125;while(true);&#125;// 消费者逻辑void consumer()&#123;    //do-while 死循环，不断生消费，配合生产者模拟程序并发    do&#123;        // 消费者要消费一个物质，将生产者的生产的 full--        wait(full);        // 加锁，mutex--        wait(mutex);        // 其实这里不用太深究，这里就是消费者从缓冲区拿走了一个生产者的生产的物质        // 从缓冲区中取出产品        nextc = buffer[out];        // 导向下一个缓冲区的地址        out = (out + 1) % n;        // 解锁，mutex++        signal(mutex);        // 消费者已经取走了，空位置 empty++        signal(empty);        // 消费者消费物质        consume the item nextc;        ...    &#125;while(true)&#125;void main()&#123;    cobegin    producer();consumer()    coend&#125;简单的写法：\n简单伪代码Producer():  Repeat    生产一个商品    wait(empty);    wait(mutex);    将商品送至缓冲区;    signal(mutex);    signal(full);  Until falseComsumer():  Repeat    wait(full);    wait(mutex);    从缓冲区取走一个物品    singal(mutex);    signal(empty);  Until falseProgram main()  empty,full,mutex;    begin      empty = n;      full = 0;      mutex = 1;      cobegin        producer();consumer();      coend    end# 练习一下！\n桌上有个能盛得下五个水果的空子。爸爸不停地向盘中放苹果或橘子，儿子不停地从盘中取出桔子享用，女儿不停地从盘中取出苹果享用。规定三人不能同时从盘子中取放水果。使用信号量实现爸爸、儿子和女儿这三个循环进程之间的同步。\nempty = 5,orange = 0,apple = 0,mutex = 1;Dad()&#123;    while(1)&#123;        wait(empty);        wait(mutex);        将水果放入盘中;        signal(mutex);        if(放了橘子) signal(orange);        else signal(apple);    &#125;&#125;Son()&#123;    while(1)&#123;        wait(orange);        wait(mutex);        拿走一个桔子;        signal(mutex);        signal(empty);        吃桔子;    &#125;&#125;Daughter()&#123;    while(1)&#123;        wait(apple);        wait(mutex);        拿一个苹果;        signal(mutex);        signal(empty);        吃苹果;    &#125;&#125;","categories":["操作系统"],"tags":["OS"]},{"title":"操作系统细碎知识点及公式速记","url":"/Operate-system/KnowledgeSummary/","content":"# 操作系统细碎知识点总结\n\n现代操作系统两个最基本特征：并发  与  共享\nOS 主要特征是：并发、共享、虚拟 以及 异步\n操作系统出现的标志：多道程序设计 以及 分时操作系统 的出现\n高级调度：作业调度\n中级调度：内存调度\n低级调度：进程调度\n多级存储分为三部分：高速缓存、内存 / 主存 以及 外存\n进程的调度时机：时间片完、正常结束、P 操作、IO 请求 以及 优先级抢占\n文件系统的功能：按名存取、存储空间管理、文件共享与保护 以及 文件操作\nSPOOLING 系统的组成：输入进程、输入缓冲区、输入井、输出进程、输出缓冲区 以及 输出井\n进程的结构特征（有争议 / 不严谨）：独立性、动态性、异步性 以及 并发性\n虚拟存储器的主要特征：多次性、对换性 以及 虚拟性\n所学的进程调度算法中，最中庸、最能让长作业和短作业都满意的调度算法是：多级反馈队列调度算法\n进程是资源分配的基本单位，线程是调度的基本单位\n程序的几种装入方式：静态装入、可重定位装入 以及 动态运行时装入\n程序的几种链接方式：静态链接、装入时动态链接 以及 运行时动态链接\n程序的顺序执行的特征：顺序性、封闭性 以及 结果可再现性\n请求分页式虚拟存储系统必须至少具有三种硬件支持，即页表机制、缺页中断机构 以及 地址变换机构\n程序并发执行的特性有：间断性、失去封闭性、失去结果可再现性\n进程控制块的信息有：进程标识符、处理机状态、进程控制信息、进程调度信息\n扇区是磁盘空间管理的最基本单位，其物理地址是由：柱面号、扇区号、磁头号三部分组成的\n火车站售票系统属于实时系统\nI/O 管制程序的主要功能是管理设备、控制器和通道的状态信息\n依据信号量的发展过程，可将信号量分为四种：and 型信号量、信号量集、互斥信号量 以及 整型信号量\n进程通信类型有：共享存储器系统、消息传递系统、管道通信 以及 异步阻塞通信\n分段存储管理的优点有：方便编程、信息共享、动态增长、动态链接以及 信息保护\n成批性是批处理操作系统的主要特征，不是分时系统的特性\n推动操作系统发展的主要动力有：不断提高计算机资源利用率、方便用户、器件的不断更新换代 以及 计算机体系结构的不断发展\n同步机制应当遵循的规则有：空闲让进、让权等待、忙则等待 以及 有限等待\n系统中各个进程相互制约的关系称为同步\n对于死锁，一般应考虑死锁的预防、检测、避免 以及 解除\n典型的银行家算法是属于死锁避免，破坏环路等待条件是属于死锁预防，剥夺资源是属于死锁解除\n操作系统向用户提供了两种接口分别是用户接口和系统接口\n设别分别配应保证设备有高利用率和避免死锁\n对于操作系统而言，打开文件广义指令的主要作用是装入文件目录项\n为了实现多道程序设计，计算机在硬件方面必须提供两种支持，他们分别是中断和通道\n多道程序设计给存储管理提出了新课题，应考虑的三个问题是存储分配、虚存管理、存储保护\n操作系统的内核应提供终端管理、短程管理、原语管理三方面的功能\n死锁产生的主要原因为资源竞争和进程非法推进\n设备分配外部设备时。先分配设备，再分配控制器，再分配通道\n可用于文件系统管理空闲磁盘块的数据结构是位示图、空闲盘块链、文件分配表 FAT\n索引文件既利于文件的动态增长，也适合随机访问。\nPeterson 算法实现互斥访问，swap 指令与 TestAndSet 指令实现忙则等待，信号量算法实现让权等待，自旋锁实现空闲让进\n银行家算法破坏了死锁四个必要条件的循环等待条件\n死锁的四个条件为：互斥条件、请求与保持条件、不可剥夺条件 以及 循环等待条件\n\n\n# 常用公式速记\n# 进程调度有关公式\nT周转时间=T结束时间−T提交时间\n\\begin {array}{c}\nT_{\\text{周转时间}}=T_{\\text{结束时间}}-T_{\\text{提交时间}}\n\\end {array}\nT周转时间​=T结束时间​−T提交时间​​\nT带权周转时间=T周转时间T运行时间\n\\begin {array}{c}\nT_{\\text{带权周转时间}}=\\frac{T_{\\text{周转时间}}}{T_{\\text{运行时间}}}\n\\end {array}\nT带权周转时间​=T运行时间​T周转时间​​​\nT平均周转时间=∑i=1nTi周转时间n\n\\begin {array}{c}\nT_{\\text{平均周转时间}}=\\frac{\\sum_{i=1}^{n}  {T_{i\\text{周转时间}}}}{n}\n\\end {array}\nT平均周转时间​=n∑i=1n​Ti周转时间​​​\nT平均带权周转时间=∑i=1nTi带权周转时间n\n\\begin {array}{c}\nT_{\\text{平均带权周转时间}}=\\frac{\\sum_{i=1}^{n}  {T_{i\\text{带权周转时间}}}}{n}\n\\end {array}\nT平均带权周转时间​=n∑i=1n​Ti带权周转时间​​​\nR响应比=T当前周转时间T运行时间=T当前等待时间T运行时间+1\n\\begin {array}{c}\nR_{\\text{响应比}}=\\frac{T_{\\text{当前周转时间}}}{T_{\\text{运行时间}}}=\\frac{T_{\\text{当前等待时间}}}{T_{\\text{运行时间}}}+1\n\\end {array}\nR响应比​=T运行时间​T当前周转时间​​=T运行时间​T当前等待时间​​+1​\n# 典型例题\n# 逻辑地址转物理地址\n已知某分页系统，内存容量为 64KB，页面大小为 1KB，对一个 4 页大的作业，其 0、1、2、3 页分别被分配到内存的 2、4、6、7 块中。\n将十进制的逻辑地址 1023 变换为物理地址。\n内存容量64KB——216\n\\begin {array}{c}\n内存容量64KB ——2^{16}\n\\end {array}\n内存容量64KB——216​\n页面大小为1KB——210\n\\begin {array}{c}\n页面大小为1KB——2^{10}\n\\end {array}\n页面大小为1KB——210​\n共有216210=26页\n\\begin {array}{c}\n共有\\frac{2^{16}}{2^{10}} =2^{6}页\n\\end {array}\n共有210216​=26页​\n\n\n\n页\n块\n\n\n\n\n0\n2\n\n\n1\n4\n\n\n2\n6\n\n\n3\n7\n\n\n\n1023(10)=001111111111(2)\n\\begin {array}{c}\n1023_{(10)} =0011 1111 1111_{(2)}\n\\end {array}\n1023(10)​=001111111111(2)​​\n转换后101111111111(2)=3,071(10)\n\\begin {array}{c}\n转换后 1011 1111 1111_{(2)}=3,071_{(10)}\n\\end {array}\n转换后101111111111(2)​=3,071(10)​​\n","categories":["操作系统"],"tags":["OS"]},{"title":"操作系统PV代码题速记","url":"/Operate-system/OSPVCode/","content":"# 操作系统 PV 代码题速记\n进程 P 向 m 个进程 Q1、Q2、Q3…Qm 发送消息，进程 P 发消息到缓冲区，只有所有的 Q 进程都接收到消息后，进程 P 才能继续向缓冲区放消息，请写出 PV 操作逻辑。\nsemaphore mutex=1, T[i]=0, notHave=1;//mutex 是缓冲区互斥锁，T [i] 是 Qi 进程完成数组，是一个自阻塞数组//notHave 是缓冲区是否为空，1 就是空，初始没有信息int R=0;//R 是一个计数器Process_P()&#123;    while(1)&#123;        P(notHave);        P(mutex);        放入缓冲区;        for(int i=1;i&lt;=m;i++)&#123;            P(T[i]);        &#125;        R=0;        V(mutex);    &#125;&#125;Process_Qi()&#123;    while(1)&#123;        P(T[i]);        P(mutex);        取消息;        R=R+1;        if(R==m)&#123;            V(notHave);        &#125;        V(mutex);    &#125;&#125;# 单生产者 - 单消费者\n一组生产者进程和一组消费者进程共享一个初始为空、大小为 n 的缓冲区：只有缓冲区没满时，生产者才能把消息放入缓冲区，否则必须等待：只有缓冲区不空时，消费者才能从中取出消息，否则必须等待。由于缓冲区是临界资源，它只允许一个生产者放入消息，或一个消费者从中取出消息。\nsemaphore full = 0;\t// 已生产semaphore empty = n;// 总可用空间semaphore mutex = 1;// 互斥锁procuder()&#123;    while(1)&#123;        生产商品;        P(empty);\t// 假取空间        P(mutex);\t// 加锁        放入缓冲区;\t        V(mutex);\t// 解锁        V(full);\t// 告知以生产    &#125;&#125;consumer()&#123;    while(1)&#123;        P(full);        P(mutex);        从缓冲区取;        V(mutex);        V(empty);        消费商品;    &#125;&#125;# 多生产者 - 多消费者\n桌子上有一个盘子，每次只能向其中放入一个水果；爸爸专向盘子中放革果，妈妈专向盘子中放橘子；儿子专等吃盘子中的橘子，女儿专等吃盘子中的革果，只有盘子为空时，爸爸或妈妈才可向盘子中放一个水果，仅当盘子中有自己需要的水果时，儿子或女儿可以从盘子中取出。\nsemaphore mutex = 1;\t// 盘子锁semaphore apple = 0;semaphore orange = 0;dad()&#123;    while(1)&#123;        准备苹果;        P(mutex);        放苹果;        V(apple);    &#125;&#125;mom()&#123;    while(1)&#123;        准备橘子;        P(mutex);        放橘子;        V(orange);    &#125;&#125;son()&#123;    while(1)&#123;        P(orange);        拿苹果;        V(mutex);        吃苹果;    &#125;&#125;daughter()&#123;    while(1)&#123;        P(apple);        拿苹果;        V(mutex);        吃苹果;    &#125;&#125;# 读者写者问题（读优先）\n有读者和写者两组并发进程，共享一个文件，当两个或以上的读进程同时访问共享数据时不会产生副作用。但若某个写进程和其他进程（读进程或写进程）同时访问共享数据时则可能导致数据不一致的错误。\n因此要求:\n\n允许多个读者可以同时对文件执行读操作。\n只中许一个写者往文件中写信息。\n任一写者在完成写操作之前不允许其他读者或写者工作。\n写者执行写操作前，应让已有的读者和写者全部退出。\n\nsemaphore rw = 1;\t//r-w 读者在读时，不容许其他写者加入。写者在写时，不容许其他读者写者加入。int count = 0;\t\t// 记录已加入的读者的数量，如果不为 0，那么其中必有读者，那么其他读者随便加入。semaphore count_mutex = 1;writer()&#123;    while(1)&#123;        P(rw);        写文件;        V(rw);    &#125;&#125;reader()&#123;    while(1)&#123;        P(count_mutex);        if(count == 0)&#123;\t// 如果不为 0，那么其中必有读者，那么其他读者随便加入。            P(rw);\t// 读者为排除其他写者        &#125;        count++;\t// 在线加 1        V(count_mutex);        读文件;        P(count_mutex);        count--;\t// 在线减 1        if(count == 0)&#123;            V(rw);        &#125;        V(count_mutex);            &#125;&#125;有可能造成写饥饿\n# 读者写者问题（写优先）\nsemaphore rw = 1;int count = 0;semaphore w = 1;\t// 写进程霸占semaphore count_mutex = 1;writer()&#123;    while(1)&#123;        P(w);\t// 写进程启用霸占        P(rw);        写文件;        V(rw);        V(w);    &#125;&#125;reader()&#123;    while(1)&#123;        P(w);\t// 写进程阻碍读进程进入        P(count_mutex);        if(count == 0)&#123;            P(rw);        &#125;        count++;        V(count_mutex);        V(w);        读文件;        P(count_mutex);        count--;        if(count == 0)&#123;            V(rw);        &#125;        V(count_mutex);            &#125;&#125;# 吸烟者问题\n假设一个系统有三个抽烟者进程和一个供应者进程。每个抽烟者不停地卷烟并抽掉它，但要卷起并抽掉一支烟，抽烟者需要有三种材料：烟草、纸和胶水。三个抽烟者中，第一个拥有烟草，第二个拥有纸，第三个拥有胶水。供应者进程无限地提供三种材料，供应者每次将两种材料放到桌子上，拥有剩下那种材料的抽烟者卷一根烟并抽掉它，并给供应者一个信号告诉已完成，此时供应者就会将另外两种材料放到桌上，如此重复 (让三个抽烟者轮流地抽烟)。\nsemaphore offer1 = 0;semaphore offer2 = 0;semaphore offer3 = 0;semaphore isFinish = 1;int i = 0;provider()&#123;    while(1)&#123;        P(isFinish);    \tif(i%3 == 0)&#123;        \tV(offer1);    \t&#125;else if(i%3 == 1)&#123;        \tV(offer2);    \t&#125;else&#123;        \tV(offer3);    \t&#125;    &#125;    &#125;somker1()&#123;    while(1)&#123;        P(offer1);    \t制作香烟;    \t吸烟;        V(isFinish);    &#125; &#125;somker2()&#123;    while(1)&#123;        P(offer2);    \t制作香烟;    \t吸烟;        V(isFinish);    &#125; &#125;somker3()&#123;    while(1)&#123;        P(offer3);    \t制作香烟;    \t吸烟;        V(isFinish);    &#125; &#125;# PV 审题流程\n\n看有几个进程\n进程内部的步骤\n是否需要 while（1）\n哪里有 P？有 P 必有 V\n连续 P 是否会死锁\n定义信号量，写注释\n\n# 题目实操\n# 2020 统考真题\n【2020 统考真题】现有 5 个操作 A、B、C、D 和 E，操作 C 必须在 A 和 B 完成后执行操作 E 必须在 C 和 D 完成后执行，请使用信号量的 wait（）、signal（）操作（P、V 操作）描述上述操作之间的同步关系，并说明所用信号量及其初值。\nsemaphore ac = 0;semaphore bc = 0;semaphore ce = 0;semaphore ce = 0;A()&#123;    执行任务;    V(ac);&#125;B()&#123;    执行任务;    V(bc);&#125;C()&#123;    P(ac);    P(bc);    执行任务;    V(ce);&#125;D()&#123;    执行任务;    V(de);&#125;C()&#123;    P(ce);    P(de);    执行任务;&#125;【2009 统考真题】三个进程 P1,P2,P3, 互斥使用一个包含 N (N&gt;0) 个单元的缓冲区。\n\nP1 每次用 produce () 生成一个正整数并用 put () 送入缓冲区某一空单元；\nP2 每次用 getodd () 从该缓冲区中取出一个奇数并用 countodd () 统计奇数个数；\nP3 每次用 geteven () 从该缓冲区中取出一个偶数并用 counteven () 统计偶数个数。\n\n请用信号量机制实现这三个进程的同步与互斥活动，并说明所定义的信号量的含义 (要求用伪代码描述)。\nsemaphore mutex = 1;semaphore odd = 0;semaphore even = 0;P1()&#123;    while(1)&#123;        int num = produce();        if(num%2 == 0)&#123;            P(mutex);            put();            V(mutex);            V(even);        &#125;else&#123;            P(mutex);            put();            V(mutex);            V(odd);        &#125;    &#125;&#125;P2()&#123;    while(1)&#123;        P(odd);        P(mutex);        getodd();        V(mutex);        countodd()    &#125;&#125;P2()&#123;    while(1)&#123;        P(even);        P(mutex);        geteven();        V(mutex);        counteven()    &#125;&#125;# 2011 统考真题\n【2011 统考真题】某银行提供 1 个服务窗口和 10 个供顾客等待的座位。顾客到达银行时若有空座位，则到取号机上领取一个号，等待叫号。取号机每次仅允许一位顾客使用。当营业员空闲时，通过叫号选取一位顾客，并为其服务。\nsemaphore count = 10;semaphore mutex = 1;semaphore server = 1;semaphore full = 0;customer()&#123;    while(1)&#123;        P(count);        P(mutex);    \t从取号机取号;    \tV(mutex);    \tV(full);    \t等待叫号;    \tP(server);    \t获取服务;    &#125;    &#125;clerk()&#123;    while(1)&#123;        P(full);        叫号;        V(server);        服务;        V(count);    &#125;    &#125;# 809-24 年 真题\n崂山有一景点称作上清宫，游客在上清宫游玩后可以在宫门口免费搭乘轿车游览其他景区，游览后再返回宫门口。已知风景区游览轿车总量有 M 量，游客总数为 N，约定:\n\n每辆轿车限乘一位游客；\n如果有空闲的轿车，应当允许想游览的游客乘坐；\n无空闲轿车时，游客只能排队等待；\n若没有想游览的游客，空闲的轿车也要等待。\n\n试利用 P、V 操作实现在上清宫门口乘车点：N 个游客进程和 M 辆轿车进程的同步操作过程。\nsemaphore mutex = 1;semaphore car = 0; \t\t// 空闲车辆数量semaphore customer = 0; // 等待乘车的游客数量//M 个车辆进程car()&#123;    while(1)&#123;        等待游客;        P(customer);\t// 等待有游客请求乘车        P(mutex);\t\t// 进入临界区        V(car);\t\t\t// 提供一辆空闲轿车        V(mutex);\t\t// 离开临界区        接待游览;        释放游客;    &#125;&#125;//N 个游客进程customer()&#123;    while(1)&#123;        等待车辆;        P(car);\t\t\t// 等待空闲轿车       \tP(mutex);\t\t// 进入临界区        V(customer)\t\t// 通知有游客请求乘车        V(mutex);\t\t// 离开临界区        上车游览;        离开;    &#125;&#125;# 典例 和尚打水\n某寺庙有小和尚、老和尚若干，有一水缸，由小和尚提水入缸供老和尚饮用。水缸可容 10 桶水，水取自同一井中。水井径窄，每次只能容一个桶取水。水桶总数为 3 个。每次入缸取水仅为 1 桶水，且不可同时进行，试给出有关从缸取水、入水的算法描述。\nsemaphore total_empty = 10;semaphore total_full = 0;semaphore mutex_jing = 1;semaphore mutex_gang = 1;demaphore bucket = 3;Old ()&#123;    P(total_full);    P(bucket);    P(mutex_gang);    从缸里取水;    V(total_empty);    V(mutex_gang);\t喝水;    V(bucket);&#125;Young()&#123;    P(total_empty);    P(bucket)    P(mutex_jing);    从水井取水;    V(mutex_jing);    P(mutex_gang);\t倒进缸里;    V(total_full);    V(mutex_gang);    V(bucket);    &#125;# 典例 公交车售票\n设公共汽车上，司机和售票员的活动分别是：\n\n司机的活动：启动车辆；正常行车；到站停车；\n售票员的活动：关车门；售票；开车门；\n\n请用记录型信号量机制实现上述问题的同步。\nsemaphore door_mutex = 1;semaphore start_mutex = 0;driver()&#123;    while(1)&#123;        P(start_mutex);        启动车辆;        正常行车;        到站停车;        V(door_mutex);    &#125;&#125;busman()&#123;    while(1)&#123;        P(door_mutex);        开车门;        售票;        关车门;        V(start_mutex);    &#125;&#125;# 典例 独木桥问题\n请用信号量解决以下的 “过独木桥” 问题：同一方向的行人可连续过桥，当某一方向有人过桥时，另一方向的行人必须等待；当某一方向无人过桥时，另一方向的行人可以过桥。\nsemaphore isOK = 1;int toACount = 0;int toBCount = 0;semaphore toACount_mutex = 1;semaphore toBCount_mutex = 1;toA()&#123;    P(toACount_mutex);    if(toACount == 0)&#123;        P(isOK);    &#125;    toACount++;    V(toACount_mutex);    上桥走向A;    P(toACount_mutex);    toACount--;    if(toACount == 0)&#123;        V(isOK);    &#125;    V(toACount_mutex);&#125;toB()&#123;    P(toBCount_mutex);    if(toBCount == 0)&#123;        P(isOK);    &#125;    toBCount++;\tV(toBCount_mutex);    上桥走向B;    P(toBCount_mutex);    toBCount--;    if(toBCount == 0)&#123;        V(isOK);    &#125;\tV(toBCount_mutex);&#125;# 典例 阅览室问题\n有一阅览室，共有 100 个座位。为了很好利用它，读者进入时必须先在登记表上进行登记。该表表目设有座位号和读者姓名；离开时再将其登记项摈除。试问：\n\n为描述读者的动作，应设哪几个进程？它们之间的关系是什么？\n试用 P、V 操作描述进程之间的同步或算法。\n\n答：要设读者进入进程和离开进程，总计两个进程。他们是互斥关系。\nsemaphore seats = 100;semaphore readers = 0;semaphore table_mutex = 1;getIn()&#123;    while(1)&#123;        P(seats);        P(table_mutex);        登记;        V(table_mutex);        V(readers);    &#125;&#125;getOut()&#123;    while(1)&#123;        P(readers);        P(table_mutex);        移除登记;        V(table_mutex);        V(seats);    &#125;&#125;# 典例 考试问题\n《操作系统》课程的期末考试即将举行，假设把学生和监考老师都看作进程，学生有 N 人，教师 1 人。考场门口每次只能进出一个人，进考场的原则是先来先进。当 N 个学生都进入了考场后，教师才能发卷子。学生交卷后即可离开考场，而教师要等收上来全部卷子并封装卷子后才能离开考场。\n(1) 问共需设置几个进程？\n(2) 请用 P、V 操作解决上述问题中的同步和互斥关系。\n答：共设置 N+1 个进程，一个教师进程，N 个学生进程\nint studentCount = 0;int onPageCount = 0;semaphore door_mutex = 1;semaphore startTest = 0;semaphore endTest = 0;semaphore readyToTest = 0;semaphore onPageCount_mutex = 1;semaphore allStudentsOK = 0;teacher()&#123;    P(door_mutex);    进门;    V(door_mutex);    P(readyToTest);    发卷子;    V(startTest);    监考;    P(allStudentsOK);    整理试卷;    P(door_mutex);    出门;    V(door_mutex);&#125;students()&#123;    P(door_mutex);    进门;    studentCount++;    if(studentCont == N)&#123;        V(readyToTest);    &#125;    V(door_mutex);    P(startTest);    开始答题;    P(onPageCount_mutex);    交卷;    onPageCount++;    if(onPageCount == N)&#123;        V(allStudentsOK);    &#125;    V(onPageCount_mutex);    P(door_mutex);    出门;    studentCount--;    V(door_mutex);&#125;\n          \n          hunter_wyh CSDN 博客\n          部分题目出自此博客\n          ","categories":["操作系统"],"tags":["OS"]},{"title":"CPU与存储器的连接","url":"/Principles-of-computer-composition/Connection_between_CPU_and_memory/","content":"# 前言\n针对这一部分内容我还是推荐大家去看看书本上和其他教学视频里的内容，本篇文章只针对计算机组成原理（第 3 版）P91 页之后的内容进行简单整理汇总。\n接下来我将会按照书上的顺序进行叙述。\n# CPU 与存储器的连接\n# 存储器容量扩展\n# 字扩展\n在某些情况下，字扩展可以被视为一种扩容操作。字扩展通常指的是将一个字符或字符串转换为具有更多位数或更高精度的数据类型，以便进行更复杂的计算或存储更大的值。这个过程可能涉及到内存分配和重新分配，因此可以看作是一种扩容操作。但是，在其他情况下，字扩展可能只是简单地将一个字符或字符串进行格式化或解码，而不涉及任何内存操作。因此，是否将字扩展视为扩容取决于具体的上下文和实现方式。\n但是我们可以简单地把字扩展理解为扩容。\n比如，两片 1K*8 位通过字扩展扩容位逻辑上的一片 2K*8 位\n# 位扩展\n位扩展是一种将一个二进制数值的位数增加的操作。在位扩展中，如果原始二进制数值的最高位为 0，则在其左侧添加 0 以增加位数；如果最高位为 1，则在其左侧添加 1 以保持符号不变。例如，将 8 位二进制数值 &quot;00110110&quot; 进行位扩展为 16 位，则结果为 &quot;00000000 00110110&quot;。位扩展通常用于将低精度数据类型转换为高精度数据类型，或者在进行算术运算时对操作数进行对齐。\n咳咳咳，简单理解就是二进制加长\n# 字位扩展\n字位扩展是一种将一个数据类型的位数增加的操作，其中 “字” 指的是计算机中的一个固定大小的数据单元。在字位扩展中，如果原始数据类型的最高位为 0，则在其左侧添加 0 以增加位数；如果最高位为 1，则在其左侧添加 1 以保持符号不变。例如，将 16 位有符号整数进行字位扩展为 32 位，则结果为在最高位之前添加 16 个相同的符号位。字位扩展通常用于将低精度数据类型转换为高精度数据类型，或者在进行算术运算时对操作数进行对齐。\n简单理解就是… 结合版\n# 例题\n怎么出题呢？一般都像这样：\nCPU 有 16 根地址线、8 根数据线，并用 MREQ 非作为访存控制信号（低电频有效），用 WR 作为读 / 写控制信号。现在有如下存储芯片：1K X 4 位 RAM、4K X 8 位 RAM、8K X 8 位 RAM、2K X 8 位 ROM、4K X 8 位 ROM、8K X 8 位 ROM 及 74138 译码器和各种门电路。现要求如下：\n\n主存地址空间分配：\n\n6000H ~ 67FFH 为系统程序区\n6800H ~ 6BFFH 为用户程序区\n\n\n合理选用上述存储芯片，说明各选几片\n详细画出芯片片选逻辑图\n\n# 解！！\n第一步：写出地址空间分配范围二进制码\n6000H ~ 67FFH 为系统程序区\n6800H ~ 6BFFH 为用户程序区\n6000H、67FFH、6800H、6BFFH 化为二进制：\n\n\n\nA15\nA14\nA13\nA12\nA11\nA10\nA9\nA8\nA7\nA6\nA5\nA4\nA3\nA2\nA1\nA0\n\n\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n首先，先确定芯片，全零到全一就是一个芯片\n先看 ROM，也就是系统区：\n\n\n\nA15\nA14\nA13\nA12\nA11\nA10\nA9\nA8\nA7\nA6\nA5\nA4\nA3\nA2\nA1\nA0\n\n\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\nA0 ~ A10 为一个 ROM，一共 11 位，理想是要一个规格为 2K X 8 位 ROM\n再来看 RAM ，也就是用户区：\n\n\n\nA15\nA14\nA13\nA12\nA11\nA10\nA9\nA8\nA7\nA6\nA5\nA4\nA3\nA2\nA1\nA0\n\n\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\nA0 ~ A9 为一个 RAM，一共 10 位，理想是要一个规格为 1K X 8 位 RAM\n然后呢，我们需要对 ROM 和 RAM 进行读写，这就涉及到切换 ROM 和 RAM 芯片的问题，我们需要弄一个片选信号：\n\n\n\nA15\nA14\nA13\nA12\nA11\nA10\nA9\nA8\nA7\nA6\nA5\nA4\nA3\nA2\nA1\nA0\n\n\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n上两行是 ROM，下两行是 RAM，可以看到 A11 及其关键的决定了到底选择哪一个芯片，是 ROM 还是 RAM。当 A11 为 0 时 ROM 被选中，当 A11 为 1 时 RAM 被选中。\n然而 74138 译码器需要三个数据输入端，这时候我们就需要再拉来两个垫背的 A12 与 A13。\n\n\n\nA15\nA14\nA13\nA12\nA11\nA10\nA9\nA8\nA7\nA6\nA5\nA4\nA3\nA2\nA1\nA0\n\n\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\nA11 A12 A13 一次排列连接至 74138 的数据输入端 A B C，这一点非常重要！！低位是起始点 A，高位时终点 C。\n数据为 100 时 ROM 被选中，74138Y4 非端被点亮，因为 100 正是数字 4。\n数据为 101 时 RAM 被选中，74138Y4 非端被点亮，因为 101 正是数字 5。\n接下来再讨论讨论芯片的选择：\nA0 ~ A10 为一个 ROM，一共 11 位，理想是要一个规格为 2K X 8 位 ROM\nA0 ~ A9 为一个 RAM，一共 10 位，理想是要一个规格为 1K X 8 位 RAM\n对于 ROM，题中刚好有一个规格为 2K X 8 位的 ROM 芯片，拿来便是！\n对于 RAM，题中没有那个规格的我们这里就需要进行扩展，这里我们选择两片 1K X 4 位的 ROM 芯片对他进行位扩展就好了。位扩展 位扩展 位扩展，说明它位数不够，数据一次无法写进一个芯片，但是一次可以写进两个芯片。这样就解决了！\n# 总结\nCPU 与存储器的连接是计算机计算机组成原理中的重要考题，更是期末考试中的重点，看着很复杂的连接图，但其实内在的原理还是很简单的，无非就是：写二进制、选芯片、扩展外带画图连接，本质还是比较容易作的\n","categories":["计算机组成原理"],"tags":["计算机组成原理"]},{"title":"政治知识点速记（持续更新）","url":"/PoliticalKnowledge/politicalNote/","content":"# 领航\n\n部分口诀来自李丽双老师\n主要内容来自《肖秀荣 1000 题》、《精讲精炼》、《30 天 70 分》的答案解析\n以及其他同学们的笔记\n版权问题请联系作者 QQ：735690757\n\n# 史纲\n# 马原理\n# 习思想\n# 琐碎\n在习近平新时代中国特色社会主义思想的指导下，中国共产党团结带领中国人民，自信自强、守正创新，统揽伟大斗争、伟大工程、伟大事业、伟大梦想，创造了新时代中国特色社会主义的 伟大成就，为实现中华民族伟大复兴提供了更为完善的制度保证、更为坚实的物质基础、更为主动的精神力量。\n# 六个坚持\n“六个必须坚持”: 必须坚持人民至上、必须坚持自信自立、必须坚持守正创新、必须坚持问题导向、必须坚持系统观念、必须坚持胸怀天下。\n\n人心正道系天下\n\n# 党的根本宗旨\n党的根本宗旨是全心全意为人民服务\n# 实现途径、行动指南、根本保障、精神力量\n改革开放以来我们取得一切成绩和进步的根本原因，归结起来就是；开辟了中国特色社会主义道路，形成了中国特色社会主义理论体系，确立了中国特色社会主义制度，发展了中国特色社会主义文化。其中，道路是实现途径，理论体系是行动指南，制度是根本保障，文化是精神力量，四者统一于中国特色社会主义伟大实践。\n# 四个自信理论支撑和根本依据\n马克思主义深刻揭示了社会主义必然代替资本主义的客观规律，这是我们坚定 “四个自信” 的理论支撑和根本依据。\n# 四个自信的来源\n中国特色社会主义道路自信、理论自信、制度自信、文化自信，来源于实践、来源于人民、来源于真理。\n# 四个全面处于主导地位的是\n“四个全面” 战略布局既有战略目标又有战略举措，每个 “全面” 之间具有紧密的内在逻辑是一个整体战略部署的有序展开。全面建设社会主义现代化国家是战略目标，在 “四个全面” 中居于引领地位；全面深化改革、全面依法治国、全面从严治党是三大战略举措，为全面建设社会主义现代化国家提供重要保障。\n# 党的基本路线\n党的基本路线是国家的生命线、人民的幸福线。\n# 科学社会主义的主张\n科学社会主义的主张受到中国人民热烈欢迎，并最终扎根中国大地、开花结果，是因为：第一，它回答了近代以来中华民族面临的历史性课题 (求得民族独立和人民解放；实现国家富强和人民幸福)、指明了实现中华民族伟大复兴的正确道路 (中国特色社会主义是实现中华民族复兴的正确道路); 第二，它同我国传承了几千年的优秀历史文化和广大人民日用而不觉的价值观念具有高度的契合性\n# 百年奋斗目标 - 不是顺序的而是交错的 100 年\n第一个一百年（共产党成立 100 周年），是到中国共产党成立 100 年时全面建成小康社会（1921-2021）\n第二个一百年（新中国成立 100 周年），是到新中国成立 100 年时建成富强民主文明和谐美丽的社会主义现代化强国（1949-2049）\n# 新形势下做好新闻舆论工作的关键\n互联网是意识形态工作的主阵地、主战场、最前沿，管好用好互联网是新形势下做好新闻舆论工作的关键。习近平指出：“过不了互联网这一关，就过不了长期执政这一关。\n# 文化是…\n面建成社会主义现代化强国必然要求建设社会主义文化强国。统筹推进 “五位一体” 总体布局、协调推进 “四个全面” 战略布局，文化是重要内容；推动高质量发展，文化是重要支点；满足人民日益增长的美好生活需要，文化是重要因素；战胜前进道路上各种风险挑战，文化是重要力量源泉。只有推动文化繁荣兴盛，才能为推进中国式现代化建设、实现第二个百年奋斗目标提供思想保证、舆论支持、精神动力和文化条件。\n# 中国共产党的精神之源\n伟大建党精神是中国共产党的精神之源\n# 关乎旗帜、关乎道路、关乎国家政治安全，决定文化前进方向和发展道路\n意识形态关乎旗帜、关乎道路、关乎国家政治安全，决定文化前进方向和发展道路。\n# 坚持百花齐放、百家争鸣 (“双百”)\n坚持百花齐放、百家争鸣 (“双百”) 是繁荣发展社会主义文化的重要方针。\n# 坚持为人民服务、为社会主义服务 (“二为”)\n坚持为人民服务、为社会主义服务 (“二为”) 的根本方向，是决定社会主义文化事业前途命运的关键。\n# 区别于其他国家和民族的根本特征\n中华民族创造的优秀传统文化是民族的根脉，根植在中国人内心，形成了中国人看待世界、看待社会、看待人生的独特价值体系、文化内涵和精神品质，这是我们区别于其他国家和民族的根本特征。\n# 文化的生命所在、本质特征\n创新创造是文化的生命所在，是文化的本质特征。因此\n# 共同富裕的基础性制度\n分配制度\n# 初次分配\n第一次分配的主体是市场参与的各要素主体。\n\n初次分配，即初次收入分配。在社会分配中，初次分配注重效率，是按贡献分配。该贡献包括对创造利润有益的各种因素，如资金、技术、管理、生产资料、劳动力、信息、市场、营销等。谁能利用这些要素作出贡献，就能分到一杯羹。这样群策群力，效率便得以提高。\n参考\n\n# 再分配\n第二次分配的主体是政府。\n\n再分配（二次分配）是指政府根据法律法规，在初次分配的基础上通过征收税收和政府非税收入，在各收入主体之间以现金或实物进行的收入再次分配过程。与初次分配不同，再分配中起主导作用的是政府，强调公平的原则，具有通过国家权力强制进行的特征。除了公平的目标外，再分配也通过教育、健康等基本公共服务的提供，创造机会平等的养教环境，以提升社会经济发展的可持续性。\n参考\n\n# 三次分配\n而第三次分配的主体是民间社会力量，包括企业、社会组织和个人等。\n\n三次分配有别于前两者，主要是企业、社会组织、家族、家庭和个人等基于自愿原则和道德准则，以募集、捐赠、资助、义工等慈善、公益方式对所属资源和财富进行分配。社会组织和社会力量是三次分配的中坚力量 。\n参考\n\n# 三次分配的变化\n市场👉政府👉社会\n# 军民融合发展\n习近平强军思想明确军民融合发展是兴国之举、强军之策；明确党对人民军队的绝对领导是人民军队建军之本、强军之魂；明确依法治军是我们党建军治军基本方式；明确作风优良是我军鲜明特色和政治优势。\n# 政治建军\n政治建军是人民军队的立军之本，抓军队建设首先要从政治上看。\n# 依法治军\n依法治军是我们党建军治军基本方式。\n# 一流军队要求一流科技\n一流军队要求一流科技，必须全面实施科技强军战略。\n科技是核心战斗力，是军事发展中最活跃、最具革命性的因素。\n# 坚持党对人民军队的绝对领导\n军委主席负责制是坚持党对人民军队绝对领导的根本制度和根本实现形式，在党领导军队的一整套制度体系中处于最高层次，居于统领地位。\n# 听党指挥 —— 首要、核心\n听党指挥是灵魂，决定军队建设的政治方向；能打胜仗是核心，反映军队的根本职能和军队建设的根本指向。\n# 作风优良 —— 保证、性质、宗旨、本质\n作风优良是保证，关系军队的性质、宗旨、本色，是人民军队的鲜明特色和政治优势，也是人民军队战无不胜、攻无不克的重要保证。\n# 作风优良 —— 核心\n# 关键一招\n改革是决定人民军队发展壮大、制胜未来的关键一招。\n# 四个战略支撑\n‌我军新时代使命任务的四个战略支撑是‌：‌\n\n\n为巩固‌中国共产党领导和社会主义制度提供战略支撑‌：确保党的领导和社会主义制度的安全和稳定。\n\n\n为捍卫‌国家主权、统一、‌领土完整提供战略支撑‌：维护国家的领土完整和主权不受侵犯。\n\n\n‌为维护国家海外利益提供战略支撑‌：保护国家在海外的利益和公民的安全。\n\n\n为促进‌世界和平与发展提供战略支撑‌：积极参与国际事务，推动世界和平与发展。\n\n\n# 一国两制\n“一国两制” 的根本宗旨是维护国家主权、安全、发展利益，保持香港、澳门长期繁荣稳定\n维护国家安全是 “一国两制” 的核心要义\n维护国家主权、安全、发展利益是 “一国两制” 方针的最高原则，在这个前提下，香港、澳门保持原有的资本主义制度长期不变，享有高度自治权。\n坚持 “爱国者治港”“爱国者治澳” 原则是事关国家主权、安全、发展利益，事关香港、澳门长期繁荣稳定的根本原则，是保证香港、澳门长治久安的必然要求\n# 全面贯彻新时代党解决台湾问题的总体方略\n\n根本保证坚持党中央对对台工作的集中统一领导\n历史方位在中华民族伟大复兴进程中推进祖国统一\n战略思路在祖国大陆发展进步基础上解决台湾问题\n大政方针 “和平统一、一国两制”\n政治基础一个中国原则和 “九二共识”\n实践途径推动两岸关系和平发展、融合发展\n根本动力团结台湾同胞、争取台湾民心\n必然要求粉碎 “台独” 分裂图谋\n外部条件反对外部势力干涉\n战略支撑决不承诺放弃使用武力\n\n# 外交工作主线\n服务民族复兴，促进人类进步\n# 经济建设主线\n供给侧结构性改革\n# 全面深化改革开放主线\n制度建设\n# 一带一路\n互联互通 —————————— 主线\n共商共建共享 ———————— 原则\n坚持开放、绿色、廉洁 ———— 理念\n高标准、可持续、惠民生 ——— 目标\n# 党建\n加强党的长期执政能力建设，先进性和纯洁性建设\n# 构建新型国际关系要秉持原则\n\n相互尊重\n公平正义\n合作共赢原则\n\n没有和平共处\n# 区分新型国际关系和大国关系\n国际关系不用和平，大国关系不用共赢。\n# 新型国际关系\n尊重 — 合作 — 公平\n# 新型国际关系又分为不同的具体要求\na、【大国关系格局】—— 和平共处、总体稳定、均衡发展\n b、周边外交方针\n c、周边外交理念（外交全局的首要位置）\nd、发展中国家 —— 真实亲诚\n e、新型政党关系 —— 求同存异、相互尊重、互学互鉴\n# 党的政治建设首要任务\n保证全党服从党中央，维护党中央权威和集中统一领导\n# 毛中特\n# 琐碎\n“工农武装割据” 和农村包围城市、武装夺取政权的思想是毛泽东在土地革命战争时期提出的\n实事求是，就是一切从实际出发，理论联系实际，坚持在实践中检验真理和发展真理，实事求是是 A. 中国共产党的思想路线，B. 中国共产党人认识世界、改造世界的根本要求，C. 中国共产党的基本思想方法、工作方法、领导方法，D. 马克思主义的根本观点\n\n群众路线是党的生命线和根本工作路线\n\n# 南方谈话的内容\n\n计划和市场都是经济手段；\n阐明了社会主义本质；\n提出了 “发展才是硬道理” 的重要论断\n提出判断改革开放和各项工作成败得失的 “三个有利于” 标准；\n强调加强党的建设；\n关于社会主义初级阶段的长期性和前途\n\n# 速记口诀一二三四\n一大立党，二大立纲，三大对国共合作的方针和办法做了正式决定，即 “党内合作”，四大提出无产阶级领导校和工农同盟军思想，五大批评陈独秀右倾错误，六大提出中国革命的性质是资产阶级民主革命，七大将毛理论命名为毛思想，并将其作为党的指导思想写入党章，八大提出主要矛盾，十二大提出建设中国特色社会主义社会，十三大系统地论述了关于社会主义初级阶段的理论，十四大概括了中国特色社会主义理论的主要内容，系统释了这一理论的历史地位和指导意义，十五大邓理论被确立为中国共产党的指导思想，并写入了党章。\n\n一党\n二纲\n三合作\n四权\n五批\n六性\n七入\n八矛\n十二大建中特\n十三大论阶段 （邓小平轮廓）\n十四大系阐释\n十五大邓入党章\n十八大进入新时代\n\n毛思想 -&gt; 七大（毛七七）\n邓理论 -&gt; 十五大\n三个代表 -&gt; 十六大\n科学发展观 -&gt; 十七大\n习思想 -&gt; 十九大\n# 一个神奇的口诀\n三线五纲，四体六康\n# 十三大\n党的十一届三中全会的胜利召开，结束了粉碎 “四人帮” 后党和国家工作在徘徊中前进的局面，标志着中国共产党重新确立了马克思主义的思想路线、政治路线和组织路线，开启了我国改革开放和社会主义现代化建设历史新时期，实现了历史性的伟大转折。\n# 十八大\n中共十八大的召开，标志着中国已经进入全面建成小康社会的决定性阶段，开启了中国特 色社会主义新时代。\n# 毛泽东思想的地位\n毛泽东思想是马克思主义中国化的第一次历史性飞跃的理论成 果；是中国革命和建设的科学指南；是党和人民的宝贵精神财富。\n# 有关中心问题\n①无产阶级的领导权是中国革命的中心问题。\n②在中国民主革命中，无产阶级领导权的中心问题是农民问题。\n③毛泽东在《国民革命与农民运动》中还提到 “农民问题乃国民革命的中心问题”。\n回答问题的时候要注意题干具体考查的角度。\n# 革命的性质（1948 毛泽东指出…）\n中国是一个封建经济占明显优势的半殖民地半封建社会，革命是为了终结这个半殖民地半封建的社会形态；中国革命的对象主要是帝国主义和封建主义势力；革命的性质是为了推翻这两个主要敌人的民族革命和民主革命\n# 谁？\n中国革命的中心问题、新民主主义苹命理论的核心问题：无产阶级的领导权问题\n中国革命的实质：党领导下的农民革命\n中国革命的最基本动力：无产阶级；\n主力军：农民阶级\n# 打谁？\n中国革命的首要问题：分清敌友\n中国革命的对象：帝国主义 (首要对象)、封建主义和官僚资本主义\n# 为什么打？\n中国革命的根本问题：国家政权问题\n中国革命的基本问题：农民问题\n# 怎么打？\n中国革命的主要内容：没收封建地主阶级的士地归农民所有\n# 中国民主革命的基本内容\n中国革命走农村包围城市、武装夺取政权的道路，必须处理好土地革命、武装斗争、农村革命根据地建设三者之间的关系。土地革命是民主革命的基本内容；武装斗争是中国革命的主要形式，是农村革命根据地建设和土地革命的强有力保证；农村革命根据地是中国革命的战略阵地，是进行武装斗争和开展土地革命的依托。\n# 统一战线的经验\n新民主主义革命时期，中国共产党领导的统一战线，先后经过了第一次国共合作的统一战线、工农民主统一战线、抗日民族统一战线、人民民主统一战线等几个时期，积累了丰富的经验。其中，最根本的经验就是正确处理好与资产阶级的关系当党能够正确处理与资产阶级建立统一战线或被迫分裂统一战线的问题时，党的发展和巩固就会前进；反之，党的发展和巩固就会后退。\n# 建立统一战线的必要性\n建立最广泛的统一战线，是由中国半殖民地半封建社会的阶级状况所决定的。毛泽东指出：“中国社会是一个两头小中间大的社会，无产阶级和地主大资产阶级都只占少数，最广大的人民是农民、城市小资产阶级以及其他的中间阶级。” 作为无产阶级先锋队的中国共产党所领导的革命力量，要战胜作为地主阶级和官僚资产阶级集中代表的国民党所领导的强大的反革命力量，就必须把农民、城市小资产阶级以及其他的中间阶级都团结在自己的周围，结成最广泛的统一战线。\n# 新民主主义革命时期的统一战线\n党领导的统一战线，先后经过了第一次国共合作的统一战线、工农民主统一战线、抗日民族统一战线、人民民主统一战线等几个时期。\n以国共合作为基础的是国民革命联合战线 (第一次国共合作的统一战线) 与抗日民族统一战线。\n国共就合作了 2 次，一次是推翻封建主义制度，一次是抗日合作\n# 新民主主义革命理论的基本纲领\n新民主主义文化是科学的，是反对一切封建思想和迷信思想主张实事求是、客观真理及理论和实践的一致性。对于封建时代创造的文化，应剔除其封建糟粕，吸收其民主性精华。同时，要尊重中国的历史，反对民族虚无主义，以历史唯物主义的态度对待古今中外文化，发展民族新文化，提高民族自信心。\n# ” 两步走 “\n由于中国革命的反帝反封建的双重任务，决定了革命分两步走：第一步是先进行推翻三座大山的新民主主义革命，第二步是进行社会主义革命。\n# “一次革命论” 和 “二次革命论” 都是错误的\n“左” 倾教条主义的 “一次革命论”(如 “无间断” 革命论) 的错误在于，只看到了民主革命与社会主义革命的联系，而混淆了民主革命与社会主义革命的区别，在反帝反封建的同时，也反对民族资产阶级，否定了中国革命的阶段性。而右的 “二次革命论” 的错误在于，只看到了民主革命与社会主义革命的区别，而没有看到两个革命阶段的联系，放弃了党对民主革命的领导权。\n# ” 两头小中间大” 的社会\n毛泽东指出：“中国社会是一个两头小中间大的社会，无产阶级和地主大资产阶级都只占少数，最广大的人民是农民、城市小资产阶级以及其他的中间阶级。”\n# 三件 “法宝”\n统一战线、武装斗争、党的建设\n\n战斗党\n\n# 战胜敌人的两个基本武器\n统一战线、武装斗争\n\n战斗\n\n# 中国革命道路\n土地革命、武装斗争、农村革命根据地建设\n\n土斗根\n\n# 活的灵魂\n实事求是、群众路线、独立自主\n\n实众独\n\n# 优良作风\n理论联系实际、密切联系群众、批评与自我批评\n# 新民主主义革命纲领最具特色一项\n保护民族工商业，是新民主主义经济纲领中极具特色的一项内容。在新民主主义条件下保护民族工商业，发展资本主义，是由中国落后的生产力和新民主主义革命的性质所决定的。新民主主义革命的对象是帝国主义、封建主义和官僚资本主义，而不是一般地消灭资本主义和资产阶级。\n# 新民主主义革命胜利的历史意义\n新民主主义革命的伟大成就，为实现中华民族伟大复兴创造了根本社会条件。\n# 各个阶段中国社会的主要矛盾\n1840—1949，中华民族与帝国主义的矛盾。\n1949—1952，与帝国主义、封建主义、国民党残余势力之间的矛盾。\n1952—1956，工人阶级和资产阶级之间的矛盾。（土改基本完成）\n1956—1978，人民对于建立先进的工业国的要求同落后的农业国的现实之间的矛盾，人民对于经济文化迅速发展的需要同当前经济文化不能满足人民需要的状况之间的矛盾。\n1978，人民日益增长的物质文化需要同落后的社会生产之间的矛盾。\n2018，人民日益增长的美好生活需要和不平衡不充分的发展之间的矛盾。\n# 互助组、常年互助组、初级合作社、高级合作社\n互助组 (包括临时互助组和常年互助组两种形式) 不涉及生产资料问题，只是在生产方面组织起来、互帮互助，是农业合作化的最初过渡形式，具有社会主义萌芽性质。初级农业生产合作社以土地入股和统一经营为特点，实行集体劳动，产品分配采取按劳分配和土地入股分红相结合，具有半社会主义性质。高级农业生产合作社实行生产资料农民集体所有，实行按劳分配，具有完全的社会主义性质。\n# 民族资产阶级的两面性\n既有剥削工人取得利润的一面，又有拥护中国共产党的领导、拥护宪法、愿意接受社会主义改造的一面\n# 民族资产阶级与工人阶级的矛盾的两重性\n既有剥削者与被剥削者的阶级利益相互对立的对抗性的一面，又有相互合作、具有相同利益的非对抗性的一面\n# 资本主义工商业的社会主义改造经历了三个步骤\n第一步 (初级形式的国家资本主义) 和第二步 (个别企业的公私合营) 的企业利润分配都是四马分肥。第三步 (全行业的公私合营) 在分配上实行定股定息。\n\n第一步实行初级形式的国家资本主义。国家在私营工业中实行委托加工、计划订货、统购包销，这主要是同资本家在企业外部的合作。\n第二步实行个别企业的公私合营。这是社会主义成分同资本主义成分在企业内部的合作。\n第三步实行全行业的公私合营。全行业公私合营后，国家对合营企业进行清产核资、定股定息，委派人员负责企业的生产经营管理，统一调配企业的人、财、物，生产资料为国家所有。全行业公私合营后，企业的生产关系已经发生了根本的变化，基本上成为社会主义国营性质的企业。\n\n# 推进手工业合作化的方针\n在推进手工业合作化的过程中，中国共产党采取的是积极领导、稳步前进的方针。\n# 推进农业社会主义改造的原则\n这条道路遵循自愿互利、典型示范和国家帮助的原则，以互助合作的优越性吸引农民走互 助合作道路。\n\n士力架\n\n# 资本主义工商业社会主义改造的方针\n和平赎买\n# 经济成分构成及阶级构成\n三种主要的经济成分：社会主义经济、个体经济、资本主义经济\n阶级构成：工人、农民、民族资产阶级、其他小资产阶级\n其中，半社会主义性质的合作社经济是个体经济向社会主义集体经济过渡的形式，国家资本主义经济是私人资本主义经济向社会主义国营经济过渡的形式。\n# 萌芽性质、半社会主义性质、完全社会主义性质\n社会主义萌芽：农业互助组，手工业供销小组，资本主义工商业初级国家资本主义经济\n半社会主义：农业初级生产合作社，手工业供销合作社，工商业个别企业公私合营\n完全社会主义：农业高级生产合作社，手工业生产合作社，工商业全行业公私合营\n\n工商业：高级国家资本主义 = 个别企业公私合营 + 全行业公私合营\n\n# 1956 年年底社会主义改造的基本完成\n\n标志着中国历史上长达数千年的阶级剥削制度的结束\n实现了由新民主主义向社会主义的转变\n标志着社会主义基本制度在我国确立\n标志着我国的社会主义初级阶段从此开始\n\n# 两个结合和 “两次结合”\n毛泽东的两次结合：马 + 革命、马 + 建设道路\n习近平的两个结合：马 + 中国国情、马 + 传统文化\n# 《论十大关系》—— 探索中国社会主义建设道路的良好开端\n毛泽东在 1956 年 4 月 25 日中央政治局扩大会议和 5 月 2 日最高国务会议上作《论十大关系》的报告。强调以苏为鉴、独立自主地探索适合中国情况的社会主义建设道路。其中，《论十大关系》中的第一大关系，即重工业和轻工业、农业的关系。\n在《论十大关系》的报告中，初步总结了我国社会主义建设的经验，明确提出要以苏为鉴，独立自主地探索适合中国情况的社会主义建设道路。《论十大关系》标志着党探索中国社会主义建设道路的良好开端。\n\n十全十美良好开端\n\n# 《关于正确处理人民内部矛盾的问题》\n《关于正确处理人民内部矛盾的问题》一文，是一篇重要的马克思主义文献。它创造性地阐述了社会主义社会矛盾学说，是对科学社会主义理论的重要发展，对中国社会主义事业具有长远的指导意义。\n《关于正确处理人民内部矛盾的问题》指出，对于政治思想领域的人民内部矛盾，实行 “团结 — 批评 — 团结” 的方针，坚持说服教育、讨论的方法。\n# 中国特色社会主义理论体系形成发展的帽子题\n中国特色社会主义理论体系形成发展的社会历史条件：【国际背景 + 历史条件 + 实践基础】\n而如果光问历史条件就不一样，社会历史条件范围要大一些\n\n国际形势的深刻变化和世界发展新趋势 —— 国际背景；\n建设社会主义正反两方面经验和我国发展的历史方位 —— 历史条件；\n改革开放和社会主义现代化建设实践 —— 实践基础\n\n# 中国特色社会主义理论体系形成发展的历史条件\n党的十一届三中全会以后，中国共产党人鲜明指出建设社会主义没有固定的模式，** 必须结合中国实际，在实践中不懈探索和回答什么是社会主义、怎样建设社会主义这一基本问题。** 正是在探索和回答这一首要的基本的理论问题的过程中，我们党开创了中国特色社会主义的伟大事业。\n# 马克思主义中国化的第一次飞跃与第二次飞跃\n== 马克思主义中国化的第一次飞跃，是党在探索中国革命道路的过程中完成的。== 在毛泽东思想的指导下，党领导人民走以农村包围城市的道路，取得了新民主主义革命的胜利；走具有中国特色的社会主义改造的道路，积极探索社会主义建设道路，取得社会主义革命和建设的伟大成就。\n马克思主义中国化时代化的第二次飞跃是中国特色社会主义理论体系。\n# 毛泽东思想的精髓\n实事求是\n# 邓小平理论的精髓\n解放思想，实事求是\n# 十三大提出社会主义初级阶段基本路线\n基本途径：一个中心，两个基本点（以经济建设为中心，坚持改革开放，坚持四项基本原则）\n跟本立足点：自力更生，艰苦创业\n领导力量和依靠力量：领导和团结各族人民\n奋斗目标：建设富强民主文明的社会主义现代化国家\n# 几个帽子\n改革 —— 是社会主义社会发展的直接动力；\n阶级斗争 —— 是阶级社会发展的直接动力；\n科技 —— 是第一生产力；\n科技革命–是社会发展的重要动力。\n解放生产力，发展生产力，是社会主义的根本任务。\n# 三个有利于\n\n是否有利于发展社会主义生产力\n是否有利于增强社会主义国家的综合国力\n是否有利于提高人民的生活水平\n\n\n生国人\n\n# 正确认识和处理改革、发展、稳定的关系\n改革是动力，发展是目的，稳定是前提。要把改革的力度、发展的速度和社会可承受的程度统一起来，把不断改善人民生活作为处理改革、发展、稳定关系的重要结合点，在社会稳定中推进改革发展，通过改革发展促进社会稳定。\n# “三个代表” 重要思想的” 花边 “\n“三个代表” 重要思想是在对冷战结束后国际局势科学判断的基础上形成的，是在科学判断党的历史方位和总结历史经验的基础上提出来的，是在建设中国特色社会主义伟大实践的基础上形成的。\n# “三个代表” 重要思想\n\n中国共产党始终代表中国先进生产力的发展要求\n始终代表中国先进文化的前进方向\n始终代表中国最广大人民的根本利益\n\n# 新民主主义革命时期 1921-1949\n\n\n\n子时期\n时间\n\n\n\n\n党的创立和大革命时期\n1921-1927\n\n\n土地革命战争时期\n1927-1937\n\n\n全民族抗日战争时期\n1937-1945\n\n\n解放战争时期\n1945-1949\n\n\n\n# 新民主主义社会时期 1949-1956\n# 社会主义建设时期 1956-1978\n# 1978 之后\n\n\n\n大事件\n时间\n\n\n\n\n十一届三中全会提出改革开放\n1978\n\n\n提出命题 “建设有中国特色的社会主义\n1982\n\n\n苏联解体，冷战结束\n1991\n\n\n南方谈话\n1992\n\n\n中共十四大，确立社会主义市场经济体制的改革目标\n1992\n\n\n邓小平理论提出\n1997\n\n\n三个代表指导思想\n2002\n\n\n\n# 科学发展观\n第一要义：发展\n核心立场：以人为本\n基本要求：全面可持续发展\n根本方法：统筹兼顾\n保持先进性是党自身建设的根本任务和永恒课题\n# 科学发展观的形成条件\n\n是在深刻把握我国基本国情和新的阶段性特征的基础 上形成和发展的。\n是在深入总结改革开放以来实践经验的基础上形成和发展的。\n是在深刻分析国际形势及借鉴国外发展经验基础上形成和发展的。\n\n# 领导人的思想精髓\n毛泽东：实事求是\n邓小平：解放思想，实事求是\n江泽民：解放思想，实事求是，与时俱进\n胡锦涛：解放思想，实事求是，与时俱进，求真务实\n# 思修\n# 琐碎\n中国特色社会主义建设实践是社会主义核心价值观的现实基础\n先进性、人民性和真实性是社会主义核心价值观的道义力量\n# 道德作用\n认识功能 — 反映揭示（行为前）\n规范功能 — 规范引导（行为中）\n调节功能 — 指导纠正（行为后）\n# 爱国主义\n爱国主义是 道德要求 政治原则 法律规范 精神纽带\n以爱国主义为核心的民族精神，以改革创新为核心的时代精神\n# 信念\n执着性、支撑性、多样性\n# 理想\n超越性、实践性、时代性\n# 价值观\n先进性、人民性、真实性\n# 区分创造精神，梦想精神，团结精神，和奋斗精神\n神话就是梦想，发明一类就是创造，开垦治理河流之类的就是奋斗，剩下的那个就是团结\n# 文化\n文化是涵养民族心理、民族个性、民族精神的摇篮，而不是祖国大好河山\n# 祖国河山\n祖国的河山在人们的心中占据着至高无上的地位，祖国的山山水水滋养哺育着她的子子孙孙。“禾苗离土即死，国家无土难存”, 祖国的大好河山，不只是自然风光，更是主权、财富、民族发 展和进步的基本载体。因此，每一个爱国者都会把 “保我国土” “爱我家乡”、维护祖国领土的完整和统一，作为自己的神圣使命和义不容辞的责任。\n# 中华传统美德的根本要求\n中华传统美德的根本要求是公义胜私欲\n# 道德的作用\n\n道德为经济基础的形成、巩固和发展服务，是一种重要的精神力量。\n道德对其他社会意识形态的存在有着重大的影响\n道德通过调整人们之间的关系维护社会秩序和稳定\n道德是提高人的精神境界、促进人的自我完善，推动人的全面发展的内在动力\n在阶级社会中，道德是阶级斗争的重要工具\n\n# 公共生活的四个特征\n\n一是活动范围的广泛性\n二是活动内容的开放性\n三是交往对象的复杂性\n四是活动方式的多样性\n\n# 全心全意为人民服务是贯穿中国革命道德的一根红线\n\n是坚持历史唯物主义必然要求\n是中国共产党践行根本宗旨\n也是社会主义道德观集中体现\n是全体中国人民共同遵循道德要求\n是社会主义经济基础和人际关系的客观要求\n是社会主义市 场经济健康发展的要求\n是先进性要求和广泛性要求的统一\n为人民服务是社会主义道德的核心\n集体主义是社会主义道德的原则\n\n# 法律的作用\n\n普遍适用（遵守）\n优先适用\n不可违返（遵守 + 制裁）\n\n# 时政（2024 年）\n# 二十届三中全会\n\n通过了《中共中央关于进一步全面深化改革、推进中国式现代化的决定》（最重要成果）\n当前和今后一个时期是以中国式现代化全面推进强国建设、民族复兴伟业的关键时期\n高水平社会主体市场经济体制是中国式现代化的重要保障\n教育、科技、人才是中国式现代化的基础性、战略性支撑\n城乡融合发展是中国式现代化的必然要求\n开放是中国式现代化的鲜明标识\n发展全过程人民民主是中国式现代化的本质要求\n法治是中国式现代化的重要保障\n在发展中保障和改善民生是中国式现代化的重大任务\n国家安全是中国式现代化行稳致远的重要基础\n国防和军队现代化是中国式现代化的重要组成部分\n党的领导是进一步全面深化改革、推进中国式现代化的根本保证\n\n# 2024 年中非合作论坛北京峰会\n2024 年中非合作论坛北京峰会的主题为 “携手推进现代化，共筑高水平中非命运共同体”。\n治国理政、工业化和农业现代化、和平安全、高质量共建 “一带一路” 分别是中非合作论坛北京峰会四场高级别会议的议题。\n# 培养社会主义法治思维\n宪法保障、立法保障、行政保障和司法保障。\n宪法保障是权利保障的前提和基础，\n立法保障是权利保障的重要条件，\n行政保障是权利保障的关键环节，\n司法保障是公民权利保障的最后防线。\n# 建设中国特色社会主义法治体系\n建设中国特色社会主义法治体系，就是要形成完备的法律规范体系、高效的法治实施体系、严密的法治监督体系、有力的法治保障体系，形成完善的党内法规体系。\n形成完备的法律规范体系，是中国特色社会主义法治体系的前提，是法治国家、法治政府、法治社会的制度基础；\n建设高效的法治实施体系，是建设中国特色社会主义法治体系的重点；\n形成严密的法治监督体系，是宪法法律有效实施的重要保障；\n建设有力的法治保障体系，是全面依法治国的重要依托；\n形成完善的党内法规体系，是中国特色社会主义法治体系的本质要求和重要内容。\n# 六个必须坚持\n\n\n\n坚持\n是…\n体现了\n\n\n\n\n人民至上\n根本价值立场\n唯物主义群众史观\n\n\n自立自信\n内在精神特质\n客观规律性和主观能动性的有机结合\n\n\n守正创新\n鲜明理论品格\n变与不变，继承与发展的内在联系\n\n\n问题导向\n重要实践要求\n矛盾的普遍性与客观性\n\n\n系统观念\n基本思想和工作方法\n辩证唯物主义普遍联系的原理\n\n\n胸怀天下\n中国共产党人的境界格局\n马克思主义追求人类进步和解放的崇高理想\n\n\n\n\n\n\n\n\n# 新质生产力口诀\n关谷悠悠业务劳累咳嗽，立志全心做点心。\n\n关键在于质优（关谷悠悠）\n重点任务是新产业（业务）\n内涵 —— 劳动者、劳动对象、劳动资料（劳累）\n核心要素 —— 科技创新（咳嗽）\n本质 —— 先进生产力（立志）\n核心标志 —— 全要素生产率（全心）\n显著特点 —— 创新（创新）\n\n# 多民族国家…\n血脉相融 —— 历史根基\n信念相同 —— 内生动力\n文化相通 —— 文化基因\n经济相依 —— 强大力量\n# 科技创新和产业创新深度融合\n基础 —— 高质量科技供给\n关键 —— 企业科技创新主体地位\n途径 —— 促进科技成果转化应用\n# 主观题解题\n# 问题分类\n\n为什么\n是什么… 如何理解？\n意义是？\n怎么做？未来怎么做？\n\n四大类型 + 主语\n# 万能三步走\n观点【出发点】+ 落脚点 + 论据\n# 中国声音关键词\n携手共建人类命运共同体、和平、公道正义、共同发展、发展与安全、公正\n一切的核心是共同发展，构建人类命运共同体。\n和平共处五项原则发表 70 周年，引出中非合作论坛峰会，引出上海合作组织、引出国际友好大会、引出金砖国家领导人第十六次会晤、引出二十国集团领导人第十九次峰会。\n抓住主旋律：\n\n观点：世界和平发展、安全、公道正义、合作、多边主义\n落脚点：人类命运共同体、全人类共同价值\n\n# 小小模板\n# 38 时政，第一问\n当前，世界百年变局加速推进，一些西方国家为维护自身霸权，大搞单边主义、保护主义，建立封闭排他的“小圈子”，危害世界和平与共同发展。\n①（主语【材料】金砖....）坚持开放包容、合作共赢的初心使命，顺应全球南方崛起大势。\n②（主语【材料】中非....）走到一起，顺应世界和平和发展发出和平之声，倡导互惠包容的经济全球化和世界多极化，坚守共同发展的大道。\n③（主语【材料】和平共处五项原则....）树立了历史标杆，为不同社会制度国家建立和发展关系提供正确指导，为发展中国家团结合汇聚强大合力，为推动国际秩序朝着更加公正合理的方向贡献了中国智慧。\n\n# 38 时政，第二问\n历史和现实告诉我们，各国必须共担维护和平责任，同走和平发展道路，要秉持公道正义的理念，展现开放包容的胸襟。\n①我们要建设“和平金砖”、“创新金砖”、“绿色金砖”、“公正金砖”，“人文金砖”。\n②我们要坚持和平的原则，夯实相互尊重的基础，实现和平安全的愿景，汇聚共筑繁荣的动力。\n③提出“坚持和平，实现共同安全” “重振发展，实现普遍繁荣” “共兴文明，实现多元和谐发展”三点主张。\n在世界变局乱局中开辟长治久安、共同繁荣的人间正道，奋力开创中国特色大国外交更有作为的新局面，为进一步全面深化改革、推进中国式现化、推为构建人类命运共同体作出新的贡献。\n\n有金砖写一二，无金砖选二三\n# 答题逻辑\n\n抛出考点\n解释考点\n阐明关系\n方法论\n结合材料\n\n# 肖四 25—— 大题逻辑练习\n# 马原部分\n\n（1）结合 “该改的坚决改，不该改的不改”，说明为什么必须坚持守正创新。\n\n“该改的坚决改，不该的不改”，体现的正是守正与创新的辩证统一。“不该改的不改”，守正才能不迷失方向，不犯颠覆性错误，“该改的坚决改”，创新才能把握时代，引领时代。\n坚持守正不动摇，所谓守正，就是坚持实事求是，坚持真理性认识，坚持正确的政治方向。\n坚持创新不停步，所谓创新，就是坚持解放思想，秉承科学的思想观念，发现和运用事物的新联系、新观点、新规律，更有效的认识世界改造世界。\n守正是创新的前提和基础，创新是守正的目的和路径。守正创新深刻的揭示了 “变” 与 “不变”、继承与发展的辩证统一。\n守正创新为党和人民事业提供了科学的观点、立场、方法。\n\n（2）为什么要 “坚持破和立的辩证统一”？\n\n唯物辩证法认为，事物发发展是通过其自身的矛盾运动以自我否定的方式来实现的，着要求我们掌握破与立的辩证统一关系。\n所谓破，就是要破除与事物发展进程不相符合的旧观点、旧理论、旧方法。所谓立，就是秉承科学的理念，寻找并发现事物的新联系、新方法、理论。\n破与立是矛盾着的对立面，即相区别，又相联系，二者对立统一，在事物的发展中起到重要的作用。该原理要求我们掌握破与立的辩证统一关系。\n\n（1）运用唯物史观原理，说明为什么 “发展新质生产力，必须进一步全面深化改革，形成与之相适应的新型生产关系”。\n\n唯物史观认为，生产力与生产关系是对立统一的有机体。\n生产力时生产的物质内容，生产关系是生产的社会形式。\n发展新质生产力就要形成与之相匹配的生产关系这是因为，生产力决定生产关系，而生产关系对生产力具有能动的反作用。生产关系对生产力能动的反作用主要体现在两个方面：当生产关系适应生产力的发展时就生产力起到促进和推动的作用，当生产关系不适应生产力的发展时就对成产力起到阻碍的作用。\n生产力与生产关系的相互作用，构成了生产关系要适应生产力的客观规律，这是社会形态发展的普遍规律。\n\n（2）善用因地制宜的 “土办法” 体现了怎样的辩证思维？（或：为什么不能 “一哄而上”“简单 套用单一发展模式”，而要 “因地制宜” 发展新质生产力？）\n\n辩证法认为，矛盾具有普遍性和特殊性，二者是辩证统一的整体。\n矛盾的普遍性就是，矛盾无处不在，无时不有，矛盾具有普遍存在的特点。矛盾的特殊性在于，事物有着不同的矛盾以及同一矛盾在不同时期体现出不同的特点，只有具体分析矛盾的特殊性，才能认清事物的本质与发展规律。\n矛盾的普遍性和特殊性辩证统一也是共性与个性的辩证统一，共性寓于个性之中，个性也离不开共性，任何事物都是个性与共性的统一体。\n这要求我们掌握矛盾的普遍性与特殊性的其二者的关系，更好的理解事物的本质。善用 “因地制宜” 的办法正是体现了具体问题具体分析的辩证思维。\n\n（3）从理论创新和实践创新的辩证关系角度，分析为什么要将已经在实践中形成的新质生产力 “从理论上进行总结、概括，用以指导新的发展实践”。\n\n人类的创新活动主要体现在理论创新和实践创新两个方面，实践创新为理论创新提供了不竭动力，理论创新为实践创新提供了科学的行动指南，理论创新与实践创新二者构成了有机的统一体，我们要坚持将理论创新与实践创新相联系。\n理论与实践相统一，理论创新不仅要以实践创新为基础，还要以科学的知道反哺实践创新。\n","categories":["political"],"tags":["政治"]},{"title":"计算机操作系统复试速览","url":"/Operate-system/os_retest/","content":"# 计算机操作系统复试\n# 操作系统的作用\n\n是用户与计算机硬件之间的接口\n是计算机系统资源的管理者\n实现了对计算机资源的抽象\n\n# 操作系统基本特征\n\n并发\n共享\n虚拟\n异步\n\n# 操作系统最基本调特征\n\n并发\n共享\n\n# 处理机的双重工作模式\n用户态、核心态\n特权指令在核心态执行，非特权指令在用户态下执行\n# 操作系统主要功能\n\n处理机调度\n\n进程控制\n进程同步\n进程通信\n调度\n\n作业调度（高级调度）\n进程调度（低级调度）\n\n\n\n\n存储器管理\n\n内存的分配与回收\n内存保护\n地址映射\n内存扩充\n\n\n设备管理\n\n缓冲管理\n设备分配\n设备处理\n\n\n文件管理\n\n文件存储空间管理\n目录管理\n文件的读写管理和保护\n\n\n接口管理\n\n用户接口（脱机用户接口、联机用户接口、图形用户接口）\n程序接口\n\n\n\n# 程序并发执行特性\n\n间断性\n失去封闭性\n不可再现性\n\n# 进程的特征\n动态性、并发性、独立性、异步性\n# 进程的 3 状态模型、5 状态模型、7 状态模型\n# 单 CPU 中的状态\n\n\n\n状态\n最多\n最少\n\n\n\n\n运行\n1\n0\n\n\n就绪\nN-1\n0\n\n\n阻塞\nN\n0\n\n\n\n# PCB 中的信息\n\n进程标识符\n处理机状态\n进程调度信息\n进程控制信息\n\n# 拥有资源？调度？\n\n进程是拥有资源的\n线程是独立调度的\n\n# 高级调度（作业调度）\n决定哪些作业从外存调入内存准备执行，控制系统的并发度。\n# 中级调度（内存调度）\n管理内存中的进程，通过挂起和激活来调整内存使用。\n# 低级调度（进程调度）\n决定哪个就绪进程获得 CPU 执行。\n# 先来先服务 FCFS\n按作业或进程到达的顺序分配 CPU。\n\n简单易实现。\n非抢占式，可能导致 “护航效应”（短作业等待长作业完成）。\n平均等待时间较长，适合长作业。\n\n# 短作业优先 SJF\n优先调度运行时间最短的作业。\n\n可抢占或非抢占。\n最小化平均等待时间。\n可能导致长作业 “饥饿”。\n\n# 优先级调度 Priority Scheduling\n按优先级分配 CPU，优先级高的先执行。\n\n可抢占或非抢占。\n可能导致低优先级进程 “饥饿”。\n优先级可以是静态或动态调整。\n\n# 时间片轮转调度 RR\n每个进程分配一个固定时间片（Time Quantum），轮流执行。\n\n抢占式，适合分时系统。\n时间片大小影响性能：太小增加上下文切换开销，太大退化为 FCFS。\n公平性好，响应时间短。\n\n# 多级队列调度 Multilevel Queue Scheduling\n将进程分为多个队列，每个队列采用不同的调度算法。\n\n队列间可设置优先级。\n适合混合型任务（如前台交互任务和后台批处理任务）。\n\n# 多级反馈队列调度 Multilevel Feedback Queue Scheduling\n进程可在多个队列间移动，根据行为动态调整优先级。\n\n结合了优先级调度和轮转调度的优点。\n动态适应进程行为，避免 “饥饿”。\n实现复杂。\n\n# 算法总结\n\n\n\n算法\n特点\n适用场景\n\n\n\n\nFCFS\n简单，非抢占，护航效应\n批处理系统\n\n\nSJF\n最小化等待时间，可能导致长作业饥饿\n批处理系统\n\n\n优先级调度\n按优先级执行，可能导致低优先级饥饿\n实时系统\n\n\n轮转调度\n公平，响应时间短，时间片大小影响性能\n交互式系统\n\n\n多级队列调度\n多队列，不同策略\n混合任务系统\n\n\n多级反馈队列调度\n动态调整优先级，避免饥饿\n通用操作系统\n\n\n\n# 死锁\n死锁是操作系统中的一种资源竞争现象，指多个进程或线程因争夺资源而陷入无限等待的状态，导致它们都无法继续执行。死锁的发生通常需要满足四个必要条件，称为死锁的四个必要条件。\n# 死锁的四个必要条件\n\n互斥条件（Mutual Exclusion）：\n\n资源一次只能被一个进程占用，其他进程必须等待。\n\n\n请求与保持推荐（Hold and Wait）：\n\n进程已经占有一些资源，同时请求其他被占用的资源。\n\n\n不剥夺条件（No Preemption）：\n\n进程已获得的资源不能被强制剥夺，只能由进程自行释放。\n\n\n循环等待条件（Circular Wait）：\n\n存在一个进程等待的循环链，每个进程都在等待下一个进程占用的资源。\n\n\n\n# 死锁的处理方法\n\n死锁预防\n死锁避免\n死锁检测\n死锁解除\n\n# 银行家算法\n\n\n检查资源分配后系统是否处于安全状态（即是否存在一个执行序列使所有进程完成）。\n\n\n如果不安全，则拒绝分配请求。\n\n\n# 两种制约关系\n\n互斥关系\n同步关系\n\n# 解决临界区同步问题须遵循以下准则\n\n空闲让进\n忙则等待\n有限等待\n让权等待\n\n# 程序的装入方式\n\n绝对装入\n\n程序中的地址是固定的，装入时不需要进行地址转换。\n适用于单道程序环境，或者内存地址固定的系统。\n灵活性差，程序必须加载到指定的内存位置，无法适应多道程序环境。\n\n\n可重定位装入\n\n程序可以在内存的不同位置加载，适应多道程序设计。\n装入时需要进行地址重定位，通常由操作系统或装入程序完成。\n适用于多道程序环境，支持内存的动态分配。\n\n\n动态运行时装入\n\n程序可以部分加载，只有在需要时才将相关部分加载到内存。\n地址转换由硬件和操作系统共同完成。\n支持虚拟内存技术，允许程序的大小超过物理内存的容量。\n\n\n\n\n绝对装入：适用于单道程序环境，地址固定，装入简单但缺乏灵活性。\n可重定位装入：适用于多道程序环境，支持地址重定位，灵活性高。\n动态运行时装入：支持虚拟内存，程序可以动态加载，适合现代操作系统。\n\n# 连续分配存储器管理方式\n\n单一连续分配\n固定分区分配\n动态分区分配\n\n# 动态分区分配的算法\n\n首次适应算法\n循环首次适应算法\n最佳适配算法\n最坏适配算法\n\n# 分页存储\n# 分段存储\n# 页面置换算法\n\n先进先出算法\n最佳页面置换算法\n最近最久未使用算法\n最少使用算法\nClock 算法\n改进型 Clock 算法\n\n# 抖动\n** 抖动（Thrashing）** 是操作系统中与虚拟内存管理相关的一种现象，通常发生在系统内存资源不足时。当系统频繁地进行页面置换（即频繁地将内存中的页面换出到外存，再从外存换入新的页面），导致大部分时间都花在页面调度上，而不是执行实际任务时，就发生了抖动。\n# I/O 设别控制方式\n\n使用轮询的可编程 I/O 方式（落后）\n使用中断的可编程 I/O 方式（用的比较多）\n使用 DMA（减少了 CPU 对 I/O 的干预）\n使用 I/O 通道\n\n# 假脱机技术 - SPOOLing\n假脱机技术（SPOOLing，Simultaneous Peripheral Operations On-Line） 是一种用于管理输入 / 输出（I/O）操作的技术，特别是在处理低速外设（如打印机）时。SPOOLing 的核心思想是通过缓冲和排队机制，将外设的操作与主机的计算任务分离，从而提高系统的效率和资源利用率。\nSPOOLing 的基本原理\n\n输入井和输出井：\n\nSPOOLing 系统在磁盘上创建两个专门的存储区域：输入井和输出井。\n输入井用于暂存从输入设备（如读卡器）读取的数据。\n输出井用于暂存需要发送到输出设备（如打印机）的数据。\n\n\n缓冲和排队：\n\n当用户提交任务时，数据首先被写入磁盘的输入井或输出井，而不是直接发送到外设。\n外设（如打印机）从输出井中按顺序读取数据并执行操作，而主机可以继续执行其他任务。\n\n\n并行操作：\n\n主机和外设可以并行工作。主机不需要等待外设完成操作，而是将数据交给 SPOOLing 系统后继续执行其他任务。\n\n\n\nSPOOLing 的应用场景\n\n打印任务管理：\n\n打印机是典型的低速外设，SPOOLing 技术可以有效地管理多个打印任务，避免用户等待。\n\n\n批处理系统：\n\n在批处理系统中，SPOOLing 技术可以用于管理输入作业和输出结果。\n\n\n网络打印：\n\n现代网络打印机通常使用 SPOOLing 技术来管理来自多个用户的打印任务。\n\n\n\nSPOOLing 技术就是将独占设备转变为逻辑上的共享设备，提高了 I/O 速度，实现了虚拟设备的功能。\n# 文件在外存的组织方式\n\n连续组织方式\n链接组织方式（显示连接 FAT、隐式链接）\n索引组织方式（一级索引、二级索引…）\n\n# 显示链接\n显示链接通过一个专门的链接表来记录文件中各个块的物理地址。这个表通常称为文件分配表（FAT, File Allocation Table）。\n# 隐式链接\n隐式链接将每个块的指针存储在块本身中，而不是使用一个全局的链接表。每个块中除了存储数据外，还包含下一个块的地址。\n# 显示链接 vs 隐式链接\n\n\n\n特性\n显示链接（FAT）\n隐式链接\n\n\n\n\n存储结构\n使用全局的 FAT 表存储链接信息\n链接信息存储在块内部\n\n\n访问方式\n支持随机访问\n仅支持顺序访问\n\n\n空间开销\nFAT 表占用额外空间\n无额外空间开销\n\n\n实现复杂度\n较复杂\n较简单\n\n\n可靠性\nFAT 损坏可能导致文件系统失效\n单个块损坏可能影响文件完整性\n\n\n适用场景\n需要随机访问的场景\n顺序访问为主的场景\n\n\n\n# 小问题\n# 进程和程序有什么区别\n\n程序\n\n程序是存储在磁盘或其他存储介质上的静态实体，由一系列指令和数据组成。\n它是一个被动的实体，只有在被加载到内存并执行时才会发挥作用。\n\n\n进程\n\n进程是程序在内存中的动态执行实例。\n它是一个主动的实体，包含了程序的执行状态（如程序计数器、寄存器、堆栈等）以及系统分配的资源（如内存、文件句柄等）。\n\n\n\n\n\n\n特性\n程序（Program）\n进程（Process）\n\n\n\n\n定义\n静态的指令和数据集合\n程序的动态执行实例\n\n\n生命周期\n永久存在\n临时存在，从创建到终止\n\n\n状态\n无状态\n有状态（运行、就绪、阻塞等）\n\n\n资源占用\n不占用系统资源\n占用系统资源（CPU、内存等）\n\n\n并发性\n不支持并发\n支持并发\n\n\n独立性\n独立存在\n进程间独立，可通过 IPC 交互\n\n\n创建方式\n开发者编写，编译生成\n操作系统创建\n\n\n示例\n可执行文件（如  a.exe ）\n运行中的程序实例\n\n\n\n# 死锁是什么？死锁是怎么产生的？如何处理死锁？\n死锁是操作系统中的一种资源竞争问题，指的是多个进程或线程因为竞争资源而相互等待，导致它们都无法继续执行的状态。死锁是一种严重的系统问题，会导致系统资源浪费和程序无法正常运行。\n\n死锁的产生条件\n\n互斥条件\n不剥夺条件\n请求与保持条件\n循环等待条件\n\n\n死锁的处理方法\n\n死锁的预防\n死锁的检测\n死锁的避免\n死锁的解除\n\n\n\n# 饥饿是什么？和死锁有什么区别？\n饥饿是指某个进程或线程因为资源总是被其他进程抢占，导致它长期得不到所需的资源，从而无法继续执行。\n产生原因\n\n资源分配策略不公平：\n\n某些进程总是优先获得资源，而其他进程被忽略。\n\n\n优先级调度问题：\n\n高优先级进程不断抢占资源，低优先级进程始终得不到资源。\n\n\n资源竞争激烈：\n\n资源数量有限，且竞争资源的进程过多。\n\n\n\n\n\n\n特性\n饥饿（Starvation）\n死锁（Deadlock）\n\n\n\n\n定义\n某个进程长期得不到资源\n多个进程相互等待，无法继续执行\n\n\n成因\n资源分配不公平或优先级调度问题\n四个必要条件同时满足\n\n\n涉及进程数量\n可能只有一个进程被 “饿死”\n至少有两个或多个进程相互等待\n\n\n资源状态\n资源被其他进程占用，但未被阻塞\n资源被占用且进程相互阻塞\n\n\n解决方法\n公平调度、动态优先级调整、资源预留\n预防、避免、检测与恢复、忽略\n\n\n影响范围\n只影响被 “饿死” 的进程\n影响所有参与死锁的进程\n\n\n是否可恢复\n可以通过调整资源分配恢复\n需要外部干预（如终止进程）才能恢复\n\n\n\n","categories":["操作系统"],"tags":["OS"]},{"title":"操作系统课程设计","url":"/Operate-system/kcsj/","content":"# 虚拟存储器管理\n \n# 什么是虚拟存储器？\n​ 虚拟存储器是一种计算机内存管理技术，它通过将计算机的硬盘空间作为辅助存储器，允许程序使用比物理内存更大的地址空间。虚拟存储器的目标是提供更大的可用内存空间，以便同时运行更多的程序，而不受物理内存的限制。\n​ 虚拟存储的实现是基于局部存储原理的，要理解虚拟存储我们就要去理解局部存储的是实现，即：分页存储、分段存储、段页式存储。\n（这里我们重点掌握具有块表的分页存储）\n# 课程设计核心目的？\n将逻辑地址转为物理地址\n# 页表？\n\n\n\n页号\n块号\n\n\n\n\n0\n0\n\n\n1\n2\n\n\n2\n4\n\n\n3\n6\n\n\n\n针对这个课程设计，我们简单的理解为应用页表就是通过页号，来去找到块号\n\n由虚拟地址（逻辑地址）转换为物理地址，这就是页表最核心的作用！\n# 为什么要用虚拟内存\n但是，我们的内存很小，还想运行多个应用程序，这必然无法满足所有的有的应用程序一次性全部加载到内存之中。根据局部性理论基础，我们又引申出页面的对换算法（页面置换算法）。\n本次课程设计采用的是 Clock 算法\n将页表扩充，引入访问位、修改位、有效位（本次仅用于算法实现，它其实用其他\n更有用的功能，在此不做介绍）\n\n\n\n修改位\n访问位\n重要排名\n\n\n\n\n0\n0\n4\n\n\n0\n1\n3\n\n\n1\n0\n2\n\n\n1\n1\n1\n\n\n\n在置换时，优先考虑不那么重要的（排名越低越不重要），对于不重要的页优先换出\n# 状态转换？\n这里稍后同步，大家可以先看视频里的有关状态转换的图。\n# 代码实现\n/** * @Author KarryLiu * @Creed may all the beauty be blessed * @Date 2023/12/4 上午 10:04 * @Description TODO 诗岸梦行舟 * @Version 1.0 */#include &lt;limits>#include \"iostream\"#include \"windows.h\"#include \"unistd.h\"// 颜色枚举，为了好看enum ConsoleColor &#123;    Black = 0,    Blue = 1,    Green = 2,    Cyan = 3,    Red = 4,    Magenta = 5,    Yellow = 6,    White = 7,    Gray = 8&#125;;// 设置文本颜色函数void setConsoleColor(ConsoleColor text, ConsoleColor background) &#123;    int color = text + background * 16;    SetConsoleTextAttribute(GetStdHandle(STD_OUTPUT_HANDLE), color);&#125;// 块大小#define blockSize 4// 页大小#define pageSize 2// 快表尺寸#define fastTableDimensions 4// 页表尺寸#define pageTableDimensions 6// 程序最大逻辑地址const int logicalMaxAddress = 100;// 逻辑地址数据结构 -> 页struct logicalAddressDataStruct &#123;    int pageNumber;    int inPageAddress;&#125;;// 页逻辑地址实现机构logicalAddressDataStruct logicalAddress;// 模拟外存中单条数据的数据结构struct externalMemoryDataStruct &#123;    int externalPageNumber;    int externalBlockNumber;    // 访问位    bool accessBit;    // 修改位    bool modifyBit;    // 有效位    bool validBit;&#125;;// 外存所有存储结构，也就是最大逻辑地址 100 的一半，50 页externalMemoryDataStruct externalMemory[logicalMaxAddress / pageSize];// 模拟内存中单条数据的数据结构 (页表)struct internalStorageDataStruct &#123;    int internalPageNumber;    int internalBlockNumber;    // 访问位    bool accessBit;    // 修改位    bool modifyBit;    // 有效位    bool validBit;&#125;;// 页表实体internalStorageDataStruct pageTable[pageTableDimensions];// 快表数据结构struct fastTableDataStruct &#123;    int fastTablePageNumber;    int fastTableBlockNumber;    // 访问位    bool accessBit;    // 修改位    bool modifyBit;    // 有效位    bool validBit;&#125;;// 快表实体fastTableDataStruct fastTable[fastTableDimensions];// 快表置换排行榜数据结构struct rankForFastTableDataStruct &#123;    int pageNumber;    int fastTableIndex;&#125;;// 页表置换排行榜数据结构struct rankForPageTableDataStruct &#123;    int pageNumber;    int pageTableIndex;&#125;;// 物理地址数据结构struct physicalAddressDataStruct &#123;    int blockNumber;    int internalBlockAddress;&#125;;// 转换后的物理地址实体physicalAddressDataStruct physicalAddress;using namespace std;/** * @Describe 初始化外存数据 */void InitializeExternalData() &#123;    for (int i = 0; i &lt; logicalMaxAddress / pageSize; i++) &#123;        // 计算页号与块号        externalMemory[i].externalPageNumber = i;        externalMemory[i].externalBlockNumber = i / blockSize;    &#125;//    for (int i = 0; i &lt; logicalMaxAddress / pageSize; ++i) &#123;//        cout&lt;&lt;externalMemory[i].externalPageNumber&lt;&lt;\"   \"&lt;&lt;externalMemory[i].externalBlockNumber&lt;&lt;endl;//    &#125;&#125;/** * @Describe 从外存中寻找所缺失的页 * @return externalMemoryDataStruct 返回找到的外存数据页 */externalMemoryDataStruct LookMissingPageFromExternalMemory(int logicalAddressPageNumber) &#123;    for (int i = 0; i &lt; logicalMaxAddress / pageSize; i++) &#123;        // 计算页号与块号        if (externalMemory[i].externalPageNumber == logicalAddressPageNumber) &#123;            return externalMemory[i];        &#125;    &#125;&#125;void PageTableVisualization() &#123;    setConsoleColor(Magenta, Black);    cout &lt;&lt; \"-------------------页表（内存）-------------------+\" &lt;&lt; endl;    for (int i = 0; i &lt; pageTableDimensions; i++) &#123;        cout &lt;&lt; \"页号：\" &lt;&lt; pageTable[i].internalPageNumber &lt;&lt; \"  块号：\" &lt;&lt; pageTable[i].internalBlockNumber &lt;&lt; \"  访问位：\"             &lt;&lt; pageTable[i].accessBit &lt;&lt; \"  修改位：\" &lt;&lt; pageTable[i].modifyBit &lt;&lt; \"  有效位：\" &lt;&lt; pageTable[i].validBit             &lt;&lt; endl;    &#125;    cout &lt;&lt; \"--------------------------------------------------+\" &lt;&lt; endl;&#125;void FastTableVisualization() &#123;    setConsoleColor(Cyan, Black);//    sleep(1);    cout &lt;&lt; \"----------------------快表-----------------------+\" &lt;&lt; endl;    for (int i = 0; i &lt; fastTableDimensions; i++) &#123;        cout &lt;&lt; \"页号：\" &lt;&lt; fastTable[i].fastTablePageNumber &lt;&lt; \"  块号：\" &lt;&lt; fastTable[i].fastTableBlockNumber             &lt;&lt; \"  访问位：\" &lt;&lt; fastTable[i].accessBit &lt;&lt; \"  修改位：\" &lt;&lt; fastTable[i].modifyBit &lt;&lt; \"  有效位：\"             &lt;&lt; fastTable[i].validBit &lt;&lt; endl;    &#125;    cout &lt;&lt; \"-------------------------------------------------+\" &lt;&lt; endl;&#125;/** * @Describe 初始化页表 */void InitializePageTable() &#123;&#125;int main() &#123;    setConsoleColor(Cyan, Black);    cout&lt;&lt;\"诗岸梦行舟\"&lt;&lt;endl;    cout&lt;&lt;\"操作系统课程设计：虚拟存储器管理\"&lt;&lt;endl;    sleep(2);    // 系统请求的逻辑地址    int requestLogicalAddressByOS = 0;    // 初始化外存数据结构    InitializeExternalData();    // 快表发现位，真就是发现了，假就是没发现    bool fastTableFind = false;    // 在快表的哪一位发现的？    int fastTableFindPoint;    // 页表发现位，真就是发现了，假就是没发现    bool pageTableFind = false;    // 在页表的哪一位发现的？    int pageTableFindPoint;    // 当前内存容量（页表剩余容量）    int remainingCapacityOfPageTable = 6;    // 当前快表容量（快表剩余容量）    int remainingCapacityOfFastTable = 4;    while (true) &#123;        // 初始化物理地址数据暂存        physicalAddress.blockNumber = -1;        physicalAddress.internalBlockAddress = -1;        setConsoleColor(Blue, Black);//        sleep(2);        cout &lt;&lt; \"系统基本信息：\" &lt;&lt; endl;        cout &lt;&lt; \" +------------------------+\" &lt;&lt;\"----------------------+\"&lt;&lt; endl;        cout &lt;&lt; \" |  块大小：\" &lt;&lt; blockSize &lt;&lt; \"             |\" &lt;&lt;\"       xx大学       |\"&lt;&lt; endl;        cout &lt;&lt; \" |  页大小：\" &lt;&lt; pageSize &lt;&lt; \"             |\" &lt;&lt;\"     专业     |\"&lt;&lt; endl;        cout &lt;&lt; \" |  最大逻辑地址：\" &lt;&lt; logicalMaxAddress &lt;&lt; \"     |\" &lt;&lt;\"     OS  课程设计     |\"&lt;&lt; endl;        cout &lt;&lt; \" |  快表尺寸：\" &lt;&lt; fastTableDimensions &lt;&lt; \"           |\" &lt;&lt;\"   诗岸梦行舟  |\"&lt;&lt; endl;        cout &lt;&lt; \" |  页表尺寸：\" &lt;&lt; pageTableDimensions &lt;&lt; \"           |\" &lt;&lt;\" 虚 拟 存 储 器 管 理 |\"&lt;&lt; endl;        cout &lt;&lt; \" +------------------------+\" &lt;&lt;\"----------------------+\"&lt;&lt; endl;        FastTableVisualization();        PageTableVisualization();        setConsoleColor(White, Black);        cout &lt;&lt; \"请输入应用程序请求的逻辑地址：\" &lt;&lt; endl;        cin >> requestLogicalAddressByOS;        // 结束程序        if (requestLogicalAddressByOS &lt; 0) &#123; break; &#125;        // 逻辑地址越界        if (requestLogicalAddressByOS > logicalMaxAddress) &#123;            setConsoleColor(Red, Black);            cout &lt;&lt; \"您请求的地址超出最大逻辑地址！产生越界中断！\" &lt;&lt; endl;            setConsoleColor(White, Black);            sleep(2);            continue;        &#125;        setConsoleColor(Cyan, Black);        cout &lt;&lt; \"您请求的十进制地址：\" &lt;&lt; requestLogicalAddressByOS &lt;&lt; endl;        setConsoleColor(White, Black);        /**         * 计算逻辑地址数据         * P = [A / L]         * d = A % L         */        logicalAddress.pageNumber = requestLogicalAddressByOS / pageSize;        logicalAddress.inPageAddress = requestLogicalAddressByOS % pageSize;        cout &lt;&lt; \"计算后形成逻辑地址：\" &lt;&lt; endl;        setConsoleColor(Magenta, Black);        cout &lt;&lt; \"+---页号---+---页内地址----+\" &lt;&lt; endl;        cout &lt;&lt; \"|    \" &lt;&lt; logicalAddress.pageNumber &lt;&lt; \"    |       \" &lt;&lt; logicalAddress.inPageAddress &lt;&lt; \"       |\"             &lt;&lt; endl;        cout &lt;&lt; \"+----------+---------------+\" &lt;&lt; endl;//        cout&lt;&lt;\"按回车继续执行...\";//        cin.get();cin.get();        setConsoleColor(White, Black);        // CPU 检索块表        cout &lt;&lt; \"操作系统检索快表...\" &lt;&lt; endl;        for (int i = 0; i &lt; fastTableDimensions; i++) &#123;            if (fastTable[i].fastTablePageNumber == logicalAddress.pageNumber &amp;&amp; fastTable[i].validBit) &#123;                // 在快表中发现了与之对应的页号的数据页                setConsoleColor(Green, Black);                cout &lt;&lt; \"操作系统在快表中发现了相应的数据页!\" &lt;&lt; endl;                fastTableFindPoint = i;                // 将发现检查位置真                fastTableFind = true;                setConsoleColor(White, Black);                break;            &#125;        &#125;        if (!fastTableFind) &#123;            // 快表中没有，访问页表            setConsoleColor(Yellow, Black);            cout &lt;&lt; \"快表中没有发现相应的数据页!\" &lt;&lt; endl;            setConsoleColor(White, Black);            cout &lt;&lt; \"操作系统检索页表...\" &lt;&lt; endl;            // 从内存中（页表）寻找所缺失的页            for (int i = 0; i &lt; pageTableDimensions; i++) &#123;                if (pageTable[i].internalPageNumber == logicalAddress.pageNumber &amp;&amp; pageTable[i].validBit) &#123;                    // 在内存中（页表）找到了数据页                    setConsoleColor(Green, Black);                    cout &lt;&lt; \"操作系统在页表中发现了相应的数据页!\" &lt;&lt; endl;                    pageTableFind = true;                    pageTableFindPoint = i;                    setConsoleColor(White, Black);                    break;                &#125;            &#125;            if (!pageTableFind) &#123;                // 页表（内存）中没有                setConsoleColor(Yellow, Black);                cout &lt;&lt; \"页中没有发现相应的数据页!\" &lt;&lt; endl;                setConsoleColor(White, Black);                cout &lt;&lt; \"操作系统会正在从外存中抽取页...\" &lt;&lt; endl;                // 从外存中抽取页                externalMemoryDataStruct externalMemoryFindPage = LookMissingPageFromExternalMemory(                        logicalAddress.pageNumber);                /*cout&lt;&lt;externalMemoryFindPage.externalPageNumber&lt;&lt;\"  \"&lt;&lt;externalMemoryFindPage.externalBlockNumber;*/                cout &lt;&lt; \"成功抽取到了，现在正在检查内存是否已满...\" &lt;&lt; endl;                sleep(1);                if (!(remainingCapacityOfPageTable > 0)) &#123;                    // 内存（页表）已满                    // 这里得选择一个最没用的一页对换出去                    setConsoleColor(Red, Black);                    cout &lt;&lt; \"内存已满，操作系统需要进行页面置换，正在选择一个最没用的一页进行换出...\" &lt;&lt; endl;;                    setConsoleColor(White, Black);                    // 开始选择一个最没用的一页                    // 页表置换排行榜                    rankForPageTableDataStruct rankForPageTable[pageTableDimensions];                    // 初始化排行榜                    for (int i = 0; i &lt; pageTableDimensions; i++) &#123;                        rankForPageTable[i].pageNumber - 1;                        rankForPageTable[i].pageTableIndex = -1;                    &#125;                    // 模拟态桶排序                    for (int i = 0; i &lt; pageTableDimensions; i++) &#123;                        if (pageTable[i].validBit                            &amp;&amp; !pageTable[i].accessBit                            &amp;&amp; !pageTable[i].modifyBit) &#123;                            rankForPageTable[0].pageTableIndex = i;                            rankForPageTable[0].pageNumber = pageTable[i].internalPageNumber;                        &#125; else if (pageTable[i].validBit                                   &amp;&amp; pageTable[i].accessBit                                   &amp;&amp; !pageTable[i].modifyBit) &#123;                            rankForPageTable[1].pageTableIndex = i;                            rankForPageTable[1].pageNumber = pageTable[i].internalPageNumber;                        &#125; else if (pageTable[i].validBit                                   &amp;&amp; !pageTable[i].accessBit                                   &amp;&amp; pageTable[i].modifyBit) &#123;                            rankForPageTable[2].pageTableIndex = i;                            rankForPageTable[2].pageNumber = pageTable[i].internalPageNumber;                        &#125; else &#123;                            rankForPageTable[3].pageTableIndex = i;                            rankForPageTable[3].pageNumber = pageTable[i].internalPageNumber;                        &#125;                    &#125;                    for (int i = 0; i &lt; pageTableDimensions; i++) &#123;                        if (rankForPageTable[i].pageNumber != -1) &#123;                            // 找到一个没用的页                            setConsoleColor(Green, Black);                            cout &lt;&lt; \"操作系统页表表选择了一个页号为\"                                 &lt;&lt; pageTable[rankForPageTable[i].pageTableIndex].internalPageNumber                                 &lt;&lt; \"的页进行换出...\"                                 &lt;&lt; endl;;                            setConsoleColor(White, Black);                            sleep(1);                            // 把页表中对应的页号置为无效                            pageTable[rankForPageTable[i].pageTableIndex].validBit = false;                            // 页表页面置换                            pageTable[rankForPageTable[i].pageTableIndex].internalPageNumber = externalMemoryFindPage.externalPageNumber;                            pageTable[rankForPageTable[i].pageTableIndex].internalBlockNumber = externalMemoryFindPage.externalBlockNumber;                            pageTable[rankForPageTable[i].pageTableIndex].accessBit = false;                            pageTable[rankForPageTable[i].pageTableIndex].modifyBit = false;                            pageTable[rankForPageTable[i].pageTableIndex].validBit = true;                            break;                        &#125;                    &#125;                &#125; else &#123;                    // 内存（页表）仍有空间                    // 有空间就可以塞进去                    setConsoleColor(Blue, Black);                    cout &lt;&lt; \"内存还有空间，操作系统正在读取缺页并回写内存中...\" &lt;&lt; endl;;                    setConsoleColor(White, Black);                    for (int i = 0; i &lt; pageTableDimensions; i++) &#123;                        if (pageTable[i].validBit == false) &#123;                            pageTable[i].validBit = true;                            pageTable[i].internalPageNumber = externalMemoryFindPage.externalPageNumber;                            pageTable[i].internalBlockNumber = externalMemoryFindPage.externalBlockNumber;                            break;                        &#125;                    &#125;                    setConsoleColor(Green, Black);                    cout &lt;&lt; \"页表回写成功！\" &lt;&lt; endl;;                    setConsoleColor(White, Black);                    // 塞完就减减                    remainingCapacityOfPageTable--;                &#125;                if (!(remainingCapacityOfFastTable > 0)) &#123;                    // 快表没有空间了                    setConsoleColor(Red, Black);                    cout &lt;&lt; \"快表已满，操作系统需要进行页面置换，正在选择一个最没用的一页进行换出...\" &lt;&lt; endl;;                    setConsoleColor(White, Black);                    // 选择一个最没用的一页进行换出！！！！！！！！！！                    bool fastTableFindCheckIsTrue = false;                    int fastTableFindCheckIndex;                    // 快表置换排行榜                    rankForFastTableDataStruct rankForFastTable[fastTableDimensions];                    // 初始化排行榜                    for (int i = 0; i &lt; fastTableDimensions; i++) &#123;                        rankForFastTable[i].pageNumber = -1;                        rankForFastTable[i].fastTableIndex = -1;                    &#125;                    // 模拟态桶排序                    for (int i = 0; i &lt; fastTableDimensions; i++) &#123;                        if (fastTable[i].validBit                            &amp;&amp; !fastTable[i].accessBit                            &amp;&amp; !fastTable[i].modifyBit) &#123;                            rankForFastTable[0].fastTableIndex = i;                            rankForFastTable[0].pageNumber = fastTable[i].fastTablePageNumber;                        &#125; else if (fastTable[i].validBit                                   &amp;&amp; fastTable[i].accessBit                                   &amp;&amp; !fastTable[i].modifyBit) &#123;                            rankForFastTable[1].fastTableIndex = i;                            rankForFastTable[1].pageNumber = fastTable[i].fastTablePageNumber;                        &#125; else if (fastTable[i].validBit                                   &amp;&amp; !fastTable[i].accessBit                                   &amp;&amp; fastTable[i].modifyBit) &#123;                            rankForFastTable[2].fastTableIndex = i;                            rankForFastTable[2].pageNumber = fastTable[i].fastTablePageNumber;                        &#125; else &#123;                            rankForFastTable[3].fastTableIndex = i;                            rankForFastTable[3].pageNumber = fastTable[i].fastTablePageNumber;                        &#125;                    &#125;                    for (int i = 0; i &lt; fastTableDimensions; i++) &#123;                        if (rankForFastTable[i].pageNumber != -1) &#123;                            // 找到一个没用的页                            setConsoleColor(Green, Black);                            cout &lt;&lt; \"操作系统在快表选择了一个页号为\"                                 &lt;&lt; fastTable[rankForFastTable[i].fastTableIndex].fastTablePageNumber                                 &lt;&lt; \"的页进行换出...\"                                 &lt;&lt; endl;                            setConsoleColor(White, Black);                            sleep(1);                            // 把快表中对应的页号置为无效                            fastTable[rankForFastTable[i].fastTableIndex].validBit = false;                            // 快表页面置换                            fastTable[rankForFastTable[i].fastTableIndex].fastTablePageNumber = externalMemoryFindPage.externalPageNumber;                            fastTable[rankForFastTable[i].fastTableIndex].fastTableBlockNumber = externalMemoryFindPage.externalBlockNumber;                            fastTable[rankForFastTable[i].fastTableIndex].accessBit = false;                            fastTable[rankForFastTable[i].fastTableIndex].modifyBit = false;                            fastTable[rankForFastTable[i].fastTableIndex].validBit = true;                            // 把页表中对应的页号置为无效？                            fastTableFindCheckIsTrue = true;                            break;                        &#125;                    &#125;                    if (fastTableFindCheckIsTrue) &#123;                    &#125; else &#123;                        setConsoleColor(Red, Black);                        cout &lt;&lt; \"一般不存在这种情况，以防万一留着DeBug用\" &lt;&lt; endl;                        setConsoleColor(White, Black);                    &#125;                &#125; else &#123;                    // 快表仍然有空间                    // 有空间就塞                    setConsoleColor(Blue, Black);                    cout &lt;&lt; \"快表还有空间，操作系统正在读取缺页并回写内存中...\" &lt;&lt; endl;;                    setConsoleColor(White, Black);                    for (int i = 0; i &lt; fastTableDimensions; i++) &#123;                        if (fastTable[i].validBit == false) &#123;                            fastTable[i].validBit = true;                            fastTable[i].fastTablePageNumber = externalMemoryFindPage.externalPageNumber;                            fastTable[i].fastTableBlockNumber = externalMemoryFindPage.externalBlockNumber;                            break;                        &#125;                    &#125;                    setConsoleColor(Green, Black);                    cout &lt;&lt; \"快表回写成功！\" &lt;&lt; endl;;                    setConsoleColor(White, Black);                    // 塞完就减减                    remainingCapacityOfFastTable--;                &#125;                //!!!!!!!!!!!!!!                physicalAddress.blockNumber = externalMemoryFindPage.externalBlockNumber;                physicalAddress.internalBlockAddress = logicalAddress.inPageAddress;                cout &lt;&lt; \"计算后得到物理地址：\" &lt;&lt; endl;                setConsoleColor(Magenta, Black);                cout &lt;&lt; \"+---块号---+---块内地址----+\" &lt;&lt; endl;                cout &lt;&lt; \"|    \" &lt;&lt; physicalAddress.blockNumber &lt;&lt; \"    |       \" &lt;&lt; physicalAddress.internalBlockAddress                     &lt;&lt; \"       |\"                     &lt;&lt; endl;                cout &lt;&lt; \"+----------+---------------+\" &lt;&lt; endl;                sleep(2);            &#125; else &#123;                // 页表中有                // 页表中的话，修改页表后还需要进行快表置换                /**                 * 如果页表里面有，那就更改他的访问位和修改位，                 * 为了编写简单，在这里我做出一个规定：                 * 当应用程序第一次访问时将访问位置真，                 * 当应用程序第二次访问时将修改为置真。                 * 这么做也是相对合理的，原因如下：                 * 访问位与修改位联合置换逻辑（修改，访问）即对换优先级：                 * * * 1.（0，0）                 * * * 2.（0，1）                 * * * 3.（1，0）                 * * * 4.（1，1）                 * 排名越高，置换优先级越高，                 * 相对来讲，访问位与修改位置真后，访问位优先级高于修改位                 * 其实我懒了 但我不说（qwq）                 */                // 快表置换，基于页表                // 快表置换排行榜                rankForFastTableDataStruct rankForFastTable[fastTableDimensions];                // 初始化排行榜                for (int i = 0; i &lt; fastTableDimensions; i++) &#123;                    rankForFastTable[i].pageNumber = -1;                    rankForFastTable[i].fastTableIndex = -1;                &#125;                // 模拟态桶排序                for (int i = 0; i &lt; fastTableDimensions; i++) &#123;                    if (fastTable[i].validBit                        &amp;&amp; !fastTable[i].accessBit                        &amp;&amp; !fastTable[i].modifyBit) &#123;                        rankForFastTable[0].fastTableIndex = i;                        rankForFastTable[0].pageNumber = fastTable[i].fastTablePageNumber;                    &#125; else if (fastTable[i].validBit                               &amp;&amp; fastTable[i].accessBit                               &amp;&amp; !fastTable[i].modifyBit) &#123;                        rankForFastTable[1].fastTableIndex = i;                        rankForFastTable[1].pageNumber = fastTable[i].fastTablePageNumber;                    &#125; else if (fastTable[i].validBit                               &amp;&amp; !fastTable[i].accessBit                               &amp;&amp; fastTable[i].modifyBit) &#123;                        rankForFastTable[2].fastTableIndex = i;                        rankForFastTable[2].pageNumber = fastTable[i].fastTablePageNumber;                    &#125; else &#123;                        rankForFastTable[3].fastTableIndex = i;                        rankForFastTable[3].pageNumber = fastTable[i].fastTablePageNumber;                    &#125;                &#125;                for (int i = 0; i &lt; fastTableDimensions; i++) &#123;                    if (rankForFastTable[i].pageNumber != -1) &#123;                        // 找到一个没用的页                        setConsoleColor(Green, Black);                        cout &lt;&lt; \"操作系统在快表选择了一个页号为\"                             &lt;&lt; fastTable[rankForFastTable[i].fastTableIndex].fastTablePageNumber                             &lt;&lt; \"的页进行换出...\"                             &lt;&lt; endl;;                        setConsoleColor(White, Black);                        sleep(1);                        // 把快表中对应的页号置为无效                        fastTable[rankForFastTable[i].fastTableIndex].validBit = false;                        // 快表页面置换                        fastTable[rankForFastTable[i].fastTableIndex].fastTablePageNumber = pageTable[pageTableFindPoint].internalPageNumber;                        fastTable[rankForFastTable[i].fastTableIndex].fastTableBlockNumber = pageTable[pageTableFindPoint].internalBlockNumber;                        fastTable[rankForFastTable[i].fastTableIndex].accessBit = false;                        fastTable[rankForFastTable[i].fastTableIndex].modifyBit = false;                        fastTable[rankForFastTable[i].fastTableIndex].validBit = true;                        // 物理地址存储                        physicalAddress.blockNumber = pageTable[pageTableFindPoint].internalBlockNumber;                        physicalAddress.internalBlockAddress = logicalAddress.inPageAddress;                        break;                    &#125;                &#125;                cout &lt;&lt; \"计算后得到物理地址：\" &lt;&lt; endl;                setConsoleColor(Magenta, Black);                cout &lt;&lt; \"+---块号---+---块内地址----+\" &lt;&lt; endl;                cout &lt;&lt; \"|    \" &lt;&lt; physicalAddress.blockNumber &lt;&lt; \"    |       \" &lt;&lt; physicalAddress.internalBlockAddress                     &lt;&lt; \"       |\"                     &lt;&lt; endl;                cout &lt;&lt; \"+----------+---------------+\" &lt;&lt; endl;                // 修改页表                if (pageTable[pageTableFindPoint].validBit                    &amp;&amp; !pageTable[pageTableFindPoint].accessBit                    &amp;&amp; !pageTable[pageTableFindPoint].modifyBit) &#123;                    pageTable[pageTableFindPoint].accessBit = true;                &#125; else if (pageTable[pageTableFindPoint].validBit                           &amp;&amp; pageTable[pageTableFindPoint].accessBit                           &amp;&amp; !pageTable[pageTableFindPoint].modifyBit) &#123;                    pageTable[pageTableFindPoint].accessBit = false;                    pageTable[pageTableFindPoint].modifyBit = true;                &#125; else if (pageTable[pageTableFindPoint].validBit                           &amp;&amp; !pageTable[pageTableFindPoint].accessBit                           &amp;&amp; pageTable[pageTableFindPoint].modifyBit) &#123;                    pageTable[pageTableFindPoint].accessBit = true;                    pageTable[pageTableFindPoint].modifyBit = true;                &#125;                // 同步快表                for (int i = 0; i &lt; fastTableDimensions; i++) &#123;                    if (fastTable[i].fastTablePageNumber == pageTable[pageTableFindPoint].internalPageNumber) &#123;                        if (fastTable[i].validBit                            &amp;&amp; !fastTable[i].accessBit                            &amp;&amp; !fastTable[i].modifyBit) &#123;                            fastTable[i].accessBit = true;                        &#125; else if (fastTable[i].validBit                                   &amp;&amp; fastTable[i].accessBit                                   &amp;&amp; !fastTable[i].modifyBit) &#123;                            fastTable[i].accessBit = false;                            fastTable[i].modifyBit = true;                        &#125; else if (fastTable[i].validBit                                   &amp;&amp; !fastTable[i].accessBit                                   &amp;&amp; fastTable[i].modifyBit) &#123;                            fastTable[i].accessBit = true;                            fastTable[i].modifyBit = true;                        &#125;                        break;                    &#125;                &#125;            &#125;        &#125; else &#123;            // 快表中有            // 修改快表            fastTable[fastTableFindPoint];            if (fastTable[fastTableFindPoint].validBit                &amp;&amp; !fastTable[fastTableFindPoint].accessBit                &amp;&amp; !fastTable[fastTableFindPoint].modifyBit) &#123;                fastTable[fastTableFindPoint].accessBit = true;            &#125; else if (fastTable[fastTableFindPoint].validBit                       &amp;&amp; fastTable[fastTableFindPoint].accessBit                       &amp;&amp; !fastTable[fastTableFindPoint].modifyBit) &#123;                fastTable[fastTableFindPoint].accessBit = false;                fastTable[fastTableFindPoint].modifyBit = true;            &#125; else if (fastTable[fastTableFindPoint].validBit                       &amp;&amp; !fastTable[fastTableFindPoint].accessBit                       &amp;&amp; fastTable[fastTableFindPoint].modifyBit) &#123;                fastTable[fastTableFindPoint].accessBit = true;                fastTable[fastTableFindPoint].modifyBit = true;            &#125;            // 同步页表            for (int i = 0; i &lt; pageTableDimensions; i++) &#123;                if (pageTable[i].internalPageNumber == fastTable[fastTableFindPoint].fastTablePageNumber) &#123;                    if (pageTable[i].validBit                        &amp;&amp; !pageTable[i].accessBit                        &amp;&amp; !pageTable[i].modifyBit) &#123;                        pageTable[i].accessBit = true;                    &#125; else if (pageTable[i].validBit                               &amp;&amp; pageTable[i].accessBit                               &amp;&amp; !pageTable[i].modifyBit) &#123;                        pageTable[i].accessBit = false;                        pageTable[i].modifyBit = true;                    &#125; else if (pageTable[i].validBit                               &amp;&amp; !pageTable[i].accessBit                               &amp;&amp; pageTable[i].modifyBit) &#123;                        pageTable[i].accessBit = true;                        pageTable[i].modifyBit = true;                    &#125;                    break;                &#125;            &#125;            // 直接从快表中提取            physicalAddress.blockNumber = fastTable[fastTableFindPoint].fastTableBlockNumber;            physicalAddress.internalBlockAddress = logicalAddress.inPageAddress;            cout &lt;&lt; \"计算后得到物理地址：\" &lt;&lt; endl;            setConsoleColor(Magenta, Black);            cout &lt;&lt; \"+---块号---+---块内地址----+\" &lt;&lt; endl;            cout &lt;&lt; \"|    \" &lt;&lt; physicalAddress.blockNumber &lt;&lt; \"    |       \" &lt;&lt; physicalAddress.internalBlockAddress                 &lt;&lt; \"       |\"                 &lt;&lt; endl;            cout &lt;&lt; \"+----------+---------------+\" &lt;&lt; endl;            sleep(2);        &#125;        // 恢复初始状态        fastTableFind = false;        pageTableFind = false;        setConsoleColor(Green, Black);        sleep(1);        cout &lt;&lt; \"---------------地址变换结束---------------\";        sleep(1);        cout &lt;&lt; endl;        setConsoleColor(White, Black);    &#125;    cout &lt;&lt; \"下次再见！\";&#125;","categories":["操作系统"],"tags":["OS"]},{"title":"DB中的范式","url":"/database/DataBase01/","content":"# 数据库范式\n# 什么是数据库范式\n数据库范式（Normalization）是一种设计关系型数据库的方法，它旨在减少数据冗余，提高数据的一致性和可靠性，避免数据修改时出现异常。数据库范式通常分为一到五个级别，每个级别对应一组设计规则。\n范式越高，数据冗余越少，数据一致性和可靠性越高。但是，范式过高也可能会带来一些问题，如增加了数据库的复杂性和查询的开销，需要根据实际应用场景来确定使用哪个范式。\n# 层级分类\n# 1NF（第一范式）\n关系模式中的所有属性都是原子性的，即不可再分。\n# 2NF（第二范式）\n关系模式必须满足第一范式，且不存在非关键字属性对主键的部分依赖。\n# 3NF（第三范式）\n关系模式必须满足第二范式，且不存在非关键字属性对其他非关键字属性的传递依赖。\n# BCNF（Boyce-Codd 范式）\n关系模式必须满足第一范式，且不存在非主属性对主属性的非平凡依赖关系。\n# 4NF（第四范式）\n关系模式必须满足第三范式，且不存在多值依赖。\n# 5NF（第五范式）\n关系模式必须满足第四范式，且不存在连接依赖。\n# 小题目\n让我们通过一些题目来感受一下吧！！\n# Ex1\n\n设有关系 R（工号，姓名，工种，定额），则 R 是属于第 2 范式，将其转化为第三范式\n\n根据关系 R（工号，姓名，工种，定额），我们可以看出工号和姓名是候选键，因为它们可以唯一地标识每个元组。同时，我们也可以看出工种和定额这两个属性完全依赖于候选键，因此关系 R 符合 2NF 的要求。\n现在我们来考虑将 R 转化为 3NF 的过程。根据 3NF 的要求，我们需要消除任何非主属性对主键的传递依赖。因此，我们需要检查每个非主属性是否直接依赖于主键，或者是直接依赖于其他非主属性。\n在关系 R 中，我们可以发现定额属性直接依赖于工种属性，而工种属性并不是主键的一部分。因此，我们可以将定额和工种拆分到一个新的关系中，以消除定额对于工种的传递依赖，从而使得关系 R 满足 3NF 的要求。\n拆分后的两个关系如下：\n\nR1（工号，姓名，工种）：该关系包含了工号、姓名和工种三个属性，其中工号和姓名作为候选键，工种是一个非主属性，但它直接依赖于主键。因此，关系 R1 符合 2NF 和 3NF 的要求。\nR2（工种，定额）：该关系包含了工种和定额两个属性，其中工种作为主键，定额是一个非主属性，但它直接依赖于主键。因此，关系 R2 符合 2NF 和 3NF 的要求。\n\n最终，我们将关系 R 转化为两个符合 3NF 的关系 R1 和 R2，符合 3NF 的要求。\n# Ex2\n\n设有关系 STUDENT (S#,SNAME,SDEPT,MNAME,CNAME,GRADE)，S#,CNAME 为候选码，设关系中有如下函数依赖：\nS#,CNAME→SNAME,SDEPT,MNAME\nS#→SNAME,SDEPT,MNAME\nS#,CNAME→GRADE\nSDEPT→MNAME\n 试求下列问题：\n（1）关系 STUDENT 属于第几范式？\n（2）如果关系 STUDENT 不属于 BCNF，请将关系 STUDENT 逐步分解为 BCNF。\n要求：写出达到每一级范式的分解过程，并指明消除什么类型的函数依赖。\n\n（1）1NF\n（2）首先消除非主属性对候选码的部分依赖，即消除部分函数依赖\n（S#，CNAME）-&gt;（SNAME,SDEPT,MNAME）\nR 分解为：\nR1(S#,SNAME,SDEPT,MNAME)\nR2(S#,CNAME,GRADE)\nR 至此分解为 2NF\n此时 R1 中存在非主属性对候选码的传递依赖，即消除传递依赖\n（S#）-&gt;（SDEPT）\n（SDEPT）-&gt;（MNAME）\nR1 分解为：\nR11(S#,SNAME,SDEPT)\nR12(SDEPT,MNAME)\nR1 至此分解为 3NF\n此时 R11，R12，R2 可以得出的函数依赖：\n（S#,CNAME）-&gt;（GRADE）\n（S#）-&gt;（SNAME）,（S#）-&gt;（SDEPT）\n（SDEPT）-&gt;（MNAME）\n​ 由此可以看出，数据表中的每个非主属性都完全依赖于候选键，即每个非主属性都不只依赖于候选键的一部分，且非主属性无法决定另一个非主属性，即不存在非平凡函数依赖。\n​ 上述关系表 R11，R12，R2 满足 BCNF\n# Ex3\n\n一个关系模式不属于第二范式可能会产生 ()、() 和 ( ) 等几个问题，解决的办法是 ( )。\n\n\n\n第一空：\n插入异常\n\n\n第二空：\n删除异常\n\n\n第三空：\n修改异常\n\n\n第四空：\n投影分解\n\n\n# 关键词\n# 候选码和非主属性\n候选码和非主属性是关系数据库中的两个概念，它们之间有一定的关系。\n候选码（Candidate Key）指的是能够唯一标识关系模式中每一条记录的属性集合。一个关系模式可能有多个候选码，但是其中只有一个会被选择作为主键（Primary Key）来标识每一条记录。通常情况下，主键会被用来建立关系模式之间的联系，因此选择一个恰当的候选码作为主键是关系数据库设计中的重要步骤。\n非主属性指的是关系模式中除了主键之外的属性。一个关系模式可能包含多个非主属性，它们之间可以存在依赖关系。例如，一个关系模式 R（A，B，C）中，如果存在函数依赖 A→B，则 B 是非主属性。\n候选码和非主属性之间的关系在于，一个候选码包含了所有的主属性，而非主属性可能依赖于主属性，也可能存在依赖于其他非主属性的情况。在进行关系数据库设计时，我们需要通过确定候选码和依赖关系来规范化关系模式，从而消除数据冗余和不一致性的问题。通常情况下，我们会选择最小的候选码作为主键，并确保所有非主属性都完全依赖于主键，以保证数据的完整性和一致性。\n# 平凡依赖关系\n平凡依赖关系是指对于一个属性集合 X，在一个关系 R 中如果 X 的任意超集都可以唯一地确定 X，那么 X 对于 R 就是平凡的依赖。例如，在一个关系 R（A, B, C），如果存在函数依赖 A→A，那么 A 对于 R 来说是一个平凡依赖。因为 A 的任意超集都可以唯一地确定 A 本身，即 A 已经包含了它所依赖的属性。\n可以这样理解，一个属性集合如果能够唯一地确定自己，那么它就是平凡的依赖。例如，在一个关系 R（A, B, C），如果存在函数依赖 A→A，那么 A 对于 R 来说是一个平凡依赖，因为属性集合 A 已经包含了它所依赖的属性 A 本身，即 A 已经能够唯一地确定自己。而非平凡的依赖是指一个属性集合不能唯一地确定自己，需要依赖于其他属性才能确定自己，例如在关系 R（A, B, C）中，存在函数依赖 A→B，B 就是一个非平凡依赖，因为 B 不能唯一地确定自己，需要依赖于 A 才能确定。\n# 传递依赖\n传递依赖（Transitive Dependency）指的是一个非主属性依赖于另一个非主属性的非主属性。例如，一个关系模式 R（A，B，C）中，如果存在函数依赖 A→B 和 B→C，则 C 对 A 存在传递依赖。这种情况下，如果我们要更新 A 的值，就会导致 C 的值也随之改变，从而引起数据不一致的问题。\n","categories":["数据库"],"tags":["数据库"]},{"title":"booth算法","url":"/Principles-of-computer-composition/booth_algorithm/","content":"# Booth 算法\n这里写点什么捏？就先来点简介吧！！比较法是 Booth 夫妇先提出来的，也称 booth 算法。\n那么！！咱们怎么解题呢，不要担心，其实 booth 算法非常简单，不要被长长的算式所迷惑鸭！！\n接下来就由我来简单快速的叙述一下如何利用 booth 算法去做题！\n# 准备开始！\n首先题目里一般会这样给：已知 [x] 补 = xxxxx，[y] 补 = xxxxx，利用 booth 算法求解 [x*y]。\nbooth 算法的符号位是成双成对的，和你不一样，因为你还没有成双成对哈哈哈哈哈哈（不是）\n另外，我们还需要额外记住的四个固定形态：\n\n\n\n尾数形态\n作法\n\n\n\n\n00\n右移一位\n\n\n01\n+[x] 补 后 右移一位\n\n\n10\n+[-x] 补后 右移一位\n\n\n11\n右移一位\n\n\n\n# 例题一\n已知 [x] 补 = 0.0101，[y] 补 = 1.0101，利用 booth 算法求解 [x*y]\n做题前我们需要做一些准备，首先我们写出双符号位形态的 [x] 补、[-x] 补以及 [y] 补（无需双符号）\n\n\n[x] 补 = 00.0101\n\n\n[-x] 补 = 11.1011\n\n\n[y] 补 = 1.0101\n\n\n然后列出如下表格：\n\n\n\n部分积\n乘数\n辅助\n\n\n\n\n\n\n0\n\n\n\n我先写到这，如果有想先试试的小伙伴可以先按照自己的思路去写，还没记起来的的小伙伴们可以继续往下看，一会我还会写出两道例题供大家去练习。\n解答开始！\n辅助位置默认为 0，乘数就是 [y] 补 = 11.0101，部分积初始值也为 0.\n\n\n\n\n部分积\n乘数\n辅助\n\n\n\n\n\n\n00 0000\n1010 1\n0\n一次判断\n\n\n10 还记得加什么嘛？没错就是加 [-x] 补 +\n11 1011\n\n\n\n\n\n=\n11 1011\n\n\n\n\n\n右移\n11 1101\n1101 0\n1\n二次判断\n\n\n01 还记得加什么嘛？没错就是加 [x] 补 +\n00 0101\n\n\n\n\n\n=\n00 0010\n\n\n\n\n\n右移\n00 0001\n0110 1\n0\n三次判断\n\n\n10 还记得加什么嘛？没错就是加 [-x] 补 +\n11 1011\n\n\n\n\n\n=\n11 1100\n\n\n\n\n\n右移\n11 1110\n0011 0\n1\n四次判断\n\n\n01 还记得加什么嘛？没错就是加 [x] 补 +\n00 0101\n\n\n\n\n\n=\n00 0011\n\n\n\n\n\n右移\n00 0001\n1001 1\n0\n五次判断\n\n\n10 还记得加什么嘛？没错就是加 [-x] 补 +\n11 1011\n\n\n\n\n\n\n11 1100\n\n\n\n\n\n\nOK 兄弟们，小数点后有四位，那么我们需要进行五次判断，现在已经完成计算过程了，答案已经呼之欲出了！！\n[x*y] = 1.1100 1001（如果要双符号为就这样去写：[x*y] = 11.1100 1001）\n总结一下，辅助位配合乘数最后一位作判断操作，00 和 11 只需右移就好，而 01 要 +[x] 补，11 要 +[-x] 补，这样我们就能得到最终的结果了。\n接下来再来一道爽题！！！\n# 例题二\n已知 [x] 补 = 1.0101，[y] 补 = 1.0011，利用 booth 算法求解 [x*y]\n老规矩：\n\n[x] 补 = 11.0101\n[-x] 补 = 00.1011\n[y] 补 = 1.0011\n\n列表：\n\n\n\n\n部分积\n乘数\n辅助\n\n\n\n\n\n\n00 0000\n10011\n0\n一次判断\n\n\n10 +\n00 1011\n\n\n\n\n\n=\n00 1011\n\n\n\n\n\n右移\n00 0101\n1100 1\n1\n二次判断\n\n\n11 右移\n00 0010\n1110 0\n1\n三次判断\n\n\n01 +\n11 0101\n\n\n\n\n\n=\n11 0111\n\n\n\n\n\n右移\n11 1011\n1111 0\n0\n四次判断\n\n\n00 右移\n11 1101\n1111 1\n0\n五次判断\n\n\n10 +\n00 1011\n\n\n\n\n\n\n00 1000\n\n\n\n\n\n\n第五次判断后无需移动\n答案：0.1000 1111\n怎么样，是不是非常爽，在五次判断中出现了两次数字相同的情况，这时候我们只需要进行右移，无需进行计算。\n最后我在给大家一道练习题，自己练习一下叭～～～\n# 练习\n已知 [x] 补 = 0.1101，[y] 补 = 0.1011，利用 booth 算法求解 [x*y]\n\n\n\n部分积\n乘数\n辅助\n\n\n\n\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 鸣谢\n感谢 B 站 up 主制作的视频，真的非常不错\n","categories":["计算机组成原理"],"tags":["计算机组成原理"]},{"title":"计算机中的除法运算","url":"/Principles-of-computer-composition/Division_in_Computers/","content":"# 计算机中的除法运算\n# 简述\n计算的中的除法十分有趣。对于 6 与 4，我们可以很容易的判断出那个大，进而进行后续的除法运算，而计算机中却不是。比此更复杂的除法运算，又是怎么实现的呢？\n# 方法一：恢复余数法\n题目源自《计算机组成原理（第三版）》例 6.24\n\n\n\n\n被除数（余数）\n商\n说明\n\n\n\n\n0.1011+1.0011=1.1110\n【0.0000】\n第一步无条件 - y*\n\n\n1.1110+0.1101=0.1011\n【0.0000】\n1.1110 小于零 恢复余数\n\n\n1.0110\n【0.000】0\n上 0 左移 （补位来自商）\n\n\n1.0110+1.0011=0.1001\n【0.000】0\n0.1001 大于 0 无需恢复\n\n\n1.0010\n【0.00】01\n上 1 左移 （补位来自商）\n\n\n1.0010+1.0011=0.0101\n【0.00】01\n0.0101 大于 0 无需恢复\n\n\n0.1010\n【0.0】011\n上 1 左移 （补位来自商）\n\n\n0.1010+1.0011=1.1101\n【0.0】011\n1.1101 小于 0 恢复余数\n\n\n1.1101+0.1101=0.1010\n【0.0】011\n\n\n\n1.0100\n【0】.0110\n上 0 左移（补位来自商）\n\n\n1.0100+1.0011=0.0111\n【0】.0110\n0.0111 大于零 无需恢复\n\n\n0.0111\n0.1101\n上 1\n\n\n\n值：0.1101\n符号位：\n\n恢复余数法总结：首先第一步无条件加 - y*，然后看余数大于零还是小于 0，如果大于 0 无需恢复 上 1 左移，如果小于 0 需要恢复 上 0 左移，如此循环往复，直到用完所有的商位。\n# 方法二：加减交替法\n加减交替法可以说是恢复余数的改良方法，我们无需恢复 直接进行运算，详情见下题：\n题目源自《计算机组成原理（第三版）》例 6.25\n\n\n\n\n被除数（余数）\n商\n说明\n\n\n\n\n0.1011+1.0011=1.1110\n【0.0000】\n无条件加 - y*\n\n\n1.1100\n【0.000】0\n1.1110 小于 0 上 0 左移\n\n\n1.1100+0.1101=0.1001\n\n\n\n\n1.0010\n【0.00】01\n0.1001 大于 0 上 1 左移\n\n\n1.0010+1.0011=0.0101\n\n\n\n\n0.1010\n【0.0】011\n0.0101 大于 0 上 1 左移\n\n\n0.1010+1.0011=1.1101\n\n\n\n\n1.1010\n【0】.0110\n1.1101 小于 0 上 0 左移\n\n\n1.1010+0.1101=0.0111\n\n\n\n\n0.0111\n0.1101\n0.0111 大于 0 上 1\n\n\n\n核心：上 0 加正，上 1 加负，此口诀为上一步上的 0 或 1 在下一步体现，如果上了 0 那么下一步就加正，如果上了 1 那么下一步就加负。那么我们怎么知道到底是上 0 还是 1 呢？这个主要看余数，如果它是正的就上 1，如果他是负的就上 0\n","categories":["计算机组成原理"],"tags":["计算机组成原理"]},{"title":"数据库设计","url":"/database/DataBase02/","content":"# 数据库设计\n# 什么是数据库设计\n数据库设计是指根据需求分析和业务流程，设计出符合应用需求的数据库结构和表结构的过程。通常包括以下几个步骤：\n\n需求分析：对应用的功能和数据进行分析，确定应用所需的数据和数据间的关系，明确数据的属性和约束条件。\n概念设计：将需求分析的结果转化为概念模型，包括实体 - 关系模型和 E-R 图等，以及各个实体之间的联系、属性及约束等。\n逻辑设计：将概念模型转化为逻辑模型，包括表的定义、属性定义、关键字、完整性规则等，确定表与表之间的联系，构建数据模型。\n物理设计：在逻辑设计的基础上，结合具体的数据库管理系统（如 MySQL、Oracle、SQL Server 等），考虑实现方式和优化策略，设计出物理存储结构，包括数据表的存储方式、索引、分区等等。\n\n在数据库设计的过程中，需要遵循以下原则：\n\n数据库的设计应该符合应用需求和业务流程，具有可扩展性、可维护性和可靠性。\n数据库应该遵循数据规范化的原则，减少数据冗余和数据异常，提高数据存储效率。\n数据库设计应该考虑数据的完整性和安全性，采取适当的权限管理和数据备份策略，保证数据的安全性和可靠性。\n数据库设计应该遵循一定的命名规范，使数据库的结构清晰易懂，便于后期的维护和管理。\n\n综上所述，数据库设计是一个综合性较强的任务，需要综合考虑多方面的因素，才能设计出符合应用需求和业务流程，高效可靠的数据库结构。\n# 数据库设计过程\n\n需求分析\n概念结构设计\n逻辑结构设计\n数据库物理设计\n数据库实施\n数据库运行与维护\n\n# 数据库设计过程各个阶段上的设计描述\n数据库设计过程一般包括以下几个阶段：\n\n需求分析阶段：在需求分析阶段，需要进行数据需求的调研和收集，明确系统需要存储的数据、数据之间的关系以及对数据的操作等，设计描述的主要内容包括数据流图、数据字典等。\n概念设计阶段：在概念设计阶段，需要将需求分析得到的数据需求转化为概念模型，设计描述的主要内容包括实体关系图（ER 图）。\n逻辑设计阶段：在逻辑设计阶段，需要将概念模型转化为具体的逻辑模型，设计描述的主要内容包括关系模式、属性、主键、外键、索引等。\n物理设计阶段：在物理设计阶段，需要将逻辑模型转化为数据库实现模型，设计描述的主要内容包括表结构、存储方式、物理索引等。\n\n在每个阶段中，设计描述的内容都不同，但都是在将需求转化为具体的数据库结构的过程中所必需的，而且在设计描述的过程中，需要对需求进行细致的分析和理解，以保证设计结构的正确性和实用性。\n","categories":["数据库"],"tags":["数据库"]},{"title":"基于用户的协同过滤算法","url":"/experience/CollaborativeFiltering/","content":"# 基于用户的协同过滤算法\n本篇文档将会用通俗易懂的方式带你来了解并实现基于用户的协同过滤算法，不再基于特定开源库，使用原生代码完成协同过滤的构建，实际上，基于用户的协同过滤很简单，只需要弄懂那几个公式是怎么用的就好了，真正的难点是公式的提出与证明，不过呢今天我不想讲相关的证明了。\n本篇文章主要是带你打开协同过滤的大门，我只是想告诉你，作为人的你我具有不断认识的能力，且在认识中不断消除谬误，并无限趋近于真理，我们有能力也必将认识这个世界，而协同过滤正是其中之一，究其本质并不复杂。\n那么请坐好，协同过滤的列车要发车啦！\n# 算法介绍\n基于用户的协同过滤，实际上就是找与目标用户最为相似的一个或多个用户，这里的目标用户实际上就是我们要为其进行信息推送的用户。假如瑞瑞喜欢 A 和 B，鸭鸭喜欢 A、B 和 C，他们都对喜欢的商品给予较高的评价，嘎嘎非常讨厌 A 和 B，但是钟爱 D。\n那么我们可以看到似乎鸭鸭的 A 和 B 与瑞瑞的 A 和 B 是一样的，但是鸭鸭买过 C 而瑞瑞没有买过 C，瑞瑞和鸭鸭是高度相似的，那么我们就可以给瑞瑞推荐鸭鸭喜欢的 C，或许瑞瑞也喜欢呢！\n而对于嘎嘎来说瑞瑞喜欢的 A 和 B 正是嘎嘎他所讨厌的，而嘎嘎对 D 情有独钟，我们可以了解到，瑞瑞或许是讨厌嘎嘎所喜欢的，那么嘎嘎和瑞瑞是低相似的，嘎嘎喜欢的 D 也许不应该给瑞瑞推荐！\n\n# 计算用户间的相似度\n# Jaccard 相似系数\nJaccard 相似系数主要是衡量两个集合的相似度 J (A,B) 是它的数学表达\nJ(A,B)=∣A∩B∣∣A∪B∣\\begin {array}{c}\nJ(A,B)=\\frac{\\left | A\\cap B \\right | }{\\left | A\\cup  B \\right |}\n\\end {array}\nJ(A,B)=∣A∪B∣∣A∩B∣​​\nJaccard 相似系数特别适用于处理二值数据（即只有两个值：0 或 1）。例如，在文本处理中，可以通过是否包含某个词来表示文档中的词汇出现与否，这种情况下 Jaccard 相似系数非常有效。然而 Jaccard 相似度不考虑元素的权重或频率。如果两个集合中有的元素出现频率较高，而有的频率较低，Jaccard 相似度会将这些元素视为等价，不适用于需要考虑元素重要性的场景。\n# 余弦相似度\n余弦相似度是 n 维空间中两个 n 维向量之间角度的余弦。它等于两个向量的点积（向量积）除以两个向量长度（或大小）的乘积。\nsim(A,B)=A⋅B∣∣A∣∣×∣∣B∣∣=∑i=1n(Ai×Bi)∑i=1nAi2×∑i=1nBi2sim(A,B)=\\frac{A·B}{\\left | \\left | A \\right |  \\right | \\times \\left | \\left | B \\right |  \\right | } =\\frac{\\sum_{i=1}^{n}(A_{i}\\times B_{}i)}{\\sqrt{\\sum_{i=1}^{n}A_{i}^{2} } \\times \\sqrt{\\sum_{i=1}^{n}B_{i}^{2} }} \nsim(A,B)=∣∣A∣∣×∣∣B∣∣A⋅B​=∑i=1n​Ai2​​×∑i=1n​Bi2​​∑i=1n​(Ai​×B​i)​\nsim(A,B)sim(A,B)sim(A,B) 是有取值范围的，在[−1,1][-1,1][−1,1] 之间，1 是完全相似，-1 是完全不相似，余弦相似度的计算公式简单，通常只涉及向量的点积和模长的运算，因此在实际应用中计算速度较快。\n# 皮尔逊相关系数\n皮尔逊相关系数，也称为皮尔逊积矩相关系数，是衡量两个变量之间线性关系的强度和方向的统计量。其值范围从 - 1 到 + 1，广泛应用于统计学、数据分析、机器学习等领域，特别是在衡量变量之间的相关性时。\n标准差：\nS=∑i=1n(Xi−X‾)2N−1S=\\sqrt{\\frac{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^2}{N-1}}\nS=N−1∑i=1n​(Xi​−X)2​​\nNNN 是数据总量，X‾\\overline{X}X 是平均值。\n协方差：\nCov(X,Y)=∑i=1n(Xi−X‾)(Yi−Y‾)N−1Cov(X,Y)=\\frac{\\sum_{i=1}^{n}(X_{i}-\\overline{X})(Y_{i}-\\overline{Y}) }{N-1}\nCov(X,Y)=N−1∑i=1n​(Xi​−X)(Yi​−Y)​\n皮尔逊相关系数\nρ=Cov(X,Y)SxSy\\rho =\\frac{Cov(X,Y)}{S_{x}S_{y}}\nρ=Sx​Sy​Cov(X,Y)​\nSxS_{x}Sx​ 是 X 的标准差。同样他是有取值范围的，在[−1,1][-1,1][−1,1] 之间，1 是完全相似，-1 是完全不相似\n皮尔逊相关系数的计算公式相对简单，容易理解，且其值的意义非常直观，可以清楚地表示变量之间的线性关系强度和方向。 它不仅衡量了两个变量之间的协方差，还考虑了变量的标准差，因此能够归一化协方差，使得相关系数的值独立于变量的尺度。\n# 应用示例\n\n\n\n用户 - 商品评分表\n用户 1001\n用户 1002\n用户 1003\n用户 1004\n用户 1005\n\n\n\n\n商品 1\n5\n4\n5\n4\n5\n\n\n商品 2\n4\n3\n4\n4\n4\n\n\n商品 3\n5\n2\n5\n3\n5\n\n\n商品 4\n4\n2\n4\n4\n3\n\n\n\n这个时候我们要找与用户 1003 最相似的用户，可见 1003 给出了（5，4，5，4）的评分向量，这与用户 1001 的（5，4，5，4）评分向量完全一致，用户 1003 与用户 1001 高度相似，我们还可以看到用户 1005 给出了（5，4，5，3）的评分向量，这也与用户 1003 高度相似。\n这些都是显然的，但是计算机怎么能知道呢？本篇文章采用皮尔逊相关系数完成协同过滤。\n# 计算标准差\ndouble standardDeviationCalculations(int[] ratings) &#123;    double std = 0;        // 标准差    double avarage = 0;    // 平均值    int sum = 0;           // 评分和    double varianceUP = 0; // 方差分子    double variance = 0; // 方差    for (int i = 0; i &lt; ratings.length; i++) &#123;        sum += ratings[i];    &#125;    avarage = (double) sum / ratings.length;    for (int i = 0; i &lt; 4; i++) &#123; // 方差分子        varianceUP += Math.pow(ratings[i] - avarage, 2);    &#125;    variance = varianceUP / (ratings.length - 1);    std = Math.sqrt(variance);    return std;&#125;# 计算协方差\ndouble covarianceCalculation(int[] X, int[] Y) &#123;    double covariance = 0;       // 协方差    double avarageX = 0;         // 平均值 X    double avarageY = 0;         // 平均值 Y    for (int i = 0; i &lt; X.length; i++) &#123;        avarageX += X[i];    &#125;    avarageX = avarageX / X.length;    for (int i = 0; i &lt; Y.length; i++) &#123;        avarageY += Y[i];    &#125;    avarageY = avarageY / Y.length;    for (int i = 0; i &lt; X.length; i++) &#123;        covariance += (X[i] - avarageX) * (Y[i] - avarageY);    &#125;    covariance = covariance / (X.length - 1);    return covariance;&#125;# 计算皮尔逊相关系数\ndouble pearsonCorrelationCoefficient(double covarianceXY, double stdX, double stdY) &#123;    return covarianceXY / (stdX * stdY);&#125;# 调用\npublic class SuiBianTest &#123;    double standardDeviationCalculations(int[] ratings) &#123;        double std = 0;        // 标准差        double avarage = 0;    // 平均值        int sum = 0;           // 评分和        double varianceUP = 0; // 方差分子        double variance = 0; // 方差        for (int i = 0; i &lt; ratings.length; i++) &#123;            sum += ratings[i];        &#125;        avarage = (double) sum / ratings.length;        for (int i = 0; i &lt; 4; i++) &#123; // 方差分子            varianceUP += Math.pow(ratings[i] - avarage, 2);        &#125;        variance = varianceUP / (ratings.length - 1);        std = Math.sqrt(variance);        return std;    &#125;    double covarianceCalculation(int[] X, int[] Y) &#123;        double covariance = 0;       // 协方差        double avarageX = 0;         // 平均值 X        double avarageY = 0;         // 平均值 Y        for (int i = 0; i &lt; X.length; i++) &#123;            avarageX += X[i];        &#125;        avarageX = avarageX / X.length;        for (int i = 0; i &lt; Y.length; i++) &#123;            avarageY += Y[i];        &#125;        avarageY = avarageY / Y.length;        for (int i = 0; i &lt; X.length; i++) &#123;            covariance += (X[i] - avarageX) * (Y[i] - avarageY);        &#125;        covariance = covariance / (X.length - 1);        return covariance;    &#125;    double pearsonCorrelationCoefficient(double covarianceXY, double stdX, double stdY) &#123;        return covarianceXY / (stdX * stdY);    &#125;    @Test    public void testCollaborativeFiltering() &#123;        int[][] ratings_1_5 = &#123;                &#123;5, 4, 5, 4&#125;,       //1001 对商品 1、2、3、4 的评分                &#123;4, 3, 2, 2&#125;,       //1002 对商品 1、2、3、4 的评分                &#123;5, 4, 5, 4&#125;,       //1003 对商品 1、2、3、4 的评分                &#123;4, 4, 2, 4&#125;,       //1004 对商品 1、2、3、4 的评分                &#123;5, 4, 5, 3&#125;,       //1005 对商品 1、2、3、4 的评分        &#125;;        //--------------------- 计算方差 -----------------        ArrayList&lt;Double> stdVector = new ArrayList&lt;>();    // 方差向量        for (int i = 0; i &lt; ratings_1_5.length; i++) &#123;            stdVector.add(standardDeviationCalculations(ratings_1_5[i]));        &#125;        System.out.println(\"方差向量：\" + stdVector);        //---------------- 协方差计算 cov (X,Y)------------        ArrayList&lt;Double> covarianceVector = new ArrayList&lt;>(); // 协方差向量        for (int i = 0; i &lt; ratings_1_5.length; i++) &#123;            covarianceVector.add(covarianceCalculation(ratings_1_5[2], ratings_1_5[i]));        &#125;        System.out.println(\"协方差向量：\" + covarianceVector);        //-------------- 皮尔逊相关系数 ----------------        ArrayList&lt;Double> pearsonVector = new ArrayList&lt;>();    // 皮尔逊相关系数向量        for (int i = 0; i &lt; ratings_1_5.length; i++) &#123;            pearsonVector.add(pearsonCorrelationCoefficient(covarianceVector.get(i), stdVector.get(2), stdVector.get(i)));        &#125;        System.out.println(\"皮尔逊相关系数向量：\" + pearsonVector);        System.out.println(\"----------------------------\");        HashMap&lt;String, Double> map = new HashMap&lt;>();        map.put(\"1001\", pearsonVector.get(0));        map.put(\"1002\", pearsonVector.get(1));//        map.put(\"1003\", pearsonVector.get(2));        map.put(\"1004\", pearsonVector.get(3));        map.put(\"1005\", pearsonVector.get(4));        // 使用 Stream API 获取最大的两个值及它们对应的键        List&lt;Map.Entry&lt;String, Double>> topTwoEntries = map.entrySet()                .stream()                .sorted(Map.Entry.&lt;String, Double>comparingByValue().reversed()) // 按照 value 降序排序                .limit(2) // 只取前两项                .toList();        // 输出结果        topTwoEntries.forEach(entry ->                System.out.println(\"最大皮尔逊相关系数之一对应的键：\" + entry.getKey() + \"，值为：\" + entry.getValue())        );        if (topTwoEntries.size() &lt; 2) &#123;            System.out.println(\"注意：Map中的元素不足两个\");        &#125;    &#125;&#125;# 输出\n\n# 后续\n此时我们已经找到了，与用户 1003 最为相似的两个用户，分别是用户 1001——1.000 和用户 1005——0.905，这与我们之前的” 显然猜想 “相一致。\n然后我们就可以根据用户 1001 和用户 1005 已经购买的商品但用户 1003 没有购买的商品，将他们两个加权综合计算，为用户 1003 推荐。\n# 小结\n其实，利用皮尔逊相关系数计算用户之间的相关性并不复杂，而在计算之前，我们应该怎么设计根据用户 1003 的已购买商品的评分信息，抽取与他购买的相同商品的其他用户的商品评分信息的 SQL 或者是其他信息提取方法，我想，这才是更为重要的点。\n这可能就是简单的 “数据清洗” 吧！哈哈哈哈哈哈哈。\n","categories":["开发经验"],"tags":["协同过滤"]},{"title":"MD5计算时间","url":"/experience/MD5CalculationTimePractice/","content":"# MD5 计算时间的实践\n# 引言\n最近在做一个项目，里面用到了 MD5 算法来生成文件名，并且可以对相同文件进行合并存储，不禁让我想到了一个问题 ——MD5 的计算很耗时吗还是 MD5 是一个轻量级的算法，在网上翻阅了一些资料，得到了结果，确实 MD5 的计算是很轻量的。\n但是，他到底需要多长时间呢？我直接开始实践一下。\n# 配置\n机器采用：AMD Ryzen 7 5800H 处理器 + 32GB 内存 频率为 3200 使用约 50%\n# 实践\n先对小文件进行测试，现选用文件大小为 2.83 MB 的图片，在 anaconda 环境下，对比不同 chunk_size 下的计算时间。\nimport hashlibimport timedef file_to_md5(file_path, chunk_size=8192):    md5 = hashlib.md5()    with open(file_path, 'rb') as f:        while chunk := f.read(chunk_size):            md5.update(chunk)    return md5.hexdigest()t = time.perf_counter()file_to_md5('D:\\\\ZzMu\\\\Aping\\\\bz\\\\zb.png')print(f'&#123;time.perf_counter() - t:.8f&#125;s')\n\n\nchunk_size\n时间\n\n\n\n\n1024\n0.00551090s\n\n\n2048\n0.00501470s\n\n\n4096\n0.00491630s\n\n\n8192\n0.00470980s\n\n\n16384\n0.00436570s\n\n\n\n随着 chunk_size 的增大，计算时间不断缩短，不过确实微乎其微，0.00 几的时间无法感知，那么大一点的文件能放大影响吗？\n下面对文件大小为 957 MB 的 4 分 44 秒的 5k 视频进行计算。\n\n\n\nchunk_size\n时间\n\n\n\n\n1024\n1.68504020s\n\n\n2048\n1.61570920s\n\n\n4096\n1.57428070s\n\n\n8192\n1.55151410s\n\n\n16384\n1.35576710s\n\n\n32768\n1.28117580s\n\n\n\n大文件对 chunk_size 的感知更为明显，变现为 0. 几的波动。\n下面试试更大的文件，文件大小为 9.58 GB 的 Linux 安装文件 CentOS-7-x86_64-Everything-2207-02.iso 进行计算。\n\n\n\nchunk_size\n时间\n\n\n\n\n1024\n17.36447650s\n\n\n2048\n17.03117640s\n\n\n4096\n16.44976660s\n\n\n8192\n16.01030290s\n\n\n16384\n14.48009140s\n\n\n32768\n13.57228980s\n\n\n\n得出结论，越大的文件对 chunk_size 越敏感。到此为止，我仍然对这个执行时间有疑问，不同的变成语言对计算时间有什么影响呢，以上是使用 Python 语言完成的测试，接下来试试 Java 的语言。\n原生：\nimport java.io.FileInputStream;import java.io.IOException;import java.io.InputStream;import java.security.MessageDigest;import java.security.NoSuchAlgorithmException;public class Main &#123;    private static final int BUFFER_SIZE = 32768;    public static String fileToMD5(String filePath) throws IOException, NoSuchAlgorithmException &#123;        try (InputStream is = new FileInputStream(filePath)) &#123;            MessageDigest md5Digest = MessageDigest.getInstance(\"MD5\");            byte[] buffer = new byte[BUFFER_SIZE];            int bytesRead;            while ((bytesRead = is.read(buffer)) != -1) &#123;                md5Digest.update(buffer, 0, bytesRead);            &#125;            return bytesToHex(md5Digest.digest());        &#125;    &#125;    private static String bytesToHex(byte[] bytes) &#123;        StringBuilder hexString = new StringBuilder();        for (byte b : bytes) &#123;            String hex = Integer.toHexString(0xff &amp; b);            if (hex.length() == 1) hexString.append('0');            hexString.append(hex);        &#125;        return hexString.toString();    &#125;    public static void main(String[] args) throws IOException, NoSuchAlgorithmException &#123;        long startTime = System.nanoTime();        fileToMD5(\"D:\\\\ZzMu\\\\Aping\\\\bz\\\\zb.png\");        long endTime = System.nanoTime();        System.out.printf(\"Time taken: %.8fs%n\", (endTime - startTime) / 1e9);    &#125;&#125;使用 java 8：\n\n\n\nchunk_size\n时间\n\n\n\n\n8192\n22.89824750\n\n\n16384\n21.72088020\n\n\n32768\n20.98676390\n\n\n\n没想到啊，java 表现的更为糟糕，是不是和 java 版本有关系呢？\n使用 java 17：\n\n\n\nchunk_size\n时间\n\n\n\n\n1024\n27.40612860\n\n\n8192\n15.47918140\n\n\n16384\n14.53450360\n\n\n32768\n13.85291620\n\n\n\n确实 java 17 的表现要比 java 8 的表现好。不过还是弱于 Python？难道是我写的代码太烂了？\n又引入了 DigestUtils 工具类，DigestUtils 是 apache. commons 包下的的 MD5 计算工具，默认 chunk_size 大小为 1024。\nimport org.apache.commons.codec.digest.DigestUtils;import org.junit.jupiter.api.Test;import java.io.FileInputStream;import java.io.IOException;public class MD5 &#123;    @Test    public void test() throws IOException &#123;        FileInputStream inputStream = new FileInputStream(\"E:\\\\LinuxIso\\\\CentOS-7-x86_64-Everything-2207-02.iso\");        long startTime = System.nanoTime();        DigestUtils.md5Hex(inputStream);        long endTime = System.nanoTime();        System.out.printf(\"Time taken: %.8fs%n\", (endTime - startTime) / 1e9);    &#125;&#125;\n\n\nchunk_size\n时间\n\n\n\n\n1024\n27.50305870\n\n\n\n彼此彼此吧，哈哈哈哈哈哈。\n下面又引入了 SecureUtil，该工具是 HuTool 包下一个计算 MD5 的工具，默认 chunk_size 为 1024。\nimport cn.hutool.crypto.SecureUtil;import org.junit.jupiter.api.Test;import java.io.FileInputStream;import java.io.IOException;public class MD5 &#123;    @Test    public void test() throws IOException &#123;        FileInputStream inputStream = new FileInputStream(\"E:\\\\LinuxIso\\\\CentOS-7-x86_64-Everything-2207-02.iso\");        long startTime = System.nanoTime();        SecureUtil.md5(inputStream);        long endTime = System.nanoTime();        System.out.printf(\"Time taken: %.8fs%n\", (endTime - startTime) / 1e9);    &#125;&#125;\n\n\nchunk_size\n时间\n\n\n\n\n1024\n15.42640900\n\n\n\nHuTool 的 SecureUtil 确实更高效，应该是做了一些特殊优化，源码中显示的是 MD5 没有使用 BC 库，完全依赖于 jdk 库。\n","categories":["开发经验"],"tags":["MD5"]},{"title":"写给我这一年多的考研时光","url":"/life/shangan/","content":"# 2024.01——2025.03\n昨天收到了学校的拟录取通知，很幸运我的名字出现在了拟录取名单里，一年多的努力于此刻兑现。回想一年来的旅途，我真的有很多话想说。\n# 前奏\n我其实很早之前就决定考研了，并不是跟风考研党，最初的考研目的也很简单，一是想与自己和解，和十八岁的自己和解，以此来填补高考的遗憾，二是去提升一下自己自身的竞争力。\n就这样，我在刚刚进入大学的时候，就已经决定好考研了。事实上，我是一名转专业的学生，高考的时候由于成绩不理想，导致没能学上自己想要的学的专业，对此我十分懊悔。\n和大多数普通人一样，我是一个堪称绝对平凡的人。普通人究竟该怎样向上，究竟该如何一步步的提升自己？我的回答是：脚踏实地，仰望星空。\n# 启程\n2024 年 1 月，我真正意义上的开启的我的考研之路，我认为我这个时间节点不算早也不算晚。\n2024 的 “年” 显得格外压抑，我觉得考研就是一场完全黑暗的旅途，你能 “听见” 同学们的声音，但是 “看不见” 他们在哪里。有人说，考研就是在黑夜里洗衣服，你不知道你是否洗干净了，你只能就这样在黑暗中继续的洗下去，直到天明。\n# 月升\n要说科目是我投入最大的，那一定是数学。我本身数学基础较差，我我花费很长时间去学习数学，遗憾的是最终的数学成绩并不是那么理想，在这里我就不献丑去讲我是怎么学习数学的了，我一定是头号反面教材。讲一句就是少看课，多练题，多练题，多练题。\n英语是我的老大难问题，英语的问题实际上要比数学还糟糕，在大学期间，我仅仅是 470 分过了四级，六级考了三次没考过，最后一次六级差了一道选择题的分数。我想说，英语你要掌握足够的词汇量，一定要多背单词，只有你认识单词了，才会看懂一句话，才知道他想表达什么意思，其次是语法，语法也很重要，但我认为，语法是一个锦上添花的。单词一定是前提，认识了足够多的单词之后，再学一学语法，那会让你更加如鱼得水。其次是做题技巧，尤其是阅读的做题技巧，多和网上的老师学一学，但是不要依赖这个，在学习之后一定要形成一套自己的解题方式。作文我觉得没什么好说的，多背多写就好了。\n我是一名纯纯理工科的学生，上一次认真学政治还是在初中，我在这里讲一讲怎么用理工科的思维去学政治，我再补充一句，如果你完全用理工的思维去学政治，那我想说，那很难学好政治。这里的理工思维一定要融入一些文科思想。政治一定要去了解历史，历史是由人民群众创造的，只有你了解了历史，理解当时他们的处境，与之共情，你才能明白，为什么他们这样做，以及这样做的好处。在答题时候，加入一些理工思维的公式，如果真正想学好仅靠一些答题公式还是不行的，但是呢我们这个是一个速成，或者是面对一个完全没怎么好好学过政治的人来说的。必要时用一些模板，公式化答题，是十分有利的。\n专业课方面我就不多讲了，我认为我这方面还是比较擅长的。还是多练多理解，毕竟这是你吃饭的工具呀。\n# 月落\n黎明前的日子最煎熬，网上我好像从来没看见过，有人说初试结束后，复试有多难。其实也不能说难，但是这段日子极其痛苦、折磨与难熬。\n成绩出来后，学校不公布排名，同学们自发在网上去排名，每一天你看见你的排名往下掉，整个人都 down down 的，这种感觉很难形容，很难受。最后在复试前，学校还是公布了排名。今年招生人数 35 人，进复试 44 人，我排在 35 名。这个时候我觉得真的很难保持一个良好的心态，这是我觉得考研最折磨的一个点。没办法，复试还要继续准备，此刻便是至暗。\n# 微光\n复试结束，一天后公布了最终排名，我以复试笔试第一名，复试成绩第四名，总成绩第十七名拟录取了。\n我想说复试真的很重要，我们学校的复试 1 分，就是初试 1.07 分，复试和初始同等重要。拟录取的那一刻犹如做梦一样，我不敢相信，我反复确认我真的拟录取了，生怕这一切全是梦境。\n是真的。\n# 回望\n回望我这一年多的旅程，有欢笑，有泪水，此刻我想对自己说，你真的很棒，你站在你曾经幻想过的未来，于此刻，不再是幻想。\n“夹岸群芳相送远帆过千障碍”\n","categories":["上岸"],"tags":["生活"]},{"title":"SSM框架整合","url":"/experience/SSM01/","content":"# 是什么 SSM\nSSM 是三个框架的简写，本别是 Spring，SpringMVC，Mybatis，这三个框架作为 JavaWeb 强有力的支撑件，极大地提高了开发效率，降低了维护成本，是 Java 程序员学习 Web 技术的必经之路（至少现在是这样）\n在此之前，我希望你能熟练掌握，额，哪怕不熟练也要了解 Web 部分的老祖：请求，响应以及 Servlet 技术，这些技术能帮助你更好、更快、更流畅的掌握 SSM 技术。\n其次本文主要讲解 SSM 整合的相关细节，并不会深入讲解 Spring，SpringMVC，Mybatis 每个框架的知识，希望大家在看本文之前能了解 Spring，SpringMVC，Mybatis 单个框架的使用方法。\n# 整合开始\n首先我先放出最终的整合状态，其实也不一定非要按照我这样来做。\n\n\n\n\nbean 包：大多数的实体类config 包：整合的核心配置类controller 包：外部控制器暴露点mapper 包：Mybiats SQL 标准化service 包：业务逻辑，包含接口以及实现resource 包：资源文件、配置文件webapp：前端资源\n\n\n\n那么接下来我将带着大家一步一步的来做，从创建项目到最终完成整合。\n# 创建项目（Maven 构建方式）\n选择新建一个项目（或模块），选择 Maven Archetype 形式创建，我给他命名叫做 SSM_demo。\nJDK：1.8（Java 8）\nArchetype：选择最后以 webapp 结尾的，这是一个创建模板\n组 ID：公司域名反写\n\n# 创建包结构\n在 main 目录下创建 java 目录和必要的包结构\n\n# 依赖导入、必要启动配置\n我们需要在 pom.xml 文件下导入我们 SSM 所需要的依赖，并配置必要的启动项。\n# 导入详解\n第一步：导包\n需要思考我们需要什么依赖：\njunit：单元测试。\nspring-webmvc：整个依赖就比较强大了她不仅包含了 SpringMVC 的部分也包含了 Spring-Context 部分，其实也不难理解，因为 SpringMVC 是基于 Spring 开发的，那也就必然包含了 Spring 框架的部分。\n至此我们还剩下数据库的部分，我们继续将其填充完整。\nMybatis：这个是 Mybatis 的核心依赖。\nmybatis-spring：整个是将 Mybatis 与 Spring 进行整合。\nspring-jdbc：简化 Java 应用程序与数据库之间的交互，Spring JDBC 是 MyBatis-Spring 的底层依赖，提供了连接管理、事务管理等基础功能。\nmysql-connector-java：这个是 MySQL 的驱动依赖。\ndruid：数据库连接池。\nservlet-api：Spring MVC 架构是基于 Servlet 规范的。它使用 Servlet API 来处理和分发 Web 请求，以及与 Web 容器进行交互。\njackson-databind：用于在 Java 对象和 JSON 数据之间进行序列化（对象到 JSON）和反序列化（JSON 到对象）的核心部分。\n&lt;dependencies>  &lt;dependency>    &lt;groupId>junit&lt;/groupId>    &lt;artifactId>junit&lt;/artifactId>    &lt;version>4.13.2&lt;/version>    &lt;scope>test&lt;/scope>  &lt;/dependency>  &lt;dependency>    &lt;groupId>org.springframework&lt;/groupId>    &lt;artifactId>spring-webmvc&lt;/artifactId>    &lt;version>5.2.10.RELEASE&lt;/version>  &lt;/dependency>  &lt;dependency>    &lt;groupId>org.mybatis&lt;/groupId>    &lt;artifactId>mybatis&lt;/artifactId>    &lt;version>3.5.9&lt;/version>  &lt;/dependency>  &lt;dependency>    &lt;groupId>org.mybatis&lt;/groupId>    &lt;artifactId>mybatis-spring&lt;/artifactId>    &lt;version>2.0.7&lt;/version>  &lt;/dependency>  &lt;dependency>    &lt;groupId>org.springframework&lt;/groupId>    &lt;artifactId>spring-jdbc&lt;/artifactId>    &lt;version>6.0.6&lt;/version>  &lt;/dependency>  &lt;dependency>    &lt;groupId>mysql&lt;/groupId>    &lt;artifactId>mysql-connector-java&lt;/artifactId>    &lt;version>8.0.31&lt;/version>  &lt;/dependency>  &lt;dependency>    &lt;groupId>javax.servlet&lt;/groupId>    &lt;artifactId>servlet-api&lt;/artifactId>    &lt;version>2.5&lt;/version>    &lt;scope>provided&lt;/scope>  &lt;/dependency>  &lt;dependency>    &lt;groupId>com.fasterxml.jackson.core&lt;/groupId>    &lt;artifactId>jackson-databind&lt;/artifactId>    &lt;version>2.13.4&lt;/version>  &lt;/dependency>&lt;/dependencies>一定要为我们的 servlet-api 配置作用域为 provided，这是非常大的一个坑点！！！\n如果您不将  servlet-api  的作用域设置为  provided ，而是将其作用域设置为默认的  compile ，则可能会导致以下问题和影响：\n\n冲突问题： 如果您的应用程序包含了自己的 Servlet API 实现（例如 jar 包），而 Web 容器也提供了自己的实现，可能会导致冲突和不稳定性。\n版本不一致： 不同的 Web 容器可能使用不同版本的 Servlet API，如果您将 API 包含在应用程序中，可能会导致版本不一致的问题。\n应用程序大小增加： 将 Servlet API 包含在应用程序中会增加应用程序的大小，尽管这在绝大多数情况下可能不会对性能产生明显影响，但仍然会浪费一些资源。\n维护困难： 如果您在多个项目中重复地包含 Servlet API，可能会导致维护上的困难，特别是在更新版本或切换到不同的 Web 容器时。\n\n&lt;scope>provided&lt;/scope>第二步：启动配置\n在这里你可以选择外挂 Tomcat 和 Maven 插件的两种启动方式，在这里我选择插件法，因为外挂启动真的是太慢了 QWQ\npom.xml 文件中的 build 标签下配置 tomcat 插件：\n&lt;build>  &lt;finalName>SSM_demo&lt;/finalName>  &lt;plugins>    &lt;plugin>      &lt;groupId>org.apache.tomcat.maven&lt;/groupId>      &lt;artifactId>tomcat7-maven-plugin&lt;/artifactId>      &lt;version>2.2&lt;/version>      &lt;configuration>        &lt;port>80&lt;/port>        &lt;path>/&lt;/path>      &lt;/configuration>    &lt;/plugin>  &lt;/plugins>&lt;/build>80 端口为启动端口，你也可以进行修改，例如修改到 8090\n&lt;port>8090&lt;/port>第三步：Java 版本配置\n其实这是一个坑点我们需要在 pom 文件加上这两行：\n&lt;properties>  &lt;maven.compiler.source>8&lt;/maven.compiler.source>  &lt;maven.compiler.target>8&lt;/maven.compiler.target>&lt;/properties>这是在声明我们的 Java 版本为 java8，这个一定要根据我们项目的 java 版本来填写\n# 最终展示\npom.xml&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\"          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0                       http://maven.apache.org/maven-v4_0_0.xsd\">  &lt;modelVersion>4.0.0&lt;/modelVersion>  &lt;groupId>com.KarryCode&lt;/groupId>  &lt;artifactId>SSM_demo&lt;/artifactId>  &lt;packaging>war&lt;/packaging>  &lt;properties>    &lt;maven.compiler.source>8&lt;/maven.compiler.source>    &lt;maven.compiler.target>8&lt;/maven.compiler.target>  &lt;/properties>  &lt;version>1.0-SNAPSHOT&lt;/version>  &lt;name>SSM_demo Maven Webapp&lt;/name>  &lt;url>http://maven.apache.org&lt;/url>  &lt;dependencies>    &lt;dependency>      &lt;groupId>junit&lt;/groupId>      &lt;artifactId>junit&lt;/artifactId>      &lt;version>4.13.2&lt;/version>      &lt;scope>test&lt;/scope>    &lt;/dependency>    &lt;dependency>      &lt;groupId>org.springframework&lt;/groupId>      &lt;artifactId>spring-webmvc&lt;/artifactId>      &lt;version>5.2.10.RELEASE&lt;/version>    &lt;/dependency>    &lt;dependency>      &lt;groupId>org.mybatis&lt;/groupId>      &lt;artifactId>mybatis&lt;/artifactId>      &lt;version>3.5.9&lt;/version>    &lt;/dependency>    &lt;dependency>      &lt;groupId>org.mybatis&lt;/groupId>      &lt;artifactId>mybatis-spring&lt;/artifactId>      &lt;version>2.0.7&lt;/version>    &lt;/dependency>    &lt;dependency>      &lt;groupId>org.springframework&lt;/groupId>      &lt;artifactId>spring-jdbc&lt;/artifactId>      &lt;version>5.0.2.RELEASE&lt;/version>    &lt;/dependency>    &lt;dependency>      &lt;groupId>mysql&lt;/groupId>      &lt;artifactId>mysql-connector-java&lt;/artifactId>      &lt;version>8.0.31&lt;/version>    &lt;/dependency>    &lt;dependency>      &lt;groupId>com.alibaba&lt;/groupId>      &lt;artifactId>druid&lt;/artifactId>      &lt;version>1.2.16&lt;/version>    &lt;/dependency>    &lt;dependency>      &lt;groupId>javax.servlet&lt;/groupId>      &lt;artifactId>servlet-api&lt;/artifactId>      &lt;version>2.5&lt;/version>      &lt;scope>provided&lt;/scope>    &lt;/dependency>    &lt;dependency>      &lt;groupId>com.fasterxml.jackson.core&lt;/groupId>      &lt;artifactId>jackson-databind&lt;/artifactId>      &lt;version>2.13.4&lt;/version>    &lt;/dependency>  &lt;/dependencies>  &lt;build>    &lt;finalName>SSM_demo&lt;/finalName>    &lt;plugins>      &lt;plugin>        &lt;groupId>org.apache.tomcat.maven&lt;/groupId>        &lt;artifactId>tomcat7-maven-plugin&lt;/artifactId>        &lt;version>2.2&lt;/version>        &lt;configuration>          &lt;port>80&lt;/port>          &lt;path>/&lt;/path>        &lt;/configuration>      &lt;/plugin>    &lt;/plugins>  &lt;/build>&lt;/project># 创建 JDBC.properties 文件\n在 resource  目录下创建 jdbc.properties\nJdbcPropertiesjdbc.driver = com.mysql.cj.jdbc.Driverjdbc.url = jdbc.username = jdbc.password =等号后面的根据自己的需要进行填写\n为了测试，我还创建的一张表：\n\n这是我的生成脚本：\n生成脚本create table tb_book(    id          int auto_increment        primary key,    type        varchar(20)  null,    name        varchar(50)  null,    description varchar(255) null);# 创建 mapper 代理包\n首先在 resource 目录下创建与 java 源代码目录一样的包，切记一定要以 “/” 的分割形式去创建，\n例如：com/KarryCode/mapper\n创建好后，后置在这个目录下创建对应的 xml，我们稍后再说！\n# 创建配置文件（配置类）\n这部分将以配置类的形式进行配置。\n# Spring_Mybatis 整合配置\n首先在 config 包下创建 Spring_Mybatis 配置类\npackage com.KarryCode.config;import com.alibaba.druid.pool.DruidDataSource;import org.mybatis.spring.SqlSessionFactoryBean;import org.mybatis.spring.annotation.MapperScan;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.PropertySource;import org.springframework.jdbc.datasource.DataSourceTransactionManager;import org.springframework.transaction.PlatformTransactionManager;import org.springframework.transaction.annotation.EnableTransactionManagement;import javax.sql.DataSource;/** * @Author KarryLiu * @Creed may all the beauty be blessed * @ClassName SpringConfig * @Description TODO Spring 核心配置类 * @Version 1.0 */@Configuration                                  //TODO 核心配置唯一标识@ComponentScan(&#123;\"com.KarryCode.service\",\"com.KarryCode.bean\"&#125;)     //TODO 注解扫描指定包@PropertySource(\"classpath:jdbc.properties\")    //TODO 加载 JDBC 配置类@MapperScan(\"com.KarryCode.mapper\")             //TODO MyBatis 基于包扫描方式识别 Mapper@EnableTransactionManagement                    //TODO 事务的自动代理，注解驱动public class Spring_MybatisConfig &#123;    @Bean    //TODO DruidDataSource 数据源的产生    public DataSource dataSource(            @Value(\"$&#123;jdbc.driver&#125;\") String driver,            @Value(\"$&#123;jdbc.url&#125;\") String url,            @Value(\"$&#123;jdbc.username&#125;\") String username,            @Value(\"$&#123;jdbc.password&#125;\") String password    ) &#123;        DruidDataSource dataSource = new DruidDataSource();        dataSource.setDriverClassName(driver);        dataSource.setUrl(url);        dataSource.setUsername(username);        dataSource.setPassword(password);        // 里面还可以配置更多关于数据库连接池的选项.......        return dataSource;    &#125;    @Bean    public SqlSessionFactoryBean sqlSessionFactoryBean(DataSource dataSource) &#123;        //TODO sqlSessionFactoryBean MybatisBeans 加载        SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean();        sqlSessionFactoryBean.setDataSource(dataSource);        return sqlSessionFactoryBean;    &#125;    @Bean    public PlatformTransactionManager transactionManager(DataSource dataSource) &#123;        //TODO 平台事务管理        DataSourceTransactionManager dataSourceTransactionManager = new DataSourceTransactionManager();        dataSourceTransactionManager.setDataSource(dataSource);        return dataSourceTransactionManager;    &#125;&#125;具体内容不展开讲解，主要概述一下注解以及方法的作用：\n@MapperScan：指定包扫描的路径。\nDataSource dataSource：数据源的产生。\nPlatformTransactionManager transactionManager：事务交由 Spring 管理。\n# SpringMVC 整合配置\n同样地，在 config 包下创建 SpringMvcConfig\npackage com.KarryCode.config;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.config.annotation.EnableWebMvc;/** * @Author KarryLiu * @Creed may all the beauty be blessed * @PackageName com.KarryCode.config * @ClassName SpringMvcConfig * @Description TODO * @Version 1.0 */@Configuration@EnableWebMvc@ComponentScan(\"com.KarryCode.controller\")public class SpringMvcConfig &#123;&#125;# Servlet-api 整合配置\n同样地，在 config 包下创建 ServletConfig\npackage com.KarryCode.config;import org.springframework.web.servlet.support.AbstractAnnotationConfigDispatcherServletInitializer;public class ServletConfig extends AbstractAnnotationConfigDispatcherServletInitializer &#123;    @Override    protected Class&lt;?>[] getRootConfigClasses() &#123;        return new Class[]&#123;Spring_MybatisConfig.class&#125;;    &#125;    @Override    protected Class&lt;?>[] getServletConfigClasses() &#123;        return new Class[]&#123;SpringMvcConfig.class&#125;;    &#125;    @Override    protected String[] getServletMappings() &#123;        return new String[]&#123;\"/\"&#125;;    &#125;&#125;此时的目录结构是这样的：\n\n至此，整合已经基本完成！\n# Bean 实体类创建\n根据数据库里的以及逻辑关系创建 Bean 实体，在这里我先创建一个 Book 实体\npackage com.KarryCode.bean;/** * @Author KarryLiu * @Creed may all the beauty be blessed * @PackageName com.KarryCode.bean * @ClassName Book * @Description TODO * @Version 1.0 */public class Book &#123;    private Integer id;    private String type;    private String name;    private String description;    // 省略 getter/setter/ 构造器（有参 / 无参）/toString&#125;# mapper 接口创建\n在 mapper 包下创建一个 mapper 接口 BookMapper\npackage com.KarryCode.mapper;import com.KarryCode.bean.Book;import org.apache.ibatis.annotations.*;import java.util.List;/** * @Author KarryLiu * @Creed may all the beauty be blessed * @PackageName com.KarryCode.mapper * @ClassName BookMapper * @Description TODO * @Version 1.0 */public interface BookMapper &#123;    @Insert(\"insert into ssm_db.tb_book values (null,#&#123;type&#125;,#&#123;name&#125;,#&#123;description&#125;)\")    int save(Book book);    @Update(\"update ssm_db.tb_book set type=#&#123;type&#125;,name=#&#123;name&#125;,description=#&#123;description&#125; where id=#&#123;id&#125;\")    int update(Book book);    @Delete(\"delete from ssm_db.tb_book where tb_book.id=#&#123;id&#125;\")    int delete(Integer id);    @Select(\"select * from ssm_db.tb_book where id=#&#123;id&#125;\")    @ResultType(Book.class)    Book getBookById(Integer id);    @Select(\"select * from ssm_db.tb_book\")    List&lt;Book> getBookList();&#125;通过注解方式配置了增删改查四种方法\n# mapper 代理 xml 创建\n虽然没啥用，但是创建了也没什么坏处，在 resource 的对应目录下创建 BookMapper.xml\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?>&lt;!DOCTYPE mapper        PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\"        \"https://mybatis.org/dtd/mybatis-3-mapper.dtd\">&lt;mapper namespace=\"com.KarryCode.mapper.BookMapper\">&lt;/mapper># Service 逻辑创建\nBookService 接口\npackage com.KarryCode.service;import com.KarryCode.bean.Book;import org.springframework.transaction.annotation.Transactional;import java.util.List;/** * @Author KarryLiu * @Creed may all the beauty be blessed * @ClassName BookService * @Description TODO * @Version 1.0 */@Transactionalpublic interface BookService &#123;    /**     * @param book     * @return boolean     * @Author KarryLiu_刘珂瑞     * @Date 2023/8/14 下午 10:58     * @Description TODO 保存     */    boolean save(Book book);    /**     * @param book     * @return boolean     * @Author KarryLiu_刘珂瑞     * @Date 2023/8/14 下午 10:58     * @Description TODO 修改     */    boolean update(Book book);    /**     * @param id     * @return boolean     * @Author KarryLiu_刘珂瑞     * @Date 2023/8/14 下午 10:58     * @Description TODO 根据 id 删除     */    boolean delete(Integer id);    /**     * @param id     * @return edu.beihua.bean.Book     * @Author KarryLiu_刘珂瑞     * @Date 2023/8/14 下午 10:59     * @Description TODO 根据 id 查询     */    Book getBookById(Integer id);    /**     * @param     * @return java.util.List&lt;edu.beihua.bean.Book>     * @Author KarryLiu_刘珂瑞     * @Date 2023/8/14 下午 10:59     * @Description TODO 查询全部     */    List&lt;Book> getBookList();&#125;BookService 接口实现\npackage com.KarryCode.service.impl;import com.KarryCode.bean.Book;import com.KarryCode.mapper.BookMapper;import com.KarryCode.service.BookService;import org.springframework.stereotype.Service;import javax.annotation.Resource;import java.util.List;/** * @Author KarryLiu * @Creed may all the beauty be blessed * @Date 2023/8/14 下午 10:43 * @ClassName BookServiceImpl * @Description TODO * @Version 1.0 */@Servicepublic class BookServiceImpl implements BookService &#123;    @Resource    private BookMapper bookMapper;    @Override    public boolean save(Book book) &#123;        System.out.println(\"save=====>Service\");        int save = bookMapper.save(book);        if (save>0)&#123;            return true;        &#125;else &#123;            return false;        &#125;    &#125;    @Override    public boolean update(Book book) &#123;        System.out.println(\"update=====>Service\");        bookMapper.update(book);        return true;    &#125;    @Override    public boolean delete(Integer id) &#123;        System.out.println(\"delete=====>Service\");        bookMapper.delete(id);        return true;    &#125;    @Override    public Book getBookById(Integer id) &#123;        System.out.println(\"getBookById=====>Service\");        Book book = bookMapper.getBookById(id);        return book;    &#125;    @Override    public List&lt;Book> getBookList() &#123;        System.out.println(\"getBookList=====>Service\");        return bookMapper.getBookList();    &#125;&#125;# Spring—Mybatis 通路检测\n编写测试类，观察输出\nimport com.KarryCode.bean.Book;import com.KarryCode.config.Spring_MybatisConfig;import com.KarryCode.service.BookService;import org.junit.Test;import org.springframework.context.annotation.AnnotationConfigApplicationContext;import java.util.List;/** * @Author KarryLiu * @Creed may all the beauty be blessed * @PackageName PACKAGE_NAME * @ClassName mybatisTest * @Description TODO * @Version 1.0 */public class mybatisSpringTest &#123;    @Test    public void testMybatisSpring()&#123;        AnnotationConfigApplicationContext annotationConfigApplicationContext = new AnnotationConfigApplicationContext(Spring_MybatisConfig.class);        BookService bookService = annotationConfigApplicationContext.getBean(BookService.class);        List&lt;Book> bookList = bookService.getBookList();        System.out.println(bookList);    &#125;&#125;输出\n\n信息: {dataSource-1} inited\ngetBookList=====&gt;Service\n [Book {id=1, type=‘计算机’, name=‘软件工程’, description=‘吼吼吼吼’}, Book {id=2, type=‘文学’, name=‘儒家经典’, description=‘儒家好书啊！！！！！’}, Book {id=3, type=‘科技’, name=‘百科全书’, description=‘真不错！！！！’}, Book {id=4, type=‘美术’, name=‘当代顶流美术’, description=‘好书啊好书！！！！’}, Book {id=6, type=‘科技’, name=‘百科全书’, description=‘真不错！！！！’}, Book {id=7, type=‘科技’, name=‘百科全书’, description=‘真不错！！！！’}]\n进程已结束，退出代码 0\n\n\n至此，Spring—Mbatis 整合完成\n# 构建 Controller\n在 controller 包下创建 BookController\npackage com.KarryCode.controller;import com.KarryCode.bean.Book;import com.KarryCode.service.BookService;import org.springframework.transaction.annotation.Transactional;import org.springframework.web.bind.annotation.*;import javax.annotation.Resource;import java.util.List;/** * @Author KarryLiu * @Creed may all the beauty be blessed * @PackageName com.KarryCode.controller * @ClassName BookController * @Description TODO * @Version 1.0 */@RestController@Transactional@RequestMapping(\"/books\")public class BookController &#123;    @Resource    private BookService bookService;    @PostMapping    public boolean save(@RequestBody Book book) &#123;        return bookService.save(book);    &#125;    @PutMapping    public boolean update(@RequestBody Book book) &#123;        return bookService.update(book);    &#125;    @DeleteMapping(\"/&#123;id&#125;\")    public boolean delete(@PathVariable Integer id) &#123;        return bookService.delete(id);    &#125;    @GetMapping(\"/&#123;id&#125;\")    public Book getBookById(@PathVariable Integer id) &#123;        return bookService.getBookById(id);    &#125;    @GetMapping    public List&lt;Book> getBookList() &#123;        return bookService.getBookList();    &#125;&#125;符合 Restful 设计建议。\n# 启动 Tomcat\n\n# Postman 接口测试\n# 存书测试\n\n\nsuccess\n\n# 查书测试\n# 查询全部\n\n# 指定 id 查书\n\n# 更新书\n\n\n# 删除测试\n\n\n所有接口测试调通，至此 SSM 整合完毕\n# 总结\n其实 SSM 整合还是比较简单的，通过三个框架的配置就可以完成了，后面的工作都是在进行业务创建。\n好了！本篇文章到这里就结束了，有什么不理解的地方或者有疑问的地方请在评论区留言，或者添加我的联系方式，欢迎各位大佬积极批评讨论！谢谢！\n","categories":["开发经验"],"tags":["SSM"]},{"title":"矩阵论","url":"/math/matrix_anl/","content":"# 矩阵论\n# 线性变换 T 在基底下的矩阵\n定义如下：\nT(α1,α2,α3…αn)=(α1,α2,α3…αn)AT(\\alpha_1,\\alpha_2,\\alpha_3\\dots\\alpha_n)=(\\alpha_1,\\alpha_2,\\alpha_3\\dots\\alpha_n)A\nT(α1​,α2​,α3​…αn​)=(α1​,α2​,α3​…αn​)A\n这个题目较为灵活，最复杂情况如下：\nT(α1,α2,α3…αn)=(α1,α2,α3…αn)AT(β1,β2,β3…βn)=(β1,β2,β3…βn)B(β1,β2,β3…βn)=(α1,α2,α3…αn)CT(\\alpha_1,\\alpha_2,\\alpha_3\\dots\\alpha_n)=(\\alpha_1,\\alpha_2,\\alpha_3\\dots\\alpha_n)A\n\\\\\nT(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)=(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)B\n\\\\\n(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)=(\\alpha_1,\\alpha_2,\\alpha_3\\dots\\alpha_n)C\nT(α1​,α2​,α3​…αn​)=(α1​,α2​,α3​…αn​)AT(β1​,β2​,β3​…βn​)=(β1​,β2​,β3​…βn​)B(β1​,β2​,β3​…βn​)=(α1​,α2​,α3​…αn​)C\n一般已知过度矩阵 C 和基下矩阵 A，C 是由α\\alphaα，过渡到β\\betaβ 的，核心在于求 B：\nT(α1,α2,α3…αn)=(α1,α2,α3…αn)A(β1,β2,β3…βn)C−1=(α1,α2,α3…αn)T(β1,β2,β3…βn)C−1=(β1,β2,β3…βn)C−1AT(β1,β2,β3…βn)=(β1,β2,β3…βn)C−1AC(β1,β2,β3…βn)C−1AC=(β1,β2,β3…βn)BB=C−1ACT(\\alpha_1,\\alpha_2,\\alpha_3\\dots\\alpha_n)=(\\alpha_1,\\alpha_2,\\alpha_3\\dots\\alpha_n)A\n\\\\\n(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)C^{-1}=(\\alpha_1,\\alpha_2,\\alpha_3\\dots\\alpha_n)\n\\\\\nT(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)C^{-1}=(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)C^{-1}A\n\\\\\nT(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)=(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)C^{-1}AC\n\\\\\n(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)C^{-1}AC=(\\beta_1,\\beta_2,\\beta_3\\dots\\beta_n)B\n\\\\\nB=C^{-1}AC\nT(α1​,α2​,α3​…αn​)=(α1​,α2​,α3​…αn​)A(β1​,β2​,β3​…βn​)C−1=(α1​,α2​,α3​…αn​)T(β1​,β2​,β3​…βn​)C−1=(β1​,β2​,β3​…βn​)C−1AT(β1​,β2​,β3​…βn​)=(β1​,β2​,β3​…βn​)C−1AC(β1​,β2​,β3​…βn​)C−1AC=(β1​,β2​,β3​…βn​)BB=C−1AC\n\n例一：已知线性变换 T 在基底η1=(−1,1,1)T,η2=(1,0,−1)T,η3=(0,1,1)T\\eta_1=(-1,1,1)^T,\\eta_2=(1,0,-1)^T,\\eta_3=(0,1,1)^Tη1​=(−1,1,1)T,η2​=(1,0,−1)T,η3​=(0,1,1)T 下的矩阵\nA=[101110−121]A=\\begin{bmatrix}\n 1 &amp; 0 &amp; 1\\\\\n 1 &amp; 1 &amp; 0\\\\\n -1 &amp; 2 &amp; 1\n\\end{bmatrix}\nA=⎣⎢⎡​11−1​012​101​⎦⎥⎤​\n求在基底e1=(1,0,0)T,e2=(0,1,0)T,e3=(0,0,1)Te_1=(1,0,0)^T,e_2=(0,1,0)^T,e_3=(0,0,1)^Te1​=(1,0,0)T,e2​=(0,1,0)T,e3​=(0,0,1)T 下的矩阵。\n解\n由题可知：\nT(η1,η2,η3)=(η1,η2,η3)AT(e1,e2,e3)=(e1,e2,e3)B(e1,e2,e3)=(η1,η2,η3)CT(\\eta_1,\\eta_2,\\eta_3)=(\\eta_1,\\eta_2,\\eta_3)A\n\\\\\nT(e_1,e_2,e_3)=(e_1,e_2,e_3)B\n\\\\\n(e_1,e_2,e_3)=(\\eta_1,\\eta_2,\\eta_3)C\nT(η1​,η2​,η3​)=(η1​,η2​,η3​)AT(e1​,e2​,e3​)=(e1​,e2​,e3​)B(e1​,e2​,e3​)=(η1​,η2​,η3​)C\nC 可以轻易的求解出来：\nC=(η1,η2,η3)−1(e1,e2,e3)=[−1101011−11]−1⋅[100010001]=[−11−101−1101]C−1=[−1101011−11]C=(\\eta_1,\\eta_2,\\eta_3)^{-1}(e_1,e_2,e_3)=\\begin{bmatrix}\n -1 &amp; 1 &amp; 0\\\\\n 1 &amp; 0 &amp; 1\\\\\n 1 &amp; -1 &amp; 1\n\\end{bmatrix}^{-1} \\cdot \\begin{bmatrix}\n 1 &amp; 0 &amp; 0\\\\\n 0 &amp; 1 &amp; 0\\\\\n 0 &amp; 0 &amp; 1\n\\end{bmatrix}=\\begin{bmatrix}\n -1 &amp; 1 &amp; -1\\\\\n 0 &amp; 1 &amp; -1\\\\\n 1 &amp; 0 &amp; 1\n\\end{bmatrix}\n\\\\\nC^{-1}=\\begin{bmatrix}\n -1 &amp; 1 &amp; 0\\\\\n 1 &amp; 0 &amp; 1\\\\\n 1 &amp; -1 &amp; 1\n\\end{bmatrix}\nC=(η1​,η2​,η3​)−1(e1​,e2​,e3​)=⎣⎢⎡​−111​10−1​011​⎦⎥⎤​−1⋅⎣⎢⎡​100​010​001​⎦⎥⎤​=⎣⎢⎡​−101​110​−1−11​⎦⎥⎤​C−1=⎣⎢⎡​−111​10−1​011​⎦⎥⎤​\n求基底 e 下的矩阵正是矩阵 B，于是有：\nB=C−1ACB=C^{-1}AC\nB=C−1AC\n\n# 基下的坐标\n一般是已知坐标(y1,y2,y3)T(y_1,y_2,y_3)^{T}(y1​,y2​,y3​)T，求其在基底(η1,η2,η3)(\\eta_1,\\eta_2,\\eta_3)(η1​,η2​,η3​)，下的坐标(x1,x2,x3)T(x_1,x_2,x_3)^T(x1​,x2​,x3​)T，一般表示为：\nY=AX[y1y2y3]=[η1,η2,η3]⋅[x1x2x3]Y=AX\n\\\\\n\\begin{bmatrix}\n y_1 \\\\\n y_2 \\\\\n y_3 \n\\end{bmatrix}=\\begin{bmatrix}\\eta_1,\\eta_2,\\eta_3\\end{bmatrix}\\cdot\\begin{bmatrix}\n x_1 \\\\\n x_2 \\\\\n x_3 \n\\end{bmatrix}\nY=AX⎣⎢⎡​y1​y2​y3​​⎦⎥⎤​=[η1​,η2​,η3​​]⋅⎣⎢⎡​x1​x2​x3​​⎦⎥⎤​\n求解 x：\n[x1x2x3]=[η1,η2,η3]−1⋅[y1y2y3]\\begin{bmatrix}\n x_1 \\\\\n x_2 \\\\\n x_3 \n\\end{bmatrix}=\\begin{bmatrix}\\eta_1,\\eta_2,\\eta_3\\end{bmatrix}^{-1}\\cdot\\begin{bmatrix}\n y_1 \\\\\n y_2 \\\\\n y_3 \n\\end{bmatrix}\n⎣⎢⎡​x1​x2​x3​​⎦⎥⎤​=[η1​,η2​,η3​​]−1⋅⎣⎢⎡​y1​y2​y3​​⎦⎥⎤​\n# 内积的证明\n要满足如下性质：\n(α,β)=(β,α)‾(kα,β)=k(α,β)(α+β,γ)=(α,γ)+(β,γ)当α≠0时(α,α)&gt;0(\\alpha,\\beta)=\\overline{(\\beta,\\alpha)}\\\\\n(k\\alpha,\\beta)=k(\\alpha,\\beta)\\\\\n(\\alpha+\\beta,\\gamma)=(\\alpha,\\gamma)+(\\beta,\\gamma)\\\\\n当\\alpha\\ne 0时(\\alpha,\\alpha)&gt;0\n(α,β)=(β,α)​(kα,β)=k(α,β)(α+β,γ)=(α,γ)+(β,γ)当α=0时(α,α)&gt;0\n# 求矩阵 A 的最小多项式\n\n先用λE−A\\lambda E-AλE−A 求出特征多项式。\n由低次方向高次方验证A−λEA-\\lambda EA−λE，直到出现零矩阵。\n\n\n例一：求矩阵\nA=[3−32−15−2−130]A=\\begin{bmatrix}\n 3 &amp; -3 &amp; 2\\\\\n -1 &amp; 5 &amp; -2\\\\\n -1 &amp; 3 &amp; 0\n\\end{bmatrix}\nA=⎣⎢⎡​3−1−1​−353​2−20​⎦⎥⎤​\n的最小多项式。\n解\n∣λE−A∣=(λ−2)2(λ−4)|\\lambda E-A|=(\\lambda-2)^2(\\lambda-4)\n∣λE−A∣=(λ−2)2(λ−4)\n由此最小特征是可能是(λ−2)2(λ−4)(\\lambda-2)^2(\\lambda-4)(λ−2)2(λ−4) 或者(λ−2)(λ−4)(\\lambda-2)(\\lambda-4)(λ−2)(λ−4)，先从低次开始验证，也就是(λ−2)(λ−4)(\\lambda-2)(\\lambda-4)(λ−2)(λ−4)。\n(A−2E)(A−4E)=[1−32−13−2−13−2]⋅[−1−32−11−2−13−4]=[000000000](A-2E)(A-4E)=\\begin{bmatrix}\n 1 &amp; -3 &amp; 2\\\\\n -1 &amp; 3 &amp; -2\\\\\n -1 &amp; 3 &amp; -2\n\\end{bmatrix}\\cdot\\begin{bmatrix}\n -1 &amp; -3 &amp; 2\\\\\n -1 &amp; 1 &amp; -2\\\\\n -1 &amp; 3 &amp; -4\n\\end{bmatrix}=\\begin{bmatrix}\n 0 &amp; 0 &amp; 0\\\\\n 0 &amp; 0 &amp; 0\\\\\n 0 &amp; 0 &amp; 0\n\\end{bmatrix}\n(A−2E)(A−4E)=⎣⎢⎡​1−1−1​−333​2−2−2​⎦⎥⎤​⋅⎣⎢⎡​−1−1−1​−313​2−2−4​⎦⎥⎤​=⎣⎢⎡​000​000​000​⎦⎥⎤​\n由此可知，由(λ−2)(λ−4)(\\lambda-2)(\\lambda-4)(λ−2)(λ−4) 可构成零矩阵，(λ−2)(λ−4)(\\lambda-2)(\\lambda-4)(λ−2)(λ−4) 就是最小多项式。\n\n# 行列式因子、不变因子、初等因子与 Jordan 标准型\n推荐通过 Smith 标准型来求解一般这个最快且出错率较低，如果 Smith 标准型求解困难，再通过子式来求。\n求矩阵\nA=[−1−26−103−1−14]A =\\begin{bmatrix}\n -1 &amp; -2 &amp; 6\\\\\n -1 &amp; 0 &amp; 3\\\\\n -1 &amp; -1 &amp; 4\n\\end{bmatrix}\nA=⎣⎢⎡​−1−1−1​−20−1​634​⎦⎥⎤​\n的行列式因子、不变因子、初等因子与 Jordan 标准型。\n解\n构造∣λE−A∣|\\lambda E-A|∣λE−A∣：\n∣λE−A∣=[λ+12−61λ−311λ−4]|\\lambda E-A|=\\begin{bmatrix}\n \\lambda+1 &amp; 2 &amp; -6\\\\\n 1 &amp; \\lambda &amp; -3\\\\\n 1 &amp; 1 &amp; \\lambda-4\n\\end{bmatrix}\n∣λE−A∣=⎣⎢⎡​λ+111​2λ1​−6−3λ−4​⎦⎥⎤​\n观察是否有公因子，并没有，1 行 1 列必是 1：\n[1λ−311λ−4λ+126]≃[10001−λ1−λ01−λλ2−3λ+2]\\begin{bmatrix}\n 1 &amp; \\lambda &amp; -3\\\\\n 1 &amp; 1 &amp; \\lambda-4\\\\\n \\lambda+1 &amp; 2 &amp; 6\\\\\n\\end{bmatrix}\\simeq\\begin{bmatrix}\n 1 &amp; 0 &amp; 0\\\\\n 0 &amp; 1-\\lambda &amp; 1-\\lambda\\\\\n 0 &amp; 1-\\lambda &amp; \\lambda^2-3\\lambda+2\\\\\n\\end{bmatrix}\n⎣⎢⎡​11λ+1​λ12​−3λ−46​⎦⎥⎤​≃⎣⎢⎡​100​01−λ1−λ​01−λλ2−3λ+2​⎦⎥⎤​\n观察右下角 2x2 子式是否有公因子，有的，是(1−λ)(1-\\lambda)(1−λ)，所以 2 行 2 列必是(1−λ)(1-\\lambda)(1−λ)。\n[10001−λ1−λ01−λ−λ2+3λ−2]≃[1000λ−1000(λ−1)2]\\begin{bmatrix}\n 1 &amp; 0 &amp; 0\\\\\n 0 &amp; 1-\\lambda &amp; 1-\\lambda\\\\\n 0 &amp; 1-\\lambda &amp; -\\lambda^2+3\\lambda-2\\\\\n\\end{bmatrix}\\simeq\\begin{bmatrix}\n 1 &amp; 0 &amp; 0\\\\\n 0 &amp; \\lambda-1 &amp; 0\\\\\n 0 &amp; 0 &amp; (\\lambda-1)^2\\\\\n\\end{bmatrix}\n⎣⎢⎡​100​01−λ1−λ​01−λ−λ2+3λ−2​⎦⎥⎤​≃⎣⎢⎡​100​0λ−10​00(λ−1)2​⎦⎥⎤​\n由此可知\n不变因子：d1=1,d2=λ−1,d3=(λ−1)2d_1=1,d_2=\\lambda-1,d_3=(\\lambda-1)^2d1​=1,d2​=λ−1,d3​=(λ−1)2\n行列式因子：D1=1,D2=λ−1,D3=(λ−1)(λ−1)2D_1=1,D_2=\\lambda-1,D_3=(\\lambda-1)(\\lambda-1)^2D1​=1,D2​=λ−1,D3​=(λ−1)(λ−1)2\n初等因子：(λ−1),(λ−1)2(\\lambda-1),(\\lambda-1)^2(λ−1),(λ−1)2\n由此可得出 Jordan 标准型：\nJ=[100011001]J=\\begin{bmatrix}\n 1 &amp; 0 &amp; 0\\\\\n 0 &amp; 1 &amp; 1\\\\\n 0 &amp; 0 &amp; 1\\\\\n\\end{bmatrix}\nJ=⎣⎢⎡​100​010​011​⎦⎥⎤​\n# 证明向量范数\n非负性\nx≠0时∣∣x∣∣&gt;0,当且仅当x=0时∣∣x∣∣=0x\\ne 0时||x||&gt;0,当且仅当x=0时||x||=0\nx=0时∣∣x∣∣&gt;0,当且仅当x=0时∣∣x∣∣=0\n齐次性\n∣∣kx∣∣=∣k∣⋅∣∣x∣∣||kx||=|k|\\cdot||x||\n∣∣kx∣∣=∣k∣⋅∣∣x∣∣\n三角不等式\n∣∣x+y∣∣≤∣∣x∣∣+∣∣y∣∣||x+y||\\le ||x||+||y||\n∣∣x+y∣∣≤∣∣x∣∣+∣∣y∣∣\n# 向量范数\n一范数\n∣∣x∣∣1=∑i=1n∣xi∣||x||_1=\\sum_{i=1}^{n}|x_i|\n∣∣x∣∣1​=i=1∑n​∣xi​∣\n二范数\n∣∣x∣∣2=∑i=1nxi2||x||_2=\\sqrt{\\sum_{i=1}^{n}x_i^2}\n∣∣x∣∣2​=i=1∑n​xi2​​\n无穷范数\n∣∣x∣∣∞=max∣xi∣||x||_\\infty=max|x_i|\n∣∣x∣∣∞​=max∣xi​∣\n# 证明矩阵范数\n非负性\n∣∣A∣∣&gt;0,当且仅当A=0时∣∣A∣∣=0||A||&gt;0,当且仅当A=0时||A||=0\n∣∣A∣∣&gt;0,当且仅当A=0时∣∣A∣∣=0\n齐次性\n∣∣kA∣∣=∣k∣⋅∣∣A∣∣||kA||=|k|\\cdot||A||\n∣∣kA∣∣=∣k∣⋅∣∣A∣∣\n三角不等式\n∣∣A+B∣∣≤∣A∣+∣B∣||A+B||\\le|A|+|B|\n∣∣A+B∣∣≤∣A∣+∣B∣\n相容性\n∣∣A⋅B∣∣≤∣∣A∣∣⋅∣∣B∣∣||A\\cdot B||\\le||A||\\cdot||B||\n∣∣A⋅B∣∣≤∣∣A∣∣⋅∣∣B∣∣\n# 矩阵范数\n一范数\n∣∣A∣∣1=max(列模长之和)||A||_1=max(列模长之和)\n∣∣A∣∣1​=max(列模长之和)\n二范数\n∣∣A∣∣2=λmax,其中λmax是AHA的最大特征值||A||_2=\\sqrt {\\lambda_{max}},其中\\lambda_{max}是A^HA的最大特征值\n∣∣A∣∣2​=λmax​​,其中λmax​是AHA的最大特征值\n无穷范数\n∣∣A∣∣∞=max(行模长之和)||A||_\\infty=max(行模长之和)\n∣∣A∣∣∞​=max(行模长之和)\nM1 范数\n∣∣AM1∣∣=∑i=1m∑j=1n∣aij∣||A_{M1}||=\\sum_{i=1}^{m}\\sum_{j=1}^{n}|a_{ij}|\n∣∣AM1​∣∣=i=1∑m​j=1∑n​∣aij​∣\nM2 范数\n∣∣AM2∣∣=∑i=1m∑j=1n∣aij∣2||A_{M2}||=\\sum_{i=1}^{m}\\sum_{j=1}^{n}\\sqrt{|a_{ij}|^2}\n∣∣AM2​∣∣=i=1∑m​j=1∑n​∣aij​∣2​\nM 无穷范数\n∣∣AM∞∣∣=max(aij)||A_{M\\infty}||=max(a_{ij})\n∣∣AM∞​∣∣=max(aij​)\n# QR 分解\n先将源矩阵写成列向量\nA=[a11a12a13a21a22a23a31a32a33]=[α1α2α3]A=\\begin{bmatrix}\n a_{11} &amp; a_{12} &amp; a_{13}\\\\\n a_{21} &amp; a_{22} &amp; a_{23}\\\\\n a_{31} &amp; a_{32} &amp; a_{33}\\\\\n\\end{bmatrix}=\\begin{bmatrix}\n \\alpha_{1} &amp; \\alpha_{2} &amp; \\alpha_{3}\\\\\n\\end{bmatrix}\nA=⎣⎢⎡​a11​a21​a31​​a12​a22​a32​​a13​a23​a33​​⎦⎥⎤​=[α1​​α2​​α3​​]\n正交化，并保留正交化过程记录\nβ1=α1β2=α2−(α2,β1)(β1,β1)β1β3=α3−(α3,β1)(β1,β1)β1−(α3,β2)(β2,β2)β2\\beta_1=\\alpha_1\\\\\n\\beta_2=\\alpha_2-\\frac{(\\alpha_2,\\beta_1)}{(\\beta_1,\\beta_1)}\\beta_1\\\\\n\\beta_3=\\alpha_3-\\frac{(\\alpha_3,\\beta_1)}{(\\beta_1,\\beta_1)}\\beta_1-\\frac{(\\alpha_3,\\beta_2)}{(\\beta_2,\\beta_2)}\\beta_2\\\\\nβ1​=α1​β2​=α2​−(β1​,β1​)(α2​,β1​)​β1​β3​=α3​−(β1​,β1​)(α3​,β1​)​β1​−(β2​,β2​)(α3​,β2​)​β2​\n反写正交化（用β\\betaβ 表示α\\alphaα）\nα1=β1α2=(α2,β1)(β1,β1)β1+β2α3=(α3,β1)(β1,β1)β1+(α3,β2)(β2,β2)β2+β3\\alpha_1=\\beta_1\\\\\n\\alpha_2=\\frac{(\\alpha_2,\\beta_1)}{(\\beta_1,\\beta_1)}\\beta_1+\\beta_2\\\\\n\\alpha_3=\\frac{(\\alpha_3,\\beta_1)}{(\\beta_1,\\beta_1)}\\beta_1+\\frac{(\\alpha_3,\\beta_2)}{(\\beta_2,\\beta_2)}\\beta_2+\\beta_3\nα1​=β1​α2​=(β1​,β1​)(α2​,β1​)​β1​+β2​α3​=(β1​,β1​)(α3​,β1​)​β1​+(β2​,β2​)(α3​,β2​)​β2​+β3​\n单位化β\\betaβ\nγ1=β1∣β1∣γ2=β2∣β2∣γ3=β3∣β3∣\\gamma_1=\\frac{\\beta_1}{|\\beta_1|}\\\\\n\\gamma_2=\\frac{\\beta_2}{|\\beta_2|}\\\\\n\\gamma_3=\\frac{\\beta_3}{|\\beta_3|}\\\\\nγ1​=∣β1​∣β1​​γ2​=∣β2​∣β2​​γ3​=∣β3​∣β3​​\n把β\\betaβ 用γ\\gammaγ 换掉\nα1=∣β1∣γ1α2=(α2,β1)(β1,β1)∣β1∣γ1+∣β2∣γ2α3=(α3,β1)(β1,β1)∣β1∣γ1+(α3,β2)(β2,β2)∣β2∣γ2+∣β3∣γ3\\alpha_1=|\\beta_1|\\gamma_1\\\\\n\\alpha_2=\\frac{(\\alpha_2,\\beta_1)}{(\\beta_1,\\beta_1)}|\\beta_1|\\gamma_1+|\\beta_2|\\gamma_2\\\\\n\\alpha_3=\\frac{(\\alpha_3,\\beta_1)}{(\\beta_1,\\beta_1)}|\\beta_1|\\gamma_1+\\frac{(\\alpha_3,\\beta_2)}{(\\beta_2,\\beta_2)}|\\beta_2|\\gamma_2+|\\beta_3|\\gamma_3\nα1​=∣β1​∣γ1​α2​=(β1​,β1​)(α2​,β1​)​∣β1​∣γ1​+∣β2​∣γ2​α3​=(β1​,β1​)(α3​,β1​)​∣β1​∣γ1​+(β2​,β2​)(α3​,β2​)​∣β2​∣γ2​+∣β3​∣γ3​\nγ\\gammaγ 系列就是 Q，上述替换过的就是 R\nA=[γ1γ2γ3]⋅[∣β1∣(α2,β1)(β1,β1)∣β1∣(α3,β1)(β1,β1)∣β1∣0∣β2∣(α3,β2)(β2,β2)∣β2∣00∣β3∣]A=\\begin{bmatrix}\n \\gamma_1 &amp; \\gamma_2 &amp; \\gamma_3\n\\end{bmatrix}\\cdot\\begin{bmatrix}\n |\\beta_1| &amp; \\frac{(\\alpha_2,\\beta_1)}{(\\beta_1,\\beta_1)}|\\beta_1| &amp; \\frac{(\\alpha_3,\\beta_1)}{(\\beta_1,\\beta_1)}|\\beta_1|\\\\\n 0 &amp; |\\beta_2| &amp; \\frac{(\\alpha_3,\\beta_2)}{(\\beta_2,\\beta_2)}|\\beta_2|\\\\\n 0 &amp; 0 &amp; |\\beta_3|\\\\\n\\end{bmatrix}\nA=[γ1​​γ2​​γ3​​]⋅⎣⎢⎢⎡​∣β1​∣00​(β1​,β1​)(α2​,β1​)​∣β1​∣∣β2​∣0​(β1​,β1​)(α3​,β1​)​∣β1​∣(β2​,β2​)(α3​,β2​)​∣β2​∣∣β3​∣​⎦⎥⎥⎤​\nQR 分解完毕。\n# 正交矩阵谱分解\n求特征值，特征向量，正交化单位化得到η1,η2,η3\\eta_1,\\eta_2,\\eta_3η1​,η2​,η3​\nA1=η1T⋅η1A2=η2T⋅η2…A_1=\\eta_1^T\\cdot\\eta_1\\\\\nA_2=\\eta_2^T\\cdot\\eta_2\\\\\n\\dots\nA1​=η1T​⋅η1​A2​=η2T​⋅η2​…\nA=λ1A1+λ2A2+…A=\\lambda_1A_1+\\lambda_2A_2+\\dots\nA=λ1​A1​+λ2​A2​+…\n# 可对角化 / 异根谱分解\n求特征值，设特征函数，如果有重根只计算一次，这里只做对方的特征值，不管自己的特征值\nφ1(λ)=(λ−λ2)(λ−λ3)φ2(λ)=(λ−λ1)(λ−λ3)φ3(λ)=(λ−λ1)(λ−λ2)\\varphi _1(\\lambda)=(\\lambda-\\lambda_2)(\\lambda-\\lambda_3)\\\\\n\\varphi _2(\\lambda)=(\\lambda-\\lambda_1)(\\lambda-\\lambda_3)\\\\\n\\varphi _3(\\lambda)=(\\lambda-\\lambda_1)(\\lambda-\\lambda_2)\\\\\nφ1​(λ)=(λ−λ2​)(λ−λ3​)φ2​(λ)=(λ−λ1​)(λ−λ3​)φ3​(λ)=(λ−λ1​)(λ−λ2​)\n把特征值和矩阵带入\nA1=φ1(A)φ1(λ1)A2=φ2(A)φ2(λ2)A3=φ3(A)φ3(λ3)A_1=\\frac{\\varphi _1(A)}{\\varphi _1(\\lambda_1)}\\\\\nA_2=\\frac{\\varphi _2(A)}{\\varphi _2(\\lambda_2)}\\\\\nA_3=\\frac{\\varphi _3(A)}{\\varphi _3(\\lambda_3)}\nA1​=φ1​(λ1​)φ1​(A)​A2​=φ2​(λ2​)φ2​(A)​A3​=φ3​(λ3​)φ3​(A)​\nA=λ1A1+λ2A2+λ3A3…A=\\lambda_1A_1+\\lambda_2A_2+\\lambda_3A_3\\dots\nA=λ1​A1​+λ2​A2​+λ3​A3​…\n# 满秩分解\n满秩分解先将矩阵化为行最简，然后选取指定的 n 列，指定 n 行，这里前后指的是化简前后，而具体指定哪列由化简后的阶梯矩阵决定。\nA=[14−1562000−14−12−40126−55−7]A=\\begin{bmatrix}\n 1 &amp; 4 &amp; -1 &amp; 5 &amp; 6\\\\\n 2 &amp; 0 &amp; 0 &amp; 0 &amp; -14 \\\\\n -1 &amp; 2 &amp; -4 &amp; 0 &amp; 1\\\\\n 2 &amp; 6 &amp; -5 &amp; 5 &amp; -7\\\\\n\\end{bmatrix}\nA=⎣⎢⎢⎢⎡​12−12​4026​−10−4−5​5005​6−141−7​⎦⎥⎥⎥⎤​\n[1000−70101072970015725700000]\\begin{bmatrix}\n 1 &amp; 0 &amp; 0 &amp; 0 &amp; -7\\\\\n 0 &amp; 1 &amp; 0 &amp; \\frac{10}{7} &amp; \\frac{29}{7} \\\\\n 0 &amp; 0 &amp; 1 &amp; \\frac{5}{7} &amp; \\frac{25}{7}\\\\\n 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\\n\\end{bmatrix}\n⎣⎢⎢⎢⎡​1000​0100​0010​0710​75​0​−7729​725​0​⎦⎥⎥⎥⎤​\n前列后行\nB=[14−1200−12−426−5]B=\\begin{bmatrix}\n 1 &amp; 4 &amp; -1\\\\\n 2 &amp; 0 &amp; 0\\\\\n -1 &amp; 2 &amp; -4\\\\\n 2 &amp; 6 &amp; -5 \\\\\n\\end{bmatrix}\nB=⎣⎢⎢⎢⎡​12−12​4026​−10−4−5​⎦⎥⎥⎥⎤​\nC=[1000−701010729700157257]C=\\begin{bmatrix}\n 1 &amp; 0 &amp; 0 &amp; 0 &amp; -7\\\\\n 0 &amp; 1 &amp; 0 &amp; \\frac{10}{7} &amp; \\frac{29}{7} \\\\\n 0 &amp; 0 &amp; 1 &amp; \\frac{5}{7} &amp; \\frac{25}{7}\\\\\n\\end{bmatrix}\nC=⎣⎢⎡​100​010​001​0710​75​​−7729​725​​⎦⎥⎤​\nA=BCA=BC\nA=BC\n具体流程请看书 P74-P75\n# 奇异值分解\n具体流程请看书 P85\n核心在于这两个公式，下面我会给予做题提示。\nUn=AVnΔ−1A=USVHU_n=AV_n\\Delta^{-1}\\\\\nA=USV^H\nUn​=AVn​Δ−1A=USVH\n其中 S 一定与 A 同型，Δ\\DeltaΔ 是非零奇异值张成的矩阵且一定是方阵，如果时间紧张我们可以通过 A、S、Δ\\DeltaΔ 的大小猜测出其他元素的大小。注意！奇异值要开根号！！！Δ−1\\Delta^{-1}Δ−1 一定要求逆！！！\n\n求出AHAA^HAAHA 的全部特征值及奇异值，由所有非零奇异值得到正线对角矩阵Δ\\DeltaΔ 进而得到奇异矩阵SSS\n对于AHAA^HAAHA 的每一个不同的特征根，求出与之相应的特征向量的极大无关组，经正交化、单位化得AHAA^HAAHA 相应于该特征根的标准正交特征向量组。将其中与非零特征根相应的那些小组（作为一些列向量）顺序排成矩阵V1V_1V1​ 其次序应与Δ\\DeltaΔ 中相关奇异值在对角线上的排列顺序相一致。再以AHAA^HAAHA 相应于零特征根的标准正交特征向量（极大无关）组排成矩阵V2V_2V2​。于是可得酉矩阵V=(V1,V2)V=(V_1,V_2)V=(V1​,V2​)\n计算U1=AV1Δ−1U_1=AV_1\\Delta^{-1} \\kern 0.1ptU1​=AV1​Δ−1\n求出AAHAA^HAAH 相应于零特征根的一个标准正交特征向量（极大无关）组，由它们排成m∗(m−r)m*(m-r)m∗(m−r) 的部分酉阵U2U_2U2​，可得U=(U1,U2)U=(U_1,U_2)U=(U1​,U2​) 综上，便得到奇异值分解A=USVHA=USV^HA=USVH\n\n# 矩阵函数\n推荐使用待定系数法求解：\n\n写出∣λE−A∣|\\lambda E-A|∣λE−A∣，求出特征多项式\n求出最小多项式，详见最小多项式章节\n观察最小多项式次数 n，设 n-1 次多项式\n将特征值带入右侧等于目标函数，当有重根时两侧同时求导再带入\n\n例：已知矩阵\nA=[221−261004]A=\\begin{bmatrix}\n 2 &amp; 2 &amp; 1\\\\\n -2 &amp; 6 &amp; 1\\\\\n 0 &amp; 0 &amp; 4\\\\\n\\end{bmatrix}\nA=⎣⎢⎡​2−20​260​114​⎦⎥⎤​\n求矩阵函数A\\sqrt{A} \\kern 0.1ptA​\n\n∣λE−A∣=(λ−4)3|\\lambda E-A|=(\\lambda-4)^3\n∣λE−A∣=(λ−4)3\n将矩阵 A 带入，先从最低次试起\n(A−4E)≠O(A−4E)2=O(A-4E)\\ne O\\\\\n(A-4E)^2= O\n(A−4E)=O(A−4E)2=O\n因此最小多项式就是(λ−4)2(\\lambda-4)^2(λ−4)2，平方项设 1 次式\naλ+b=λa\\lambda +b=\\sqrt{\\lambda}\naλ+b=λ​\n特征值带入，重根求导\n4a+b=2a=144a+b=2\\\\\na=\\frac{1}{4}\n4a+b=2a=41​\n解得 a 与 b，b=1\nA=14A+E\\sqrt{A}=\\frac{1}{4}A+E\nA​=41​A+E\n# 2011\n\n设ε1,ε2,ε3\\varepsilon_1,\\varepsilon_2,\\varepsilon_3ε1​,ε2​,ε3​ 是线性空间VVV 的一组基，线性变换δ\\deltaδ 使δ(ε1)=ε3,δ(ε2)=ε2,δ(ε3)=ε1\\delta(\\varepsilon_1)=\\varepsilon_3,\\delta(\\varepsilon_2)=\\varepsilon_2,\\delta(\\varepsilon_3)=\\varepsilon_1δ(ε1​)=ε3​,δ(ε2​)=ε2​,δ(ε3​)=ε1​，求δ\\deltaδ 的所有特征根及全部特征向量。\nδ(ε1,ε2,ε3)=(ε1,ε2,ε3)[001010100]\\delta(\\varepsilon_1,\\varepsilon_2,\\varepsilon_3)=(\\varepsilon_1,\\varepsilon_2,\\varepsilon_3)\\begin{bmatrix}\n 0 &amp; 0 &amp; 1\\\\\n 0 &amp; 1 &amp; 0\\\\\n 1 &amp; 0 &amp; 0\\\\\n\\end{bmatrix}\nδ(ε1​,ε2​,ε3​)=(ε1​,ε2​,ε3​)⎣⎢⎡​001​010​100​⎦⎥⎤​\n∣λE−A∣=[λ0−10λ−10−10λ]|\\lambda E-A|=\\begin{bmatrix}\n \\lambda &amp; 0 &amp; -1\\\\\n 0 &amp; \\lambda-1 &amp; 0\\\\\n -1 &amp; 0 &amp; \\lambda\\\\\n\\end{bmatrix}\n∣λE−A∣=⎣⎢⎡​λ0−1​0λ−10​−10λ​⎦⎥⎤​\n得到特征值λ1=λ2=1,λ3=−1\\lambda_1=\\lambda_2=1,\\lambda_3=-1λ1​=λ2​=1,λ3​=−1\n得到特征向量\nl1=(−1,0,1)Tl2=(1,0,1)Tl3=(0,1,0)Tl_1=(-1,0,1)^T\\\\\nl_2=(1,0,1)^T\\\\\nl_3=(0,1,0)^T\nl1​=(−1,0,1)Tl2​=(1,0,1)Tl3​=(0,1,0)T\n对于特征值1k1(ε1+ε3)+k2(ε2)对于特征值−1k3(−ε1+ε3)对于特征值1\\\\\nk_1(\\varepsilon_1+\\varepsilon_3)+k_2(\\varepsilon_2)\\\\\n对于特征值-1\\\\\nk3(-\\varepsilon_1+\\varepsilon_3)\n对于特征值1k1​(ε1​+ε3​)+k2​(ε2​)对于特征值−1k3(−ε1​+ε3​)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC 是 nxn 的笔误了。。\n\n\n\n# 2013-J\n\n\n# 2021\n\n\n\n\n\n\n\n\n\n\n\n# 2022-?\n这里针对长春理工大学 2022-2023 年矩阵论期末考试最后一道题（8 题）的相关内容做出个人解题见解。\n我认为 PDF 中的题解有问题，如果同样对这道题有疑问的同学，可在文章评论区（我可能看不到），微信（Sa9329Mxz），QQ（735690757）进一步讨论。\n\n\n基变换公式来自于 p8\n 坐标变换公式来自于 p9\n 线性变换在基下矩阵公式来自于 p19\n 线性变换作用在坐标下公式来自于 p22\n\n\n\n\n# 手写版\n这个手写版我会放在我的书里，但是可能会弄丢了，保险起见我把它扫描上来了。\n\n\n\n\n\n\n\n# 特别鸣谢\n特别鸣谢我的室友：求求你别骗我，对本文详细审阅。\n","categories":["笔记"],"tags":["矩阵论解题笔记"]},{"title":"计算机网络复试速览","url":"/network/computer_network_retest/","content":"# 计算机网络复试\n# 物理层\n传输比特流，规定机械特性、电气特性、功能特性。\n# 数据链路层\n将原始比特流封装成帧，将物理层提供的可能出错的物理连接改造为逻辑上无差错的数据链路，是对物理层的一次增强。\n# 网络层\n传输单位是数据报，进行路由选择，实现流量控制、拥塞控制、差错控制、网际互连。\n# 传输层\n负责主机中两个进程通信，提供端到端的可靠传输。\n端到端：进程的端口号之间\n点对点：IP 之间\n# 应用层\nOSI 最高层，是用户与网络之间的接口。\n# 信道、串行与并行\n是通信的通道，即传输介质。串行适合长距离通信，并行适用于并行通信。\n# 单工、半双工与全双工\n单工：只单向传输。\n半双工：双向，但是不能同时。\n全双工：双向，允许同时。\n# 奈奎斯特定理\n理想信道中，为避免串码干扰极限速率。\n# 香农定理\n实际信道中，传输速率上界。\n# 双绞线、同轴电缆、光纤与无线传输\n双绞线、同轴电缆、光纤与无线传输这些都是物理层的传输介质。\n# 物理层设备\n中继器：信号的转发、放大、整形，以防止失真，无差错控制。\n集线器：实际上集线器是一个多端口中继器。\n# 帧\n是数据链路层对等实体之间进行的逻辑通信单元。\n# 为网络层提供的三种服务\n\n无确认、无连接的服务，适用于误码率低的比如以太网\n有确认、无连接的服务，适用于无线通信\n有确认的面向连接的服务，适用于有高可靠要求的网络\n\n# 封装成帧\n在一点数据的头和尾加一些信息，构成帧，主要是：标志、地址、控制、信息、帧检验序列、标志组成。\n# 透明传输\n透明传输的作用是你无论传输了什么数据，都会保证你的传输原数据，不会被” 误解 “\n# 流量控制\n链路两端存在工作速率差异，为以防淹没现象，使用流量控制。接收方控制发送方。\n# 差错检测\n传输时可能产生错误，为修复这些错误使用差错检测机制\n\n位错：用 CRC 冗余校验\n帧错：这个是传输错误\n\n# 字节填充法\n在开头填充和末尾填充\n# 零比特填充法\n每遇五个‘1’在后面填充一个‘0’\n# 差错控制\n自动重传请求 ARQ / 前向纠错 FEC：ARQ 是一种基于重传的错误控制机制。当接收端检测到数据包错误或丢失时，会请求发送端重新发送数据。FEC 是一种通过在发送的数据中添加冗余信息来纠正错误的机制。接收端利用这些冗余信息检测并纠正错误，而无需请求重传。\n差错控制又可分为：检错编码（奇偶校验码、循环冗余码）和纠错编码（海明码）\n\n奇偶校验码（检错）就是让 1 的个数是奇数个还是偶数个\n海明码（纠错）就是在信息位中加入几个就错位\n\n# 停止 - 等待流量控制（一个一个的）\n发送方每次发送一个帧，只有接收方回复确认帧才发下一帧\n# 滑动窗口流量控制（一块一块的）\n集中发送一块，收到最左侧的帧后向右滑动，如果未收确认消息到那就重发。\n# ARQ 协议（自动重传请求）!\n\n停止 - 等待 Stop-and-Wait：发送端发送一个数据包后，等待接收端的确认（ACK）。如果收到 ACK，发送下一个数据包；如果超时未收到 ACK，则重传。简单但效率较低，适合低速率通信。\n后退 N 帧 Go-Back-N：如果某个数据包出错，接收端会丢弃该包及其后续的所有包，发送端需要从出错的数据包开始重传。\n选择性重传 Selective Repeat：如果某个数据包出错，只重传出错的数据包，而不是后续的所有包。\n\n1-&gt;2-&gt;3，越来越优秀。\n# 介质访问控制\n数据链路层的一个子层，主要负责管理多个设备共享同一通信介质时的访问权限。它的核心目标是解决多路访问问题，即如何高效、公平地协调多个设备对共享介质的访问，以避免冲突并提高网络性能。\n\n频分复用 FDM：将总带宽划分为多个子频带，每个子频带分配给一个信道（用户或设备）\n时分复用 TDM：将时间划分为固定长度的时隙，每个时隙分配给一个信道。\n波分复用 WDM：在光纤通信中，将不同波长的光信号复用到同一根光纤中传输。\n码分复用 CDM：每个信道使用唯一的编码序列对数据进行调制，多个信道可以在同一频率和时间内传输。\n\n\n\n\n技术\n划分依据\n适用场景\n优点\n缺点\n\n\n\n\nFDM\n频率\n广播、模拟通信\n简单、适合恒定速率数据\n频率利用率低\n\n\nTDM\n时间\n数字电话、数字通信\n高效、适合数字信号\n对同步要求高\n\n\nWDM\n波长（光频率）\n光纤通信\n高容量、长距离传输\n设备成本高\n\n\nCDM\n编码\n移动通信、卫星通信\n抗干扰、多用户同时通信\n实现复杂\n\n\n\n# ALOHA\n最简单的随机访问协议，设备随时发送数据。如果发生冲突，随机等待一段时间后重传。效率较低，最大信道利用率为 18.4%。\n# 载波侦听多路访问（CSMA）\n设备在发送数据前先侦听介质是否空闲。如果介质空闲，则发送数据；否则等待。\n\n\nCSMA/CD（冲突检测）：用于有线以太网，检测到冲突后立即停止发送。\n\n\nCSMA/CA（冲突避免）：用于无线网络（如 Wi-Fi），通过随机退避避免冲突。\n\n\nCSMA 有：1 - 坚持 CSMA、非坚持 CSMA、p - 坚持 CSMA\n\n“1 - 坚持” 表示设备以概率 1 发送数据（即一旦介质空闲，必定发送）。\n设备不会持续侦听介质，而是随机退避。\np 是一个可调参数，用于控制发送概率，结合了 1 - 坚持和非坚持的特点。\n\n# 载波侦听多路访问 / 冲突检测 CSMA/CD （有线网络）\n# 载波侦听（Carrier Sense）\n\n设备在发送数据前先侦听介质是否空闲。\n如果介质空闲，则开始发送数据。\n如果介质忙，则等待直到介质空闲。\n\n# 多路访问（Multiple Access）\n\n多个设备共享同一通信介质，可能同时尝试发送数据。\n\n# 冲突检测（Collision Detection）（CD）\n\n设备在发送数据的同时继续侦听介质。\n如果检测到冲突（即信号强度异常），立即停止发送数据。\n发送一个冲突强化信号（Jamming Signal），通知其他设备发生了冲突。\n\n# 随机退避（Random Backoff）\n\n设备等待一个随机时间后重新尝试发送数据。\n随机时间通过二进制指数退避算法计算，以减少再次冲突的概率。\n\n先听再发、边发边听、冲突停发、随机重发\n# 争用期 2τ\nτ 是信号在网络中传播的单程最大传播时延，争用期（2τ） 是 CSMA/CD 协议中冲突检测的时间窗口。决定了网络的最大物理范围和最小帧长。通过合理设计争用期，可以确保冲突检测的有效性，从而提高网络的可靠性。\n# 二进制指数退避算法\n冲突之后要等待，等？到底等多久？利用二进制指数退避算法就能知道。\n# 冲突避免（CA）\n冲突避免通过预先协调和随机退避来尽量避免冲突的发生，而不是在冲突发生后再进行处理。\n# 载波侦听多路访问 / 冲突避免 CSMA/CA （无线网络）\n是一种用于无线网络（如 Wi-Fi）的介质访问控制协议。\n# CSMA/CA 与 CSMA/CD 的区别\n\n\n\n特性\nCSMA/CA（冲突避免）\nCSMA/CD（冲突检测）\n\n\n\n\n适用网络\n无线网络（如 Wi-Fi）\n有线网络（如以太网）\n\n\n冲突处理\n通过 RTS/CTS 和退避避免冲突\n通过检测冲突并重发数据\n\n\n隐藏终端问题\n有效解决\n无法解决\n\n\n开销\n较高（RTS/CTS 和退避机制）\n较低\n\n\n延迟\n较高\n较低\n\n\n\n# 常见拓扑结构\n\n星型\n环型\n总线型\n\n# 以太网\n逻辑拓扑结构是总线型，物理拓扑是星型结构\n以太网采用两项措施简化通信\n\n采用无连接工作方式\n使用曼彻斯特编码\n\n# MAC 地址\n设备身份证，绝对唯一的地址，MAC 地址 48 位。\n# 单播、多播、广播\n\n单播：一对一\n多播：一对多\n广播：一对所有\n\n# 以太网的目的地址、源地址\n\n目的地址：标识帧的接收方。\n源地址：标识帧的发送方。\n类型：指示帧内数据的协议类型。（IPV4、IPV6、ARP）\n\n# VLAN 虚拟局域网\nVLAN（Virtual Local Area Network） 是一种将物理局域网划分为多个逻辑局域网的技术。通过 VLAN，可以在同一物理网络设备（如交换机）上创建多个独立的广播域，从而实现网络的逻辑隔离和优化。\n# PPP 协议（Point-to-Point Protocol）\nPPP（点对点协议） 是一种数据链路层协议，用于在两个节点之间直接传输数据。它广泛应用于拨号连接、DSL、串行通信等场景，支持多种网络层协议（如 IP、IPX 等），并提供了身份验证、错误检测和压缩等功能。\n\n只保证无差错接收（CRC 校验），是不可靠的服务\n只支持全双工的点对点链路\nPPP 两端可以运行不同的网络层协议\n\nPPP 无连接、不可靠、速度快\n# 集线器\n广播传输：数据会被发送到所有端口，无论目标设备是哪一个。\n无智能过滤：集线器无法识别数据的目标地址，因此无法优化数据传输。\n冲突域：所有设备共享同一个冲突域，容易发生数据冲突，导致网络效率低下。\n带宽共享：所有端口共享总带宽（例如，10 Mbps 的集线器连接 5 台设备，每台设备平均只能使用 2 Mbps）。\n# 网桥\n网桥可以识别帧转发帧，解决冲突的问题\n# 集线器与网桥的主要区别\n\n\n\n特性\n集线器（Hub）\n网桥（Bridge）\n\n\n\n\n工作层次\n物理层（Layer 1）\n数据链路层（Layer 2）\n\n\n数据传输方式\n广播到所有端口\n根据 MAC 地址转发到目标端口\n\n\n冲突域\n所有端口共享一个冲突域\n每个端口是一个独立的冲突域\n\n\n广播域\n所有端口共享一个广播域\n所有端口共享一个广播域\n\n\n带宽管理\n所有端口共享总带宽\n每个端口有独立带宽\n\n\n智能过滤\n无\n有（基于 MAC 地址）\n\n\n应用场景\n小型网络或临时连接\n连接两个局域网或扩展网络\n\n\n现代替代品\n交换机\n交换机\n\n\n\n# 交换机\n是多接口的网桥，是网桥的增强版，拥有自学习算法\n记录各个端口的 MAC 地址（交换表），冷启动阶段通过泛洪，找到对应的 MAC\n# 集线器、交换机\n集线器不隔离冲突域，也不隔离广播域。\n交换机隔离冲突域，但不隔离广播域。\n# 设备\n\n\n\n特性\n能否隔离冲突域\n能否隔离广播域\n\n\n\n\n集线器\n不能\n不能\n\n\n中继器\n不能\n不能\n\n\n交换机\n能\n不能\n\n\n网桥\n能\n不能\n\n\n路由器\n能\n能\n\n\n\n# 网络层功能\n\n路由选择：选择数据从源设备到目标设备的最佳路径。\n分组转发：根据路由表将数据包从输入接口转发到输出接口。\n逻辑寻址：为网络中的设备分配唯一的逻辑地址（如 IP 地址）。\n分段与重组：将大数据包分割成适合传输的小数据包，并在目标设备处重新组装。\n拥塞控制：防止网络因数据流量过大而出现拥塞。\n异构网络互联：连接不同类型的网络（如以太网、Wi-Fi、帧中继等）。\n错误处理与诊断：检测和处理网络中的错误。\n服务质量：为不同类型的数据流提供优先级和带宽保障。\n安全性：保护网络层数据的安全性和隐私性。\n\n# 路由器功能\n\n路由选择\n分组转发\n\n# SDN 软件定义网络\n软件定义网络（SDN，Software-Defined Networking） 是一种网络架构，旨在通过将网络控制平面与数据平面分离，提升网络的可编程性、灵活性和管理效率。SDN 的核心思想是通过集中式的控制器来管理网络设备，简化配置和优化流量。\nSDN 的优势\n\n集中控制：\n\n通过集中控制器管理网络，简化配置和故障排查。\n\n\n灵活性与可编程性：\n\n网络行为可通过软件编程动态调整，适应不同需求。\n\n\n自动化与智能化：\n\n支持自动化配置和优化，减少人工干预。\n\n\n网络虚拟化：\n\n支持多租户网络虚拟化，提升资源利用率。\n\n\n快速创新：\n\n新功能可通过软件快速部署，无需更换硬件。\n\n\n\n# 拥塞控制\n因为网络中出现过多的分组而引起的网络性能下降。\n控制拥塞的两种方法：\n\n开环控制：事发前\n闭环控制：事发中 / 后\n\n# IPv4（32 位）\n控制网络数据传送的网络协议。其主要由首部（20 字节）加数据部\n网络号 + 主机号\n全 0 是本机地址\n全 1 是广播地址\n# IPv6（128 位）\n# 源地址字段、目的地址字段\n# MTU（最大传送单元）\n# OSI 五层模型 vs OSI 七层模型\n\n\n\nOSI 五层模型\nOSI 七层模型\n功能描述\n\n\n\n\n应用层\n应用层（Application）\n提供应用程序接口和服务（如 HTTP、FTP、DNS 等）。\n\n\n\n表示层（Presentation）\n数据格式化、加密和解密（如 SSL/TLS）。\n\n\n\n会话层（Session）\n建立、管理和终止会话（如 RPC、NetBIOS）。\n\n\n传输层\n传输层（Transport）\n提供端到端的可靠数据传输（如 TCP、UDP）。\n\n\n网络层\n网络层（Network）\n负责逻辑地址寻址和路由选择（如 IP、ICMP）。\n\n\n数据链路层\n数据链路层（Data Link）\n提供节点到节点的可靠数据传输（如 Ethernet、PPP）。\n\n\n物理层\n物理层（Physical）\n传输原始的比特流（如网线、光纤、Wi-Fi）。\n\n\n\n# NAT 网络地址转换\nNAT（Network Address Translation，网络地址转换） 是一种用于将私有网络中的 IP 地址映射到公共 IP 地址的技术。它主要用于解决 IPv4 地址不足的问题，并提高网络的安全性。\n# 子网掩码\n子网掩码（Subnet Mask） 是用于划分 IP 地址的网络部分和主机部分的关键工具。它的主要作用是帮助网络设备确定一个 IP 地址属于哪个子网，从而实现高效的路由和网络管理。\n\n划分网络和主机\n支持子网划分\n路由选择\n广播域控制\n\n# IP 地址的分类\n\n\n\n类别\n前缀\n地址范围\n默认子网掩码\n用途\n\n\n\n\nA 类\n0\n1.0.0.0  到  126.0.0.0\n255.0.0.0\n大型网络\n\n\nB 类\n10\n128.0.0.0  到  191.255.0.0\n255.255.0.0\n中型网络\n\n\nC 类\n110\n192.0.0.0  到  223.255.255.0\n255.255.255.0\n小型网络\n\n\nD 类\n1110\n224.0.0.0  到  239.255.255.255\n无\n多播（Multicast）\n\n\nE 类\n1111\n240.0.0.0  到  255.255.255.255\n无\n保留（实验用途）\n\n\n\n# 路由聚合\n将好几个路由器逻辑上视为一个，是地址聚合。\n# 最长前缀匹配\n# ARP 地址解析协议\n# DHCP 动态主机配置协议\n动态分配 IP 地址，允许一台主机无需手动配置即可获取到 IP 地址，基于 UDP。\n# ICMP 网际控制报文协议\nICMP 是传输了一个控制报文，专门用来报告报告错误和异常的情况\n# RIP 的优势和劣势\n好消息传得快、坏消息传得慢\n","categories":["计算机网络"],"tags":["计算机网络"]},{"title":"Attention is all you need","url":"/paper/attention_is_all_your_need/","content":"# Attention is all you need\n\n\n【Original Link】 Attention Is All You Need\nUpdated on Aug 2023\nAuthors\n\nAshish Vaswani Google Brain  avaswani@google.com\nNoam Shazeer Google Brain  noam@google.com\nNiki Parmar Google Research nikip@google.com\nJakob Uszkoreit Google Research usz@google.com\nLlion Jones Google Research llion@google.com\nAidan N. Gomez † University of Toronto aidan@cs.toronto.edu\nŁukasz Kaiser Google Brain  lukaszkaiser@google.com\nIllia Polosukhin ‡  illia.polosukhin@gmail.com\n\n\n\n\n# Abstract\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.\n显性序列转导模型基于复杂的循环或卷积神经网络，其中包括编码器和解码器。\nThe best performing models also connect the encoder and decoder through an attention mechanism.\n性能最佳的模型还通过注意力机制连接编码器和解码器。\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n我们提出了一种新的简单网络架构，即 Transformer，它仅基于注意力机制，完全省去了重复和卷积。\nExperiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.\n对两项机器翻译任务的实验表明，这些模型在质量上表现出色，同时更具可并行化性，并且需要更少的训练时间。\nOur model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU.\n我们的模型在 WMT 2014 英德翻译任务中达到了 28.4 BLEU，比现有的最佳结果（包括合奏）提高了 2 BLEU 以上。\nOn the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n在 WMT 2014 英法翻译任务中，我们的模型在 8 个 GPU 上训练 3.5 天后，建立了新的单模型最先进的 BLEU 分数 41.8，这只是文献中最佳模型训练成本的一小部分。\nWe show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n我们证明了 Transformer 模型能够很好地推广到其他任务，通过将其成功应用于英语成分句法分析，无论是在大量训练数据还是有限训练数据的情况下。\n# Introduction\nRecurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation.\n循环神经网络，尤其是长短期记忆网络（LSTM）和门控循环单元（GRU），已经在序列建模和转导问题（如语言建模和机器翻译）中被牢固地确立为最先进的方法。\nNumerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.\n自那以后，人们不断努力突破循环语言模型和编码器 - 解码器架构的极限。\nRecurrent models typically factor computation along the symbol positions of the input and output sequences.\n循环模型通常沿着输入和输出序列的符号位置进行计算分解。\nAligning the positions to steps in computation time, they generate a sequence of hidden states hth_tht​, as a function of the previous hidden state ht−1h_t−1ht​−1 and the input for position ttt.\n将位置与计算时间的步骤对齐，它们生成一系列隐藏状态hth_tht​，这些隐藏状态是前一个隐藏状态ht−1h_t−1ht​−1 和位置ttt 的输入的函数。\nThis inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.\n这种固有的顺序性使得在训练样本内部无法进行并行化处理，这在序列长度较长时变得尤为关键，因为内存限制会限制跨样本的批处理。\nRecent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation, while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n最近的研究通过分解技巧和条件计算在计算效率方面取得了显著的改进，同时也提高了后者的模型性能。然而，顺序计算的基本限制仍然存在。\n\n分解技巧可以用于优化矩阵乘法、卷积操作等。例如，通过将一个大的矩阵乘法分解为多个小的矩阵乘法，可以利用现代硬件的并行计算能力，从而提高计算效率。\n条件计算可以用于动态调整模型的计算量，例如在某些情况下跳过某些层的计算，或者根据输入数据的特征选择性地激活某些神经元。\n\nAttention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences\n注意力机制已经成为各种任务中令人信服的序列建模和转导模型的一个重要组成部分，它允许在不考虑输入或输出序列中依赖项距离的情况下对依赖关系进行建模。\nIn all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.\n然而，在几乎所有情况下，这些注意力机制都是与循环网络结合使用的。\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n在这项工作中，我们提出了 Transformer 模型，这是一种摒弃了循环结构，完全依赖于注意力机制来捕捉输入和输出之间全局依赖关系的架构。Transformer 模型能够实现显著更高的并行化程度，并且在仅使用八块 P100 GPU 训练十二小时后，就能在翻译质量上达到新的最高水平。\n# Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet and ConvS2S, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.\n减少顺序计算的目标也是扩展型神经 GPU（Extended Neural GPU）、ByteNet 和 ConvS2S 的基础，这些模型都使用卷积神经网络（CNN）作为基本构建块，能够并行计算所有输入和输出位置的隐藏表示。\nIn these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.\n在这些模型中，将两个任意输入或输出位置的信号关联起来所需的运算次数会随着这两个位置之间的距离增加而增长。对于 ConvS2S 模型，这种增长是线性的；而对于 ByteNet 模型，这种增长是对数的。\nit more difficult to learn dependencies between distant positions. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n这使得学习远距离位置之间的依赖关系变得更加困难。在 Transformer 模型中，这种困难被减少到了一个固定的运算次数，尽管这以平均注意力加权位置导致的有效分辨率降低为代价，而我们通过第 3.2 节中描述的多头注意力（Multi-Head Attention）机制来抵消这种影响。\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations\n自注意力（Self-attention），有时也称为内部注意力（intra-attention），是一种注意力机制，它通过关联单个序列中不同位置的信息来计算该序列的表示。自注意力已经在多种任务中成功应用，包括阅读理解、摘要生成、文本蕴含以及学习与任务无关的句子表示。\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks.\n端到端记忆网络（End-to-end Memory Networks）基于一种循环注意力机制，而不是基于序列对齐的循环，已经在简单语言问答和语言建模任务上表现良好。\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as and.\n然而，据我们所知，Transformer 是第一个完全依赖自注意力来计算其输入和输出的表示的转导模型，而不使用序列对齐的循环神经网络（RNN）或卷积。在接下来的章节中，我们将介绍 Transformer 模型，阐述自注意力的动机，并讨论其相对于其他模型（如 RNN 和卷积模型）的优势。\n# Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations (x1,...,xn)(x_1, ..., x_n)(x1​,...,xn​) to a sequence of continuous representations z=(z1,...,zn)z = (z_1, ..., z_n)z=(z1​,...,zn​).\n大多数具有竞争力的神经序列转导模型都采用了编码器 - 解码器结构。在这种结构中，编码器将输入符号序列的表示(x1,...,xn)(x_1, ..., x_n)(x1​,...,xn​) 映射到一个连续表示的序列 z=(z1,...,zn)z = (z_1, ..., z_n)z=(z1​,...,zn​)。\nGiven zzz, the decoder then generates an output sequence (y1,...,ym)(y_1, ..., y_m)(y1​,...,ym​) of symbols one element at a time.\n给定 zzz，解码器随后逐个生成符号的输出序列 (y1,...,ym)(y_1, ..., y_m)(y1​,...,ym​)。\nAt each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.\n在每一步中，模型都是自回归的，在生成下一个符号时，会将之前生成的符号作为额外的输入。\n\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\nTransformer 模型遵循这种总体架构，使用堆叠的自注意力层和逐点全连接层来构建编码器和解码器，分别如图 1 的左半部分和右半部分所示。\n# Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N=6N = 6N=6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x+Sublayer(x))LayerNorm(x + Sublayer(x))LayerNorm(x+Sublayer(x)), where Sublayer(x)Sublayer(x)Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel=512.d_{model} = 512.dmodel​=512.\n编码器由 N=6N = 6N=6 个相同的层堆叠而成。每一层包含两个子层。第一个子层是多头自注意力机制（Multi-Head Self-Attention），第二个子层是一个简单的逐点全连接前馈网络（Position-wise Fully Connected Feed-Forward Network）。我们在每个子层周围使用残差连接（Residual Connection），并随后进行层归一化（Layer Normalization）。具体来说，每个子层的输出是LayerNorm(x+Sublayer(x))LayerNorm(x + Sublayer(x))LayerNorm(x+Sublayer(x))，其中Sublayer(x)Sublayer(x)Sublayer(x) 是子层自身实现的函数。为了便于实现这些残差连接，模型中的所有子层以及嵌入层都产生维度为 dmodel=512d_{model} = 512dmodel​=512 的输出。\nDecoder: The decoder is also composed of a stack of N=6N = 6N=6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position iii can depend only on the known outputs at positions less than iii.\n解码器同样由N=6N = 6N=6 个相同的层堆叠而成。除了编码器层中的两个子层外，解码器插入了一个第三个子层，该子层对编码器堆栈的输出执行多头注意力操作。与编码器类似，我们在每个子层周围使用残差连接，并随后进行层归一化。我们还修改了解码器堆栈中的自注意力子层，以防止位置关注后续位置。这种掩码（masking）与输出嵌入偏移一个位置的事实相结合，确保了位置 iii 的预测只能依赖于小于 iii 的已知输出。\n# Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n注意力函数可以被描述为将一个查询（query）和一组键值对（key-value pairs）映射到一个输出，其中查询、键、值和输出都是向量。输出是值的加权和，其中每个值的权重是由查询与相应键的兼容性函数计算得出的。\n\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n图 2：（左）缩放点积注意力。（右）多头注意力由并行运行的多个注意力层组成。\n# Scaled Dot-Product Attention (缩放点积注意力)\nWe call our particular attention “Scaled Dot-Product Attention” (Figure 2).\n我们将我们的特殊注意力称为 “尺度点积注意力”（图 2）。\nThe input consists of queries and keys of dimension dkd_kdk​, and values of dimension dvd_vdv​. We compute the dot products of the  query with all keys, divide each by dk\\sqrt{d_k}dk​​, and apply a softmax function to obtain the weights on the values.\n输入由维度 dkd_kdk​ 的查询和键以及维度 dvd_vdv​ 的值组成。我们计算所有键的查询的点积，将每个键除以 dk\\sqrt{d_k}dk​​，并应用 softmax 函数来获得值的权重。\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix QQQ. The keys and values are also packed together into matrices KKK and V . We compute the matrix of outputs as:\n在实践中，我们同时计算一组查询的注意力函数，并打包成矩阵 QQQ。键和值也打包到矩阵 KKK 和 VVV 中。我们将输出矩阵计算为：\nAttention(Q,K,V)=softmax(QKTdk)V\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\nAttention(Q,K,V)=softmax(dk​​QKT​)V\nThe two most commonly used attention functions are additive attention, and dot-product (multiplicative) attention.\n两个最常用的注意力函数是加法注意力和点积（乘法）注意力。\nDot-product attention is identical to our algorithm, except for the scaling factor of 1dk\\frac{1}{\\sqrt{d_k}}dk​​1​  .\n点积注意力与我们的算法相同，只是缩放因子为 1dk\\frac{1}{\\sqrt{d_k}}dk​​1​ 。\nAdditive attention computes the compatibility function using a feed-forward network with  a single hidden layer\n加性注意力使用具有单个隐藏层的前馈网络计算兼容性函数\nWhile the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n虽然两者在理论复杂性上相似，但点积注意力在实践中速度更快、空间效率更高，因为它可以使用高度优化的矩阵乘法代码来实现。\nWhile for small values of dkd_kdk​ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dkd_kdk​\n虽然对于较小的 dkd_kdk​ 值，这两种机制的性能相似，但对于较大的 dkd_kdk​ 值，加法注意力在不缩放的情况下优于点积注意力。\nWe suspect that for large values of dkd_kdk​, the dot products grow large in magnitude, pushing the softmax function into regions where it has  extremely small gradients\n我们怀疑，对于较大的 dkd_kdk​ 值，点积的幅度会变大，将 softmax 函数推入梯度极小的区域\nTo counteract this effect, we scale the dot products by √1dk\n为了抵消这种影响，我们将点积缩放为1dk\\frac{1}{\\sqrt{d_k}}dk​​1​\n# Multi-Head Attention（多头注意力）\nInstead of performing a single attention function with dmodeld_{model}dmodel​-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dkd_kdk​, dkd_kdk​ and dvd_vdv​ dimensions, respectively.\n我们发现，与其对 dmodeld_{model}dmodel​ 维度的键、值和查询执行单个注意力函数，不如分别使用不同的学习线性投影将查询、键和值线性投影到 dkd_kdk​, dkd_kdk​ 和dvd_vdv​ 维度。\nOn each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dvd_vdv​-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n在这些查询、键和值的投影版本上，我们并行地执行注意力函数，得到维度为 dvd_vdv​ 的输出值。这些输出值被拼接在一起，并再次进行投影，从而得到最终的值，如图 2 所示。\n\n假设查询 QQQ、键 KKK  和值 VVV 的维度分别为  dmodeld_{\\text{model}}dmodel​ ，多头注意力机制可以表示为：\n\n投影\n\nQi=QWiQ,Ki=KWiK,Vi=VWiVfor i=1,…,hQ_i = Q W_i^Q, \\quad K_i = K W_i^K, \\quad V_i = V W_i^V \\quad \\text{for } i = 1, \\ldots, h\nQi​=QWiQ​,Ki​=KWiK​,Vi​=VWiV​for i=1,…,h\n其中，WiQW_i^QWiQ​、WiKW_i^KWiK​ 和 WiVW_i^VWiV​ 是每个头的权重矩阵，hhh 是头的数量。\n\n\n并行执行注意力函数\nheadi=Attention(Qi,Ki,Vi)=softmax(QiKiTdk)Vi\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i) = \\text{softmax}\\left(\\frac{Q_i K_i^T}{\\sqrt{d_k}}\\right) V_i\nheadi​=Attention(Qi​,Ki​,Vi​)=softmax(dk​​Qi​KiT​​)Vi​\n\n\n拼接\nConcat(head1,head2,…,headh)\\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h)\nConcat(head1​,head2​,…,headh​)\n\n\n再次投影\nMultiHead(Q,K,V)=Concat(head1,head2,…,headh)WO\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h) W^O\nMultiHead(Q,K,V)=Concat(head1​,head2​,…,headh​)WO\n\n\n\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n多头注意力允许模型共同关注来自不同位置的不同表示子空间的信息。对于单个注意力头，平均会抑制这种情况。\nMultiHead(Q,K,V)=Concat(head1,head2,…,headh)where headi=Attention(QWiQ,KWiK,VWiV)\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h)\\\\\n\\text{where} \\ head_i=Attention(QW_{i}^Q,KW_{i}^K,VW_{i}^V)\nMultiHead(Q,K,V)=Concat(head1​,head2​,…,headh​)where headi​=Attention(QWiQ​,KWiK​,VWiV​)\nWhere the projections are parameter matrices WiQ∈Rdmodel×dk,WiK∈Rdmodel×dk,WiV∈Rdmodel×dvW^Q_i\\in\\mathbb{R^{d_{model}\\times d_k}}, W^K_i\\in\\mathbb{R^{d_{model}\\times d_k}}, W^V_i\\in\\mathbb{R^{d_{model}\\times d_v}}WiQ​∈Rdmodel​×dk​,WiK​∈Rdmodel​×dk​,WiV​∈Rdmodel​×dv​ and WiQ∈Rhdmodel×dkW^Q_i\\in\\mathbb{R^{hd_{model}\\times d_k}}WiQ​∈Rhdmodel​×dk​\n其中投影是参数矩阵WiQ∈Rdmodel×dk,WiK∈Rdmodel×dk,WiV∈Rdmodel×dvW^Q_i\\in\\mathbb{R^{d_{model}\\times d_k}}, W^K_i\\in\\mathbb{R^{d_{model}\\times d_k}}, W^V_i\\in\\mathbb{R^{d_{model}\\times d_v}}WiQ​∈Rdmodel​×dk​,WiK​∈Rdmodel​×dk​,WiV​∈Rdmodel​×dv​ 和 WiQ∈Rhdmodel×dkW^Q_i\\in\\mathbb{R^{hd_{model}\\times d_k}}WiQ​∈Rhdmodel​×dk​\nIn this work we employ h=8h = 8h=8 parallel attention layers, or heads. For each of these we use dk=dv=dmodelh=64d_k = d_v = \\frac{d_{\\text{model}}}{h} = 64dk​=dv​=hdmodel​​=64.Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n在这项工作中，我们使用了h=8h = 8h=8 个并行的注意力层，或者称为头。对于每一个头，我们使用dk=dv=dmodelh=64d_k = d_v = \\frac{d_{\\text{model}}}{h} = 64dk​=dv​=hdmodel​​=64。由于每个头的维度减小，总计算成本与全维的单头注意力相似。\n# Applications of Attention in our Model (注意力在我们模型中的应用)\nThe Transformer uses multi-head attention in three different ways:\nTransformer 以三种不同的方式使用多头注意力：\n\nIn “encoder-decoder attention” layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. 在 “编码器 - 解码器注意” 层中，查询来自前一个解码器层，内存键和值来自编码器的输出。This allows every position in the decoder to attend over all positions in the input sequence. 这允许解码器中的每个位置都关注输入序列中的所有位置。This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. 这模仿了序列到序列模型（如 [38,2,9]）中典型的编码器 - 解码器注意力机制。\nThe encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. 编码器包含自注意力层。在自注意力层中，所有键、值和查询都来自同一个位置，在本例中，是编码器中上一层的输出。编码器中的每个位置都可以关注编码器上一层中的所有位置。\nSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. 类似地，解码器中的自注意力层允许解码器中的每个位置关注解码器中的所有位置，直到该位置并包括该位置。We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2. 我们需要防止解码器中信息向左流动，以保留自回归属性。我们通过屏蔽（设置为 -∞）softmax 输入中对应于非法连接的所有值来实现这一点。参见图 2。\n\n# Position-wise Feed-Forward Networks（位置前馈网络）\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n除了注意力子层外，我们的编码器和解码器中的每一层都包含一个完全连接的前馈网络，该网络分别且相同地应用于每个位置。这包括两个线性变换，中间有一个 ReLU 激活。\nFFN(x)=max(0,xW1+b1)W2+b2FFN(x)=max(0,xW_1+b_1)W_2+b_2\nFFN(x)=max(0,xW1​+b1​)W2​+b2​\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n虽然线性变换在不同位置上是相同的，但它们在层与层之间使用不同的参数。另一种描述这一点的方法是两个内核大小为 1 的卷积。\nThe dimensionality of input and output is dmdoel=512d_{mdoel}=512dmdoel​=512,and the inner-layer has dimensionalitydff=2048d_{ff}=2048dff​=2048\n输入和输出的维数为 dmdoel=512d_{mdoel}=512dmdoel​=512，内层的维数为 dff=2048d_{ff}=2048dff​=2048。\n# Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodeld_{model}dmodel​.\n与其他序列转导模型类似，我们使用学习的嵌入将输入标记和输出标记转换为维度  dmodeld_{model}dmodel​ 的向量。\nWe also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.\n们还使用通常学习的线性变换和 softmax 函数将解码器输出转换为预测的下一个标记概率。\nIn our model, we share the same weight matrix between the two embedding layers and the pre-softmax  linear transformation, similar to [30].In the embedding layers, we multiply those weights by dmodel\\sqrt{d_{model}}dmodel​​\n在我们的模型中，我们在两个嵌入层和预 softmax 线性变换之间共享相同的权重矩阵，类似于 [30]。在嵌入层中，我们将这些权重乘以dmodel\\sqrt{d_{model}}dmodel​​\n","categories":["论文阅读"],"tags":["python","深度学习","attention","transformer"]},{"title":"模糊数学25习题","url":"/math/mh_math/","content":"# 第一章\n\n\n\n\n\n\n\n\n# 第二章\n\n\n\n\n\n\n\n \n# 第三章\n\n\n\n\n\n\n\n\n# 第四章\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 居卡莫特方法 —— 哥弟姐妹公式\n居卡莫特方法是求解模糊关系方程的核心所在，对于考试来说，主要目的在于求解形如：\n\n这样的题。\n居卡莫特方法定义了如下两种运算方法：\nbεa={b,当a&gt;b时[b,1],当a=b时∅,当b&gt;a时b\\varepsilon a=\\left\\{\\begin{matrix}\n b, &amp; 当a&gt;b时\\\\\n [b,1], &amp; 当a=b时\\\\\n \\emptyset , &amp; 当b&gt;a时\n\\end{matrix}\\right.\nbεa=⎩⎪⎨⎪⎧​b,[b,1],∅,​当a&gt;b时当a=b时当b&gt;a时​\nbε^a={[0,b],当a&gt;b时[0,1],当a≤b时b \\hat{\\varepsilon} a=\\left\\{\\begin{matrix}\n [0,b], &amp; 当a&gt;b时\\\\\n [0,1], &amp; 当a\\le b时\n\\end{matrix}\\right.\nbε^a={[0,b],[0,1],​当a&gt;b时当a≤b时​\n背下来就行，问题是怎么背，我在这里提出一个原创故事来帮助你记忆 —— 哥弟姐妹公式。\n不带帽子的称为哥弟公式，b 是哥哥，a 是弟弟。\nbεa={b,当a&gt;b时[b,1],当a=b时∅,当b&gt;a时b\\varepsilon a=\\left\\{\\begin{matrix}\n b, &amp; 当a&gt;b时\\\\\n [b,1], &amp; 当a=b时\\\\\n \\emptyset , &amp; 当b&gt;a时\n\\end{matrix}\\right.\nbεa=⎩⎪⎨⎪⎧​b,[b,1],∅,​当a&gt;b时当a=b时当b&gt;a时​\n故事开始了…\n哥哥让着弟弟（当 a&gt;b 时），我们说这是一个有担当的哥哥，我们应该褒奖哥哥（b）\n哥哥与弟弟一样时，这说明兄弟二人相处良好，哥哥爱着弟弟（这个 [b, 1] 可以理解为 b 是哥哥，1 就像字母 i 一样，哥哥 b，i 弟弟，[b, 1]）\n当哥哥比弟弟大，也就是说他不爱弟弟，爱归于空寂，所以不爱了是空集。\n有帽子的称为姐妹公式，b 是姐姐，a 是妹妹。\nbε^a={[0,b],当a&gt;b时[0,1],当a≤b时b \\hat{\\varepsilon} a=\\left\\{\\begin{matrix}\n [0,b], &amp; 当a&gt;b时\\\\\n [0,1], &amp; 当a\\le b时\n\\end{matrix}\\right.\nbε^a={[0,b],[0,1],​当a&gt;b时当a≤b时​\n故事开始了…\n妹妹比较牛，当妹妹生气的时候（a&gt;b），这个时候姐姐也很生气，无论妹妹做什么姐姐都不想理妹妹，姐姐睁一只眼闭一只眼（睁一只眼闭一只眼，[0,b]，这个睁一只眼就像 0 一样，闭一只眼就是 b，所以是 [0,b]）。\n后来（其他情况），吵来吵去终究姐妹重归于好，他们要共同成长，共赴未来（[0,1] 从零到一描绘了从小到大，共同成长）。\n# 碎碎念\n模糊数学复习就到这里了，感谢你能看到这里，祝复习顺利。\n# 特别鸣谢\n特别鸣谢我的室友：求求你别骗我，对本文详细审阅。\n","categories":["笔记"],"tags":["模糊数学解题笔记"]},{"title":"3DCNN视频行为检测平台","url":"/python/3D_CNN/","content":"# 3DCNN 视频行为检测平台笔记\n学习内容基于：https://www.bilibili.com/video/BV1kc411Q7Tj\n# 图像的本质\n图像是由 RGB 三通道组成的，一般数值范围是 0 到 255.\n因此像素矩阵为：H * W * 3，其中 H 为高，W 是宽。\n# 从图像到视频\n视频本质上是由连续的图片（视频帧），快速播放构成的。\n图片的像素矩阵是 H * W * 3\n视频就是 D * H * W * 3，其中 D 是深度，是视频帧的叠加。\n不同的库对与这 4 个维度顺序表述不同，但这四个维度诚然如此。\n# 2D 检测方法对比\n\n\n\n# C3D 网络流程\n\n这个（1，1，1），三个数字是有点手法的，第一个数是代表了深度，第二个、第三个代表了高和宽。\n我们可以看到，一般来说这个卷积核大小一般是（3，3，3），填充是（1，1，1），经过这样的卷积过后，特征图大小不变。\n\n这里是 up 的解释，已经是非常的直观易懂。\n# 动手搭建一个 C3D\nimport torchfrom torch import nnfrom torchsummary import summaryclass C3D(nn.Module):    def __init__(self, num_classes, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.block1 = nn.Sequential(            nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1)),            nn.ReLU(),            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))        )        self.block2 = nn.Sequential(            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1)),            nn.ReLU(),            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))        )        self.block3 = nn.Sequential(            nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1)),            nn.ReLU(),            nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1)),            nn.ReLU(),            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))        )        self.block4 = nn.Sequential(            nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1)),            nn.ReLU(),            nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1)),            nn.ReLU(),            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))        )        self.block5 = nn.Sequential(            nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1)),            nn.ReLU(),            nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1)),            nn.ReLU(),            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1))        )        self.fc = nn.Sequential(            nn.Linear(8192, 4096),            nn.ReLU(),            nn.Dropout(p=0.5),            nn.Linear(4096, 4096),            nn.ReLU(),            nn.Dropout(p=0.5),            nn.Linear(4096, num_classes)        )        self._init_weights()    def forward(self, x):        x = self.block1(x)        x = self.block2(x)        x = self.block3(x)        x = self.block4(x)        x = self.block5(x)        x = x.flatten(start_dim=1)        x = self.fc(x)        return x    def _init_weights(self):        for m in self.modules():            if isinstance(m, nn.Conv3d):                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')                if m.bias is not None:                    nn.init.constant_(m.bias, 0)            elif isinstance(m, nn.Linear):                nn.init.normal_(m.weight, 0, 0.01)if __name__ == '__main__':    model = C3D(num_classes=101)    print(summary(model, (3, 16, 112, 112), device='cpu'))# 预训练权重\n比如，有一个猫狗分类的模型，权重和偏置已经训练好了，但是现在有一个新任务，就是狼和老虎进行分类，我们就可以用猫狗分类的权重来继续训练狼和老虎分类，这样的话会加速我们的训练速度，非常方便。\n@staticmethod    def __load__pretrained_model():        p_dict = torch.load('./ucf101-caffe.pth')        s_dict = model.state_dict()        corresp_name = &#123;            'features.0.weight': 'block1.0.weight',            'features.0.bias': 'block1.0.bias',            'features.3.weight': 'block2.0.weight',            'features.3.bias': 'block2.0.bias',            'features.6.weight': 'block3.0.weight',            'features.6.bias': 'block3.0.bias',            'features.8.weight': 'block3.2.weight',            'features.8.bias': 'block3.2.bias',            'features.11.weight': 'block4.0.weight',            'features.11.bias': 'block4.0.bias',            'features.13.weight': 'block4.2.weight',            'features.13.bias': 'block4.2.bias',            'features.16.weight': 'block5.0.weight',            'features.16.bias': 'block5.0.bias',            'features.18.weight': 'block5.2.weight',            'features.18.bias': 'block5.2.bias',            'classifier.0.weight': 'fc.0.weight',            'classifier.0.bias': 'fc.0.bias',            'classifier.3.weight': 'fc.3.weight',            'classifier.3.bias': 'fc.3.bias'        &#125;        for name in p_dict:            if name not in corresp_name:                continue            s_dict[corresp_name[name]] = p_dict[name]像这样，我们就可以做参数移植了\n","categories":["深度学习"],"tags":["python","深度学习","CNN","3D_CNN"]},{"title":"CNN经典卷积神经网络与实战","url":"/python/CNN_Acc/","content":"# 卷积神经网络\n学习内容基于：Pytorch 框架与经典卷积神经网络与实战\n# CNN 卷积神经网络算法原理\n# 全连接神经网络\n\n输入层是我们输入的数据，这里看到的第一列节点并不是输入层，中间为隐藏层。\n输入层就像 X（自变量），模型或者说这些网络就是 F（函数），我们得到的输出就是 Y（因变量）。\n# 为什么要使用激活函数\n在神经网络中使用激活函数的根本原因是引入非线性，从而使模型能够拟合和表达复杂的函数关系。如果没有激活函数，神经网络无论堆叠多少层，本质上都是一个线性模型，能力极其有限。多层线性变换的叠加依然是线性变换，最终的模型只能拟合直线 / 平面，完全无法处理复杂的数据模式\n# Sigmoid 激活函数\n\n优点：简单、非常适用分类任务。\n缺点：反向传播训练时有梯度消失的问题；输出值区间为 (0,1)，关于 0 不对称；梯度更新在不同方向走得太远，使得优化难度增大，训练耗时。\n# Tanh 激活函数\n\n优点：解决了 Sigmoid 函数输出值非 0 对称的问题，训练比 Sigmoid 函数快，更容易收敛\n缺点：反向传播训练时有梯度消失的问题，Tanh 函数和 Sigmoid 函数非常相似。\n# ReLU 激活函数\n\n优点：解决了梯度消失的问题；计算更为简单，没有 Sigmoid 函数和 Tanh 函数的指数运算\n缺点：训练时可能出现神经元死亡\n# Leaky ReLU 激活函数\n\n优点：解决了 ReLU 的神经元死亡问题\n缺点：无法为正负输入值提供一致的关系预测 (不同区间函数不同)\n# 前向传播\n前向传播是神经网络中数据从输入层依次流向输出层的过程，它的核心目标是根据当前的模型参数（权重和偏置）计算出预测结果。\n前向传播就是 “把输入数据依次喂给每一层，经过线性计算 + 激活函数，逐层输出，最后得到预测值” 的过程。\n# 损失函数\n# 均方误差\n\n\n\n\n\n\n\n\n前面有可能有出现 1/2，那只是为了方便求导，都是均方误差。\n# 梯度下降法\n\n\n# 全连接神经网络在图片中存在的问题\n# 1. 参数量巨大\n\n全连接层的每一个神经元都与上一层的所有神经元相连。\n对于图片来说，输入通常是高维的，例如一张 224×224 的 RGB 图片就是 224×224×3 = 150,528 个输入特征。\n假设第一层有 1000 个神经元，那么权重数量就是 150,528 × 1000 ≈ 1.5 亿个参数！\n问题：参数太多 → 容易过拟合 → 训练时间长 → 需要大量显存。\n\n\n# 2. 忽略空间结构\n\n图片是二维或三维（RGB）的网格数据，有局部空间相关性（邻近像素往往相关）。\n全连接层把图片 “拉平” 成一维向量，然后再进行矩阵乘法。\n问题：丢失了图片的空间信息（如边缘、纹理、形状），无法有效捕捉局部特征。\n\n\n# 3. 缺乏平移不变性\n\n图像中物体的位置可能变化。\n全连接网络对输入的每个位置都固定，物体稍微移动，输出可能完全不同。\n问题：无法自动识别图像中的平移或局部位移，泛化能力差。\n\n\n# 4. 计算效率低\n\n全连接层计算复杂度高（矩阵乘法量大）。\n对高分辨率图像，训练和推理速度都很慢。\n对比卷积神经网络（CNN），后者通过卷积核共享权重大幅减少计算量。\n\n\n# 5. 不适合捕捉层次特征\n\n图片的特征是有层次结构的：边缘 → 纹理 → 形状 → 对象。\n全连接层一次性处理所有像素，无法自然学习层次特征。\nCNN 则通过卷积和池化层逐步抽象特征，更符合视觉认知规律。\n\n# 卷积、步幅、填充、池化\n这部分请看：\n\n          \n          PyTorch深度学习convolution 卷积操作\n          https://735690757.github.io/python/pytorch\n          \n# 经过卷积后特征图大小\n\nFH：卷积核（filter）的高度（Filter Height）\nFW：卷积核的宽度（Filter Width）\nC_in：输入通道数（Input Channels）\nC_out：输出通道数（Output Channels）\nS：步幅（Stride）\nP：填充（Padding）\n如果算出来是小数，一般是向下取整。\n# LeNet 与 AlexNet 原理\n# LeNet-5 诞生背景\n简单来说，LeNet-5 的诞生背景是为了解决手写数字识别这一实际应用问题，它是世界上首个成功商用的卷积神经网络，奠定了现代深度学习的基础。\n# LeNet-5 网络结构\n\n\n输入 1 * 28 * 28\n5 * 5 卷积（6），填充 2\n2 * 2 平均池化层，步幅 2\n5 * 5 卷积（16），填充 0\n2 * 2 平均池化层，步幅 2\n全连接（120）\n全连接（84）\n全连接（10）\n\n\n再强调一下：\n\n这个计算公式非常重要。\n\n# AlexNet 诞生背景\n在算力达到临界点、大数据已经就位的环境下，一个古老但曾被忽视的算法（深度学习 / 卷积神经网络）迎来了证明自己的最佳时机。AlexNet 不仅仅是一个优秀的模型，它更是一个时代的开创者，是人工智能发展史上的一个关键转折点。\n# AlexNet 网络结构\n\n5 层卷积，3 层全连接，共 8 层，激活函数使用 ReLU。\n\n输入 3 * 227 * 227（？这个可能有问题）\n11 * 11 卷积（96），步幅 4，ReLU\n3 * 3 最大池化，步幅 2\n5 * 5 卷积（256），填充 2，ReLU\n3 * 3 最大池化，步幅 2\n3 * 3 卷积（384），填充 1，ReLU\n3 * 3 卷积（384），填充 1，ReLU\n3 * 3 卷积（256），填充 1，ReLU\n3 * 3 最大池化，步幅 2\n全连接（4096）\n全连接（4096）\n全连接（10）\n\n这里在全连接层有很多的参数，参数太多容易过拟合，我们引入了 Dropout 操作。\n# 图像增强 - 水平翻转\n\n# 图像增强 - 随机裁剪\n\n# 图像增强 - PCA\n\n\n\n\n方面\n描述\n\n\n\n\n核心思想\n利用 PCA 找到图像颜色的主要变化方向，并沿这些方向添加随机扰动来模拟光照变化。\n\n\n目的\n增强模型对颜色和光照变化的鲁棒性，是一种高效的数据增强手段。\n\n\n优点\n变化方式基于图像自身的统计特性，生成的图像颜色变化自然、合理。\n\n\n缺点\n计算成本相对较高（需要对每张图或每批图做 PCA）。\n\n\n遗产\n是 AlexNet 的一个创新点，启发了对颜色增强的重视，但已被更简单高效的方法所取代。\n\n\n\n# LRN 正则化\n这是一个针对通道间的计算\n\n尽管 LRN 是 AlexNet 的一个关键创新，但在后续更深、更先进的网络（如 VGG、ResNet）中，它几乎被完全弃用了。主要原因如下：\n\n效果有限且不稳定：后续研究发现，LRN 带来的性能提升非常微弱，甚至有时不稳定。其正则化效果远不如 Dropout 和 Batch Normalization（BN） 那样显著和可靠。\n被更好的技术取代：\n\nDropout：通过随机断开神经元连接来防止过拟合，更为直接有效。\nBatch Normalization（批归一化）：这是革命性的技术。BN 对整个 Batch 的每个通道进行归一化（均值为 0，方差为 1），极大地改善了梯度流动，加速了训练，同时本身也具有轻微的正则化效果。BN 的效果远超 LRN，并且已经成为现代深度网络的标准组件。\n\n\n增加计算开销和超参数：LRN 引入了额外的计算量，并且  k, α, β, n  这些超参数需要调优，增加了模型设计的复杂性。\n\n# 重叠池化\n与 LRN 类似，重叠池化在现代深度学习架构中也已经不常用了。\n\n计算成本更高：由于存在重叠，为了得到相同尺寸的输出特征图，重叠池化需要进行更多次池化操作。例如，将  5x5  降采样到  2x2 ，非重叠池化 ( 2x2 , stride=2) 需要 4 次操作，而重叠池化 ( 3x3 , stride=2) 也需要 4 次操作，但每次操作的窗口更大，计算量稍高。\n被更有效的技术取代：如今，防止过拟合和提升性能的重任更多地由 Batch Normalization、更深的网络结构（如 ResNet 的残差连接）、更先进的优化器 和 Dropout 等方法来承担。\n设计趋势变化：现代网络有时甚至会完全摒弃池化层，转而使用步长大于 1 的卷积（Strided Convolution） 来同时实现特征提取和降采样，这被认为能提供更大的模型容量和灵活性。\n\n# LeNet 实战\n# 模型\nclass LeNet(nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.block = nn.Sequential(            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2),            nn.Sigmoid(),            nn.AvgPool2d(kernel_size=2, stride=2),            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0),            nn.Sigmoid(),            nn.AvgPool2d(kernel_size=2, stride=2),            nn.Flatten(),            nn.Linear(in_features=400, out_features=120),            nn.Sigmoid(),            nn.Linear(in_features=120, out_features=84),            nn.Sigmoid(),            nn.Linear(in_features=84, out_features=10)        )    def forward(self, x):        return self.block(x)# 设备、实例化与 summary\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")net = LeNet().to(device)summary(net, (1, 28, 28))\n这里的 -1 代表的是 批量大小，更具体地说：-1 是一个占位符，表示这个维度的大小是由其他维度推断出来的，而不是一个固定值。\n批量大小：在深度学习训练中，数据通常是按批次（batch）输入的。比如，你可能会一次输入 32 张图片、64 张图片等。这个数量就是批量大小。\n为什么是 -1？：PyTorch 模型在设计时，其核心计算逻辑不依赖于具体的批量大小。为了增加灵活性，在定义模型的前向传播时，我们通常将输入张量的第一个维度设为批量大小。当打印模型摘要时，库（如 torchsummary）无法预先知道你会用多大的批量大小来训练，所以它使用 -1 来代表 “任何尺寸”。\n动态推断：在实际运行中，这个 -1 会被你输入数据的真实批量大小所替代。\n例如，如果你用一批 32 张图片输入到模型，那么 [-1, 1, 32, 32] 就会变成 [32, 1, 32, 32]。\n如果你用 128 张图片，它就会变成 [128, 1, 32, 32]。\n# 加载 FashionMNIST\nfrom torchvision.datasets import FashionMNISTfrom torchvision.transforms import transformsimport numpy as nptransform = transforms.Compose([    transforms.Resize((28, 28)),    transforms.ToTensor(),    transforms.Normalize((0.5,), (0.5,))])train_data = FashionMNIST(root=\"./data/FashionMNIST\", train=True, download=True, transform=transform)test_data = FashionMNIST(root=\"./data/FashionMNIST\", train=False, download=True, transform=transform)from torch.utils.data import DataLoadertrain_dataLoader = DataLoader(train_data, batch_size=64, shuffle=True)test_dataLoader = DataLoader(test_data, batch_size=64, shuffle=False)# 展示\nimport matplotlib.pyplot as pltfor setp, (features, label) in enumerate(train_dataLoader):    if setp == 0:        x = features.squeeze().numpy()        y = label.numpy()        breakplt.figure(figsize=(12, 5))for ii in np.arange(len(y)):    plt.subplot(4, 16, ii+1)    plt.imshow(x[ii,:,:], cmap=plt.cm.gray)    plt.title(y[ii])    plt.axis(\"off\")plt.show()\n# 训练、验证（此处为最佳实践）\nfrom torch.optim.lr_scheduler import CosineAnnealingLRfrom tqdm import tqdmimport copyimport timeimport pandas as pddef train_model_process(model, train_dataloader, val_dataloader, num_epochs):    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)    scheduler = CosineAnnealingLR(        optimizer,        T_max=num_epochs,        eta_min=1e-6    )    criterion = nn.CrossEntropyLoss().to(device)    model = model.to(device)    best_model_wts = copy.deepcopy(model.state_dict())    best_acc = 0.0    train_loss_all = []    val_loss_all = []    train_acc_all = []    val_acc_all = []    learning_rates = []    since = time.time()    for epoch in range(num_epochs):        print(\"Epoch &#123;&#125;/&#123;&#125;\".format(epoch, num_epochs - 1))        print(\"-\" * 10)        current_lr = optimizer.param_groups[0]['lr']        learning_rates.append(current_lr)        print(f\"当前学习率: &#123;current_lr:.6f&#125;\")        train_loss = 0.0        train_corrects = 0        val_loss = 0.0        val_corrects = 0        train_num = 0        val_num = 0        for step, (b_x, b_y) in tqdm(enumerate(train_dataloader), desc=f\"总步骤：&#123;len(train_dataloader)&#125;\",                                     leave=False, unit=\"step\", total=len(train_dataloader),                                     bar_format=\"&#123;desc&#125;: |&#123;bar:30&#125;| &#123;percentage:3.0f&#125;% 唱跳Rap🏀，Music~\",                                     ascii=\"🏀🥰🥰😘\"):            b_x = b_x.to(device)            b_y = b_y.to(device)            model.train()            output = model(b_x)            pre_lab = torch.argmax(output, dim=1)            loss = criterion(output, b_y)            optimizer.zero_grad()            loss.backward()            optimizer.step()            train_loss += loss.item() * b_x.size(0)            # 如果预测正确，则准确度 train_corrects 加 1            train_corrects += torch.sum(pre_lab == b_y.data)            train_num += b_x.size(0)        for step, (b_x, b_y) in enumerate(val_dataloader):            b_x = b_x.to(device)            b_y = b_y.to(device)            model.eval()            output = model(b_x)            pre_lab = torch.argmax(output, dim=1)            loss = criterion(output, b_y)            val_loss += loss.item() * b_x.size(0)            val_corrects += torch.sum(pre_lab == b_y.data)            val_num += b_x.size(0)        train_loss_all.append(train_loss / train_num)        train_acc_all.append(train_corrects.double().item() / train_num)        val_loss_all.append(val_loss / val_num)        val_acc_all.append(val_corrects.double().item() / val_num)        print(\"&#123;&#125; train loss:&#123;:.4f&#125; train acc: &#123;:.4f&#125;\".format(epoch, train_loss_all[-1], train_acc_all[-1]))        print(\"&#123;&#125; val loss:&#123;:.4f&#125; val acc: &#123;:.4f&#125;\".format(epoch, val_loss_all[-1], val_acc_all[-1]))        if val_acc_all[-1] > best_acc:            best_acc = val_acc_all[-1]            best_model_wts = copy.deepcopy(model.state_dict())        time_use = time.time() - since        print(\"训练和验证耗费的时间&#123;:.0f&#125;m&#123;:.0f&#125;s\".format(time_use // 60, time_use % 60))        scheduler.step()    model.load_state_dict(best_model_wts)    torch.save(best_model_wts, \"models/best_model.pth\")    train_process = pd.DataFrame(data=&#123;\"epoch\": range(num_epochs),                                       \"train_loss_all\": train_loss_all,                                       \"val_loss_all\": val_loss_all,                                       \"train_acc_all\": train_acc_all,                                       \"val_acc_all\": val_acc_all,                                       \"learn_rates\": learning_rates&#125;)    return train_processdef matplot_acc_loss(train_process):    # 显示每一次迭代后的训练集和验证集的损失函数和准确率    plt.figure(figsize=(12, 4))    plt.subplot(1, 3, 1)    plt.plot(train_process['epoch'], train_process.train_loss_all, \"ro-\", label=\"Train loss\")    plt.plot(train_process['epoch'], train_process.val_loss_all, \"bs-\", label=\"Val loss\")    plt.legend()    plt.xlabel(\"epoch\")    plt.ylabel(\"Loss\")    plt.subplot(1, 3, 2)    plt.plot(train_process['epoch'], train_process.train_acc_all, \"ro-\", label=\"Train acc\")    plt.plot(train_process['epoch'], train_process.val_acc_all, \"bs-\", label=\"Val acc\")    plt.xlabel(\"epoch\")    plt.ylabel(\"acc\")    plt.legend()    plt.subplot(1, 3, 3)    plt.plot(train_process['epoch'], train_process.learn_rates, \"go-\", label=\"Learn rates\")    plt.xlabel(\"epoch\")    plt.ylabel(\"Learn rates\")    plt.legend()    plt.tight_layout()    plt.show()# 这里是优化后的版本\nfrom torch.optim import lr_schedulerimport copyimport timeimport pandas as pddef train_model_process(model, train_dataloader, val_dataloader, num_epochs):    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    if not os.path.exists(\"models\"):        os.mkdir(\"models\")    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=0.05)    scheduler = lr_scheduler.SequentialLR(        optimizer,        schedulers=[            lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=15),            lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs-15, eta_min=1e-6)        ],        milestones=[15]    )    criterion = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)    model = model.to(device)    best_model_wts = copy.deepcopy(model.state_dict())    best_acc = 0.0    train_loss_all = []    val_loss_all = []    train_acc_all = []    val_acc_all = []    learning_rates = []    since = time.time()    for epoch in range(num_epochs):        print(\"Epoch &#123;&#125;/&#123;&#125;\".format(epoch, num_epochs - 1))        print(\"-\" * 10)        current_lr = optimizer.param_groups[0]['lr']        learning_rates.append(current_lr)        print(f\"当前学习率: &#123;current_lr:.6f&#125;\")        train_loss = 0.0        train_corrects = 0        val_loss = 0.0        val_corrects = 0        train_num = 0        val_num = 0        for step, (b_x, b_y) in tqdm(enumerate(train_dataloader), desc=f\"总步骤：&#123;len(train_dataloader)&#125;\",                                     leave=False, unit=\"step\", total=len(train_dataloader),                                     bar_format=\"&#123;desc&#125;: |&#123;bar:30&#125;| &#123;percentage:3.0f&#125;% 我在努力训练，唱跳Rap🏀，Music~\",                                     ascii=\"🏀🥰🥰😘\"):            b_x = b_x.to(device)            b_y = b_y.to(device)            model.train()            output = model(b_x)            pre_lab = torch.argmax(output, dim=1)            loss = criterion(output, b_y)            optimizer.zero_grad()            loss.backward()            optimizer.step()            train_loss += loss.item() * b_x.size(0)            # 如果预测正确，则准确度 train_corrects 加 1            train_corrects += torch.sum(pre_lab == b_y.data)            train_num += b_x.size(0)        for step, (b_x, b_y) in tqdm(enumerate(val_dataloader), desc=f\"总步骤：&#123;len(val_dataloader)&#125;\",                                     leave=False, unit=\"step\", total=len(val_dataloader),                                     bar_format=\"&#123;desc&#125;: |&#123;bar:30&#125;| &#123;percentage:3.0f&#125;% 该我上场表演了，唱跳Rap🏀，Music~\",                                     ascii=\"🏀🥰🥰😘\"):            b_x = b_x.to(device)            b_y = b_y.to(device)            model.eval()            output = model(b_x)            pre_lab = torch.argmax(output, dim=1)            loss = criterion(output, b_y)            val_loss += loss.item() * b_x.size(0)            val_corrects += torch.sum(pre_lab == b_y.data)            val_num += b_x.size(0)        train_loss_all.append(train_loss / train_num)        train_acc_all.append(train_corrects.double().item() / train_num)        val_loss_all.append(val_loss / val_num)        val_acc_all.append(val_corrects.double().item() / val_num)        print(\"&#123;&#125; train loss:&#123;:.4f&#125; train acc: &#123;:.4f&#125;\".format(epoch, train_loss_all[-1], train_acc_all[-1]))        print(\"&#123;&#125; val loss:&#123;:.4f&#125; val acc: &#123;:.4f&#125;\".format(epoch, val_loss_all[-1], val_acc_all[-1]))        if val_acc_all[-1] > best_acc:            best_acc = val_acc_all[-1]            best_model_wts = copy.deepcopy(model.state_dict())            checkpoint = &#123;                'epoch': epoch,                'model_state_dict': model.state_dict(),                'optimizer_state_dict': optimizer.state_dict(),                'scheduler_state_dict': scheduler.state_dict(),                'best_acc': best_acc,            &#125;            torch.save(checkpoint, \"models/best_checkpoint.pth\")        time_use = time.time() - since        print(\"训练和验证耗费的时间&#123;:.0f&#125;m&#123;:.0f&#125;s\".format(time_use // 60, time_use % 60))        scheduler.step()    model.load_state_dict(best_model_wts)    torch.save(best_model_wts, \"models/theBest.pth\")    train_process = pd.DataFrame(data=&#123;\"epoch\": range(num_epochs),                                       \"train_loss_all\": train_loss_all,                                       \"val_loss_all\": val_loss_all,                                       \"train_acc_all\": train_acc_all,                                       \"val_acc_all\": val_acc_all,                                       \"learn_rates\": learning_rates&#125;)    return train_process这里是学习率使用 0.01，可以得到更好的效果，学习率为 0.001 在图像上更美观。\n# 测试（此处为最佳实践）\ndef test_model_process(model, test_dataloader):    device = \"cuda\" if torch.cuda.is_available() else 'cpu'    model = model.to(device)    test_corrects = 0.0    test_num = 0    with torch.no_grad():        for test_data_x, test_data_y in test_dataloader:            test_data_x = test_data_x.to(device)            test_data_y = test_data_y.to(device)            model.eval()            output= model(test_data_x)            pre_lab = torch.argmax(output, dim=1)            test_corrects += torch.sum(pre_lab == test_data_y.data)            test_num += test_data_x.size(0)    test_acc = test_corrects.double().item() / test_num    print(\"测试的准确率为：\", test_acc)# AlexNet 实战\nclass AlexNet(nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.block = nn.Sequential(            nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11, stride=4),            nn.ReLU(),            nn.MaxPool2d(kernel_size=3, stride=2),            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2),            nn.ReLU(),            nn.MaxPool2d(kernel_size=3, stride=2),            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),            nn.ReLU(),            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),            nn.ReLU(),            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=3, stride=2),            nn.Flatten(),            nn.Linear(256 * 5 * 5, 4096),            nn.ReLU(),            nn.Dropout(0.5),            nn.Linear(4096, 4096),            nn.ReLU(),            nn.Dropout(0.5),            nn.Linear(4096, 10)        )    def forward(self, x):        x = self.block(x)        return x此处并不是标准是 AlexNet 模型，主要是为了适配 FashionMNIST。\ntrain_process = train_model_process(AlexNet(), train_dataloader, val_dataloader, num_epochs=5)matplot_acc_loss(train_process)由于主包的 GPU 实在是太慢了，就不放结果图了。\n# VGG 网络原理\nVGGNet 有 6 种不同的结构，主要以 VGG-16 为核心拆解。\n\nvgg-block 内的卷积层都是同结构的，池化层都得上一层的卷积层特征缩减一半，深度较深，参数量够大，较小的 filter size/kernel size\nVGG 大量使用了 3 x 3 的卷积核，参数很小而且效果还不错。\n还有 VGG 使用了块状结构，相当于一个小单元，非常方便。\n# 模型\nclass VGG16(nn.Module):    def __init__(self):        super(VGG16, self).__init__()        self.block1 = nn.Sequential(            nn.Conv2d(1, 64, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(64, 64, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2)        )        self.block2 = nn.Sequential(            nn.Conv2d(64, 128, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(128, 128, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2)        )        self.block3 = nn.Sequential(            nn.Conv2d(128, 256, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(256, 256, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(256, 256, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2)        )        self.block4 = nn.Sequential(            nn.Conv2d(256, 512, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(512, 512, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(512, 512, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2)        )        self.block5 = nn.Sequential(            nn.Conv2d(512, 512, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(512, 512, kernel_size=3, padding=1),            nn.ReLU(),            nn.Conv2d(512, 512, kernel_size=3, padding=1),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2)        )        self.block6 = nn.Sequential(            nn.Flatten(),            nn.Linear(512 * 7 * 7, 4096),            nn.ReLU(),            nn.Linear(4096, 4096),            nn.ReLU(),            nn.Linear(4096, 10)        )    def forward(self, x):        x = self.block1(x)        x = self.block2(x)        x = self.block3(x)        x = self.block4(x)        x = self.block5(x)        x = self.block6(x)        return x\n# 最佳实践 —— 权重初始化\n在我们训练的时候，我们的模型可能不收敛，训练出来的结果图很难看，实际上大概率可能是出现了梯度消失问题，核心原因是我们的权重初始化过于随机了。\n为什么权重初始化如此重要？\n在深度神经网络中，权重初始化直接影响：\n\n激活值的分布（前向传播）\n梯度的大小和稳定性（反向传播）\n模型是否收敛、收敛速度、最终性能\n\n如果权重初始化不当，比如：\n\n权重太小 → 激活值趋近于 0 → 梯度消失\n权重太大 → 激活值饱和 → 梯度爆炸\n\n所以我们必须要引入权重初始化！\n# 何凯明 - 凯明初始化法\n凯明初始化法（Kaiming Initialization），又称 He 初始化，由 何恺明（Kaiming He） 在 2015 年提出，专为 ReLU 及其变种（如 LeakyReLU） 设计的权重初始化方法。\nfor param in self.modules():    if isinstance(param, nn.Conv2d):        nn.init.kaiming_normal_(param.weight, nonlinearity='relu')        if param.bias is not None:            nn.init.constant_(param.bias, 0)for param in self.modules():    # 卷积层初始化    if isinstance(param, nn.Conv2d):        nn.init.kaiming_normal_(param.weight, nonlinearity='relu')        if param.bias is not None:            nn.init.constant_(param.bias, 0)    # 全连接层初始化    elif isinstance(param, nn.Linear):        nn.init.normal_(param.weight, 0, 0.01)        if param.bias is not None:            nn.init.constant_(param.bias, 0)以上可以放在：\nclass VGG16_init(nn.Module):    def __init__(self):        super(VGG16, self).__init__()初始化函数之下。\n# 最佳实践 —— 调整批次\n在深度学习中，调整批次大小（Batch Size） 是最有效、最低成本的性能调优手段之一。“最佳实践” 不是越大越好，也不是越小越精，而是根据硬件、任务、训练阶段动态权衡。\n# GoogLeNet 网络原理\n\n这里面最唬人的地方就是这个 Inception 块，实际上没有那么吓人。\n以前流行的网络使用小到 1×1，大到 7×7 的卷积核。 本文的一个观点：有时使用不同大小的卷积核组合是有利的。\n\n通道合并 ：将四个路线输出的通道合并。\n# 1 x 1 卷积的优点\n** 在不改变空间结构的前提下，高效地融合通道信息、调整通道维度、引入非线性，从而提升模型表达能力并降低计算成本。** 实现跨通道的交互和信息整合，卷积核通道数的降维和升维，减少网络参数。\n# 全局平均池化 GAP\n优点：“无参降维 + 抗过拟合”—— 把特征图全局平均成单个数值，直接当分类 logits，省掉全连接层，大幅减少参数量且强制保留通道级语义，降低过拟合风险。\n缺点：“丢细节 + 强假设”—— 空间信息被压成一点，对细粒度特征或目标定位任务无能为力，并隐含 “通道即类别” 的假设，若类别间特征重叠则易混淆。\n注意区别：全局平均池化 GAP 与直接 Flatten 平展的区别\n# 如何训练自己的数据集\n在深度学习中，设计一个良好的模型需要基础知识与运气，在此基础之上，数据的预处理往往是拉开差距的关键点。\n# 数据集的划分\n如何将这样的数据目录：\ndata_cat_dog├── cat└── dog变成：\ndata├── train│   ├── cat│   └── dog└── test    ├── cat    └── dog有这样的脚本：\nimport osfrom shutil import copyimport randomdef mkfile(file):    if not os.path.exists(file):        os.makedirs(file)# 获取 data 文件夹下所有文件夹名（即需要分类的类名）file_path = 'data_cat_dog'flower_class = [cla for cla in os.listdir(file_path)]# 创建 训练集 train 文件夹，并由类名在其目录下创建 5 个子目录mkfile('data/train')for cla in flower_class:    mkfile('data/train/' + cla)# 创建 验证集 val 文件夹，并由类名在其目录下创建子目录mkfile('data/test')for cla in flower_class:    mkfile('data/test/' + cla)# 划分比例，训练集：测试集 = 9 : 1split_rate = 0.1# 遍历所有类别的全部图像并按比例分成训练集和验证集for cla in flower_class:    cla_path = file_path + '/' + cla + '/'  # 某一类别的子目录    images = os.listdir(cla_path)  # iamges 列表存储了该目录下所有图像的名称    num = len(images)    eval_index = random.sample(images, k=int(num * split_rate))  # 从 images 列表中随机抽取 k 个图像名称    for index, image in enumerate(images):        # eval_index 中保存验证集 val 的图像名称        if image in eval_index:            image_path = cla_path + image            new_path = 'data/test/' + cla            copy(image_path, new_path)  # 将选中的图像复制到新路径        # 其余的图像保存在训练集 train 中        else:            image_path = cla_path + image            new_path = 'data/train/' + cla            copy(image_path, new_path)        print(\"\\r[&#123;&#125;] processing [&#123;&#125;/&#123;&#125;]\".format(cla, index + 1, num), end=\"\")  # processing bar    print()print(\"processing done!\")# 数据的预处理\n# 重调整\ntransforms.Resize(160)或\ntransforms.Resize((160,160))第一个是的比例裁剪，第二个是指定像素裁剪，他们都是重调整。\n# 随机裁剪\ntransforms.RandomResizedCrop(128, scale=(0.8, 1.0))先随机在原图里裁出一块面积占 80 %–100 % 的区域，再直接 resize 成 128×128，既做了随机裁剪又做了尺度增强。\n# 原始数据增强\ntransforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET),自动从 ImageNet 预训练好的 25 种增强策略里，随机挑一条子策略（含 5 种强度可变的图像变换）作用到输入图上，属于 “自动数据增强” 里的经典算法，无需手工设计组合。\n\n策略搜索阶段用强化学习在 ImageNet 上离线搜出 25 条子策略（每条含 5 个变换）。\n每次训练迭代时：\n\n随机选一条子策略；\n按该子策略里指定的概率、幅度依次对图像做 5 次变换；\n变换列表包括  ShearX/Y ,  TranslateX/Y ,  Rotate ,  Color ,  Posterize ,  Solarize ,  Contrast ,  Sharpness ,  Brightness ,  AutoContrast ,  Equalize  等。\n\n\n\n# 标准化\ntransforms.ToTensor()本身就具有归一化的功能，他将数值转化为 0-1 的区间，但这只是 “线性缩放”，不是真正意义上的 “标准化 (normalization)”。\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])这才是真正的标准化：把数值变成均值为 0、方差为 1 的分布，加速模型收敛。\n怎么算呢？还是来一个预处理。\nfrom PIL import Imageimport osimport numpy as np# 文件夹路径，包含所有图片文件folder_path = 'data_cat_dog'# 初始化累积变量total_pixels = 0sum_normalized_pixel_values = np.zeros(3)  # 如果是 RGB 图像，需要三个通道的均值和方差# 遍历文件夹中的图片文件for root, dirs, files in os.walk(folder_path):    for filename in files:        if filename.endswith(('.jpg', '.jpeg', '.png', '.bmp')):            image_path = os.path.join(root, filename)            image = Image.open(image_path)            image_array = np.array(image)            # 归一化像素值到 0-1 之间            normalized_image_array = image_array / 255.0            # 累积归一化后的像素值和像素数量            total_pixels += normalized_image_array.size            sum_normalized_pixel_values += np.sum(normalized_image_array, axis=(0, 1))# 计算均值和方差mean = sum_normalized_pixel_values / total_pixelssum_squared_diff = np.zeros(3)for root, dirs, files in os.walk(folder_path):    for filename in files:        if filename.endswith(('.jpg', '.jpeg', '.png', '.bmp')):            image_path = os.path.join(root, filename)            image = Image.open(image_path)            image_array = np.array(image)            # 归一化像素值到 0-1 之间            normalized_image_array = image_array / 255.0            try:                diff = (normalized_image_array - mean) ** 2                sum_squared_diff += np.sum(diff, axis=(0, 1))            except:                print(f\"捕获到自定义异常\")variance = sum_squared_diff / total_pixelsprint(\"Mean:\", mean)print(\"Variance:\", variance)# 随机擦除\ntransforms.RandomErasing(p=0.3, scale=(0.02, 0.2))以 30 % 的概率在图像上随机挖掉一块矩形区域（面积占图 2 %–20 %），用随机值（灰色、白色或黑色）填平，迫使模型学会 “靠局部也能猜对”，属于简单的正则化 / 抗遮挡增强。\n# ResNet 原理与实战\n\n# 网络持续加深带来那些问题\n一、优化难题：网络越深越 “学不动”\n二、表达难题：网络越深越 “记不住”\n三、工程难题：网络越深越 “养不起”\n“深” 本身不是错，错的是深 + Plain 堆叠；，\n\nPlain Network = 只有 “卷积–BN–ReLU” 一路串行下去，不带任何跳跃连接的直筒式架构。\n\n“链式求导” 本身就是根源 —— 但要把话拆成两句说：\n\n链式求导必然导致深度网络里的梯度是 “连乘” 形式；\n连乘的因子一旦持续小于 1（或大于 1），层数一多就指数级衰减 / 爆炸，这就是梯度消失 / 爆炸的数学本质。\n\n所以 Plain Net 的退化问题虽然表现形式是 “越深层训练误差越大”，但底层机制仍然绕不开链式求导带来的数值不稳定。\nResNet 的 skip connection 正是人为在链式乘积里插进一项 1，把 “连乘” 改写成 “连乘 + 1”，从而打断指数衰减 —— 用加法给链式法则打了一个补丁。\n# 残差块\nResNet（Residual Network）的核心创新就是残差连接（Residual Connection），它解决了深层网络的梯度消失问题，使得训练非常深的网络成为可能。\n残差块的设计是深度学习领域的重大突破，它不仅在图像识别任务中表现出色，还被广泛应用于各种深度学习架构中。a = h (x) + x\n\n上图有一个错误，填充应该是 0，步幅是 1。\n# Batch Normalization 归一化\nBatch Normalization（批归一化，简称 BN） 的目的是 让神经网络训练更快、更稳定、更容易收敛。\n\nBatchNorm 不是 “锦上添花”，而是 “深度网络能训得动” 的刚需 —— 它把每层的输入分布强行拉回 N (0,1)，切断梯度消失 / 爆炸与内部协变量偏移的恶性循环，让链式求导的连乘因子始终落在 1 附近，于是非常深的 Plain/ResNet 才吃得下大学习率、快速收敛且不用特别精调初始化。\nBN 的解决方案非常直观有力：既然层输入的分布老在变，那我们就强制把它拉回一个稳定、标准的分布。\nBN 的位置通常放在：\n全连接层/卷积层 → BatchNorm → 激活函数(ReLU等)# ResNet 的基本实现\n# 残差块\nclass ResidualBlock(nn.Module):    def __init__(self, in_channels, out_channels, use_conv1x1=False, stride=1) -> None:        super(ResidualBlock).__init__()        self.RelU = nn.ReLU(inplace=True)        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride,                               padding=1)        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=stride,                               padding=1)        self.BN1 = nn.BatchNorm2d(out_channels)        self.BN2 = nn.BatchNorm2d(out_channels)        if use_conv1x1:            self.conv3 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=stride,                                   stride=stride, padding=0)        else:            self.conv3 = None    def forward(self, x):        y = self.RelU(self.BN1(self.conv1(x)))        y = self.BN2(self.conv2(y))        if self.conv3 is not None:            x = self.conv3(x)        y = self.RelU(y + x)        return y# ResNet18\nclass ResNet18(nn.Module):    def __init__(self, ResidualBlock) -> None:        super(ResNet18, self).__init__()        self.b1 = nn.Sequential(            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3),            nn.BatchNorm2d(64),            nn.ReLU(inplace=True),            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)        )        self.b2 = nn.Sequential(            ResidualBlock(64, 64, use_conv1x1=False, stride=1),            ResidualBlock(64, 64, use_conv1x1=False, stride=1)        )        self.b3 = nn.Sequential(            ResidualBlock(64, 128, use_conv1x1=True, stride=2),            ResidualBlock(128, 128, use_conv1x1=False, stride=1)        )        self.b4 = nn.Sequential(            ResidualBlock(128, 256, use_conv1x1=True, stride=2),            ResidualBlock(256, 256, use_conv1x1=False, stride=1)        )        self.b5 = nn.Sequential(            ResidualBlock(256, 512, use_conv1x1=True, stride=2),            ResidualBlock(512, 512, use_conv1x1=False, stride=1)        )        self.b6 = nn.Sequential(            nn.AdaptiveAvgPool2d((1, 1)),            nn.Flatten(),            nn.Linear(512, 10)        )    def forward(self, x):        x = self.b1(x)        x = self.b2(x)        x = self.b3(x)        x = self.b4(x)        x = self.b5(x)        x = self.b6(x)        return x","categories":["深度学习"],"tags":["python","深度学习","CNN"]},{"title":"从公式角度看深度学习","url":"/python/DLFromFormula/","content":"# 从公式角度看深度学习\n学习基于尚硅谷：https://www.bilibili.com/video/BV1MRJmzSEaa\n# 损失函数\n# 均方误差 MSE / L2 Loss（回归问题较适用）\nMean Squared Error（MSE）\nL=1n∑i=1n(yi−ti)2\\begin {array}{c}\nL=\\frac{1}{n} \\sum_{i=1}^{n}(y_{i}-t_{i})^{2}  \n\\end{array}\nL=n1​∑i=1n​(yi​−ti​)2​\n这个 ti 是目标标签 (向量)，yi 是预测标签 (向量)，n 是总个数。\n往往我们在是实际使用的是时候，使用下面这个公式。\nL=12n∑i=1n(yi−ti)2\\begin {array}{c}\nL=\\frac{1}{2n} \\sum_{i=1}^{n}(y_{i}-t_{i})^{2}\n\\end{array}\nL=2n1​∑i=1n​(yi​−ti​)2​\nL2 Loss 对异常值敏感，遇到异常值时易发生梯度爆炸。\ndef mean_squared_error(y, t):    return 0.5 * np.sum((y-t)**2)# 交叉熵误差 CEE（分类问题较适用）\nCross Entropy Error（CEE）\nL=−1n∑i=1ntilog⁡(yi)\\begin {array}{c}\nL=-\\frac{1}{n} \\sum_{i=1}^{n} t_{i} \\log (y_{i})\n\\end{array}\nL=−n1​∑i=1n​ti​log(yi​)​\nyi 表示神经网络的输出，ti 表示正确解标签；而且，ti 中只有正确解标签对应的值为 1，其它均为 0（one-hot 表示）。\ndef cross_entropy_error(y, t):    if y.ndim == 1:        t = t.reshape(1, t.size)        y = y.reshape(1, y.size)            # 监督数据是 one-hot 向量的情况下，转换为正确解标签的索引    if t.size == y.size:        t = t.argmax(axis=1)                 batch_size = y.shape[0]    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size# 二元交叉熵误差 BCEE\nBinary Cross-Entropy Loss\nL=−1n∑i=1ntilog⁡(yi)+(1−ti)log⁡(1−yi)\\begin {array}{c}\nL=-\\frac{1}{n} \\sum_{i=1}^{n} t_{i} \\log (y_{i})+(1-t_{i}) \\log (1-y_{i})\n\\end{array}\nL=−n1​∑i=1n​ti​log(yi​)+(1−ti​)log(1−yi​)​\n# 多类交叉熵误差 CCEE\nCategorical Cross-Entropy Loss\nL=−1n∑i=1n∑j=1Ctijlog⁡(yij)\\begin {array}{c}\nL=-\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{C} t_{i j} \\log (y_{i j})\n\\end{array}\nL=−n1​∑i=1n​∑j=1C​tij​log(yij​)​\n# 平均绝对误差 MAE / L1 Loss\nMean Absolute Erro\nL=1n∑i=1n∣yi−ti∣\\begin {array}{c}\nL=\\frac{1}{n} \\sum_{i=1}^{n}|y_{i}-t_{i}|\n\\end{array}\nL=n1​∑i=1n​∣yi​−ti​∣​\nL1 Loss 对异常值鲁棒，但在 0 点处不可导。\n# 平滑 L1 误差 / Smooth L1\nSmooth L1\nSmooth L1={12(yi−y^i)2,if ∣yi−y^i∣&lt;1∣yi−y^i∣−12,if ∣yi−y^i∣≥1\\begin {array}{c}\n\\text{Smooth } L1 =\n\\begin{cases}\n\\frac{1}{2}(y_i - \\hat{y}_i)^2, &amp; \\text{if } |y_i - \\hat{y}_i| &lt; 1 \\\\\n|y_i - \\hat{y}_i| - \\frac{1}{2}, &amp; \\text{if } |y_i - \\hat{y}_i| \\geq 1\n\\end{cases}\n\\end{array}\nSmooth L1={21​(yi​−y^​i​)2,∣yi​−y^​i​∣−21​,​if ∣yi​−y^​i​∣&lt;1if ∣yi​−y^​i​∣≥1​​\n# 梯度\n梯度\n∇L=∂L∂w1∂w1∂x+∂L∂w2∂w2∂x+⋯+∂L∂wn∂wn∂x\\begin {array}{c}\n\\nabla L=\\frac{\\partial L}{\\partial w_{1}} \\frac{\\partial w_{1}}{\\partial x}+\\frac{\\partial L}{\\partial w_{2}} \\frac{\\partial w_{2}}{\\partial x}+\\cdots+\\frac{\\partial L}{\\partial w_{n}} \\frac{\\partial w_{n}}{\\partial x}\n\\end{array}\n∇L=∂w1​∂L​∂x∂w1​​+∂w2​∂L​∂x∂w2​​+⋯+∂wn​∂L​∂x∂wn​​​\n梯度是关于参数的导数，即对于参数 w1，w2，w3，w4，w5…wn，求关于 x 的导数，这些偏导数组成的向量就是梯度，如下面这个形式。\n∇L=[∂L∂w1,∂L∂w2,∂L∂w3,∂L∂w4,∂L∂w5,⋯ ,∂L∂wn]\\begin {array}{c}\n\\nabla L=\\left[\\frac{\\partial L}{\\partial w_{1}}, \\frac{\\partial L}{\\partial w_{2}}, \\frac{\\partial L}{\\partial w_{3}}, \\frac{\\partial L}{\\partial w_{4}}, \\frac{\\partial L}{\\partial w_{5}}, \\cdots, \\frac{\\partial L}{\\partial w_{n}}\\right]\n\\end{array}\n∇L=[∂w1​∂L​,∂w2​∂L​,∂w3​∂L​,∂w4​∂L​,∂w5​∂L​,⋯,∂wn​∂L​]​\n在极小值处、极大值处和鞍点处，梯度向量都为 0。\n梯度代表的其实是函数值增大最快的方向；在实际应用中，我们需要寻找损失函数的最小值，所以一般选择负梯度\n向量。同样地，负梯度代表的是函数值减小最快的方向，并不一定直接指向函数图像的最低点。\n# 优化函数\n# 随机梯度下降 SGD\nstochastic Gradient Descent\nwt+1=wt−η∇L(wt)\\begin {array}{c}\nw_{t+1}=w_{t}-\\eta \\nabla L(w_{t})\n\\end{array}\nwt+1​=wt​−η∇L(wt​)​\nη\\etaη 是学习率，一般取 0.01。\nSGD 有以下问题：\n\n局部最优解：陷入局部最优，尤其在非凸函数中，难以找到全局最优解。\n鞍点：陷入鞍点，梯度为 0，导致训练停滞。\n收敛速度慢：高维或非凸函数中，收敛速度较慢。\n学习率选择：学习率过大导致震荡或不收敛，过小则收敛速度慢。\n\n# 自适应梯度 AdaGrad\nAdaGrad\nwt+1=wt−η1∑i=1n(wt,i)2+ϵ∇L(wt)\\begin {array}{c}\nw_{t+1}=w_{t}-\\eta \\frac{1}{\\sqrt{\\sum_{i=1}^{n}\\left(w_{t, i}\\right)^{2}+\\epsilon}} \\nabla L(w_{t})\n\\end{array}\nwt+1​=wt​−η∑i=1n​(wt,i​)2+ϵ​1​∇L(wt​)​\nAdaGrad 的优点是：\n\n快速收敛：在非凸函数中，AdaGrad 能够快速收敛到全局最优解。\n梯度加速：在凸函数中，AdaGrad 能够加速梯度下降，使得训练速度更快。\n\n# 动量法 Momentum\nMomentum\nwt+1=wt+ηmt\\begin {array}{c}\nw_{t+1}=w_{t}+\\eta m_{t}\n\\end{array}\nwt+1​=wt​+ηmt​​\nmt=βmt−1+(1−β)∇L(wt)\\begin {array}{c}\nm_{t}=\\beta m_{t-1}+\\left(1-\\beta\\right) \\nabla L(w_{t})\n\\end{array}\nmt​=βmt−1​+(1−β)∇L(wt​)​\nβ\\betaβ 是动量的超参数，一般取 0.9。\n动量的作用是加速梯度下降，会保存历史梯度并给予一定的权重，使其也参与到参数更新中，使得梯度下降速度更快。\n其中，mt-1 是历史梯度，mt 是当前梯度。\n在当前梯度中，也包含了历史梯度，因此，mt-1 中的历史梯度也会被加入到当前梯度中，从而实现梯度加速。\n# Adam\nAdam\nmt=β1mt−1+(1−β1)∇L(wt)\\begin {array}{c}\nm_{t}=\\beta_{1} m_{t-1}+(1-\\beta_{1}) \\nabla L(w_{t})\n\\end{array}\nmt​=β1​mt−1​+(1−β1​)∇L(wt​)​\nvt=β2vt−1+(1−β2)∇L(wt)2\\begin {array}{c}\nv_{t}=\\beta_{2} v_{t-1}+(1-\\beta_{2}) \\nabla L(w_{t})^{2}\n\\end{array}\nvt​=β2​vt−1​+(1−β2​)∇L(wt​)2​\nwt+1=wt−ηv^t+ϵm^t\\begin {array}{c}\nw_{t+1}=w_{t}-\\frac{\\eta}{\\sqrt{\\hat{v}_{t}}+\\epsilon} \\hat{m}_{t}\n\\end{array}\nwt+1​=wt​−v^t​​+ϵη​m^t​​\nβ1\\beta_{1}β1​ 和β2\\beta_{2}β2​ 是 Adam 算法中的两个超参数，一般取 0.9 和 0.999。\nϵ\\epsilonϵ 是 Adam 算法中的超参数，一般取 1e-8。\nm^t\\hat{m}_{t}m^t​ 和v^t\\hat{v}_{t}v^t​ 是 Adam 算法中的两个中间变量，分别表示mtm_{t}mt​ 和vtv_{t}vt​ 的估计值。\nm^t\\hat{m}_{t}m^t​ 和v^t\\hat{v}_{t}v^t​ 的计算公式如下：\nm^t=mt1−β1t\\begin {array}{c}\n\\hat{m}_{t}=\\frac{m_{t}}{1-\\beta_{1}^{t}}\n\\end{array}\nm^t​=1−β1t​mt​​​\nv^t=vt1−β2t\\begin {array}{c}\n\\hat{v}_{t}=\\frac{v_{t}}{1-\\beta_{2}^{t}}\n\\end{array}\nv^t​=1−β2t​vt​​​\n# AdamW\nAdamW\nwt+1=wt−ηv^t+ϵm^t+λwt\\begin {array}{c}\nw_{t+1}=w_{t}-\\frac{\\eta}{\\sqrt{\\hat{v}_{t}}+\\epsilon} \\hat{m}_{t}+\\lambda w_{t}\n\\end{array}\nwt+1​=wt​−v^t​​+ϵη​m^t​+λwt​​\nλ\\lambdaλ 是 AdamW 算法中的超参数，一般取 0.01。\nm^t\\hat{m}_{t}m^t​ 和v^t\\hat{v}_{t}v^t​ 的计算公式如下：\nm^t=mt1−β1t\\begin {array}{c}\n\\hat{m}_{t}=\\frac{m_{t}}{1-\\beta_{1}^{t}}\n\\end{array}\nm^t​=1−β1t​mt​​​\nv^t=vt1−β2t\\begin {array}{c}\n\\hat{v}_{t}=\\frac{v_{t}}{1-\\beta_{2}^{t}}\n\\end{array}\nv^t​=1−β2t​vt​​​\n# 学习率衰减机制\n# 等间隔衰减\nηt=η0(1−tT)p\\begin {array}{c}\n\\eta_{t}=\\eta_{0}\\left(1-\\frac{t}{T}\\right)^{p}\n\\end{array}\nηt​=η0​(1−Tt​)p​\nη0\\eta_{0}η0​ 是初始学习率，TTT 是总迭代次数，ppp 是衰减因子，一般取 0.5。\n等间隔衰减的缺点是，学习率衰减过快，导致训练效果变差。\n# 指定间隔衰减\n在指定的 epoch，让学习率按照一定的系数衰减，直到指定 epoch 结束，再按照初始学习率继续训练。\n# 指数衰减\nηt=η0×(0.99)t/T\\begin {array}{c}\n\\eta_{t}=\\eta_{0} \\times \\left(0.99\\right)^{t / T}\n\\end{array}\nηt​=η0​×(0.99)t/T​\nη0\\eta_{0}η0​ 是初始学习率，TTT 是总迭代次数。\n指数衰减的缺点是，学习率衰减过慢，导致训练效果变差。\n# 余弦退火\nηt=η0×(0.5(1+cos⁡(πtT)))\\begin {array}{c}\n\\eta_{t}=\\eta_{0} \\times \\left(0.5\\left(1+\\cos\\left(\\frac{\\pi t}{T}\\right)\\right)\\right)\n\\end{array}\nηt​=η0​×(0.5(1+cos(Tπt​)))​\nη0\\eta_{0}η0​ 是初始学习率，TTT 是总迭代次数。\n余弦退火算法的优点是，学习率衰减速度适中，训练效果较好。\n# 初始化\n# He 初始化（Kaiming 初始化）\nHe 初始化的优点是，初始化后的权重分布更接近于标准正态分布，从而加速训练。\nimport torch.nn as nnlinear = nn.Linear(5, 2)# Kaiming 正态分布初始化nn.init.kaiming_normal_(linear.weight)print(linear.weight)# Kaiming 均匀分布初始化nn.init.kaiming_uniform_(linear.weight)print(linear.weight)# 正则化\n机器学习的问题中过拟合是一个很常见的问题。\n过拟合指的是能较好拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据。机器学习的目标是提高泛化能力，希望即便是不包含在训练数据里的未观测数据，模型也可以进行正确的预测。因此可以通过\n正则化方法来抑制过拟合。\n常用的正则化方法有 Batch Normalization、权值衰减、Dropout、早停法等。\n# Batch Normalization 批量标准化\nBatch Normalization\ny=x−μσ2+ϵ\\begin {array}{c}\ny=\\frac{x-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}\n\\end{array}\ny=σ2+ϵ​x−μ​​\n其中，μ\\muμ 是均值，σ2\\sigma^{2}σ2 是方差，ϵ\\epsilonϵ 是一个很小的数，一般取 0.001。\n","categories":["深度学习"],"tags":["python","深度学习","CNN","RNN","LSTM"]},{"title":"Harris兴趣点检测","url":"/python/Harris/","content":"# Harris 兴趣点检测\n# 什么是 “兴趣点”\n在 Harris 兴趣点检测中，“兴趣点” 通常指图像中那些在水平和垂直两个方向上都具有显著灰度变化的像素点，也就是说在局部窗口在任意方向做小幅平移都会引起较大像素差异的点。这类点往往对应于图像中的角点或局部纹理显著的区域，因此被认为是具有判别性和稳定性的特征点。\n\n一大片平坦区域，所有方向都变化小，不是兴趣点\n高楼顶部的转角处、十字路口的交叉点，在所有方向都变化大这是兴趣点\n\n\n角点会在两个方向同时产生剧烈变化，边界只会在一个方向上产生剧烈变化，平坦区则都不会。\n# 数学原理\n# 从上帝视角理解\n如何检测本质在于计算窗口滑动前后，其像素的变化情况。\n\n假设一个窗口包含四个像素，那么这里会采集到这四个值：\nx1,x2,x3,x4x_1,x_2,x_3,x_4\nx1​,x2​,x3​,x4​\n再次假设，白色部分数值为 0，蓝色部分数值为 1，那么有：\nx1=0,x2=1,x3=0,x4=1x_1=0,x_2=1,x_3=0,x_4=1\nx1​=0,x2​=1,x3​=0,x4​=1\n水平方向移动：\n\n此时有：\nx1=0,x2=0,x3=0,x4=0x_1=0,x_2=0,x_3=0,x_4=0\nx1​=0,x2​=0,x3​=0,x4​=0\n数值产生剧烈变化。\n垂直方向移动：\n\n$$\nx_1=0,x_2=1,x_3=0,x_4=1\n$$\n数值不变。\n噢～这是边界！\n如果把这个放在蓝色矩形的左上角呢？可以很轻松的想象到，无论是在水平方向还是垂直方向，数值都会产生剧烈变化。噢～这是角点！\n如果把这个放在蓝色矩形的中间呢？可以更很轻松的想象到，无论是在水平方向还是垂直方向，数值都不会产生剧烈变化。噢～这是平坦区！\n我们一看就知道，噢，数值产生剧烈变化了，再思考一下我们就知道这是角点还是边界了，那有没有一种数学方法让计算机也知道这是角点还是边界亦或是平坦区呢？\n有的兄弟有的，这就是 Harris 兴趣点检测法。\n# Harris 检测方法\n对于图像I(x,y)I(x,y)I(x,y)，我们可以判断在点(x,y)(x,y)(x,y) 处平移(Δx,Δy)(\\Delta x,\\Delta y)(Δx,Δy) 后的自相似性。此处I(x,y)I(x,y)I(x,y) 是一个灰度图像。\n有自相似性公式：\nc(x,y,Δx,Δy)=∑(u,v)∈W(x,y)W(u,v)(I(u,v)−I(u+Δx,v+Δy))2c(x,y,\\Delta x,\\Delta y)=\\sum_{(u,v)\\in W(x,y)}W(u,v)(I(u,v)-I(u+\\Delta x,v+\\Delta y))^2\nc(x,y,Δx,Δy)=(u,v)∈W(x,y)∑​W(u,v)(I(u,v)−I(u+Δx,v+Δy))2\n其中I(u,v)I(u,v)I(u,v) 是原来的灰度值，I(u+Δx,v+Δy)I(u+\\Delta x,v+\\Delta y)I(u+Δx,v+Δy) 是经过平移变换后的灰度值，做减法是为了看看平移操作后其与之前的差异是多大的，有时候我们的灰度变化是上升的，而有时候我们的灰度变化是下降的，我们只关心他变化了多少，因此这里做了平方操作，经过平方后不仅化负为正，还进一步强化了前后的差异。\n而W(u,v)W(u,v)W(u,v) 是一个权重设计，在整个窗口中，我们没有必要平等的看待每一个像素（ps：当然也可以平等的看待，此时W(u,v)W(u,v)W(u,v) 每一项均为同一常数），通常我们使用高斯加权函数，在图像上，最常用的是二维版本：\nw(x,y)=12πσ2exp⁡(−x2+y22σ2)w(x, y) = \\frac{1}{2\\pi \\sigma^2} \\exp\\left(-\\frac{x^2 + y^2}{2\\sigma^2}\\right)\nw(x,y)=2πσ21​exp(−2σ2x2+y2​)\n其中(x,y)(x, y)(x,y)：相对于窗口中心的坐标，σ\\sigmaσ 控制模糊程度或者说是影响范围，二维权重的三维表示如图所示：\n\n像图中表示的一样，他将会更加重视窗口中心的元素，一个3×33\\times 33×3 的高斯矩阵形如：\n[121242121]\\begin{bmatrix}\n 1 &amp; 2 &amp; 1\\\\\n 2 &amp; 4 &amp; 2\\\\\n 1 &amp; 2 &amp; 1\n\\end{bmatrix}\n⎣⎢⎡​121​242​121​⎦⎥⎤​\n对I(u+Δx,v+Δy)I(u+\\Delta x,v+\\Delta y)I(u+Δx,v+Δy) 进行泰勒展开：\nI(u+Δx,v+Δy)=I(u,v)+Ix(u,v)Δx+Iy(u,v)Δy+O(Δx2,Δy2)I(u+\\Delta x,v+\\Delta y)=I(u,v)+I_x(u,v)\\Delta x+I_y(u,v)\\Delta y+O(\\Delta x^2,\\Delta y^2)\nI(u+Δx,v+Δy)=I(u,v)+Ix​(u,v)Δx+Iy​(u,v)Δy+O(Δx2,Δy2)\n如此展开到一阶就够用了，丢弃高阶无穷小后，原式近似等于：\nI(u+Δx,v+Δy)≈I(u,v)+Ix(u,v)Δx+Iy(u,v)ΔyI(u+\\Delta x,v+\\Delta y)\\approx I(u,v)+I_x(u,v)\\Delta x+I_y(u,v)\\Delta y\nI(u+Δx,v+Δy)≈I(u,v)+Ix​(u,v)Δx+Iy​(u,v)Δy\n其中IxI_xIx​ 表示其对xxx 的偏导，其中IyI_yIy​ 表示其对yyy 的偏导。\n对于原式I(u,v)−I(u+Δx,v+Δy)I(u,v)-I(u+\\Delta x,v+\\Delta y)I(u,v)−I(u+Δx,v+Δy) 我们可以观察到，其包含了I(u,v)I(u,v)I(u,v)，因此可以直接被消掉，于是我们重写c(x,y,Δx,Δy)c(x,y,\\Delta x,\\Delta y)c(x,y,Δx,Δy)，近似得到：\nc(x,y,Δx,Δy)≈∑(u,v)∈W(x,y)W(u,v)(Ix(u,v)Δx+Iy(u,v)Δy)2c(x,y,\\Delta x,\\Delta y)\\approx \\sum_{(u,v)\\in W(x,y)}W(u,v)(I_x(u,v)\\Delta x+I_y(u,v)\\Delta y)^2\nc(x,y,Δx,Δy)≈(u,v)∈W(x,y)∑​W(u,v)(Ix​(u,v)Δx+Iy​(u,v)Δy)2\n简单起见，我们将∑(u,v)∈W(x,y)W(u,v)\\sum_{(u,v)\\in W(x,y)}W(u,v)∑(u,v)∈W(x,y)​W(u,v)，收缩至求和符号中记作：\\sum_\n进一步，我们可以把上式改写成矩阵的形式，设矩阵M(x,y)M(x,y)M(x,y) 为：\nM(x,y)=∑w[Ix(x,y)2Ix(x,y)Iy(x,y)Ix(x,y)Iy(x,y)Iy(x,y)2]=[∑wIx(x,y)2∑wIx(x,y)Iy(x,y)∑wIx(x,y)Iy(x,y)∑wIy(x,y)2]M(x,y)=\\sum_{w}\\begin{bmatrix}\n I_x(x,y)^2 &amp; I_x(x,y)I_y(x,y)\\\\\n I_x(x,y)I_y(x,y) &amp; I_y(x,y)^2\n\\end{bmatrix}=\\begin{bmatrix}\n\\sum_{w} I_x(x,y)^2 &amp; \\sum_{w} I_x(x,y)I_y(x,y)\\\\\n\\sum_{w} I_x(x,y)I_y(x,y) &amp;\\sum_{w} I_y(x,y)^2\n\\end{bmatrix}\nM(x,y)=w∑​[Ix​(x,y)2Ix​(x,y)Iy​(x,y)​Ix​(x,y)Iy​(x,y)Iy​(x,y)2​]=[∑w​Ix​(x,y)2∑w​Ix​(x,y)Iy​(x,y)​∑w​Ix​(x,y)Iy​(x,y)∑w​Iy​(x,y)2​]\n观察到矩阵的副对角线完全一样，为进一步简化M(x,y)M(x,y)M(x,y)，设A=Ix(x,y)2A=I_x(x,y)^2A=Ix​(x,y)2、B=Ix(x,y)2B=I_x(x,y)^2B=Ix​(x,y)2、C=Ix(x,y)Iy(x,y)C=I_x(x,y)I_y(x,y)C=Ix​(x,y)Iy​(x,y)，于是得到：\nM(x,y)=[ACCB]M(x,y)=\\begin{bmatrix}\nA &amp; C\\\\\nC &amp; B\n\\end{bmatrix}\nM(x,y)=[AC​CB​]\n显然，M(x,y)M(x,y)M(x,y) 是一个对称阵，因此得到：\nc(x,y,Δx,Δy)≈[Δx,Δy]M(x,y)[ΔxΔy]=[Δx,Δy][ACCB][ΔxΔy]c(x,y,\\Delta x,\\Delta y)\\approx \\begin{bmatrix}\n\\Delta x, \n\\Delta y \n\\end{bmatrix}M(x,y)\\begin{bmatrix}\n\\Delta x \\\\\n\\Delta y \n\\end{bmatrix}=\\begin{bmatrix}\n\\Delta x, \n\\Delta y \n\\end{bmatrix}\\begin{bmatrix}\nA &amp; C\\\\\nC &amp; B\n\\end{bmatrix}\\begin{bmatrix}\n\\Delta x \\\\\n\\Delta y \n\\end{bmatrix}\nc(x,y,Δx,Δy)≈[Δx,Δy​]M(x,y)[ΔxΔy​]=[Δx,Δy​][AC​CB​][ΔxΔy​]\n化简可得：\nc(x,y,Δx,Δy)≈AΔx2+2CΔxΔy+BΔy2c(x,y,\\Delta x,\\Delta y)\\approx A\\Delta x^2+2C\\Delta x\\Delta y+B\\Delta y^2\nc(x,y,Δx,Δy)≈AΔx2+2CΔxΔy+BΔy2\n其中A=∑wIx2A=\\sum_{w}I_x^2A=∑w​Ix2​，B=∑wIy2B=\\sum_{w}I_y^2B=∑w​Iy2​，C=∑wIxIyC=\\sum_{w}I_x I_yC=∑w​Ix​Iy​。\n这里的AΔx2+2CΔxΔy+BΔy2A\\Delta x^2+2C\\Delta x\\Delta y+B\\Delta y^2AΔx2+2CΔxΔy+BΔy2，非常像椭圆方程，回忆椭圆一般二次型：Ax2+Bxy+Cy2+Dx+Ey+F=0Ax^2 + Bxy + Cy^2 + Dx + Ey + F = 0Ax2+Bxy+Cy2+Dx+Ey+F=0，出现交叉项时，其变为非标准形，形如：\n\n我们需要把整个非标准椭圆做标准化变换。\n为什么AΔx2+2CΔxΔy+BΔy2A\\Delta x^2+2C\\Delta x\\Delta y+B\\Delta y^2AΔx2+2CΔxΔy+BΔy2 并不标准，因为这里存在交叉项2CΔxΔy2C\\Delta x\\Delta y2CΔxΔy，如果这一项能被消掉那就变成标准的了，最直接的方法就是让CCC 等于000，此处的CCC 是由M(x,y)M(x,y)M(x,y) 产生的，观察到MMM 矩阵是一个实对称矩阵，那么我们就可以对MMM 进行相似对角化，消掉CCC，变标准椭圆。\n\n下面假设已经完成了对矩阵MMM 的形似对角化：\nP−1MP∼[λ1λ2]P^{-1}MP\\sim  \\begin{bmatrix}\n\\lambda _1&amp; \\\\\n &amp; \\lambda _2\n\\end{bmatrix}\nP−1MP∼[λ1​​λ2​​]\n其中，λ1\\lambda _1λ1​，λ2\\lambda _2λ2​，是矩阵MMM 的特征值，因此可进一步写做：\nc(x,y,Δx,Δy)≈[Δx,Δy][ACCB][ΔxΔy]≈[Δx′,Δy′][λ1λ2][Δx′Δy′]=λ1(Δx′)2+λ2(Δy′)2c(x,y,\\Delta x,\\Delta y)\\approx\\begin{bmatrix}\n\\Delta x, \n\\Delta y \n\\end{bmatrix}\\begin{bmatrix}\nA &amp; C\\\\\nC &amp; B\n\\end{bmatrix}\\begin{bmatrix}\n\\Delta x \\\\\n\\Delta y \n\\end{bmatrix}\\approx\n\\begin{bmatrix}\n\\Delta x&#x27; , \n\\Delta y&#x27; \n\\end{bmatrix}\n\n\\begin{bmatrix}\n\\lambda _1&amp; \\\\\n &amp; \\lambda _2\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta x&#x27; \\\\\n\\Delta y&#x27;\n\\end{bmatrix}=\\lambda_1 (\\Delta x&#x27;)^2 + \\lambda_2 (\\Delta y&#x27;)^2\nc(x,y,Δx,Δy)≈[Δx,Δy​][AC​CB​][ΔxΔy​]≈[Δx′,Δy′​][λ1​​λ2​​][Δx′Δy′​]=λ1​(Δx′)2+λ2​(Δy′)2\n于是我们可以用λ1,λ2\\lambda_1,\\lambda_2λ1​,λ2​ 的大小关系区分平坦、边缘亦或是角点。相似对角化不是为了方便计算，而是为了方便理解 与分类，实际实现 Harris 的时候，基本不会显式去算特征值、特征向量，对角化是为了看清λ1,λ2\\lambda_1,\\lambda_2λ1​,λ2​ 的几何意义。\n像这样的λ1(Δx′)2+λ2(Δy′)2\\lambda_1 (\\Delta x&#x27;)^2 + \\lambda_2 (\\Delta y&#x27;)^2λ1​(Δx′)2+λ2​(Δy′)2，我们可以把它改变一下，让其与标准式对齐，得到：\nΔx′2(1λ1)2+Δy′2(1λ2)2\\frac{\\Delta x&#x27;^2}{(\\sqrt{\\frac{1}{\\lambda _1} } )^2} +\\frac{\\Delta y&#x27;^2}{(\\sqrt{\\frac{1}{\\lambda _2} } )^2}\n(λ1​1​​)2Δx′2​+(λ2​1​​)2Δy′2​\n太复杂了，简单点写做：\nΔx′2(λ1−12)2+Δy′2(λ2−12)2\\frac{\\Delta x&#x27;^2}{(\\lambda_1^{-\\frac{1}{2} })^2} +\\frac{\\Delta y&#x27;^2}{(\\lambda_2^{-\\frac{1}{2} })^2}\n(λ1−21​​)2Δx′2​+(λ2−21​​)2Δy′2​\n\n现在有三种情况：\n\n平坦区域：λ1≈0, λ2≈0\\lambda_1 \\approx 0,\\ \\lambda_2 \\approx 0λ1​≈0, λ2​≈0（两个都很小）\n边缘区域：λ1≫0, λ2≈0\\lambda_1 \\gg 0,\\ \\lambda_2 \\approx 0λ1​≫0, λ2​≈0 或者λ2≫0, λ1≈0\\lambda_2 \\gg 0,\\ \\lambda_1 \\approx 0λ2​≫0, λ1​≈0（一大一小）\n角点：λ1≫0, λ2≫0\\lambda_1 \\gg 0,\\ \\lambda_2 \\gg 0λ1​≫0, λ2​≫0（两个都很大）\n\n然后，使用det⁡(M)\\det(M)det(M) 和trace(M)\\text{trace}(M)trace(M) 构造角点响应RRR 值：\nR=det⁡(M)−k⋅trace2(M)R = \\det(M) - k \\cdot \\text{trace}^2(M)\nR=det(M)−k⋅trace2(M)\n其中det⁡(M)\\det(M)det(M) 是矩阵MMM 的行列式，trace(M)\\text{trace}(M)trace(M) 是矩阵MMM 的迹，det⁡(M)=λ1λ2\\det(M) = \\lambda_1\\lambda_2det(M)=λ1​λ2​，trace(M)=λ1+λ2\\text{trace}(M) = \\lambda_1 + \\lambda_2trace(M)=λ1​+λ2​，这就是 Harris 设计出来的一个角点评分公式。\n进一步重述上述三种情况：\n\n平坦区域：λ1≈0, λ2≈0\\lambda_1 \\approx 0,\\ \\lambda_2 \\approx 0λ1​≈0, λ2​≈0（两个都很小），det⁡=λ1λ2≈0\\det = \\lambda_1\\lambda_2 \\approx 0det=λ1​λ2​≈0，trace2≈0⇒(R≈0)\\text{trace}^2 \\approx 0 ⇒ (R \\approx 0)trace2≈0⇒(R≈0)\n边缘区域：λ1≫0, λ2≈0\\lambda_1 \\gg 0,\\ \\lambda_2 \\approx 0λ1​≫0, λ2​≈0 或者λ2≫0, λ1≈0\\lambda_2 \\gg 0,\\ \\lambda_1 \\approx 0λ2​≫0, λ1​≈0（一大一小），det⁡=λ1λ2\\det = \\lambda_1\\lambda_2det=λ1​λ2​ 不算大，trace2=(λ1+λ2)2≈λ12\\text{trace}^2 = (\\lambda_1+\\lambda_2)^2 \\approx \\lambda_1^2trace2=(λ1​+λ2​)2≈λ12​ 很大，$⇒ R $ 往往是负的（这里 Harris 故意用 $-k \\cdot \\text {trace}^2 $ 把边缘压下去）\n角点：λ1≫0, λ2≫0\\lambda_1 \\gg 0,\\ \\lambda_2 \\gg 0λ1​≫0, λ2​≫0（两个都很大），det⁡=λ1λ2\\det = \\lambda_1\\lambda_2det=λ1​λ2​ 很大，即便减去了k(λ1+λ2)2k(\\lambda_1+\\lambda_2)^2k(λ1​+λ2​)2，但整体还是会是个比较大的正数\n\n\n综上所述，RRR 有如下特性：\n\nR≈0R \\approx 0R≈0 ⇒ 平坦区\nR&lt;0R &lt; 0R&lt;0 ⇒ 更像边缘\nR≫0R \\gg 0R≫0 ⇒ 很像角点（兴趣点）\n\ndet⁡(M)\\det(M)det(M) 大，就说明两个方向上变化都大，trace(M)2\\text{trace}(M)^2trace(M)2 大，可能只是有一个方向特别大，比如边缘，R=det⁡−k⋅trace2R = \\det - k \\cdot \\text{trace}^2R=det−k⋅trace2，用 det⁡\\detdet 奖励两个方向都大的情况，用 −ktrace2-k\\text{trace}^2−ktrace2 惩罚只有一个方向大、另一个方向小的情况。\n# 在 OpenCV 中的使用\nimport cv2img = cv2.imread('1.jpg')gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)dst = cv2.cornerHarris(gray, 2, 3, 0.04)img[dst > 0.1 * dst.max()]=[0, 0, 255]cv2.imshow('dst', img)cv2.imwrite('1_corner.png', img)cv2.waitKey(0)cv2.destroyAllWindows()\nblockSize=2 是窗口大小\nksize =3 是 Sobel 求导算子核大小，Harris 需要先算图像在 x，y 方向的梯度 Ix,IyI_x, I_yIx​,Iy​，OpenCV 用的是 Sobel 算子。\nk = 0.04 是 Harris 公式里的经验参数 k，R=det⁡(M)−k⋅trace2(M)R = \\det(M) - k \\cdot \\text{trace}^2(M)R=det(M)−k⋅trace2(M)\n\n# 对比\n# 简单场景\n\n\n# 复杂场景\n\n# 参考\n[1]  Harris C G, Stephens M. A combined corner and edge detector[C] Alvey vision conference. 1988, 15(50): 10-5244.\n[2]  Harris 角点检测与 SIFT 特征匹配全解析 https://www.bilibili.com/video/BV1Zi1eYLEiR\n[3]  Harris Corner Detection https://docs.opencv.org/4.x/dc/d0d/tutorial_py_features_harris.html\n[4]  Harris corner detector https://en.wikipedia.org/wiki/Harris_corner_detector?utm_source=chatgpt.com\n","categories":["图像处理"],"tags":["python","Harris"]},{"title":"NLP自然语言处理","url":"/python/NLP/","content":"# NLP 自然语言处理\n\n学习笔记\n\n# 常见任务\n\n文本分类：情感分析（积极 / 消极）、垃圾邮件识别、新闻主题分类【句子级别】\n序列标注：命名实体识别（找人名、地名、手机号）、文本生成、信息抽取、文本转化【Token 级别】\n\n# 文本表示\n# 分词\n\n词级分词：将文本按照词切分在英语中空格往往是天然的切词标志，但是容易出现 OOV 问题（未登录词问题）\n字符级分词：一个字母、数字、标点甚至空格，都会被视作一个独立的 token，不会有 OOV 问题，但模型必须依赖更长的上下文来推断词义和结构，这显著增加了建模难度和训练成本。\n子词级分词：将词语切分为更小的单元 —— 子词（subword），例如词根、前缀、后缀或常见词片段。\n\n以上是英文适用的分词方法，下面是中文适用的分词方法。\n\n字符级分词：一个字就进行一次切分，比英文中的字符分词，中文的字符分词更加 “语义友好”。\n词级分词：由于中文没有空格等天然词边界，词级分词通常依赖词典、规则或模型来识别词语边界。\n子词级分词：它们以汉字为基本单位，通过学习语料中高频的字组合（如 “自然”、“语言”、“处理”），自动构建子词词表。在当前主流的中文大模型（如通义千问、DeepSeek）中，子词分词已成为广泛采用的文本切分策略。\n\n# JiebBa 分词组件\nword_gen = jieba.cut(\"我的名字叫Karry，来自计算机科学技术学院\")for word in word_gen:    print(word)\n我\n的\n名字\n叫\n Karry\n，\n来自\n计算机\n科学技术\n学院\n\n# 词表示\n\nOne-hot 编码（独热编码）：它将词汇表中的每个词映射为一个稀疏向量，向量的长度等于整个词表的大小。该词在对应的位置为 1，其他位置为 0。在实际自然语言处理任务中，one-hot 表示已经很少被直接使用。\n语义化词向量：它通过对大规模语料的学习，为每个词生成一个具有语义意义的稠密向量表示。比如 “女人” 和 “女孩”，这两个词向量就很接近。Word2Vec。\n\n# Word2Vec\n\n左侧是 CBOW，中间词是教师，以此来学习上下文。\n右侧是 Skip-gram，上下文是教师，以此来学习中间词。\n# GENSIM 词向量组件\n# 加载与使用公开词向量\nfrom gensim.models import KeyedVectorsmodel_path = 'sgns.weibo.word.bz2'model = KeyedVectors.load_word2vec_format(model_path)similarity = model.similarity('公交', '地铁')print('公交和地铁的相似度', similarity)\n公交和地铁的相似度 0.65458214\n\nmodel.most_similar(positive=['男人', '女孩'], negative=['男孩'], topn=10)男人 + 女孩 - 男孩 = 女人\n\n[(‘女人’, 0.6578881740570068),\n(‘女孩子’, 0.515068531036377),\n(‘女生’, 0.4519447982311249),\n(‘女人真’, 0.44206273555755615),\n(‘女人们’, 0.4369858503341675),\n(‘女人爱’, 0.435453325510025),\n(‘寡言少语’, 0.42479249835014343),\n(‘男孩子’, 0.42177465558052063),\n(‘看女人’, 0.41949161887168884),\n(‘笨女人’, 0.4182003140449524)]\n\n# 训练自己的词向量\nimport pandas as pdimport jiebafrom gensim.models import Word2Veccomments = pd.read_csv('./data/online_shopping_10_cats.csv', encoding='utf-8')reviews = comments['review']reviews = reviews.dropna()sentences = [[token for token in jieba.__lcut(review) if token.strip() != ''] for review in reviews]model = Word2Vec(    sentences,            # 已分词的句子序列    vector_size=100,      # 词向量维度    window=5,             # 上下文窗口大小    min_count=2,          # 最小词频（低于将被忽略）    sg=1,                 # 1:Skip-Gram，0:CBOW    workers=4             # 并行训练线程数)model.wv.save_word2vec_format('./data/word2vec.txt')# 词向量的应用\nimport torchfrom torch import nnfrom gensim.models import KeyedVectors# 加载词向量wv = KeyedVectors.load_word2vec_format('./data/word2vec.txt')# 构建词向量矩阵num_embedding = len(wv.key_to_index)embedding_dim = wv.vector_sizeembedding_matrix = torch.randn(num_embedding, embedding_dim)for word, index in wv.key_to_index.items():    embedding_matrix[index] = torch.from_numpy(wv[word])print(embedding_matrix.shape)# 创建 Embeddingembedding = nn.Embedding.from_pretrained(embedding_matrix)text = '我喜欢乘坐地铁'tokens= jieba.lcut(text)input_ids = [wv.key_to_index[token] for token in tokens if token in wv.key_to_index]input_tensor = torch.LongTensor(input_ids)embedding(input_tensor)# ELMo 模型 —— 一词多义问题\nNNLM 模型是在预测下一个词，而词向量是副产品。\nWord2Vec 模型是在专门做词向量，有 CBOW 和 Skip-gram。\nELMo 模型解决的是一词多义的问题。\n\nELMo 不仅仅是训练了一个 Q 矩阵，还把这个词的上下文信息融入到这个 Q 矩阵中，左边的 LSTM 获取 E2 的上文信息，右侧的 LSTM 获取下文信息。\nT1 包含了第一个词的特征，与此同时也包含了语法特征和语义特征。\n然而 LSTM 无法并行计算，因此我们引入了注意力机制。\n# RNN 基本结构\nRNN 以时间步为基本单位，逐个处理每一个 Token，新的隐藏状态是由，上一个隐藏状态与当前步共同决定的。\n\nx1，x2，x3…xt 这是一组特征，但是这是与时间有关的特征，hₜ是每时每刻的预测。\n比如一句话：“我出生在中国，我会说中文”\n我们可以按时间步展开：\n\n\n\n时间步\n输入 xₜ（词向量）\n隐藏状态 hₜ（编码了前面的上下文）\n可能的预测 yₜ（下一个词）\n\n\n\n\nt=1\n“我”\nh₁（编码了 “我”）\n“出生”\n\n\nt=2\n“出生”\nh₂（编码了 “我 出生”）\n“在”\n\n\nt=3\n“在”\nh₃（编码了 “我 出生 在”）\n“中国”\n\n\nt=4\n“中国”\nh₄（编码了 “我 出生 在 中国”）\n“，”\n\n\nt=5\n“，”\nh₅（编码了 “我 出生 在中国 ，”）\n“我”\n\n\nt=6\n“我”\nh₆（编码了前半句 +“我”）\n“会”\n\n\nt=7\n“会”\nh₇（编码了前半句 +“我 会”）\n“说”\n\n\nt=8\n“说”\nh₈（编码了前半句 +“我 会 说”）\n“中文”\n\n\n\nRNN 的隐藏状态 hₜ 起到了 **“记忆”** 的作用：\n\n当模型看到 “中国” 时，h₄ 已经编码了 “我出生在中国” 这个信息。\n当模型看到后半句 “我会说中文” 时，h₈ 已经编码了整句话的上下文，因此它可以推断出 “中文” 是合理的下一个词，因为 “出生在中国” 和 “会说中文” 之间有语义关联。\n\n在 RNN 中，每个词 xₜ 是随时间输入的特征，隐藏状态 hₜ 是模型对 “到目前为止所有词” 的理解，而预测 yₜ 是基于这种理解做出的下一步判断。\n# RNN 与 FCNN（全连接神经网络）的区别\n\n左侧是 RNN，右侧是全连接神经网络。全连接神经网络是一个直筒的结构，RNN 是一个与时间有关的循环结构。\n# FCNN 的展开\n\n# RNN 的展开\n\n# 为什么？\nFCNN 把每个输入当作独立静态向量\nRNN 把输入看成随时间展开的序列，用共享参数 + 循环隐状态来显式建模 “过去” 对 “现在” 的影响。\n# 数学表达\nht=tanh⁡(xtWx+ht−1Wh+b),\\begin {array}{c}\nh_t = \\tanh(x_t W_x + h_{t-1} W_h + b),\n\\end{array}\nht​=tanh(xt​Wx​+ht−1​Wh​+b),​\n[ht1ht2ht3ht4]=tanh⁡([ht−11ht−12ht−13ht−14][wh11wh12wh13wh14wh21wh22wh23wh24wh31wh32wh33wh34wh41wh42wh43wh44]+[xt1xt2xt3xt4][wx11wx12wx13wx14wx21wx22wx23wx24wx31wx32wx33wx34]+[b1b2b3b4])\\begin {array}{c}\n\\begin{bmatrix}\nh_t^1 &amp; h_t^2 &amp; h_t^3 &amp; h_t^4\n\\end{bmatrix}\n=\n\\tanh(\n\\begin{bmatrix}\nh_{t-1}^1 &amp; h_{t-1}^2 &amp; h_{t-1}^3 &amp; h_{t-1}^4\n\\end{bmatrix}\n\\begin{bmatrix}\nw_h^{11} &amp; w_h^{12} &amp; w_h^{13} &amp; w_h^{14} \\\\\nw_h^{21} &amp; w_h^{22} &amp; w_h^{23} &amp; w_h^{24} \\\\\nw_h^{31} &amp; w_h^{32} &amp; w_h^{33} &amp; w_h^{34} \\\\\nw_h^{41} &amp; w_h^{42} &amp; w_h^{43} &amp; w_h^{44}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\nx_t^1 &amp; x_t^2 &amp; x_t^3 &amp; x_t^4\n\\end{bmatrix}\n\\begin{bmatrix}\nw_x^{11} &amp; w_x^{12} &amp; w_x^{13} &amp; w_x^{14} \\\\\nw_x^{21} &amp; w_x^{22} &amp; w_x^{23} &amp; w_x^{24} \\\\\nw_x^{31} &amp; w_x^{32} &amp; w_x^{33} &amp; w_x^{34}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\nb_1 &amp; b_2 &amp; b_3 &amp; b_4\n\\end{bmatrix}\n)\n\\end{array}\n[ht1​​ht2​​ht3​​ht4​​]=tanh([ht−11​​ht−12​​ht−13​​ht−14​​]⎣⎢⎢⎢⎡​wh11​wh21​wh31​wh41​​wh12​wh22​wh32​wh42​​wh13​wh23​wh33​wh43​​wh14​wh24​wh34​wh44​​⎦⎥⎥⎥⎤​+[xt1​​xt2​​xt3​​xt4​​]⎣⎢⎡​wx11​wx21​wx31​​wx12​wx22​wx32​​wx13​wx23​wx33​​wx14​wx24​wx34​​⎦⎥⎤​+[b1​​b2​​b3​​b4​​])​\n# 权重共享\n权重共享” 是 RNN 与 CNN 里最常被提到的一个核心设计，但它不是 “所有神经元共用同一个数”，而是同一组参数（矩阵 / 向量）在多个位置、多个时间步或空间位置反复使用。\n# 双向 RNN\n双向递归神经网络（Bidirectional Recurrent Neural Network，简称 Bi-RNN）是一种特殊的递归神经网络（RNN），它在处理序列数据时，不仅考虑了过去的信息，还考虑了未来的信息。这使得 Bi-RNN 在处理诸如自然语言处理（NLP）任务时特别有用，因为它能够同时利用上下文信息。\n# 在 PyTorch 使用 RNN\nimport torch.nn as nn# 双层双向 RNNrnn = nn.RNN(input_size=3, hidden_size=4, num_layers=2, batch_first=True, bidirectional=True)# shape: (batch, seq_len, input_size)input = torch.randn(2, 4, 3)output, hn = rnn(input)print(output.shape)print(hn.shape)\ntorch.Size([2, 4, 8])\ntorch.Size([4, 2, 4])\n\n# 词嵌入层 API 应用\n# 作用\n把词或词对应的索引转为词向量\n# 使用框架实现\nimport torchimport jiebaimport torch.nn as nndef dm01():    text = \"当您登录平台即表示您接受本隐私政策的全部内容，并同意平台按本政策收集、使用和保护您的相关个人信息。\"    words = jieba.lcut(text)    print(words)    '''    len(words) : 词表大小    embedding_dim : 词向量维度    '''    embed = nn.Embedding(len(words), embedding_dim=4)    for i, word in enumerate(words):        # 词索引（张量）转变为词向量        word2Vec = embed(torch.tensor([i]))        print(word, word2Vec)if __name__ == '__main__':    dm01()# RNN 的 API 使用\nRNN 就像是你的大脑，在看电影的过程中记住剧情。\nimport torchimport torch.nn as nn'''input_size：词向量的维度hidden_size：隐藏状态的维度num_layers：隐藏层数batch_first：批次优先'''rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=1)'''5：有5个词3：3个批次10：每个词的细节是10维度'''x = torch.rand(5, 3, 10)'''1：隐藏层层数3：句子数量20：有20个循环维度，也就是隐藏状态的维度'''h0 = torch.zeros(1, 3, 20)'''RNN处理x：本次的输入h0：上一次的隐藏状态输出output：每个时间步的输出，包含了所有时间步骤的隐藏状态h1：最后一次隐藏状态，大脑里最新的剧情'''output, h1 = rnn(x, h0)print(f'output_shape:&#123;output.shape&#125;')print(f'h1_shape:&#123;h1.shape&#125;')# RNN 的文本生成\n# _*_ coding : utf-8 _*_# @Time : 2025/10/31 10:02# @Author : KarryLiu# File : 歌词生成# @Project : pytorchSTUimport mathimport torchimport jiebafrom torch.utils.data import DataLoader, Datasetimport torch.nn as nnimport torch.optim as optimimport timefrom tqdm import tqdm\"\"\"实现步骤：    1. 获取数据，进行分词    2. 获取词表，构建数据集    3. 搭建RNN神经网络模型    4. 训练模型    5. 模型预测    6. 测试\"\"\"def build_vocab():    unique_words, all_words = [], []    with open('data/jaychou_lyrics.txt', 'r', encoding='utf-8') as f:        for line in tqdm(f, desc='正在处理数据', total=5819):            words = jieba.lcut(line)            all_words.append(words)            for word in words:                if word not in unique_words:                    unique_words.append(word)    word2index = &#123;word: index for index, word in enumerate(unique_words)&#125;    index2word = &#123;index: word for index, word in enumerate(unique_words)&#125;    # 将词表转为索引，【0，1，2，3】，这个是一句歌词    corpus_idx = []    for all_word in all_words:        tmp = []        for word in all_word:            tmp.append(word2index[word])        tmp.append(word2index[' '])        corpus_idx.extend(tmp)    word_count = len(unique_words)    return unique_words, word2index, word_count, corpus_idxclass LyricsDataset(Dataset):    def __init__(self, corpus_idx, num_chars):        super().__init__()        self.corpus_idx = corpus_idx        self.num_chars = num_chars        self.word_count = len(corpus_idx)        self.sentence_len = self.word_count // num_chars    def __len__(self):        return self.sentence_len    def __getitem__(self, index):        start = min(max(index, 0), self.word_count - self.num_chars - 1)        end = start + self.num_chars        sentence_x = self.corpus_idx[start:end]        sentence_y = self.corpus_idx[start + 1:end + 1]        sentence_x = torch.tensor(sentence_x, dtype=torch.long)        sentence_y = torch.tensor(sentence_y, dtype=torch.long)        return sentence_x, sentence_yclass TextGenerator(nn.Module):    def __init__(self, unique_words_count, ):        super().__init__()        self.embedding = nn.Embedding(unique_words_count, 128)        self.rnn = nn.RNN(128, 256, 1)        self.linear = nn.Linear(256, unique_words_count)    def forward(self, input, hidden):        embed = self.embedding(input)        output, hidden = self.rnn(embed.transpose(0, 1), hidden)        output = self.linear(output.reshape(shape=(-1, output.shape[-1])))        return output, hidden    def init_hidden(self, batch_size):        return torch.zeros(1, batch_size, 256)def train():    unique_words, word2index, unique_word_count, corpus_idx = build_vocab()    dataset = LyricsDataset(corpus_idx, 32)    dataloader = DataLoader(dataset, batch_size=5, shuffle=True)    model = TextGenerator(unique_word_count)    optimizer = optim.Adam(model.parameters(), lr=0.01)    loss_fn = nn.CrossEntropyLoss()    min_loss = math.inf    loss = 0    total_loss = 0    iter_times = 0    for epoch in range(50):        for i, (x, y) in enumerate(tqdm(dataloader, desc=f'正在训练第&#123;epoch + 1&#125;轮', total=len(dataset) // 5)):            hidden = model.init_hidden(5)            output, hidden = model(x, hidden)            y = torch.transpose(y, 0, 1).reshape(shape=(-1,))            loss = loss_fn(output, y)            optimizer.zero_grad()            loss.backward()            optimizer.step()            iter_times += 1            total_loss += loss.item()        print(f'第&#123;epoch + 1&#125;轮，总损失：&#123;total_loss / iter_times&#125;')        if loss &lt; min_loss:            min_loss = loss            torch.save(model.state_dict(), 'model.pth')def eval(start_word, seq_len):    unique_words, word2index, unique_word_count, corpus_idx = build_vocab()    model = TextGenerator(unique_word_count)    model.load_state_dict(torch.load('model.pth'))    hidden = model.init_hidden(1)    word_index = word2index[start_word]    generate_sentence = [word_index]    for i in range(seq_len):        output, hidden = model(torch.tensor([[word_index]]), hidden)        output = output.reshape(shape=(-1,))        word_index = torch.argmax(output).item()        generate_sentence.append(word_index)    for e in [unique_words[index] for index in generate_sentence]:        print(e, end='')if __name__ == '__main__':    train()    eval('分手', 100)\n分手的话像语言暴力你而香 牧草有没有 我马儿有些瘦\n天涯尽头 满脸风霜落寞 近乡情怯的我\n我说店小二 三两银够不够\n景色入秋 漫天黄沙凉过\n塞北的客栈人多 牧草有没有 我马儿有些瘦\n世事看透 江湖上潮起潮落 什么恩怨过错\n在多年以后 还是让人难过 心伤透\n娘子她人在江南等我 泪不休 语沉默\n\n# 智能提示输入法的实现\n# 最佳实践\nprint(__file__)__ file __ 是当前文件的绝对路径，py 在处理相对路径时可能会遇到问题，最好使用绝对路径。\nfrom pathlib import Pathprint(Path(__file__).parent.parent / \"data/raw/synthesized_.jsonl\")所以我们通常会这样写：\nRAW_DATA_DIR = Path(__file__).parent.parent / \"data/raw\"def process():    print(\"开始处理数据\")    # 读取原始文件    df = pd.read_json(RAW_DATA_DIR / \"synthesized_.jsonl\", lines=True, orient='records',                      encoding='utf-8')    print(df.head())    print(\"数据处理完成\")# 数据预处理\nimport jiebafrom sklearn.model_selection import train_test_splitimport pandas as pdfrom tqdm import tqdmimport configdef build_dataset(sentences, word2index, desc=\"构建数据集\"):    indexed_sentences = [[word2index.get(token, 0) for token in jieba.lcut(sentences)] for sentences in                         sentences]    dataset = []    for index, sentence in tqdm(enumerate(indexed_sentences), desc=desc):        for i in range(len(sentence) - config.SWQ_LEN):            input = sentence[i:i + config.SWQ_LEN]            target = sentence[i + config.SWQ_LEN]            dataset.append(&#123;                'input': input,                'target': target,            &#125;)    return datasetdef process():    print(\"开始处理数据\")    # 读取原始文件    df = pd.read_json(config.RAW_DATA_DIR / \"synthesized_.jsonl\", lines=True, orient='records',                      encoding='utf-8').sample(frac=0.1)    # 提取句子    sentences = []    for sentencesX in df['dialog']:        for sentencesXs in sentencesX:            sentences.append(sentencesXs.split('：')[1])    print(f\"句子的数量为：&#123;len(sentences)&#125;\")    # 划分数据集    train_sentences, valid_sentences = train_test_split(sentences, test_size=0.2)    # 构建词表    vocab_set = set()    for sentence in tqdm(train_sentences, desc='构建词表'):        vocab_set.update(jieba.lcut(sentence))    vocab_list = ['&lt;unk>'] + list(vocab_set)    print(f\"词表的数量为：&#123;len(vocab_list)&#125;\")    # 保存此表    with open(config.MODELS_DIR / \"vocab.txt\", 'w', encoding='utf-8') as f:        f.write('\\n'.join(vocab_list))    # 构建训练集    word2index = &#123;word: index for index, word in enumerate(vocab_list)&#125;    train_dataset = build_dataset(train_sentences, word2index)    # 保存训练集    pd.DataFrame(train_dataset).to_json(config.PROCESSED_DATA_DIR / \"train.jsonl\", orient='records', lines=True)    # 构建验证集    valid_dataset = build_dataset(valid_sentences, word2index)    # 保存验证集    pd.DataFrame(valid_dataset).to_json(config.PROCESSED_DATA_DIR / \"valid.jsonl\", orient='records', lines=True)    print(\"数据处理完成\")if __name__ == '__main__':    process()# 数据集\nimport pandas as pdimport torchfrom torch.utils.data import DataLoader, Datasetfrom torch.utils.data.dataset import _T_cofrom src.NLP.SmartPrompt.src import config# 定义 Datasetclass InputMethodDataset(Dataset):    def __init__(self, path):        super().__init__()        self.data = pd.read_json(path, lines=True, orient='records').to_dict('records')    def __getitem__(self, index) -> _T_co:        input_tensor = torch.tensor(self.data[index]['input'], dtype=torch.long)        target_tensor = torch.tensor(self.data[index]['target'], dtype=torch.long)        return input_tensor, target_tensor    def __len__(self) -> int:        return len(self.data)# 提供一个获取 DATALoader 的方法def get_dataloader(train=True):    path = config.PROCESSED_DATA_DIR / (\"train.jsonl\" if train else \"valid.jsonl\")    dataset = InputMethodDataset(path)    return DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=True)if __name__ == '__main__':    train_loader = get_dataloader(train=True)    valid_loader = get_dataloader(train=False)    print(len(train_loader))    print(len(valid_loader))    for input_tensor, target_tensor in train_loader:        print(input_tensor.shape)        print(target_tensor.shape)        break# 训练\nimport timeimport torchfrom torch import nnfrom torch.utils.tensorboard import SummaryWriterfrom tqdm import tqdmfrom src.NLP.SmartPrompt.src import configfrom src.NLP.SmartPrompt.src.dataset import get_dataloaderfrom src.NLP.SmartPrompt.src.model import InputMethodModeldef train_onr_epoch(model, dataloader, loss_fn, optimizer, device):    \"\"\"    :param model: 模型    :param dataloader: 数据集    :param loss_fn: 损失函数    :param optimizer: 优化器    :param device: 设备    :return: 损失值    \"\"\"    model.train()    total_loss = 0    for batch in tqdm(dataloader, desc=\"训练中\"):        features, targets = batch        features = features.to(device)        targets = targets.to(device)        outputs = model(features)        loss = loss_fn(outputs, targets)        total_loss += loss.item()        optimizer.zero_grad()        loss.backward()        optimizer.step()    return total_loss / len(dataloader)def train():    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    dataloader = get_dataloader(train=True)    with open(config.MODELS_DIR / 'vocab.txt', 'r', encoding='utf-8') as f:        vocab_list = f.readlines()        vocab_list = [vocab.strip() for vocab in vocab_list]    model = InputMethodModel(len(vocab_list)).to(device)    loss_fn = nn.CrossEntropyLoss()    optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE)    writer = SummaryWriter(log_dir=config.LOGS_DIR/time.strftime('%Y-%m-%d_%H-%M-%S'))    bestLoss = float('inf')    for epoch in range(1, 1 + config.EPOCHS):        print(\"-\" * 5, f\"Epoch &#123;epoch&#125;/&#123;config.EPOCHS&#125;\", \"-\" * 5)        loss = train_onr_epoch(model, dataloader, loss_fn, optimizer, device)        print(f\"loss:&#123;loss&#125;\")        writer.add_scalar('loss', loss, epoch)        if loss &lt; bestLoss:            bestLoss = loss            torch.save(model.state_dict(), config.MODELS_DIR / 'model.pt')    writer.close()if __name__ == '__main__':    train()# 模型\nfrom torch import nnimport configclass InputMethodModel(nn.Module):    def __init__(self, vocab_size, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=config.EMBEDDING_DIM)        self.rnn = nn.RNN(            input_size=config.EMBEDDING_DIM,            hidden_size=config.HIDDEN_SIZE,            batch_first=True,        )        self.linear = nn.Linear(in_features=config.HIDDEN_SIZE, out_features=vocab_size)    def forward(self, x, *args, **kwargs):        embedding = self.embedding(x)        output, hidden = self.rnn(embedding)        last_hidden_state = output[:,-1,:]        output = self.linear(last_hidden_state)        return output# 预测\nimport jiebaimport torchfrom src.NLP.SmartPrompt.src import configfrom src.NLP.SmartPrompt.src.model import InputMethodModeldevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')with open(config.MODELS_DIR / 'vocab.txt', 'r', encoding='utf-8') as f:    vocab_list = f.readlines()    vocab_list = [vocab.strip() for vocab in vocab_list]word2index = &#123;word: index for index, word in enumerate(vocab_list)&#125;index2word = &#123;index: word for index, word in enumerate(vocab_list)&#125;model = InputMethodModel(len(vocab_list)).to(device)model.load_state_dict(torch.load(config.MODELS_DIR / 'model.pt'))def predict(text):    tokens = jieba.lcut(text)    input = [word2index.get(token, 0) for token in tokens]    input_tenosr = torch.tensor([input], dtype=torch.long).to(device)    model.eval()    with torch.no_grad():        output = model(input_tenosr)    topk = torch.topk(output, k=5).indices.tolist()    return [index2word[index] for index in topk[0]]if __name__ == '__main__':    user_str = ''    while user_str != 'q':        now_str = input(\"请输入：\")        user_str += now_str        if now_str.strip() == 'q':            break        top5_tokens = predict(user_str)        print(top5_tokens)        print(\"当前输入：\", user_str)# LSTM\nLSTM 灵感来原与计算机逻辑门。\n\n图片来源于：LSTM - 长短期记忆递归神经网络 - 知乎\n遗忘门：主要负责遗忘过去的记忆信息，最重要的是要去结合当下的状态信息去处理。\n输入门：输入门主要负责现在要记下什么，可以看到在输入门的右侧还有一个 tanh 的激活函数，\n输出门：\n\n# 双向结构\n\n# 注意力机制\n注意力机制的起源，是因为我们往往会聚焦于重要的信息。\n怎么做？\n我（查询对象 Q），这张图（被查询对象 V）\n我看一眼这张图，我就会去判断哪些东西对我而言是重要的，那些东西对我来说是是不重要的（去计算 Q 和 V 里的事物重要程度）。\n图片来自：https://www.cnblogs.com/nickchen121/p/16470710.html\n\n解码器在生成目标序列的每一步时，不再依赖于一个静态的上下文向量，而是根据当前的解码状态，动态地从编码器各个时间步的隐藏状态中选取最相关的信息，以辅助当前步生成。\n这种机制语法赋予模型对齐的能力，使其能够自动判断源句子中那些位置对当前目标词更为重要，从而有效缓解信息瓶颈问题，提升生成质量与表达能力。\n\n注意力机制实际上是在动态的提取当先最需要关心的数据。\n这一机制通常以下四个步骤来完成。\n\n相关性计算\n注意力权重计算\n上下文向量计算\n解码信息融合\n\n# 相关性计算\n相关性的计算依赖于特定的函数，通常被称为注意力评分函数。\n\n# 注意力权重计算\n得到所有源位置的注意力评分后，使用 Softmax 函数将其归一化为概率分布，作为注意力权重。得分越高的位置，其对应的权重越大，代表模型在当前生成中更关注该位置的信息。\n\n# 上下文向量计算\n将所有编码器输出按照注意力权重进行加权求和，得到一个上下文向量。这个向量就表示当前时间步，模型从源句中提取出的关键信息。\n\n# 解码信息融合\n在得到上下文向量后，解码器将其与当前时间步的隐藏状态进行拼接，以融合两者信息，最终通过线性变换和 Softmax，生成当前时间步目标词的概率分布。\n\n# 注意力评分函数\n# 点积评分\n它通过计算解码器当前时间步的隐藏状态与编码器每个时间步的隐藏状态的点积，来衡量二者之间的相关性。\n# 通用点积评分\n通用点积评分在点积的基础上引入了一个可学习的权重矩阵 W, 用于先对编码器隐藏状态进行线性变换，再与解码器隐藏状态进行点积。\n# 拼接评分\n拼接评分是一种表达能力更强的相关性评分方法。它的核心思想是：将解码器当前隐藏状态与编码器每个时间步的隐藏状态拼接为一个长向量，经过线性变换和非线性激活，最后用一个向量进行投影，得到最终打分值。\n\n# Transformer\n\nTransformer 其实就是 Attention 的一个堆叠。\n# Transformer 主要是在做什么\n把一个输入序列（如一段文字、一段语音、或一张图片的特征序列）—— 编码成表示语义的向量序列，然后（可选）再解码出另一个序列。Transformer 不是一个单纯的 “算法”，而是一个序列建模框架。 它最擅长的事是：理解上下文中的关系，并基于此生成输出。\n\n# Transformer 的 Encoder\nTransformer 的 Encoder 由 N 层堆叠的结构（通常是 6 层） 组成，每一层的结构几乎一样，每层包含两个主要子层：\n输入 → [多头自注意力机制] → [前馈神经网络] → 输出\n\n并且每个子层后都有：\n\n残差连接（Residual Connection）\n层归一化（Layer Normalization）\n\nRNN：逐个时间步处理序列，有记忆但慢。\nTransformer Encoder：一次性处理全序列，通过注意力机制 “并行感知” 所有词之间的关系，速度快、效果好。\n说白了还是在弄一个词向量，只不过这个词向量更加优秀！！\n每个 Encoder Layer 都包含两个子层（sublayer），分别是自注意力子层（Self-Attention Sublayer）和前馈神经网络子层（Feed-Forward Sublayer）。\n\n# 自注意力层\n自注意力机制的第一步，是将输入序列中的每个位置表示映射为三个不同的向量，分别是 查询（Query）、键（Key） 和 值（Value）。\n\nQuery：表示当前词的用于发起注意力匹配的向量；\nKey：表示序列中每个位置的内容标识，用于与 Query 进行匹配；\nValue：表示该位置携带的信息，用于加权汇总得到新的表示。\n自注意力的核心思想是：每个位置用自身的 Query 向量，与整个序列中所有位置的 Key 向量进行相关性计算，从而得到注意力权重，并据此对对应的 Value 向量加权汇总，形成新的表示。\n完成 Query、Key、Value 向量的生成后，模型会使用每个位置的 Query 向量与所有位置的 Key 向量进行相关性评分。\n在得到每个位置与所有位置之间的相关性评分后，模型会使用 softmax 函数进行归一化，确保每个位置对所有位置的关注程度之和为 1，从而形成一个有效的加权分布。对于整个序列，模型要做的是对之前得到的注意力评分矩阵的每一行进行 softmax 归一化。\n\n\n综上：\n\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n$$\n# 前馈神经网络层\n\n# Transformer 的 Decoder\n解码器接收编码器生成的词向量，然后通过这个词向量生成翻译的结果。\n# 注意力与多头注意力\nimport torchimport torch.nn as nnimport mathclass SelfAttention(nn.Module):    def __init__(self, dropout=0.1) -> None:        super().__init__()        self.dropout = nn.Dropout(dropout)  # 对 10% 的数据进行 dropout，防止模型过拟合        self.softmax = nn.Softmax(dim=-1)  # 对最后一维进行 softmax，为什么是最后一个维度？        # 因为是多头注意力机制，所以是多个头，每个头对输入进行 softmax    def forward(self, Q, K, V, mask=None):        # X: [batch_size, seq_len, d_model]        # batch_size: 批次大小，一次送几个句子        # seq_len: 序列长度，一个句子中的 Token 数量        # d_model: 模型维度，这是个 Embedding 向量的维度        # Q：query 向量     [batch_size, heads, seq_len_q, d_k]        # K：key 向量       [batch_size, heads, seq_len_k, d_k]        # V：values 向量    [batch_size, heads, seq_len) v, d_v]        # mask：掩码向量    [batch_size, seq_len, seq_len]   mask 的意义是那些需要忽略，不要看见未来的信息        d_k = Q.size(-1)        # Q：                [batch_size, heads, seq_len_q, d_k]       seq_len_q, d_k        # K.T:               [batch_size, heads, d_k, seq_len_k]      d_k, seq_len_k        # attention_scores： [batch_size, heads, seq_len_q, seq_len_k]        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)  # (Q·K.T)/sqrt(d_k)        # 如果提供了 mask，masked_fill 则将 mask 中的值为 0 的元素替换为负无穷大        if mask is not None:            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))        # 获取注意力权重，对最后一位进行 softmax        attn = self.softmax(attention_scores)        attn = self.dropout(attn)        # 获取注意力结果，对 V 进行矩阵乘法，[batch_size, heads, seq_len_q, d_v]        output = torch.matmul(attn, V)        return output, attnclass MultiHeadAttention(nn.Module):    def __init__(self, d_model, n_heads, dropout=0.1) -> None:        super().__init__()        # d_model: 模型维度，这是个 Embedding 向量的维度，一般是 512        # n_heads: 多头注意力机制的个数，一般是 8        assert d_model % n_heads == 0        self.d_k = d_model // n_heads        self.n_heads = n_heads        self.W_q = nn.Linear(d_model, d_model)        self.W_k = nn.Linear(d_model, d_model)        self.W_v = nn.Linear(d_model, d_model)        self.fc = nn.Linear(d_model, d_model)        self.attention = SelfAttention(dropout)        self.layer_norm = nn.LayerNorm(d_model)        self.dropout = nn.Dropout(dropout)    def forward(self, q, k, v, mask=None):        batch_size = q.size(0)        # Q: [batch_size, seq_len, d_model] -> [batch_size, n_heads, seq_len, d_k]        # 为了获取多头注意力，需要将输入进行分割，分割成多个头，每个头对输入进行计算，然后进行拼接        Q = self.W_q(q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)        K = self.W_k(k).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)        V = self.W_v(v).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)        output, attn = self.attention(Q, K, V, mask)        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)        output = self.fc(output)        output = self.layer_norm(q + self.dropout(output))        return output, attn# Torch 中使用 TransformerAPI\ntorch.nn.Transformer(d_model=512,                      nhead=8,                      num_encoder_layers=6,                      num_decoder_layers=6,                      dim_feedforward=2048,                      dropout=0.1,                      activation='relu',                      custom_encoder=None,                      custom_decoder=None,                      layer_norm_eps=1e-05,                      batch_first=False,                      norm_first=False,                      bias=True,                      device=None,                      dtype=None)# transformer\nfrom torch import nn# 初始化 Transformertransformer = nn.Transformer(  d_model=512,   nhead=8,   num_encoder_layers=6,   num_decoder_layers=6,   batch_first=True)# forward\noutput = transformer(    src=src_emb,    tgt=tgt_emb,    src_key_padding_mask=src_pad_mask,    tgt_key_padding_mask=tgt_pad_mask,    tgt_mask=tgt_mask,    memory_key_padding_mask=src_pad_mask)# encoder\nfrom torch import nn# 初始化 Transformertransformer = nn.Transformer(    d_model=512, nhead=8,    num_encoder_layers=6, num_decoder_layers=6,    batch_first=True)# 调用编码器memory = transformer.encoder(    src=src_emb,     src_key_padding_mask=src_pad_mask)# decoder\nfrom torch import nn# 初始化 Transformertransformer = nn.Transformer(    d_model=512, nhead=8,    num_encoder_layers=6, num_decoder_layers=6,    batch_first=True)# 调用编码器memory = transformer.encoder(    src=src_emb,     src_key_padding_mask=src_pad_mask)# 调用解码器（逐步生成）output = transformer.decoder(    tgt=tgt_emb,    memory=memory,    tgt_mask=tgt_mask,    tgt_key_padding_mask=tgt_pad_mask,    memory_key_padding_mask=src_pad_mask)# 参考文献\n\n【[手把手教学] 基于 RNN、LSTM 神经网络单特征用电负荷预测】https://www.bilibili.com/video/BV1HN4y1Y7Lt\n【尚硅谷 NLP 教程，nlp 自然语言处理，Transformer、LSTM、BERT 等大模型技术全覆盖】https://www.bilibili.com/video/BV1k44LzPEhU\n【Word2Vec 模型、Attention、Transformer 】https://space.bilibili.com/383551518\n【Transformer 模型详解（图解最完整版）】https://zhuanlan.zhihu.com/p/338817680\n【黑马程序员 AI 大模型《神经网络与深度学习》】https://www.bilibili.com/video/BV1c5yrBcEEX\n\n","categories":["深度学习"],"tags":["python","深度学习","RNN","LSTM","NLP","GRU","Transformer"]},{"title":"Transformer实战","url":"/python/Transformer/","content":"# Transformer 实战\n# 背景\n为了解决在序列建模中提升并行与长距离依赖建模能力，摆脱对循环与卷积的依赖。其核心是编码器与解码器结构。\n其中有三种注意力机制：编码器多头注意力、交叉注意力、解码器多头注意力（含因果编码）\n\n# 输入数据\nTransformer 里计算的全是数值，在输出之后和输出之前，都要把词转换为词向量，或者将词词向量转化为词。\n# 词嵌入（Word Embedding）\n\n这个词嵌入矩阵的 d 相当于压缩后词向量的大小（维度）。\n\n# 位置编码（Positional Encoding）\n\n# 自注意力机制\n自注意力机制让每个输入位置都能动态地根据与其他位置的相关性重新表示自己，从而实现了信息的全局建模与动态加权融合。\n\n# 多头注意力机制\n\n# 掩码注意力（推理）\n\n# 交叉注意力\n\n# 参考文献\n\n【Transformer 算法原理与实战】https://www.bilibili.com/video/BV1ej1EBWEWu\n\n","categories":["深度学习"],"tags":["python","Transformer"]},{"title":"YOLO系列","url":"/python/YOLO_Series/","content":"# YOLO 系列\nYOLO（You Only Look Once）\n# YOLOv1\n\n输入是 448x448x3，最终输出是 7x7x30。对应原始图片对应区域的结果。就是所谓的将原始图片划分为 7x7 个小方格大小的图片。\n其实这是一种一一映射 7x7 这样的小方格，7×7 个 cell，每个 cell 负责 “这个 cell 中心落在这里的目标” 的检测任务。\n\n# x, y, w, h\nx,y 是中心点坐标，w,h 是预测框的宽和高。\n每个 bounding box 的 5 个参数：x,y,w,h,confidence,\nx,y：框中心相对于当前 cell 的坐标（一般在 0～1）\nw,h：框的宽高（通常相对于整张图像归一化）\nconfidence：这个框中有物体的置信度 × 该框与真实框的 IoU\n每个 cell 的输出维度 = B×5+C\nB×5+C=2×5+20=10+20=30\n# 置信度\n公式就是：\nIoU=Intersection AreaUnion Area\\text{IoU} = \\frac{\\text{Intersection Area}}{\\text{Union Area}}\nIoU=Union AreaIntersection Area​\n\nIntersection Area：两个框相交的那一块区域的面积（交集）\nUnion Area：两个框合起来覆盖住的总面积（并集）\n\n\n这里所说的物体真实的 box 实际是不存在的，这只是模型表达自己框出了物体的自信程度。因此此时置信度的公式为:\nC=Pr(Object)∗IoUpredtruthC=Pr(Object)*IoU_{pred}^{truth}\nC=Pr(Object)∗IoUpredtruth​\n# YOLOV1 损失函数\n损失函数为：\n\n# YOLOV1 总结\nYOLO 非常快，因为将物体检测定义为回归问题，所以检测也不需要复杂组件。\nYOLO 基于全图进行检测，所以不像晃动窗口和预选区技术，YOLO 中隐含着隐式编码的上下文信息。\nYOLOv1 的准确率不够高，YOLO 在定位小物体上表现偏差，可以检测到的目标物体较少。\n以上缺点将会在下面持续改进。\n# 参考\n\n【YOLOv1、YOLOv2、YOLOv3 目标检测算法原理与实战】https://www.bilibili.com/video/BV1WT421r72w\n\n","categories":["深度学习"],"tags":["python","深度学习","CNN","YOLO"]},{"title":"NumPy核心处理方法","url":"/python/numpy/","content":"# NumPy 核心处理方法\nNumPy（Numerical Python）是一个用于科学计算的 Python 第三方库，它提供了：\n# 核心功能：\n\n多维数组对象\n\n高效地存储和操作大规模数字数据。\n比原生 Python 的列表（list）更快、更节省内存。\n\n\n广播功能\n\n允许不同形状的数组进行数学运算，简洁高效。\n\n\n数学函数库\n\n包括线性代数、傅里叶变换、随机数生成、统计分析等。\n\n\n数组索引与切片\n\n支持布尔索引、花式索引，比 Python 原生更灵活强大。\n\n\n\n# 引用\nimport numpy as np# 多维性\narr = np.array(5)  # 创建了一个 0 维度的数组print(arr)print('arr的维度：', arr.ndim)  # 打印维度 ndimarr = np.array([1, 2, 3])  # 创建了一个 1 维度的数组print(arr)print('arr的维度：', arr.ndim)  # 打印维度 ndim# 同质性\narr = np.array([1, 'hello', 3])  #测试不同质print(arr)结果全部转化为字符串\n# ndarray 的属性\narr = np.array(1)print(arr)print('arr的维度：', arr.ndim)print('arr的形状：', arr.shape)print('arr的元素个数', arr.size)print('arr的数据类型', arr.dtype)\n1\narr 的维度： 0\narr 的形状： ()\n arr 的元素个数 1\narr 的数据类型 int64\n\narr = np.array([1, 2.7, 3])print(arr)print('arr的维度：', arr.ndim)print('arr的形状：', arr.shape)print('arr的元素个数', arr.size)print('arr的数据类型', arr.dtype)print('arr的转置', arr.T)\n[1.  2.7 3. ]\n arr 的维度： 1\narr 的形状： (3,)\n arr 的元素个数 3\narr 的数据类型 float64\narr 的转置 [1.  2.7 3.]\n\narr = np.array([[1, 2, 3], [4, 5, 6]])print(arr)print('arr的维度：', arr.ndim)print('arr的形状：', arr.shape)print('arr的元素个数', arr.size)print('arr的数据类型', arr.dtype)print('arr的转置', arr.T)\n[[1 2 3]\n[4 5 6]]\n arr 的维度： 2\narr 的形状： (2, 3)\n arr 的元素个数 6\narr 的数据类型 int64\narr 的转置 [[1 4]\n[2 5]\n[3 6]]\n\n# 指定元素数据类型\nlist1 = [4, 5, 6]arr = np.array(list1, dtype=np.float64)# 预定义数组\n# 全零阵\narr = np.zeros((4, 4), dtype=np.int64)arr = np.zeros((60,), dtype=np.int64)# 全一阵\narr = np.ones((4, 4), dtype=np.int64)# 空阵\narr = np.empty((3, 4))# 满阵\narr = np.full((3, 4), 2)\n[[2 2 2 2]\n[2 2 2 2]\n[2 2 2 2]]\n\n# 形状模仿\narr1 = np.zeros_like(arr)arr2 = np.ones_like(arr)arr3 = np.empty_like(arr)# 定增量（等差数列）\narr = np.arange(1, 106, 1)# 定间隔（等分数列）\napp = np.linspace(1, 100, 10)# 对角阵\narr = np.diag([1, 2, 3, 4, 5])# 随机数组\n# 默认零到一\narr = np.random.rand(2, 4)# 均匀分布， 0~1# 指定区间\narr = np.random.uniform(1, 4, (4, 4))# 随机整数\narr = np.random.randint(1, 10, (4, 4))# 随机正态分布\narr = np.random.randn(4, 4)# 随机种子\n# 设置随机种子，每次生成的随机数相同np.random.seed(20)arr = np.random.randint(1, 4, (4, 4))# 单位矩阵\narr = np.eye(5, dtype=int)# 数据类型\n# ndarray 的数据类型\n\n布尔类型\n整形\n浮点型\n复数型\n\n主要通过 dtype 指定\narr = np.array([1, 2, 3, 0, -1], dtype=np.int8)arr = np.array([1, 2, 3, 0, -1], dtype=np.float32)# 索引与切片\n# 索引\n# 一维数组arr = np.random.randint(10, 100, 30)# 一维索引print(arr[3])print(arr[7])# 一维布尔索引print(arr[(arr > 30) &amp; (arr &lt; 50)])# 二维数组arr = np.random.randint(10, 100, (5, 8))# 二维索引print(arr[2, 3])print(arr[2][3])# 切片\n# 一维切片print(arr[3:7])print(arr[slice(3, 7)])print(arr[slice(0, 10, 2)])# 二维切片print(arr[0, 1:3])print(arr[arr > 70])print(arr[0][arr[0] > 90])print(arr[3])print(arr[:, 3])# 运算\n# 常规运算\narr1 = np.array([1, 2, 3])arr2 = np.array([4, 5, 6])print(arr1 + arr2)print(arr1 * arr2)print(arr1 - arr2)print(arr1 / arr2)print(arr1 ** arr2)# python 原生listA = [1, 2, 3]listB = [4, 5, 6]for i in range(len(listA)):    print(listA[i] + listB[i])python 原生只是纯粹的拼接，ndarray 是在帮你做计算\narr1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])arr2 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])print(arr1 + arr2)print(arr1 * arr2)print(arr1 - arr2)print(arr1 / arr2)print(arr1 ** arr2)# 广播机制\n# 广播机制，但是需满足维度一致arr1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])arr2 = np.array([1, 2, 3])  # [1, 2, 3],[1, 2, 3],[1, 2, 3]print(arr1 + arr2)print(arr1 * arr2)arr1 = np.array([[1, 2, 3]])arr2 = np.array([[1], [2], [3]])print(arr1 + arr2)# 矩阵的运算\n# 矩阵运算arr1 = np.array([[1, 2, 3], [4, 5, 6]])arr2 = np.array([[4, 5], [1, 2], [7, 8]])print(arr1.dot(arr2))  # 矩阵的乘法，矩阵的行数必须等于矩阵的列数，不是简单的对应相乘print(arr1 @ arr2)# 常用函数\n# 数学函数\n# 平方根\nprint(np.sqrt(9))print(np.sqrt([1, 4, 9]))# 指数\n# 计算指数print(np.exp(1))  #  e^1print(np.exp([0, 2, 3]))# 自然对数\n# 计算自然对数print(np.log(np.e))print(np.log([1, np.e, np.e ** 2]))print(np.log10(100))# 三角函数\n# 计算三角函数print(np.sin(np.pi / 2))  # sin(π/2)print(np.cos(np.pi))  # cos(π)# 绝对值\n# 计算绝对值print(np.abs(-1))print(np.abs(np.array([1, -1, 0, -2, 2, -3, 3, -4, 4, -5, 5])))# a 的 b 次幂\n# 计算 a 的 b 次幂print(np.power(2, 3))# 四舍五入\n# 四舍五入print(np.round(3.14))print(np.round([3.14, 2.7, 1.5, 1.2]))# 上取整与下取整\n# 向上取整，向下取整arr = np.array([1.2, 1.5, 1.6, 1.7, 2.2, 2.5, 2.6, 2.7])print(np.ceil(arr))print(np.floor(arr))# 检测缺失值\narr = np.array([1, 2, np.nan, 4, np.nan])print(np.isnan(arr))# 统计函数\n求和、计算平均值、计算中位数、标准差、方差、最大值、最小值、计算分位数、累积和、累积差\narr = np.random.randint(1, 10, 10)print(arr)  # 创建一个 10 个元素的数组print('排序', np.sort(arr))print('求和', np.sum(arr))  # 求和print('计算平均值', np.mean(arr))  # 计算平均值print('计算中位数', np.median(arr))  # 计算中位数print('标准差', np.std(arr))  # 标准差 print('方差', np.var(arr))  # 方差 ((1-2)^2 + (2-2)^2 + ... + (9-2)^2) / 9print('最大值', np.max(arr))  # 最大值print('最小值', np.min(arr))  # 最小值print('计算分位数', np.quantile(arr, 0.5))  # 计算分位数print('累积和', np.cumsum(arr))  # 累积和print('累积差', np.cumprod(arr))  # 累积差# 比较函数\n# 是否大于print(np.greater([1, 3, 5, 1, 8, 0, -23, 3], 2))# 是否小于print(np.less([1, 3, 5, 1, 8, 0, -23, 3], 2))# 是否等于print(np.equal([1, 3, 5, 1, 8, 0, -23, 3], 3))# 逻辑运算\n# 逻辑运算print(np.logical_and([1, 3, 5, 1, 8, 0, -23, 3], [1, 3, 5, 1, 8, 0, -23, 3]))print(np.logical_or([1, 3, 5, 1, 8, 0, -23, 3], [1, 3, 5, 1, 8, 0, -23, 3]))# 自定义条件\narr = np.array([1, 3, 5, 1, 8, 0, -23, 3])print(np.where(arr > 3, arr, 0))print(np.select([arr > 3, arr &lt; 0], [arr, 0], default=arr))# 排序函数\narr = np.random.randint(1, 10, 20)print(arr)print(np.sort(arr))print(np.argsort(arr))  # 返回排序后的索引# 去重\narr = np.array([1, 3, 5, 1, 8, 0, -23, 3])print(np.unique(arr))# 数组的拼接\narr1 = np.array([1, 2, 3])arr2 = np.array([4, 5, 6])print(np.concatenate((arr1, arr2)))# 数组分割\narr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])print(np.split(arr, [3, 5]))  # 分成 3 份# 调整形状\narr = np.random.randint(1, 10, 20)print(arr)print(arr.reshape(4, 5))# 案例练习\n# 温度数据分析\n某城市一周的最高气温（℃）为 [28, 30, 29, 31, 32, 30, 29]。\n\n计算平均气温、最高气温和最低气温。\n找出气温超过 30℃ 的天数。\n\narr = np.array([28, 30, 29, 31, 32, 30, 29])print('平均天气：', np.mean(arr))print('最高气温：', np.max(arr))print('最低气温：', np.min(arr))print('气温超过30℃的天数：', np.sum(arr > 30))# 学生成绩统计\n某班级 5 名学生的数学成绩为 [85, 90, 78, 92, 88]。\n\n计算成绩的平均分、中位数和标准差。\n将成绩转换为百分制（假设满分为 100）。\n\narr = np.array([85, 90, 78, 92, 88])print('平均成绩：', np.mean(arr))print('中位数：', np.median(arr))print('标准差：', np.std(arr))print('成绩转换为百分制：', arr / 100 * 100)# 数组变形\n题目：创建一个 1 到 12 的一维数组，并转换为 (3, 4) 的二维数组。\n\n计算每行的和与每列的平均值。\n将数组展平为一维数组。\n\narr = np.linspace(1, 12, 12, dtype=int)arr = arr.reshape(3, 4)print(arr)print(arr.sum(axis=1))print(arr.mean(axis=0))print(arr.flatten())","categories":["深度学习"],"tags":["python","python_NumPy"]},{"title":"Pandas核心处理方法","url":"/python/pandas/","content":"# Pandas 核心处理方法\n# 什么是 Pandas\nPandas 是一个基于 Python 的开源数据分析和数据处理库\n它提供了两种核心数据结构：\n\n\n\n数据结构\n描述\n类似于\n索引\n数据存储\n类比\n\n\n\n\nSeries\n一维带标签的数组\n一列\n单索引\n同质化\nExcel 单列\n\n\nDataFrame\n二维带标签的表格数据结构\n一张表格（像 Excel 表）\n双索引\n各个列之间可以是不同的\n整个 Excel 表格\n\n\n\n应用场景：\n\n数据预处理（机器学习前）\n数据统计分析\n自动化报表\n数据可视化前处理\n\n# Series\nSeries 就像这样：\n\n\n\nSeries Index\nSeries Name\n\n\n\n\n1\nSeries Values\n\n\n2\nSeries Values\n\n\n3\nSeries Values\n\n\n\n# 创建方法\n# series 的创建import pandas as pds = pd.Series([1,2,3,4,5])print(s)# 自定义索引s = pd.Series([1, 2, 3, 4, 5], index=['a', 'b', 'c', 'd', 'e'])print(s)# 定义 Names = pd.Series([1, 2, 3, 4, 5], index=['a', 'b', 'c', 'd', 'e'], name='我是xxx')print(s)左侧是索引，右侧是值\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\na    1\nb    2\nc    3\nd    4\ne    5\ndtype: int64\na    1\nb    2\nc    3\nd    4\ne    5\nName: 我是 xxx, dtype: int64\n\n# 通过字典方式来创建s = pd.Series(&#123;'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5&#125;)print(s)# 通过 Series 对象来创建（截取）s1=pd.Series(s,index=['a','c'])print(s1)# 基本属性\n\n\n\n属性\n说明\n示例\n\n\n\n\nindex\n行索引（标签）对象\ns.index\n\n\nvalues\n数据值组成的  numpy  数组\ns.values\n\n\ndtype\n数据类型\ns.dtype\n\n\nsize\n元素总个数\ns.size\n\n\nshape\n数据结构的形状（长度，）\ns.shape\n\n\nndim\n维度，Series 永远是 1\ns.ndim\n\n\nname\nSeries 的名称（可自定义）\ns.name\n\n\nis_unique\n是否全是唯一值\ns.is_unique\n\n\nhasnans\n是否含有缺失值（NaN）\ns.hasnans\n\n\n\n# Series 对象属性s = pd.Series([1, 2, 3, 4, 5], index=['a', 'b', 'c', 'd', 'e'])print(s.values)print(s.index)print(s.name)print(s.dtype)print(s.size)print(s.empty)print(s.ndim)print(s.shape)print(s.is_unique)print(s.loc['e'])  # 通过索引获取，显示索引print(s.iloc[4])  # 通过位置获取，隐式索引# 支持切片print(s.loc['a':'d'])print(s.iloc[0:3])# 访问数据\nprint(s.iloc[2])print(s['b'])print(s[s &lt; 3]) # 布尔索引print(s.isin([1, 2, 3]))print(s[s.isin([1, 2, 3])])s = pd.Series([1, np.nan, 3, None, 5, 6, 7, 8, 9, 10], index=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'], name='testData')s.head()s.tail(2)# 计算与常用统计方法\n# 所有描述信息的语句s.describe()# 是否包含在其中s.isin([1, 9, 3])# 排序s.sort_values()# 分位数s.quantile(0.5)# 统计个数s.value_counts()# 众数s.mode()# Series 案例\n\n创建一个包含十名学生数学成绩的 Series，成绩范围在 50 到 100 间，计算平均分、最高分、最低分、并找出高于平均分的学生人数。\n\nprint(scores.mean())print(scores.max())print(scores.min())print(scores[scores.mean() &lt; scores].count())\n给定某城市一周每天的最高温度 Series，完成以下任务：\n找出温度超过 30 度的天数\n计算平均温度将温度从高到低排序\n找出温度变化最大的两天\n\nprint(temperatures[temperatures > 30].count())print(temperatures.sort_values(ascending=False))temperatures.diff().abs().sort_values(ascending=False).head(2)\n股票价格分析给定某股票连续 10 个交易日的收盘价 Series:\n计算每日收益率 (当日收盘价 / 前日收盘价 - 1)\n找出收益率最高和最低的日期\n计算波动率 (收益率的标准差)\n\n# 计算收益率 pct->percent price.pct_change()a = price.pct_change()a.idxmax()price.pct_change().idxmin()price.pct_change().std()\n某产品过去 12 个月的销售量 Series:\n计算季度平均销量 (每 3 个月为一个季度)\n找出销量最高的月份\n计算月环比增长率\n找出连续增长超过 2 个月的月份\n\n# resample->qs 按照季度开始重新采样sales.resample('QS').mean()sales.idxmax()# 月百分比变化，即为环比sales.pct_change()# 环比大于零，使用滑动窗口，和为 3pct_c = sales.pct_change()b = pct_c > 0b[b.rolling(window=3).sum() == 3].keys().tolist()\n某商店每小时销售额 Series:\n按天重采样计算每日总销售额\n计算每天营业时间 (8:00-22:00) 和非营业时间的销售额比例\n找出销售额最高的 3 个小时\n\nhourly_sales.resample('D').sum()# 两种方法hourly_sales[(hourly_sales.index.hour>=8)&amp;(hourly_sales.index.hour&lt;=22)].sum()hourly_sales.between_time('8:00', '22:00')# 最终计算hourly_sales[(hourly_sales.index.hour>=8)&amp;(hourly_sales.index.hour&lt;=22)].sum()/(hourly_sales.sum() - hourly_sales[(hourly_sales.index.hour>=8)&amp;(hourly_sales.index.hour&lt;=22)].sum())# 可在全部的索引中去除营业索引hourly_sales.drop(hourly_sales[(hourly_sales.index.hour>=8)&amp;(hourly_sales.index.hour&lt;=22)].index)# 销售额最高的 3 个小时，两种方法hourly_sales.sort_values(ascending=False).head(3)hourly_sales.nlargest(3).keys()# DataFrame\n# 读取 JSONL\npd.read_json(\"../data/raw/synthesized_.jsonl\", lines=True)# 保存为 JSON / JSONL\nimport pandas as pdframe = pd.DataFrame(dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)), columns=['A', 'B', 'C'], index=['x', 'y', 'z'])print(frame)print(frame.to_json(orient='index'))print(frame.to_json(orient='columns'))vprint(frame.to_json(orient='split'))print(frame.to_json(orient='records', lines=True))\nA  B  C\nx  1  4  7\ny  2  5  8\nz  3  6  9\n{“x”:{“A”:1,“B”:4,“C”:7},“y”:{“A”:2,“B”:5,“C”:8},“z”:{“A”:3,“B”:6,“C”:9}}\n\n{“A”:1,“B”:4,“C”:7}\n\n\n","categories":["深度学习"],"tags":["python","python_Pandas"]},{"title":"基于PyTorch的手写数字识别","url":"/python/pytorch_nn_digital_identification/","content":"# 基于 PyTorch 的手写数字识别\n# MNIST 数据集\nMNIST 是一个经典的手写数字识别数据集，包含 60,000 张用于训练的图片和 10,000 张用于测试的图片。每张图片分辨率为 28×28 像素，内容是数字 0–9 的手写体。我们将基于该数据集对模型进行训练和验证。\n\n# 神经网络的定义\n# 输入\n\n一张图片大小： 1 × 28 × 28  （灰度图，通道数 = 1）\n\n\n# 第一层卷积\nnn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n输入： 1 × 28 × 28\n卷积核：5×5，padding=2 → 输出大小仍然是 28×28\n输出： 16 × 28 × 28 （提取 16 个不同特征图）\n\n\n# ReLU 激活\nnn.ReLU()\n不改变形状，只是把负数变成 0。\n输出： 16 × 28 × 28\n\n\n# 第一次池化\nnn.MaxPool2d(kernel_size=2)\n池化窗口：2×2，步长 = 2 → 尺寸减半\n输出： 16 × 14 × 14\n\n\n# 第二层卷积\nnn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n输入： 16 × 14 × 14\n输出： 32 × 14 × 14\n\n\n# ReLU 激活\nnn.ReLU()\n输出： 32 × 14 × 14\n\n\n# 第二次池化\nnn.MaxPool2d(kernel_size=2)\n尺寸再减半\n输出： 32 × 7 × 7\n\n\n# 展平\nnn.Flatten()\n把  32 × 7 × 7  展平成一维向量\n输出： 32*7*7 = 1568\n\n\n# 全连接层 1\nnn.Linear(32*7*7, 500)\n输入： 1568\n输出： 500\n\n\n# ReLU 激活\nnn.ReLU()\n输出： 500\n\n\n# 全连接层 2\nnn.Linear(500, 100)\n输入： 500\n输出： 100\n\n\n# 全连接层 3（输出层）\nnn.Linear(100, 10)\n输入： 100\n输出： 10 （对应数字 0–9 的类别）\n\n# 总结\n网络的流程就是：\n输入  1×28×28  图像 → 两次卷积 + ReLU + 池化提取特征 → 展平 → 三层全连接分类 → 输出 10 个类别的分数（预测数字 0–9）。\n\n# 存在的问题\n全连接层数量和大小\n\n两个全连接层（500 → 100 → 10）对 MNIST 来说稍微大了一点\n可以简化为  500 → 10  或  256 → 10 ，训练更快，参数更少\n\n卷积层通道数\n\n第一层 16、第二层 32 可以再小一点（如 8 → 16），MNIST 太简单了，用太多特征图可能没必要\n\n# 代码\nimport torchfrom torch import nnclass Net(nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.model = nn.Sequential(            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, ceil_mode=False),            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, ceil_mode=False),            nn.Flatten(),            nn.Linear(32 * 7 * 7, 500),            nn.ReLU(),            nn.Linear(500, 100),            nn.Linear(100, 10)        )    def forward(self, x):        x = self.model(x)        return xif __name__ == '__main__':    net = Net()    print(net)    output = net(torch.ones((64, 1, 28, 28)))    print(output.shape)# 模型训练\n# 导入库\nimport torchimport torch.nn as nnimport torchvision.datasetsfrom torch.utils.data import DataLoaderfrom torchvision import transformsfrom torch.utils.tensorboard import SummaryWriterfrom nn_DI_NNFramework import Net\ntorch ：PyTorch 主库\ntorch.nn ：神经网络模块，包括各种层、损失函数\ntorchvision.datasets ：常用数据集（这里用 MNIST）\nDataLoader ：批量加载数据\ntransforms ：对图像进行转换（如标准化、张量化）\nSummaryWriter ：TensorBoard 日志写入，用于可视化训练过程\nNet ：自己定义的神经网络\n\n\n# 数据预处理\ntransform = transforms.Compose([    transforms.ToTensor(),  # 转为 PyTorch 张量，形状为 [C,H,W]    transforms.Normalize((0.1307,), (0.3081,))  # 标准化：均值 0.1307，标准差 0.3081])\n将图片从  [0,255]  映射到  [0,1]\n再做标准化，便于模型收敛更快\n\n\n# TensorBoard 日志 &amp; 设备设置\nwriter = SummaryWriter(\"log_digital\")device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSummaryWriter  用于记录训练和测试的 loss、accuracy\ndevice  判断是否有 GPU 可用，有就用 GPU 否则 CPU\n\n\n# 加载数据集\ndatasets_mnist_train = torchvision.datasets.MNIST(\"data\", train=True, transform=transform, download=True)datasets_mnist_test = torchvision.datasets.MNIST(\"data\", train=False, transform=transform, download=True)\ntrain=True ：训练集 60000 张图片\ntrain=False ：测试集 10000 张图片\n\n\n# DataLoader\ndataLoader_train = DataLoader(datasets_mnist_train, batch_size=64, shuffle=True, num_workers=0)dataLoader_test = DataLoader(datasets_mnist_test, batch_size=64, shuffle=False, num_workers=0)\nbatch_size=64 ：每次送入网络 64 张图片\nshuffle=True ：打乱训练顺序\nnum_workers=0 ：加载数据的线程数\n\n\n# 打印数据集大小\ntrain_data_size = len(datasets_mnist_train)test_data_size = len(datasets_mnist_test)print(f\"训练数据集的长度为&#123;train_data_size&#125;\")print(f\"测试数据集的长度为&#123;test_data_size&#125;\")\n方便确认数据集是否加载成功\n\n\n# 创建网络和损失函数\nnet = Net().to(device)loss_fn = nn.CrossEntropyLoss().to(device)\nNet() ：实例化你的卷积神经网络\nCrossEntropyLoss ：常用的多分类损失函数，内部包含 softmax\n\n\n# 优化器\noptimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n使用随机梯度下降（SGD）优化\nlr=0.01 ：学习率\nmomentum=0.5 ：动量，帮助加速收敛\n\n\n# 训练设置\ntotal_train_step = 0total_test_step = 0epoch = 15\ntotal_train_step ：训练总步数\ntotal_test_step ：测试总步数\nepoch=15 ：训练轮数\n\n\n# 训练循环\nfor i in range(epoch):    print(\"--------第 &#123;&#125; 轮训练开始--------\".format(i + 1))    net.train()  # 设置网络为训练模式\nnet.train() ：启用训练模式（例如 Dropout、BatchNorm 生效）\n\n# 训练步\nfor data in dataLoader_train:    imgs, targets = data    imgs = imgs.to(device)    targets = targets.to(device)    outputs = net(imgs)    loss = loss_fn(outputs, targets)    optimizer.zero_grad()    loss.backward()    optimizer.step()\nimgs ：图片 batch\ntargets ：标签 batch\nloss.backward() ：计算梯度\noptimizer.step() ：更新参数\noptimizer.zero_grad() ：清空上一步梯度\n\n# 记录日志\nif total_train_step % 100 == 0:    print(f\"训练次数：&#123;total_train_step&#125;，loss：&#123;loss&#125;\")    writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n每 100 步打印 loss\n写入 TensorBoard\n\n\n# 测试循环\nnet.eval()  # 设置网络为评估模式total_test_loss = 0total_test_accuracy = 0with torch.no_grad():  # 不计算梯度    for data in dataLoader_test:        imgs, targets = data        imgs = imgs.to(device)        targets = targets.to(device)        outputs = net(imgs)        loss = loss_fn(outputs, targets)        total_test_loss += loss.item()        accuracy = (outputs.argmax(1) == targets).sum()        total_test_accuracy += accuracy.item()\nnet.eval() ：关闭 Dropout、BatchNorm 等训练特性\ntorch.no_grad() ：节省显存，不计算梯度\noutputs.argmax(1) ：得到预测的数字\n累计 loss 和正确数\n\n# 打印和记录\nprint(\"整体测试集上的Loss：&#123;&#125;\".format(total_test_loss))print(\"整体测试集上的正确率：&#123;&#125;\".format(total_test_accuracy / test_data_size))writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)writer.add_scalar(\"test_accuracy\", total_test_accuracy / test_data_size, total_test_step)total_test_step += 1\n# 保存模型\ntorch.save(net, \"net_DI.pth\")print(\"模型已保存\")writer.close()\n将训练好的网络保存为  net_DI.pth\n关闭 TensorBoard 写入器\n\n# 全部代码\nimport torchimport torch.nn as nnimport torchvision.datasetsfrom torch.utils.data import DataLoaderfrom torchvision import transformsfrom torch.utils.tensorboard import SummaryWriterfrom nn_DI_NNFramework import Nettransform = transforms.Compose([    transforms.ToTensor(),  # 将图像转换为张量    transforms.Normalize((0.1307,), (0.3081,))  # 标准化图像])writer = SummaryWriter(\"log_digital\")device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")datasets_mnist_train = torchvision.datasets.MNIST(\"data\", train=True, transform=transform,                                                  download=True)datasets_mnist_test = torchvision.datasets.MNIST(\"data\", train=False, transform=transform,                                                 download=True)dataLoader_train = DataLoader(datasets_mnist_train, batch_size=64, shuffle=True, num_workers=0)dataLoader_test = DataLoader(datasets_mnist_test, batch_size=64, shuffle=False, num_workers=0)# 获取数据集大小train_data_size = len(datasets_mnist_train)test_data_size = len(datasets_mnist_test)print(f\"训练数据集的长度为&#123;train_data_size&#125;\")print(f\"测试数据集的长度为&#123;test_data_size&#125;\")net = Net().to(device)# 创建损失函数loss_fn = nn.CrossEntropyLoss()# 交给 GPUloss_fn = loss_fn.to(device)# 创建优化器optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.5)# 设置训练网络一些参数# 训练的轮数total_train_step = 0# 测试的轮数total_test_step = 0# 训练的轮数epoch = 15for i in range(epoch):    print(\"--------第 &#123;&#125; 轮训练开始--------\".format(i + 1))    net.train()    for data in dataLoader_train:        imgs, targets = data        imgs = imgs.to(device)        targets = targets.to(device)        outputs = net(imgs)        loss = loss_fn(outputs, targets)        optimizer.zero_grad()        loss.backward()        optimizer.step()        total_train_step += 1        if total_train_step % 100 == 0:            print(f\"训练次数：&#123;total_train_step&#125;，loss：&#123;loss&#125;\")            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)    net.eval()    total_test_loss = 0    total_test_accuracy = 0    with torch.no_grad():        for data in dataLoader_test:            imgs, targets = data            imgs = imgs.to(device)            targets = targets.to(device)            outputs = net(imgs)            loss = loss_fn(outputs, targets)            total_test_loss += loss.item()            accuracy = (outputs.argmax(1) == targets).sum()            total_test_accuracy += accuracy.item()        print(\"整体测试集上的Loss：&#123;&#125;\".format(total_test_loss))        print(\"整体测试集上的正确率：&#123;&#125;\".format(total_test_accuracy / test_data_size))        writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)        writer.add_scalar(\"test_accuracy\", total_test_accuracy / test_data_size, total_test_step)        total_test_step += 1torch.save(net, \"net_DI.pth\")print(\"模型已保存\")writer.close()# 训练过程\n\n模型在 15 轮训练后，测试集上准确率超过 99%\n# 简单验证\n# 导入库\nimport torchimport torchvisionfrom PIL import Imageimport nn_DI_NNFramework\ntorch ：PyTorch 主库\ntorchvision ：用于图像处理和变换\nPIL.Image ：处理图片文件\nnn_DI_NNFramework ：你自己定义的网络（Net）\n\n\n# 设备设置\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n判断是否有 GPU 可用，如果有就用 GPU，否则用 CPU\n\n\n# 加载模型\nmodel = torch.load(\"net_DI.pth\", weights_only=False)\n加载之前训练好的模型  net_DI.pth\nweights_only=False  表示加载整个模型对象，而不仅仅是权重\n\n\n一般推荐保存  state_dict  加载权重，这样更灵活：\n\nnet = nn_DI_NNFramework.Net().to(device)net.load_state_dict(torch.load(\"net_DI.pth\"))\n# 打开图片\nimage_path = \"images/333.png\"image = Image.open(image_path).convert(\"L\")\n打开图片\n.convert(&quot;L&quot;) ：将图片转换为灰度（1 通道）\n\n\n# 图像预处理\ntrans = torchvision.transforms.Compose([    torchvision.transforms.Resize(size=(28, 28)),  # 调整图片大小    torchvision.transforms.ToTensor(),             # 转为张量])image = trans(image)\n将图片缩放到 28×28（MNIST 输入尺寸）\n转成 PyTorch 张量，形状  [C,H,W] ，范围  [0,1]\n\n\n# 调整 batch 维度并发送到 GPU\nimage = torch.reshape(image, (1, 1, 28, 28)).to(device)print(image.shape)\n模型期望输入  [batch, channel, height, width]\n这里 batch=1，channel=1，28×28\n.to(device) ：把图片发送到 GPU 或 CPU\n\n\n# 模型推理\nmodel.eval()with torch.no_grad():    output = model(image)    print(output)\nmodel.eval() ：评估模式（关闭 Dropout/BatchNorm）\ntorch.no_grad() ：不计算梯度，节省显存\noutput ：模型输出 logits（长度为 10 的向量，每个元素对应数字 0–9 的得分）\n\n\n# 获取预测结果\nitem = torch.argmax(output).item()print(item)\ntorch.argmax(output) ：找到得分最大的索引（预测数字）\n.item() ：把张量转成 Python 整数\n\n# 全部代码\nimport torchimport torchvisionfrom PIL import Imageimport nn_DI_NNFrameworkdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")model = torch.load(\"net_DI.pth\", weights_only=False)image_path = \"images/333.png\"image = Image.open(image_path).convert(\"L\")trans = torchvision.transforms.Compose([    torchvision.transforms.Resize(size=(28, 28)),    torchvision.transforms.ToTensor(),])image = trans(image)# 添加一个维度代表 1 张图片，并交给 GPUimage = torch.reshape(image, (1, 1, 28, 28)).to(device)print(image.shape)model.eval()with torch.no_grad():    output = model(image)    print(output)    item = torch.argmax(output).item()    print(item)# 自定义测试\n\n\n\n0\n2\n3\n7\n9\n\n\n\n\n\n\n\n\n\n\n\n\n以上手写数字均准确识别\n","categories":["深度学习"],"tags":["python","python_PyTorch","pytorch_nn_digital_identification"]},{"title":"PyTorch深度学习","url":"/python/pytorch/","content":"# PyTorch 深度学习\n基于小土堆：https://www.bilibili.com/video/BV1hE411t7RN\n# 安装\n基于 conda 环境来安装\nconda create --name pytorch python=3.11查询已有环境\nconda info --envs\nconda environments:\nbase                   * D:\\anaconda\npytorch              D:\\anaconda\\envs\\pytorch\n\n激活 pytorch 环境\nconda activate pytorch其他命令\nconda remove -n xxxxx(名字) --all\t# 环境删除命令deactivate\t# 退出虚拟环境pip list\t# 查看虚拟环境的库访问 pytorch 网站：https://pytorch.org/get-started/locally/\n\n截至 2025 年 8 月 Start Locally 条目给予 python 版本提示：\nNOTE: Latest PyTorch requires Python 3.9 or later.\n\n基于：Stable (2.7.1) - Windows - Pip - Python - CUDA 11.8\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118(pytorch) C:\\Users\\Karry>pip listPackage           Version----------------- ------------filelock          3.13.1fsspec            2024.6.1Jinja2            3.1.4MarkupSafe        2.1.5mpmath            1.3.0networkx          3.3numpy             2.1.2pillow            11.0.0pip               25.1setuptools        78.1.1sympy             1.13.3torch             2.7.1+cu118torchaudio        2.7.1+cu118torchvision       0.22.1+cu118typing_extensions 4.12.2wheel             0.45.1验证：\nimport torchtorch.cuda.is_available()\t# True# 两大法宝函数 - dir 与 help\ndir 主要是来查看一个工具包下还有什么子工具包或者工具\nhelp 主要是查看一些工具有什么作用\n比如 help (torch.cuda.is_available)\nhelp(torch.cuda.is_available)Help on function is_available in module torch.cuda:is_available() -> bool    Return a bool indicating if CUDA is currently available.# PyTorch 数据读取\n数据 —— Dataset（提供一种方式获取 label） —— Dataloader（为后面的网络提供不同的数据形式）\n组织结构：\n+—— hymenoptera_data| +—— train| | —— ants| | —— bees对应代码：\nfrom torch.utils.data import Datasetfrom PIL import Imageimport os# MyData 是自定义的数据集类，他继承了 Dataset 类，MyData (Dataset) 这是继承动作class MyData(Dataset):    # 初始化函数，初始化一些数据，比如图片路径，标签路径等等，__init__这是重写父类的方法    def __init__(self, root_dir, label_dir):        # 根集路径        self.root_dir = root_dir        # 标签路径，这其实表达了图片是什么分类        self.label_dir = label_dir        # 获取图片的路径，这个路径下存放着图片        self.path = os.path.join(self.root_dir, self.label_dir)        # 获取图片路径列表 listdir        self.img_path = os.listdir(self.path)    # 获取数据，这个函数是必须写的，并且是必须返回两个值，一个是图片，一个是标签    def __getitem__(self, idx):        img_name = self.img_path[idx]        img_item_path = os.path.join(self.root_dir, self.label_dir, img_name)        img = Image.open(img_item_path)        label = self.label_dir        return img, label    def __len__(self):        return len(self.img_path)root_dir = \"hymenoptera_data\\\\train\"ant_label_dir = \"ants\"bees_label_dir = \"bees\"ants_dataset = MyData(root_dir, ant_label_dir)bees_dataset = MyData(root_dir, bees_label_dir)train_dataset = ants_dataset + bees_dataset组织结构：\n+—— hymenoptera_data_ex| +—— train| | —— ants_image| | —— ants_label| | —— bees_image| | —— bees_label对应代码：\nfrom torch.utils.data import Datasetfrom PIL import Imageimport osclass MyData(Dataset):    def __init__(self, root_dir, img_dir, label_dir):        self.root_dir = root_dir        self.img_dir = img_dir        self.label_dir = label_dir        self.path_img = os.path.join(self.root_dir, self.img_dir)        self.path_label = os.path.join(self.root_dir, self.label_dir)        self.img_path = os.listdir(self.path_img)        self.label_path = os.listdir(self.path_label)    def __getitem__(self, item):        img_name = self.img_path[item]        img_item_path = os.path.join(self.path_img, img_name)        img = Image.open(img_item_path)        label_name = self.label_path[item]        label_item_path = os.path.join(self.path_label, label_name)        with open(label_item_path, \"r\", encoding=\"utf-8\") as f:            label = f.read()            return img, labelroot_dir = \"hymenoptera_data_ex\\\\train\"img_dir = \"ants_image\"label_dir = \"ants_label\"ants_dataset = MyData(root_dir, img_dir, label_dir)# Tensorboard 的使用\n# add_scalar\nfrom torch.utils.tensorboard import SummaryWriterwriter = SummaryWriter(\"logs\")# writer.add_image()# y = xfor i in range(100):    writer.add_scalar(\"y=i^2\", i * i, i)writer.close()tensorboard --logdir=logs --port=6007\nwriter.add_scalar (“y=i^2”, i * i, i)，这个的参数分别是：标签、y、x\n# add_image\nimport numpy as npfrom PIL import Imagefrom torch.utils.tensorboard import SummaryWriterimg_path = \"hymenoptera_data/train/ants/0013035.jpg\"img_PIL = Image.open(img_path)img_array = np.array(img_PIL)# hymenoptera_data/train/ants/0013035.jpgwriter = SummaryWriter(\"logs\")writer.add_image(\"img\", img_array, 1,dataformats=\"HWC\")# y = xfor i in range(100):    writer.add_scalar(\"y=i^2\", i * i, i)writer.close()\nadd_image (“img”, img_array, 1,dataformats=“HWC”)，这个参数分别是：标签、ndarray 类型的图片、步骤、dataformats-HWC\n# Transforms 的使用\nTransforms 主要是对图片的各种变换，是预处理？\nfrom PIL import Imagefrom torchvision import transformsimg = Image.open('hymenoptera_data/train/ants/0013035.jpg')tensor_trans = transforms.ToTensor()tensor_img = tensor_trans(img)print(tensor_img)Image.open 返回了 PIL 类型的图片，通过 transforms.ToTensor () 创建了工具对象，最后使用 tensor_trans (img) 转化为 tensor 类型\n\ntensor 数据类型：包装了神经网络所需要的理论基础参数\n\n使用 SummaryWriter 将 tensor_img 写入\nfrom PIL import Imagefrom torch.utils.tensorboard import SummaryWriterfrom torchvision import transformsimg = Image.open('hymenoptera_data/train/ants/0013035.jpg')tensor_trans = transforms.ToTensor()tensor_img = tensor_trans(img)# print(tensor_img)writer = SummaryWriter('logs')writer.add_image('tensor_img', tensor_img)writer.close()\n\n题外话，理解一下 py 的面向对象：\nclass Person:    def __init__(self, name):        self.name = name    def __call__(self):        print(\"hello\", self.name)    def hello(self):        print(\"hello_ex\", self.name)person = Person(\"Karry\")person()person.hello()init 方法实际上是一个构造方法，person = Person (“Karry”) 执行后调用构造方法\ncall 方法像初次见面问好一样 person () 执行后，调用 call 方法，是让实例像函数一样用的钩子方法\nperson.hello () 只是一个类的普通方法\n\n# ToTensor 的使用\nToTensor 图片张量化工具\nfrom PIL import Imagefrom torch.utils.tensorboard import SummaryWriterfrom torchvision import transformswriter = SummaryWriter('logs')img = Image.open('images/pytorch.png')toTensorTools = transforms.ToTensor()img_tensor = toTensorTools(img)writer.add_image('ToTensor', img_tensor)writer.close()# Normalize 的使用\nNormalize 归一化、标准化，均值为 0，方差为 1，数值位于 - 1 到 1 之间\n如果图片不是 RGB 模式需要做 img.convert (‘RGB’)\nwriter = SummaryWriter('logs')img = Image.open('images/pytorch.png')# img 转化为 RGBimg = img.convert('RGB')toTensorTools = transforms.ToTensor()img_tensor = toTensorTools(img)writer.add_image('ToTensor', img_tensor)print(img_tensor[0][0][0])# 归一化transforms_normalize = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])img_normalize = transforms_normalize(img_tensor)print(img_normalize[0][0][0])writer.close()tensor(0.1333)\ntensor(-0.7333)\n0.1333*2 - 1 = -0.7333\n# Resize 的使用\nResize 重调整\n# Resize 的使用print(img.size)transforms_resize = transforms.Resize([128, 128])img_resize = transforms_resize(img_tensor)writer.add_image('Resize', img_resize, 0)print(img_resize.size())resize_2 = transforms.Resize(512)transforms_compose = transforms.Compose([resize_2, toTensorTools])img_resize_2 = transforms_compose(img)writer.add_image('Compose', img_resize_2, 1)起初我们使用 img = Image.open (‘images/testFG.jpg’)，此时这是一个 PIL 图片，如何我们使用 toTensorTools = transforms.ToTensor () 创建张量转换工具，使用 img_tensor = toTensorTools (img) 将 PIL 图片转化为 img_tensor 张量图片，紧接着我们使用 transforms_resize = transforms.Resize ([128, 128]) 创建尺寸调整工具，使用 img_resize = transforms_resize (img_tensor) 对张量图片重调整。\nCompose 的意义在于它可以将多个图像变换操作（如缩放、裁剪、归一化等）按顺序组合成一个流水线，输入图像会依次通过这些变换。\n我们使用 transforms_compose = transforms.Compose ([resize_2, toTensorTools]) 创建了一个工具链，resize_2 用于调整图像尺寸，toTensorTools 用于其转化为张量图片。\n从 img_resize_2 = transforms_compose (img) 我们可以看到，参数 img 是一个 PIL 图片他通过 Compose 先后进行了重调整和张量化，最终返回 img_resize_2 张量图片。\n\n提示：resize_2 = transforms.Resize (512) 是一个等比例调整。\ntensor(0.4039)\ntensor(0.6797)\n(3600, 2700)\ntorch.Size([3, 128, 128])\ntorch.Size([3, 512, 682])\n\n# RandomCrop 的使用\nRandomCrop 随机裁剪\n# RandomCrop 的使用# random_crop = transforms.RandomCrop(500, 1000)random_crop = transforms.RandomCrop(512)compose_random_crop = transforms.Compose([random_crop, toTensorTools])for i in range(10):    img_random_crop = compose_random_crop(img)    writer.add_image('RandomCrop', img_random_crop, i)# 使用 TorchVision 的数据集（DataSet）\nCIFAR —— Canadian Institute For Advanced Research（加拿大高级研究所）\nroot 数据集所在目录、train 是训练集还是测试集、transform 应用的变换操作或操作集合、download 是否启用下载\ntrain_set = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=transforms_compose_dataset, download=True)test_set = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=transforms_compose_dataset, download=True)\nCIFAR-10 and CIFAR-100 datasets\n\nThe CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\nCIFAR-10 数据集由 10 类的 60000 张 32x32 彩色图像组成，每类 6000 张图像。有 50000 张训练图像和 10000 张测试图像。\nThe dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n数据集分为五个训练批次和一个测试批次，每个训练批次有 10000 张图像。测试批次恰好包含每个类中随机选择的 1000 张图像。训练批次以随机顺序包含剩余的图像，但某些训练批次可能包含来自一个类的图像多于另一个类的图像。在它们之间，训练批次恰好包含每个类的 5000 张图像。\n\n# 数据集联动 Tensorboard\nimport torchvisionfrom torch.utils.tensorboard import SummaryWritertensorboard = SummaryWriter(\"p10\")transforms_compose_dataset = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])train_set = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=transforms_compose_dataset, download=True)test_set = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=transforms_compose_dataset, download=True)print(test_set[0])for i in range(20):    img, target = test_set[i]    tensorboard.add_image(\"test_set\", img, i)tensorboard.close()其中 img, target = test_set [i] 返回了一个元组\n\n0 号位是转化过后的张量图，1 号位是其标签索引，数据的标签列表可以在 test_set.classes 中看到。\n# add_image 源码提示\ndef add_image(    self, tag, img_tensor, global_step=None, walltime=None, dataformats=\"CHW\"):tag, img_tensor, global_step=None 分别对应 tensorboard 标签，张量图，以及步骤 i\ntensorboard.add_image(\"test_set\", img, i)注意：tensorboard 使用过后须关闭 tensorboard.close ()\n# DataLoader 的使用\ntorch.utils.data — PyTorch 2.8 documentation\n参数初见：\n\n\ndataset (Dataset) – dataset from which to load the data.\n\n\nbatch_size (int, optional) – how many samples per batch to load (default:  1 ).\n\n\nshuffle (bool, optional) – set to  True  to have the data reshuffled at every epoch (default:  False ).\n\n\nbatch_sampler (Sampler or Iterable*,* optional) – like  sampler , but returns a batch of indices at a time. Mutually exclusive with  batch_size ,  shuffle ,  sampler , and  drop_last .\n\n\nnum_workers (int, optional) – how many subprocesses to use for data loading.  0  means that the data will be loaded in the main process. (default:  0 )\n\n\ndrop_last (bool, optional) – set to  True  to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If  False  and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default:  False )\n\n\n# 准备的测试数据test_data = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=torchvision.transforms.ToTensor(),                                         download=True)# 创建测试数据集test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=True, num_workers=0, drop_last=True)dataset 数据集、batch_size 一次打包多少个、shuffle 是否打乱、num_workers 加载数据子进程数、drop_last 多余部分是否删除\ntorch.Size([3, 32, 32])3cat# 联动 Tensorboard\nimport torchvision.datasetsfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriter# 准备的测试数据test_data = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=torchvision.transforms.ToTensor(),                                         download=True)# 创建测试数据集test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=True, num_workers=0, drop_last=True)image, target = test_data[0]# 测试数据第一张图像的 shape 和标签print(image.shape)print(target)print(test_data.classes[target])print(\"____________________\")writer = SummaryWriter(\"DataLoader\")step = 0for loaderX in test_loader:    images, targets = loaderX    # print(images.shape)    # print(targets)    writer.add_images(\"test_data_drop_last\", images, step)    step += 1writer.close()# 利用 Epoch 变量控制训练或测试轮次\nfor epoch in range(2):    step = 0    for loaderX in test_loader:        images, targets = loaderX        # print(images.shape)        # print(targets)        writer.add_images(\"Epoch:&#123;&#125;\".format(epoch), images, step)        step += 1&quot;Epoch:&#123;&#125;&quot;.format(epoch)  是 Python 的 字符串格式化 方法，它会将  epoch  的值动态插入到字符串的  &#123;&#125;  占位符中。\n\n此时当 shuffle=True 时，Epoch0 和 Epoch1 并不一样\ntest_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=True, num_workers=0, drop_last=True)\n# 神经网络 (Neural Network) 基本骨架\ntorch.nn — PyTorch 2.8 documentation\nModule — PyTorch 2.8 documentation\nimport torch.nn as nnimport torch.nn.functional as Fclass Model(nn.Module):    def __init__(self) -> None:        super().__init__()        self.conv1 = nn.Conv2d(1, 20, 5)        self.conv2 = nn.Conv2d(20, 20, 5)    def forward(self, x):        x = F.relu(self.conv1(x))        return F.relu(self.conv2(x))forward 前向传播：\ndef forward(self, x):        x = F.relu(self.conv1(x))        return F.relu(self.conv2(x))x  先经过一次 conv1  卷积，再经过一次 relu  非线性处理 x = F.relu(self.conv1(x))\n然后 x  在经过一次 conv2  卷积，再经过一次 relu  非线性处理，最后返回 return F.relu(self.conv2(x))\n# 简单的骨架\n简单的骨架就是这样，有一个输入经过 forward 后每次加一\nimport torchfrom torch import nnclass Module(nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)    def forward(self, input):        output = input + 1        return outputkarry = Module()x = torch.tensor(1.0)print(karry(x))# convolution 卷积操作\nhttps://www.bilibili.com/video/BV1hE411t7RN?p=17\n# Stride 跨步 = 1\nStride 是每次跨步数，1 就是每次跨一步\n\n注意到红色部分就是对应位置相乘再相加\nimport torchimport torch.nn.functional as F# 创建输入和核input_matrix = torch.tensor([[1, 2, 0, 3, 1], [0, 1, 2, 3, 1], [1, 2, 1, 0, 0], [5, 2, 3, 1, 1], [2, 1, 0, 1, 1]])kernel = torch.tensor([[1, 2, 1], [0, 1, 0], [2, 1, 0]])# 改变维度input_matrix = torch.reshape(input_matrix, (1, 1, 5, 5))kernel = torch.reshape(kernel, (1, 1, 3, 3))# 卷积output_ans = F.conv2d(input_matrix, kernel, stride=1)print(input_matrix.shape)print(kernel.shape)print(output_ans)\ntorch.Size([1, 1, 5, 5])\ntorch.Size([1, 1, 3, 3])\ntensor([[[[10, 12, 12],\n[18, 16, 16],\n[13,  9,  3]]]])\n\n# Stride 跨步 = 2\n\n# 卷积output_ans = F.conv2d(input_matrix, kernel, stride=2)print(output_ans)\ntensor([[[[10, 12],\n[13,  3]]]])\n\n# Padding 填充 = 1\nPadding 填充就是在原始数据的最外侧填充一些数据（像素），一般情况下是设置为零\n\n一样地，红色部分对应位置相乘再相加，最外圈绿色为 padding=1 所产生的额外填充\n# 卷积output_ans = F.conv2d(input_matrix, kernel, stride=1, padding=1)print(output_ans)\ntensor([[[[ 1,  3,  4, 10,  8],\n[ 5, 10, 12, 12,  6],\n[ 7, 18, 16, 16,  8],\n[11, 13,  9,  3,  4],\n[14, 13,  9,  7,  4]]]])\n\n# Convolution Layers 卷积层\ntorch.nn — PyTorch 2.8 documentation\nclass torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode=‘zeros’, device=None, dtype=None)\n其中最主要的参数设置是这五个：in_channels, out_channels, kernel_size, stride=1, padding=0\n\nweight 实际上就是卷积核，input 是通道数据，bias 是偏置\n\n卷积层的 每个输出通道 都是 所有输入通道的加权卷积结果相加，权重就是  weight[j, k] 。\n假设输入是 RGB 彩色图像（3 个通道：R、G、B），卷积层的一个输出通道是这样算的：\n\n为什么要 “所有输入通道相加”？\n\n图像的特征可能跨通道（比如红绿蓝组合才能构成颜色信息）\n一个卷积核只看单个通道的信息是不完整的\n把多个输入通道的卷积结果加在一起，就相当于在融合这些通道的信息\n\n这也是为什么多通道卷积的  weight  是四维的：C_out、C_in、k_h、k_w\n\nConvolution animations：conv_arithmetic/README.md at master · vdumoulin/conv_arithmetic · GitHub\n# in_channel=1, out_channel=1\n只有一个输入通道和一个输出通道，不存在跨通道求和。\n\n1×1+2×0+3×0+4×(−1)=1+0+0−4=−3\n[[−3]]\nin_channel=1, out_channel=1 时，就是用一个卷积核直接作用于输入通道，卷积后加上 bias 得到结果。\n没有跨通道加权，没有额外求和步骤。\nimport torchfrom torch import nnnn_conv_d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride=1, padding=0, bias=False)nn_conv_d.weight.data = torch.tensor([[[[1, 0],                                        [0, -1]]]], dtype=torch.float32)input = torch.tensor([[[[1, 2], [3, 4]]]], dtype=torch.float32)print(nn_conv_d(input))\ntensor([[[[-3.]]]], grad_fn=)\n\n起步解释：\n\nnn_conv_d.weight.data 自定义卷积核\nnn.Conv2d  要求  [batch, channels, height, width] ，所以二维矩阵要用  unsqueeze  扩成 4 维，或者在初始化时就是四维的\n卷积层的权重和 bias 是 ** 浮点型 ( torch.float32 )** 因此要 dtype=torch.float32\n为了得到精确值偏置量应当设为假（不偏置）：bias=False\n\n以上均为 torch 框架中自带的参数调试变量，这些操作主要是 调试和理解卷积的计算过程\n在实际神经网络训练中：\n\n权重  weight  会被优化器自动更新\n输入通常是四维 tensor\nbias 是否启用视网络设计而定\n\n掌握这些基本参数调试方法，是为了：\n\n理解卷积计算机制\n验证卷积操作是否如预期\n为后续做图像或特征识别的神经网络打基础\n\n# in_channel=1, out_channel=2\n每个输出通道是独立的卷积结果\n\n1×1+2×0+3×0+4×(−1)=1+0+0−4=−3\n1×0+2×1+3×(−1)+4×0=0+2−3+0=−1\n输出张量 (batch=1, out_channel=2, H=1, W=1)：output=[[−3],[−1]]\nin_channel=1 时，每个输出通道都直接在这个输入通道上用不同卷积核独立运算。\n最终的两个通道结果是并列存储，不做求和。\nimport torchfrom torch import nnnn_conv_d = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=2, stride=1, padding=0, bias=False)nn_conv_d.weight.data = torch.tensor([    [[[1, 0], [0, -1]]],  # 输出通道 1 的卷积核，对应输入通道 1    [[[0, 1], [-1, 0]]]  # 输出通道 2 的卷积核，对应输入通道 1], dtype=torch.float32)input = torch.tensor([[[[1, 2], [3, 4]]]], dtype=torch.float32)print(nn_conv_d(input))\ntensor([[[[-3.]],\n​\t\t\t\t[[-1.]]]], grad_fn=)\n\n# in_channel=2, out_channel=1\n两个输入通道各自用自己的卷积核卷积 → 得到两个结果。\n\n1×1+2×0+3×0+4×(−1)=1+0+0−4=−3\n5×0+6×1+7×(−1)+8×0=0+6−7+0=−1\n加权求和（合成一个输出通道）：output=(−3)+(−1)=−4\n输出张量 (batch=1, out_channel=1, H=1, W=1)：[[−4]]\nimport torchfrom torch import nnnn_conv_d = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=2, stride=1, padding=0, bias=False)nn_conv_d.weight.data = torch.tensor([    [        [[1, 0], [0, -1]],    # 输入通道 1 的卷积核        [[0, 1], [-1, 0]]     # 输入通道 2 的卷积核    ]], dtype=torch.float32)input = torch.tensor([    [        [            [1, 2],            [3, 4]        ], [        [5, 6],        [7, 8]    ]    ]], dtype=torch.float32)print(nn_conv_d(input))\ntensor([[[[-4.]]]], grad_fn=)\n\n# in_channel=2, out_channel=2\n\nX_channel1 * W_out1_in1 = [[11+20+30+41]] = [[1+0+0+4]] = [[5]]\nX_channel2 * W_out1_in2 = [[00+11+11+00]] = [[0+1+1+0]] = [[2]]\nY_out1 = 5 + 2 = 7\nX_channel1 * W_out2_in1 = [[1+2+3+4]] = [[10]]\nX_channel2 * W_out2_in2 = [[0* -1 + 10 + 10 + 0*-1]] = [[0]]\nY_out2 = 10 + 0 = 10\nY_out1 = [[7]]\t\tY_out2 = [[10]]\n结果为：[[7],[10]]\nimport torchfrom torch import nnnn_conv_d = nn.Conv2d(in_channels=2, out_channels=2, kernel_size=2, stride=1, padding=0, bias=False)nn_conv_d.weight.data = torch.tensor([    [        [[1, 0], [0, 1]],        [[0, 1], [1, 0]]    ], [        [[1, 1], [1, 1]],        [[-1, 0], [0, -1]]    ]], dtype=torch.float32)input = torch.tensor([[    [        [1, 2],        [3, 4]    ], [        [0, 1],        [1, 0]    ]]], dtype=torch.float32)print(nn_conv_d(input))\ntensor([[[[ 7.]],\n​\t\t\t\t[[10.]]]], grad_fn=)\n\n# 向前传播的卷积（正向传播）\nimport torchimport torchvisionfrom torch import nnfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterdataset = torchvision.datasets.CIFAR10(\"data\", train=False, transform=torchvision.transforms.ToTensor(),                                       download=True)dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=0)class Net(nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)    def forward(self, x):        x = self.conv1(x)        return xwriter = SummaryWriter(\"logs\")net = Net()step = 0for data in dataloader:    imgs, targets = data    outputs = net(imgs)    print(imgs.shape)    print(outputs.shape)    # torch.Size([64, 3, 32, 32])    writer.add_images(\"input\", imgs, global_step=step)    # torch.Size([64, 6, 30, 30])    outputs = torch.reshape(outputs, (-1, 3, 30, 30))    writer.add_images(\"output\", outputs, global_step=step)    step += 1self.conv1  是一个 卷积层（ nn.Conv2d ），在  forward  里写  x = self.conv1(x) ，就是把输入  x  通过卷积层进行前向计算。\n# Pooling Layers 池化层\ntorch.nn — PyTorch 2.8 documentation\n池化是卷积神经网络（CNN）中一个很重要的操作。它的主要作用可以总结为以下几点：\n\n降维与减少计算量\n\n\n为什么：卷积层输出的特征图通常很大，如果不缩小，后面网络层的计算量会非常庞大。\n怎么做：池化通过取局部区域的最大值（Max Pooling）或平均值（Average Pooling）来缩小特征图尺寸。\n效果：减少参数量和计算量，加快训练和推理速度。\n\n\n\n特征的平移不变性\n\n\n什么意思：如果图片里一个物体稍微移动了，网络依然能识别。\n为什么能做到：池化会在一个小范围内提取统计特征（最大值或平均值），因此即使输入图像有微小的偏移，结果变化也不会太大。\n\n\n\n突出重要特征，抑制不重要信息\n\n\nMax Pooling：保留一个区域的最大值，倾向于保留最显著的边缘或纹理特征。\nAverage Pooling：保留区域的平均值，得到更加平滑的特征。\n作用：相当于 “特征压缩”，让后续层更容易提取全局信息。\n\n\n\n防止过拟合\n\n\n通过减少参数和对细节的依赖，网络更关注大局特征而不是局部噪声，从而减轻过拟合风险。\n\n\n核心参数\n\nkernel_size (Union[int, tuple[int, int]]) – the size of the window to take a max over\nstride (Union[int, tuple[int, int]]) – the stride of the window. Default value is kernel_size\npadding (Union[int, tuple[int, int]]) – Implicit negative infinity padding to be added on both sides\ndilation (Union[int, tuple[int, int]]) – a parameter that controls the stride of elements in the window\nreturn_indices (bool) – if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later\nceil_mode (bool) – when True, will use ceil instead of floor to compute the output shape\n\n池化示意图：\n\n# dilation 扩张率（空洞卷积）\n这个参数在 卷积（特别是卷积神经网络中的卷积层）里起很重要的作用。它和  kernel_size 、 stride  一样，决定了卷积核是怎么在输入特征图上取值的。\n\n dilation  卷积，其实就是我们常说的 空洞卷积 (Dilated Convolution / Atrous Convolution)。\n# ceil 模式和 floor 模式\n\nfloor 模式（默认）\n\n\n取整时向下取整（floor）。\n多余的边缘（不足一个 kernel 的区域）会被丢弃。\n比如：\n\n输入长度 5\nkernel=2, stride=2\n计算：(5−2)/2+1=2.5 (5-2)/2 + 1 = 2.5 (5−2)/2+1=2.5 → floor → 2\n输出长度就是 2（最后一个位置没覆盖到）。\n\n\n\n\nceil 模式（开启  ceil_mode=True ）\n\n\n取整时向上取整（ceil）。\n边缘不足 kernel 的部分，也会被保留（通常会用 padding 补齐）。\n上面例子：\n\n输入长度 5\nkernel=2, stride=2, ceil_mode=True\n计算：2.5 → ceil → 3\n输出长度就是 3（最后一个区域会只覆盖部分输入，或者补 0）。\n\n\n\nfloor 模式（默认）：计算稳定，常用于训练。\nceil 模式：当你希望输入和输出严格对齐，或者想保留更多边缘信息时使用（比如某些图像分割任务）。\n# 提示\n卷积是提取特征，池化是压缩特征\n1080P -&gt; 720P\n# 最大池化\n池化的默认步长等于池化核的大小，池化的默认步长等于池化核的大小，池化的默认步长等于池化核的大小\n\n# 最大池化实例\nceil_mode=True：\nimport torchimport torch.nn as nninput = torch.tensor([[1, 2, 0, 3, 1],                      [0, 1, 2, 3, 1],                      [1, 2, 1, 0, 0],                      [5, 2, 3, 1, 1],                      [2, 1, 0, 1, 1]])input = torch.reshape(input, (-1, 1, 5, 5))class Net(nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.maxPool1 = nn.MaxPool2d(kernel_size=3, ceil_mode=True)    def forward(self, input):        output = self.maxPool1(input)        return outputnet = Net()output = net(input)print(output.shape)print(output)\ntorch.Size([1, 1, 2, 2])\ntensor([[[[2, 3],\n[5, 1]]]])\n\nceil_mode=False：\nself.maxPool1 = nn.MaxPool2d(kernel_size=3, ceil_mode=False)\ntorch.Size([1, 1, 1, 1])\ntensor([[[[2]]]])\n\n# 图片操作\nimport torchimport torch.nn as nnimport torchvision.datasetsfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterinput = torch.tensor([[1, 2, 0, 3, 1],                      [0, 1, 2, 3, 1],                      [1, 2, 1, 0, 0],                      [5, 2, 3, 1, 1],                      [2, 1, 0, 1, 1]])input = torch.reshape(input, (-1, 1, 5, 5))data_set = torchvision.datasets.CIFAR10(root=\"data\", train=False, transform=torchvision.transforms.ToTensor(),                                        download=True)dataloader = DataLoader(data_set, batch_size=64, shuffle=True, num_workers=0)class Net(nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.maxPool1 = nn.MaxPool2d(kernel_size=3, ceil_mode=False)    def forward(self, input):        output = self.maxPool1(input)        return outputwriter = SummaryWriter(\"logs_maxpool\")net = Net()step = 0for data in dataloader:    img, label = data    writer.add_images(\"input\", img, global_step=step)    output = net(img)    writer.add_images(\"output\", output, global_step=step)    step += 1writer.close()可以看到，变成马赛克了：\n\n# Padding Layers 填充层\ntorch.nn — PyTorch 2.8 documentation\n填充是应用于图片外围的，主要进行一些值的填充，基本不用到\n最多的会使用到：nn.ZeroPad2d Pads the input tensor boundaries with zero.\n# Non-linear Activations 非线性激活\n最常见的是：ReLU — PyTorch 2.8 documentation\n\n其次是：Sigmoid — PyTorch 2.8 documentation\n\n# ReLU 示例\nimport torchinput = torch.tensor([[1, -0.5], [-1, 3]])output = torch.reshape(input, (-1, 1, 2, 2))print(output)class Net(torch.nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.relu = torch.nn.ReLU()    def forward(self, input) -> torch.Tensor:        output = self.relu(input)        return outputnet = Net()output = net(output)print(output)# inplace 源码提示\ndef __init__(self, inplace: bool = False):    super().__init__()    self.inplace = inplace当 inplace 为假时，不改变源数据\n当 inplace 为真时，执行时改变原始数据（就地算法）\n# 图片操作\nimport torchimport torchvision.datasetsfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterdataset = torchvision.datasets.CIFAR10(\"data\", train=False, transform=torchvision.transforms.ToTensor(), download=True)dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=0)class Net(torch.nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.relu = torch.nn.ReLU()        self.sigmoid = torch.nn.Sigmoid()    def forward(self, input) -> torch.Tensor:        # output = self.relu(input)        output = self.sigmoid(input)        return outputnet = Net()writer = SummaryWriter(\"logs_relu\")step = 0for data in dataloader:    imgs, targets = data    writer.add_images(\"input\", imgs, global_step=step)    output = net(imgs)    writer.add_images(\"output\", output, global_step=step)    step += 1writer.close()结果如下：\n\n# Normalization Layers 正则化层\nhttps://docs.pytorch.org/docs/stable/nn.html#normalization-layers\n有一篇论文提到正则化层可以加速训练\n# Recurrent Layers 循环层\n提示：RNN\ntorch.nn — PyTorch 2.8 documentation\n序列的每一步计算都会依赖前一步的隐藏状态，从而能捕捉序列的时间依赖性。\n# Transformer Layers\nhttps://docs.pytorch.org/docs/stable/nn.html#transformer-layers\nTransformer 的核心思想就是用 自注意力机制（Self-Attention） 来替代传统 RNN 或 CNN 处理序列时的缺陷。\n# Linear Layers 线性层（全连接）\ntorch.nn — PyTorch 2.8 documentation\n\n# 源码提示\ntorch.nn.Linear(in_features, out_features, bias=True)\n\nin_features  含义：输入特征的维度（每个样本的输入向量长度）。\nout_features  含义：输出特征的维度（每个样本的输出向量长度）。\nbias  是否使用偏置项，默认 True\n\nimport torchimport torchvisionfrom torch import nnfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterdataset = torchvision.datasets.CIFAR10(\"data\", train=False, transform=torchvision.transforms.ToTensor(),                                       download=True)dataLoader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=0, drop_last=True)# writer = SummaryWriter(\"log_liner\")class Net(nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.linear1 = nn.Linear(196608, 10)    def forward(self, input):        output = self.linear1(input)        return outputnet = Net()for data in dataLoader:    img, label = data    print(img.shape)    output = torch.flatten(img)    print(output.shape)    output = net(output)    print(output.shape)展平动作：torch.flatten (img)\n输入 196608，输出 10：nn.Linear (196608, 10)\n# Dropout Layers\ntorch.nn — PyTorch 2.8 documentation\n# Sequential 序列操作 —— 以简单网络模型实战为例\n相当于 transforms 的 compose，将一些操作组装到一起\n\nimport torchfrom torch import nnfrom torch.nn import Conv2d, MaxPool2d, Flatten, Linearclass Net(nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.conv1 = Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2)        self.maxPool1 = MaxPool2d(kernel_size=2, ceil_mode=False)        self.conv2 = Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2)        self.maxPool2 = MaxPool2d(kernel_size=2, ceil_mode=False)        self.conv3 = Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)        self.maxPool3 = MaxPool2d(kernel_size=2, ceil_mode=False)        self.flatten = Flatten()        self.linear0 = Linear(1024, 64)        self.linear1 = Linear(64, 10)    def forward(self, x):        x = self.conv1(x)        x = self.maxPool1(x)        x = self.conv2(x)        x = self.maxPool2(x)        x = self.conv3(x)        x = self.maxPool3(x)        x = self.flatten(x)        x = self.linear0(x)        x = self.linear1(x)        return xnet = Net()print(net)# 测试网络input = torch.ones(64, 3, 32, 32)outputs = net(input)print(outputs.shape)输入是一张彩色图片（3 个通道）\n\n首先用一个卷积层（conv1）去提取一些低级特征，比如边缘、颜色块，然后通过一次池化（maxPool1）把图片 “缩小一半”，同时保留主要特征\n接着再来一次卷积（conv2），这时候的输入已经有 32 个通道了，网络会继续在前面提取到的特征基础上，找到更复杂的形状、纹理，然后再做一次池化（maxPool2），图像再缩小一半\n然后再卷积一次（conv3），这次输出通道变成 64 个，能提取更丰富、更抽象的特征，比如局部的结构、物体的一部分，再池化一次（maxPool3），图像又缩小\n接下来把这些 “缩小后的特征图” 拉直成一维向量（flatten），然后经过一个全连接层（linear0），把大规模的特征压缩成一个 64 维的向量\n最后再经过一个全连接层（linear1），输出 10 个数，这通常对应 10 个分类的可能性\n\n总结一下：它就是一个典型的卷积神经网络，前面几层卷积 + 池化负责逐步提取和浓缩图像特征，后面的全连接层负责把这些特征转成分类结果。\n# Sequential\nclass Net(nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.conv1 = Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2)        self.maxPool1 = MaxPool2d(kernel_size=2, ceil_mode=False)        self.conv2 = Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2)        self.maxPool2 = MaxPool2d(kernel_size=2, ceil_mode=False)        self.conv3 = Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)        self.maxPool3 = MaxPool2d(kernel_size=2, ceil_mode=False)        self.flatten = Flatten()        self.linear0 = Linear(1024, 64)        self.linear1 = Linear(64, 10)        self.model1 = Sequential(self.conv1, self.maxPool1, self.conv2, self.maxPool2, self.conv3, self.maxPool3,self.flatten, self.linear0, self.linear1)    def forward(self, x):        x = self.model1(x)        return x其中 Sequential 将一些操作组装到一起：\nself.model1 = Sequential(self.conv1, self.maxPool1, self.conv2, self.maxPool2, self.conv3, self.maxPool3,self.flatten, self.linear0, self.linear1)进一步地可以这样写：\nclass Net(nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.model2 = Sequential(            Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Flatten(),            Linear(1024, 64),            Linear(64, 10))    def forward(self, x):        x = self.model2(x)        return x# 流程可视化\nwriter = SummaryWriter(\"logs_seq\")writer.add_graph(net, input)writer.close()\n可以观察到具体流程就是：卷积 1、池化 1、卷积 2、池化 2、卷积 3、池化 3、展平、全连接 1（线性 1）、全连接 2（线性 2）。\n# 损失函数与反向传播\n计算实际输出和目标之间的差距，为我们更新输出提供一定的依据（反向传播）, grad\ntorch.nn — PyTorch 2.8 documentation\n\n nn.L1Loss \nnn.L1Loss：\nX:1, 2, 3\nY:1, 2, 5\nL1loss = (0+0+2) /3=0.6，这里自然是越小越好\nimport torchfrom torch.nn import L1Lossinput = torch.tensor([1, 2, 3], dtype=torch.float32)target = torch.tensor([1, 2, 5], dtype=torch.float32)input = torch.reshape(input, (1, 1, 1, 3))target = torch.reshape(target, (1, 1, 1, 3))loss = L1Loss()result = loss(input, target)print(result)输出：tensor (0.6667)\n\n nn.MSELoss \nMSE = (0+0+2^2)/3=4/3=1.333\n注意每个位置要平方\nimport torchfrom torch.nn import L1Lossinput = torch.tensor([1, 2, 3], dtype=torch.float32)target = torch.tensor([1, 2, 5], dtype=torch.float32)input = torch.reshape(input, (1, 1, 1, 3))target = torch.reshape(target, (1, 1, 1, 3))loss_Mse = torch.nn.MSELoss()result_Mse = loss_Mse(input, target)print(result_Mse)\n交叉熵： nn.CrossEntropyLoss  分类问题\n\n此处 log 时以 e 为底数的（ln），常见 target 就是目标代号，1 就是第二个标签\n\n使用 result_loss.backward () 开启反向传播\nbackward 根据损失值  result_loss  自动计算出每个参数的梯度，并把梯度存到参数的  .grad  属性中。\nimport torchimport torchvisionfrom torch import nnfrom torch.nn import Conv2d, Sequential, MaxPool2d, Linear, Flattendataset = torchvision.datasets.CIFAR10(\"data\", train=False, transform=torchvision.transforms.ToTensor(),                                       download=True)dataloader = torch.utils.data.DataLoader(dataset, batch_size=1)class Net(nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.model1 = Sequential(            Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Flatten(),            Linear(1024, 64),            Linear(64, 10))    def forward(self, x):        x = self.model1(x)        return xnet = Net()loss = nn.CrossEntropyLoss()for data in dataloader:    imgs, targets = data    outputs = net(imgs)    result_loss = loss(outputs, targets)    result_loss.backward()    print(\"ok\")    print(result_loss)# optim 优化器\n它负责根据参数的梯度  .grad  来更新模型的参数，从而让模型越来越接近目标。\n训练一个神经网络时流程是这样的：\n\n前向传播 (forward)\n 输入数据 → 模型输出 → 计算损失  loss 。\n反向传播 (backward)\n 调用  loss.backward() ，PyTorch 会自动算出每个参数的梯度，并存到  param.grad  里。\n参数更新 (update step)\n 这一步就是 优化器的作用。\n优化器会读取参数的  .grad ，然后根据优化算法（如 SGD、Adam）来调整参数值。\n\noptim（update step）会根据 backward 计算出来的梯度来更新参数\nimport torchimport torchvisionfrom torch import nnfrom torch.nn import Conv2d, Sequential, MaxPool2d, Linear, Flattendataset = torchvision.datasets.CIFAR10(\"data\", train=False, transform=torchvision.transforms.ToTensor(),                                       download=True)dataloader = torch.utils.data.DataLoader(dataset, batch_size=1)class Net(nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.model1 = Sequential(            Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Flatten(),            Linear(1024, 64),            Linear(64, 10))    def forward(self, x):        x = self.model1(x)        return xnet = Net()loss = nn.CrossEntropyLoss()optim = torch.optim.SGD(net.parameters(), lr=0.01)for epoch in range(20):    print(\"Epoch:&#123;&#125;\".format(epoch))    running_loss = 0.0    for data in dataloader:        imgs, targets = data        outputs = net(imgs)        result_loss = loss(outputs, targets)        optim.zero_grad()        result_loss.backward()        optim.step()        running_loss += result_loss    print(\"Loss:&#123;&#125;\".format(running_loss))关键在于：\noptim.zero_grad()\nresult_loss.backward()\noptim.step()\n首先 optim 梯度数据清零，然后 backward 计算这一次的梯度。最后 optim 做参数的优化\n对于这里的循环：\nfor data in dataloader:        imgs, targets = data        outputs = net(imgs)        result_loss = loss(outputs, targets)        optim.zero_grad()        result_loss.backward()        optim.step()        running_loss += result_loss只是对数据进行了一轮的学习，我们需要多轮学习才能最优化结果：\nfor epoch in range(20):    print(\"Epoch:&#123;&#125;\".format(epoch))    running_loss = 0.0    for data in dataloader:        imgs, targets = data        outputs = net(imgs)        result_loss = loss(outputs, targets)        optim.zero_grad()        result_loss.backward()        optim.step()        running_loss += result_loss    print(\"Loss:&#123;&#125;\".format(running_loss))因此我们这样操作\n# SGD（随机梯度下降）\n训练神经网络的目标是让 损失函数 Loss 最小化\n想象损失函数是一个 “山谷地形”，我们要沿着山坡往下走，直到找到最低点\n梯度（Gradient） 就是告诉我们 “往哪个方向下坡最快”\n\noptim = torch.optim.SGD(net.parameters(), lr=0.01)模型参数即 net.parameters ()，学习速率即 lr=0.01\n学习率 (learning rate, lr)：\nlr 太小：走得慢，收敛速度慢，可能训练很久 loss 才下降。\nlr 太大：走得快，但可能 “跨过山谷底部”，造成震荡甚至发散（loss 变大）。\n# 现有模型的使用与修改（迁移学习）\n借用别人训练好的模型知识，在新任务上减少训练成本，提高效果。\n在现有 vgg16 中加一些新的层\nimport osimport torchvision.datasetsfrom torch import nnos.environ[\"TORCH_HOME\"] = \"E:/PyCharmCode/pytorchSTU/data\"vgg16_false = torchvision.models.vgg16(pretrained=False)vgg16_true = torchvision.models.vgg16(pretrained=True)print(vgg16_true)train_data = torchvision.datasets.CIFAR10(root=\"data\", train=True, transform=torchvision.transforms.ToTensor(),                                          download=True)vgg16_true.classifier.add_module(\"add_linear\", nn.Linear(1000, 10))print(vgg16_true)源：\n\n(classifier): Sequential(\n(0): Linear(in_features=25088, out_features=4096, bias=True)\n(1): ReLU(inplace=True)\n(2): Dropout(p=0.5, inplace=False)\n(3): Linear(in_features=4096, out_features=4096, bias=True)\n(4): ReLU(inplace=True)\n(5): Dropout(p=0.5, inplace=False)\n(6): Linear(in_features=4096, out_features=1000, bias=True)\n)\n\n现：\n\n(classifier): Sequential(\n(0): Linear(in_features=25088, out_features=4096, bias=True)\n(1): ReLU(inplace=True)\n(2): Dropout(p=0.5, inplace=False)\n(3): Linear(in_features=4096, out_features=4096, bias=True)\n(4): ReLU(inplace=True)\n(5): Dropout(p=0.5, inplace=False)\n(6): Linear(in_features=4096, out_features=1000, bias=True)\n(add_linear): Linear(in_features=1000, out_features=10, bias=True)\n)\n\n在现有 vgg16 中修改层\nimport osimport torchvision.datasetsfrom torch import nn# train_data = torchvision.datasets.ImageNet(root=\"data\", split=\"train\", download=True,#                                            transform=torchvision.transforms.ToTensor())os.environ[\"TORCH_HOME\"] = \"E:/PyCharmCode/pytorchSTU/data\"vgg16_false = torchvision.models.vgg16(pretrained=False)vgg16_true = torchvision.models.vgg16(pretrained=True)print(vgg16_false)vgg16_false.classifier[6] = nn.Linear(in_features=4096, out_features=10)print(vgg16_false)源：\n\n(classifier): Sequential(\n(0): Linear(in_features=25088, out_features=4096, bias=True)\n(1): ReLU(inplace=True)\n(2): Dropout(p=0.5, inplace=False)\n(3): Linear(in_features=4096, out_features=4096, bias=True)\n(4): ReLU(inplace=True)\n(5): Dropout(p=0.5, inplace=False)\n(6): Linear(in_features=4096, out_features=1000, bias=True)\n)\n\n现：\n\n(classifier): Sequential(\n(0): Linear(in_features=25088, out_features=4096, bias=True)\n(1): ReLU(inplace=True)\n(2): Dropout(p=0.5, inplace=False)\n(3): Linear(in_features=4096, out_features=4096, bias=True)\n(4): ReLU(inplace=True)\n(5): Dropout(p=0.5, inplace=False)\n(6): Linear(in_features=4096, out_features=10, bias=True)\n)\n\n# 模型的保存与加载\n保存：\nimport osimport torchimport torchvision.modelsfrom torch import nnos.environ[\"TORCH_HOME\"] = \"E:/PyCharmCode/pytorchSTU/data\"vgg16 = torchvision.models.vgg16(pretrained=False)# 方式 1：保存模型结构 + 参数torch.save(vgg16, \"vgg16_method1.pth\")# 方式 2：保存模型参数 (官方推荐)torch.save(vgg16.state_dict(), \"vgg16_method2.pth\")# 陷阱 1class Net(nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.conv1 = nn.Conv2d(3, 32, 5, 1, 2)    def forward(self, x):        x = self.conv1(x)        return xnet = Net()torch.save(net, \"net.pth\")加载：\nimport torchimport torchvision.modelsimport model_savefrom torch import nn# 方式 1：模型结构 + 参数加载 1torch_load = torch.load(\"vgg16_method1.pth\", weights_only=False)# print(torch_load)# 方式 2：模型参数加载 2vgg16 = torchvision.models.vgg16(pretrained=False)torch_load = torch.load(\"vgg16_method2.pth\", weights_only=True)vgg16.load_state_dict(torch_load)# print(vgg16)# 陷阱 1: 需要把模型引入才能进行加载 import model_savemodel = torch.load(\"net.pth\", weights_only=False)print(model)# 模型的完整训练套路\n\n准备数据集、获取数据集大小（可选）\ndataLoader 加载数据集\n搭建神经网络 Net\n创建损失函数 loss_fn、创建优化器 optimizer、可选使用 tenser board\n设置训练网络一些参数：训练的轮数 total_train_step、测试的轮数 total_test_step、总训练的轮数 epoch\n开始训练，导出 imgs, targets 并进入网络\n计算损失函数\n清零梯度、计算梯度、利用反向传播，致使优化器更新参数\n暂时关闭梯度计算、开始测试\n计算损失函数并累计、计算当前轮次准确率\n重复 6~10，直到完成所有轮次\n\nimport torchimport torchvision.datasetsfrom torch.utils.tensorboard import SummaryWriterfrom model import *from torch.utils.data import DataLoader# 准备数据集train_data = torchvision.datasets.CIFAR10(\"data\", train=True, transform=torchvision.transforms.ToTensor(),                                          download=True)test_data = torchvision.datasets.CIFAR10(\"data\", train=False, transform=torchvision.transforms.ToTensor(),                                         download=True)# 获取数据集大小train_data_size = len(train_data)test_data_size = len(test_data)print(f\"训练数据集的长度为&#123;train_data_size&#125;\")print(f\"测试数据集的长度为&#123;test_data_size&#125;\")# dataLoader 加载数据集dataLoader_train = DataLoader(train_data, batch_size=64)dataLoader_test = DataLoader(test_data, batch_size=64)# 搭建神经网络net = Net()# 创建损失函数loss_fn = nn.CrossEntropyLoss()# 创建优化器lr = 0.01optimizer = torch.optim.SGD(net.parameters(), lr=lr)# tenser boardwriter = SummaryWriter(\"logs\")# 设置训练网络一些参数# 训练的轮数total_train_step = 0# 测试的轮数total_test_step = 0# 训练的轮数epoch = 10for i in range(epoch):    print(\"--------第 &#123;&#125; 轮训练开始--------\".format(i + 1))    # 训练开始    net.train()    for data in dataLoader_train:        imgs, targets = data        outputs = net(imgs)        loss = loss_fn(outputs, targets)  # 损失函数        optimizer.zero_grad()  # 清零梯度        loss.backward()  # 反向传播        optimizer.step()  # 优化器更新参数        total_train_step += 1        if total_train_step % 100 == 0:            print(\"训练次数：&#123;&#125;, Loss: &#123;&#125;\".format(total_train_step, loss.item()))            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)    # 测试步骤开始    net.eval()    total_test_loss = 0    total_acc = 0    with torch.no_grad():  # 不进行梯度计算 with 的作用是：临时关闭梯度计算，退出后自动恢复。在上下文里关闭，出了上下文就恢复。        for data in dataLoader_test:            imgs, targets = data            outputs = net(imgs)  # 测试步骤开始            loss = loss_fn(outputs, targets)  # 损失函数            total_test_loss += loss.item()  # 求和            acc = (outputs.argmax(1) == targets).sum()            total_acc += acc        print(\"整体测试集上的Loss：&#123;&#125;\".format(total_test_loss))        print(\"整体测试集上的正确率：&#123;&#125;\".format(total_acc / test_data_size))        writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)        writer.add_scalar(\"test_acc\", total_acc / test_data_size, total_test_step)        total_test_step += 1    torch.save(net, \"net_&#123;&#125;.pth\".format(i))    print(\"模型已保存\")writer.close()10 轮学习后，整体测试集上的正确率：0.5428000092506409\n# test_acc\n\n# test_loss\n\n# train_loss\n\n# 使用 GPU 训练 1（.cuda）\n\n\n网络可以使用 GPU 加速\nnet = Net()# 交给 GPUif torch.cuda.is_available():    net = net.cuda()\n\n损失函数可以使用 GPU 加速\nloss_fn = nn.CrossEntropyLoss()# 交给 GPUif torch.cuda.is_available():    loss_fn = loss_fn.cuda()\n\n数据和标签和使用 GPU 加速\n# 交给 GPUif torch.cuda.is_available():    imgs = imgs.cuda()    targets = targets.cuda()\n\n优化后：\n# _*_ coding : utf-8 _*_# @Time : 2025/8/26 14:38# @Author : KarryLiu# File : train_gpu_1# @Project : pytorchSTUimport torchimport torchvision.datasetsfrom torch import nnfrom torch.nn import Sequential, Conv2d, MaxPool2d, Linear, Flattenfrom torch.utils.tensorboard import SummaryWriterfrom torch.utils.data import DataLoader# 准备数据集train_data = torchvision.datasets.CIFAR10(\"data\", train=True, transform=torchvision.transforms.ToTensor(),                                          download=True)test_data = torchvision.datasets.CIFAR10(\"data\", train=False, transform=torchvision.transforms.ToTensor(),                                         download=True)# 获取数据集大小train_data_size = len(train_data)test_data_size = len(test_data)print(f\"训练数据集的长度为&#123;train_data_size&#125;\")print(f\"测试数据集的长度为&#123;test_data_size&#125;\")# dataLoader 加载数据集dataLoader_train = DataLoader(train_data, batch_size=64)dataLoader_test = DataLoader(test_data, batch_size=64)# 搭建神经网络class Net(nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.model = Sequential(            Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Flatten(),            Linear(1024, 64),            Linear(64, 10)        )    def forward(self, x):        x = self.model(x)        return xnet = Net()# 交给 GPUif torch.cuda.is_available():    net = net.cuda()# 创建损失函数loss_fn = nn.CrossEntropyLoss()# 交给 GPUif torch.cuda.is_available():    loss_fn = loss_fn.cuda()# 创建优化器lr = 0.01optimizer = torch.optim.SGD(net.parameters(), lr=lr)# tenser boardwriter = SummaryWriter(\"logs\")# 设置训练网络一些参数# 训练的轮数total_train_step = 0# 测试的轮数total_test_step = 0# 训练的轮数epoch = 10for i in range(epoch):    print(\"--------第 &#123;&#125; 轮训练开始--------\".format(i + 1))    # 训练开始    net.train()    for data in dataLoader_train:        imgs, targets = data        # 交给 GPU        if torch.cuda.is_available():            imgs = imgs.cuda()            targets = targets.cuda()        outputs = net(imgs)        loss = loss_fn(outputs, targets)  # 损失函数        optimizer.zero_grad()  # 清零梯度        loss.backward()  # 反向传播        optimizer.step()  # 优化器更新参数        total_train_step += 1        if total_train_step % 100 == 0:            print(\"训练次数：&#123;&#125;, Loss: &#123;&#125;\".format(total_train_step, loss.item()))            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)    # 测试步骤开始    net.eval()    total_test_loss = 0    total_acc = 0    with torch.no_grad():  # 不进行梯度计算 with 的作用是：临时关闭梯度计算，退出后自动恢复。在上下文里关闭，出了上下文就恢复。        for data in dataLoader_test:            imgs, targets = data            # 交给 GPU            if torch.cuda.is_available():                imgs = imgs.cuda()                targets = targets.cuda()            outputs = net(imgs)  # 测试步骤开始            loss = loss_fn(outputs, targets)  # 损失函数            total_test_loss += loss.item()  # 求和            acc = (outputs.argmax(1) == targets).sum()            total_acc += acc        print(\"整体测试集上的Loss：&#123;&#125;\".format(total_test_loss))        print(\"整体测试集上的正确率：&#123;&#125;\".format(total_acc / test_data_size))        writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)        writer.add_scalar(\"test_acc\", total_acc / test_data_size, total_test_step)        total_test_step += 1    torch.save(net, \"net_&#123;&#125;.pth\".format(i))    print(\"模型已保存\")writer.close()使用 GPU 后，学习 100 次只需要大约 1.23s\n# 使用 GPU 训练 2（.to）\n使用：\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n网络可以使用 GPU 加速\nnet = Net()# 交给 GPUnet = net.to(device)\n\n损失函数可以使用 GPU 加速\nloss_fn = nn.CrossEntropyLoss()# 交给 GPUloss_fn = loss_fn.to(device)\n\n数据和标签和使用 GPU 加速\n# 交给 GPUimgs = imgs.to(device)targets = targets.to(device)\n\n优化后：\nimport torchimport torchvision.datasetsfrom torch import nnfrom torch.nn import Sequential, Conv2d, MaxPool2d, Linear, Flattenfrom torch.utils.tensorboard import SummaryWriterfrom torch.utils.data import DataLoaderdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")# 准备数据集train_data = torchvision.datasets.CIFAR10(\"data\", train=True, transform=torchvision.transforms.ToTensor(),                                          download=True)test_data = torchvision.datasets.CIFAR10(\"data\", train=False, transform=torchvision.transforms.ToTensor(),                                         download=True)# 获取数据集大小train_data_size = len(train_data)test_data_size = len(test_data)print(f\"训练数据集的长度为&#123;train_data_size&#125;\")print(f\"测试数据集的长度为&#123;test_data_size&#125;\")# dataLoader 加载数据集dataLoader_train = DataLoader(train_data, batch_size=64)dataLoader_test = DataLoader(test_data, batch_size=64)# 搭建神经网络class Net(nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.model = Sequential(            Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Flatten(),            Linear(1024, 64),            Linear(64, 10)        )    def forward(self, x):        x = self.model(x)        return xnet = Net()# 交给 GPUnet = net.to(device)# 创建损失函数loss_fn = nn.CrossEntropyLoss()# 交给 GPUloss_fn = loss_fn.to(device)# 创建优化器lr = 0.01optimizer = torch.optim.SGD(net.parameters(), lr=lr)# tenser boardwriter = SummaryWriter(\"logs\")# 设置训练网络一些参数# 训练的轮数total_train_step = 0# 测试的轮数total_test_step = 0# 训练的轮数epoch = 10for i in range(epoch):    print(\"--------第 &#123;&#125; 轮训练开始--------\".format(i + 1))    # 训练开始    net.train()    for data in dataLoader_train:        imgs, targets = data        # 交给 GPU        imgs = imgs.to(device)        targets = targets.to(device)        outputs = net(imgs)        loss = loss_fn(outputs, targets)  # 损失函数        optimizer.zero_grad()  # 清零梯度        loss.backward()  # 反向传播        optimizer.step()  # 优化器更新参数        total_train_step += 1        if total_train_step % 100 == 0:            print(\"训练次数：&#123;&#125;, Loss: &#123;&#125;\".format(total_train_step, loss.item()))            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)    # 测试步骤开始    net.eval()    total_test_loss = 0    total_acc = 0    with torch.no_grad():  # 不进行梯度计算 with 的作用是：临时关闭梯度计算，退出后自动恢复。在上下文里关闭，出了上下文就恢复。        for data in dataLoader_test:            imgs, targets = data            # 交给 GPU            imgs = imgs.to(device)            targets = targets.to(device)            outputs = net(imgs)  # 测试步骤开始            loss = loss_fn(outputs, targets)  # 损失函数            total_test_loss += loss.item()  # 求和            acc = (outputs.argmax(1) == targets).sum()            total_acc += acc        print(\"整体测试集上的Loss：&#123;&#125;\".format(total_test_loss))        print(\"整体测试集上的正确率：&#123;&#125;\".format(total_acc / test_data_size))        writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)        writer.add_scalar(\"test_acc\", total_acc / test_data_size, total_test_step)        total_test_step += 1    torch.save(net, \"net_&#123;&#125;.pth\".format(i))    print(\"模型已保存\")writer.close()# 验证套路（利用训练好的模型，给他提供输入）\nimport torchimport torchvision.transformsfrom PIL import Imagefrom torch import nnfrom torch.nn import Sequential, Conv2d, MaxPool2d, Flatten, Linear# 如果是由 GPU 训练得到模型，则需要将模型移动到 GPU 上# 如果仅想使用 CPU 测试可以在 model 中使用：map_location=\"cpu\"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")image_path = \"images/fff.png\"# 此处 png 是 RGBA 模式，我们转换成 RGB 模式image = Image.open(image_path).convert(\"RGB\")trans = torchvision.transforms.Compose([    torchvision.transforms.Resize(size=(32, 32)),    torchvision.transforms.ToTensor(),])image = trans(image)# 添加一个维度代表 1 张图片，并交给 GPUimage = torch.reshape(image, (1, 3, 32, 32)).to(device)print(image.shape)# 搭建神经网络class Net(nn.Module):    def __init__(self, *args, **kwargs) -> None:        super().__init__(*args, **kwargs)        self.model = Sequential(            Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),            MaxPool2d(kernel_size=2, ceil_mode=False),            Flatten(),            Linear(1024, 64),            Linear(64, 10)        )    def forward(self, x):        x = self.model(x)        return x# 仅仅在 CPU 测试# model = torch.load(\"net_9.pth\", weights_only=False, map_location=\"cpu\")model = torch.load(\"net_9.pth\", weights_only=False)model.eval()with torch.no_grad():    output = model(image)    print(output)    print(torch.argmax(output))\ntorch.Size([1, 3, 32, 32])\ntensor([[ 4.1824, -0.9885,  2.4846, -0.8212,  0.5465, -1.7889, -1.6802, -0.3970,\n0.7540, -0.7934]])\ntensor(0)\n\n\n\n# GPU50 轮学习后\n\n后面出现了过拟合\n# 附录\n简单入门了一下，后面还有很长的路要走，继续保持持续学习的动力。\n相关代码已公开在 GitHub 中：https://github.com/735690757/pytorch_stu_up\nSwim in the ocean of art and programming, weave the future with code art.\n","categories":["深度学习"],"tags":["python","python_PyTorch"]}]